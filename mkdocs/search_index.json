{
    "docs": [
        {
            "location": "/",
            "text": "Let there be light!\n\u00b6\n\n\nA R documentation website.\n\n\nLayout\n\u00b6\n\n\n\n\nugo_r_doc\n is a corpus: a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc.\n\n\nA Mkdocs site is automatically indexed. The \u2018docs\u2019 is a searchable knowledge-based system. \n\n\nYou type a keyword, it leads to several sources, you identify the document the source belongs to, and you go retrieve the document; whether it is a digital or a material document. Fast and easy!\n\n\nFor that matter, \ngenerously\n adding keywords to the \u2018docs\u2019 is crucial (adding them in subscript makes them stand apart).\n\n\nCitations, keywords, links, etc.; they all provide leads.\n\n\nThe corpus is unstructured. There is no unique chapter dedicated to one topic.\n\n\nKnowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information. \n\n\nInformation may be repeted among many documents, with different explanations, or some more comprehensive.\n\n\nNewer entries might also supplement or contradict older entries.",
            "title": "Home"
        },
        {
            "location": "/#layout",
            "text": "ugo_r_doc  is a corpus: a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc.  A Mkdocs site is automatically indexed. The \u2018docs\u2019 is a searchable knowledge-based system.   You type a keyword, it leads to several sources, you identify the document the source belongs to, and you go retrieve the document; whether it is a digital or a material document. Fast and easy!  For that matter,  generously  adding keywords to the \u2018docs\u2019 is crucial (adding them in subscript makes them stand apart).  Citations, keywords, links, etc.; they all provide leads.  The corpus is unstructured. There is no unique chapter dedicated to one topic.  Knowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information.   Information may be repeted among many documents, with different explanations, or some more comprehensive.  Newer entries might also supplement or contradict older entries.",
            "title": "Layout"
        },
        {
            "location": "/An Introduction to R/",
            "text": "CONTENT\n\n\n1, Introduction and Preliminaries\n\n\n2, Simple Manipulations; Numbers and Vectors\n\n\nCreate a vector\n\n\nRepetition\n\n\nLength (vector)\n\n\nBoolean\n\n\nMissing data and more\n\n\nExtract, subset (vector)\n\n\nBackslash use for some characters\n\n\nConcatenate, paste (vector)\n\n\nA note of data handling and manipulations\n\n\nExclude, remove (vector)\n\n\nReplace (vector)\n\n\nAbsolute value\n\n\n\n\n\n\n3, Objects, their Modes and Attributes\n\n\nObject type\n\n\nClasses\n\n\n\n\n\n\n4, Ordered and Unordered Factors\n\n\nConvert (.as)\n\n\n\n\n\n\n5, Arrays and Matrices\n\n\nDimension\n\n\nExtract, subset (matrix)\n\n\nCross product vs multiplication\n\n\nMatrix operation\n\n\nDiagonal and triangle\n\n\nSolving a matrix system (and more matrix algebra)\n\n\nLeast square fitting (matrix)\n\n\nConvert (.as)\n\n\n\n\n\n\n6, Lists and Data Frames\n\n\nCreate a list\n\n\nExtract, subset (list)\n\n\nConcatenate, paste\n\n\nConvert (as.)\n\n\nData frame\n\n\nConcatenate, paste (data frame)\n\n\nConvert (as.)\n\n\nLoad data into R\n\n\n\n\n\n\n7, Reading/Writing Data from/to Files (Input/Output)\n\n\nCleaning parameter\n\n\nChange the data frame or matrix format\n\n\nEncoding\n\n\nOutput\n\n\nCheck a file\n\n\nDirectory management\n\n\nSpreadsheet editor and edition in R\n\n\n\n\n\n\n8, Probability Distributions\n\n\nDistribution\n\n\nDistribution operation\n\n\nDescriptive statistics\n\n\nTest\n\n\nNormality\n\n\n\n\n\n\n9, Grouping, Loops and Conditional Execution\n\n\nControl flow\n\n\nLoops\n\n\n\n\n\n\n10, Writing you own Functions (with examples)\n\n\nFunction in a function\n\n\nCustomizing startup\n\n\nList function and method\n\n\n\n\n\n\n11, Statistical models in R\n\n\nRegression\n\n\nGeneralized least squares\n\n\nNonlinear least squares (MLE, Mixed, Local, Robust, Additive, Tree-based)\n\n\n\n\n\n\n12, Graphical Procedures\n\n\nGraphic, packages\n\n\nBasic plot\n\n\nQ-Q plot\n\n\nPictures\n\n\nGraphic arguments and parameters\n\n\nGeometric shapes\n\n\nGraphic text and mouse\n\n\nGraphic device\n\n\n\n\n\n\n13, Packages\n\n\n14, OS Facilities\n\n\n\n\n\n\n\n\nForeword\n\n\nNotes, leads, and ideas on what R can do. More at:\n\n\n\n\nR-intro\n\n\ncran.r-project.org/manuals (series of official manuals)\n\n\n\n\n\n\n1, Introduction and Preliminaries\n\u00b6\n\n\n\n\nRun R (CLI).\n\n\nQuit with \nq()\n.\n\n\n\n\n\n\nRun R in an IDE (GUI); like RStudio.\n\n\nCreate a working directory \nwork\n.\n\n\nAsk help.\n\n\nhelp(function)\n; open a web documentation in a browser or in the IDE.\n\n\n?function\n; idem.\n\n\n??function\n; idem, but showing concordances.\n\n\nhelp(\"[[\")\n; idem, searching with a string.\n\n\nhelp.start()\n; show the entire manual.\n\n\n\n\n\n\nsink()\n; divert output from the console to a connection; restore the output to the console.\n\n\nobjects()\n, \nls()\n; see the objects stored in a session.\n\n\nObjects are in a file with \n.RData\n extension.\n\n\n\n\n\n\nrm(x, y, ink, ...)\n; remove stored objects.\n\n\nAll commands entered or run are recorded in a file with \n.Rhistory\n extension.\n\n\n\n\n2, Simple Manipulations; Numbers and Vectors\n\u00b6\n\n\nCreate a vector\n\u00b6\n\n\nx <- c(1, 2) # assignment (universal).\nx\n[1] 1 2\n\nc(2, 1) -> y # alternative assignment.\ny\n[1] 2 1\n\nx = c(1, 2) # alternative assignment (some limitations).\nx <<- c(1, 2) # permanent assignment.\n\n# examples\nab <- 9\nab\n[1] 9\n\nassign(\"ab\", 10)\nab \n[1] 10\n\n\n\n\nCreate a sequence (vector)\n\n\nx <- 2 * 1:15\n\nx\n[1]  2  4  6  8 10 12 14 16 18 20 22 24 26 28 30\n\nn <- 10\nx <- 1:n-1\n\nx\n[1] 1 2 3 4 5 6 7 8 9\n\nx <- 30:1\nx\n[1] 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n[29]  2  1\n\nseq(from = 1, to = 30)\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n[29] 29 30\n\nseq(1, 30)\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n[29] 29 30\n\nseq(30, 1)\n[1] 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n[29]  2  1\n\nseq(1, 30, by = 2)\n[1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29\n\n\n\n\nRepetition\n\u00b6\n\n\nrep(2, times = 5)\n[1] 2 2 2 2 2\n\n\n\n\nx <- c(\"x\", \"y\")[rep(c(1, 2, 2, 1), times = 4)]\n\nx\n[1] \"x\" \"y\" \"y\" \"x\" \"x\" \"y\" \"y\" \"x\" \"x\" \"y\" \"y\" \"x\" \"x\" \"y\" \"y\" \"x\"\n\n\n\n\nLength (vector)\n\u00b6\n\n\n\n\nlength(vector)\n\n\n\n\nBoolean\n\u00b6\n\n\n\n\nTRUE\n or \nT\n; \nT == 1\n.\n\n\nFALSE\n or \nF\n; \nF == 0\n.\n\n\n\n\nSome operators\n\n\n\n\n<\n\n\n>=\n\n\n<\n\n\n>=\n\n\n==\n\n\n!=\n\n\n<>\n\n\n&\n\n\n&&\n\n\n|\n\n\n||\n\n\nand many more.\n\n\n\n\nMissing data and more\n\u00b6\n\n\n\n\nNA; not available.\n\n\nNaN; not a number.\n\n\nInf - Inf == NaN == 0/0; infinite number.\n\n\n\n\nx <- c(1:3, NA)\n\nx\n[1] 1 2 3 NA\n\n\n\n\n\n\nis.na(var)\n.\n\n\nvar == NA\n.\n\n\nis.na(x)\n.\n\n\n!is.na(x)\n.\n\n\n\n\nx <- c(-1:3, NA)\ny <- x[!is.na(x) & x > 0]\n\nx\n[1]  -1  0  1  2  3 NA\n\ny\n[1] 1 2 3\n\n\n\n\nExtract, subset (vector)\n\u00b6\n\n\n\n\nx[i]\n; index.\n\n\n\n\nBackslash use for some characters\n\u00b6\n\n\n\n\n\\\\\n; backslash.\n\n\n\\n\n; new line.\n\n\n\\t\n; tab.\n\n\n\\b\n; backspace.\n\n\n\n\nConcatenate, paste (vector)\n\u00b6\n\n\nlabs <- paste(c(\"X\", \"Y\"), 1:10, sep = \"\")\n\nlabs\n[1] \"X1\"  \"Y2\"  \"X3\"  \"Y4\"  \"X5\"  \"Y6\"  \"X7\"  \"Y8\"  \"X9\"  \"Y10\"\n\n\n\n\nA note of data handling and manipulations\n\u00b6\n\n\nYou can also \nsplit()\n, \nmerge()\n, \nrbind()\n, \ncbind()\n vectors. \n\n\nIt is also possible with other objects such as factors, lists, arrays, matrices, and data frames.\n\n\nThere are built-in functions to extract, exclude, subset, replace, transform or convert (\n.as\n), concatenate, paste, group, and bind.\n\n\nslice, extract, exclude, subset, replace, convert,  concatenate, paste, group, bind\n \n\n\nExclude, remove (vector)\n\u00b6\n\n\nz <- 1:20\n\nz\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\nzz <- z[-(1:5)]\n\nzz\n[1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\n\n\nReplace (vector)\n\u00b6\n\n\nx <- c(1, 1, 1, NA)\n\nx\n[1]  1  1  1 NA\n\nx[is.na(x)] <- 0\n\nx\n[1] 1 1 1 0\n# replaces any missing values in x by zeros\n\n\n\n\nAbsolute value\n\u00b6\n\n\n\n\ny <- abs(y)\n.\n\n\n\n\nConvert (\n.as\n)\n\n\n\n\nas.vector()\n.\n\n\nas.integer()\n.\n\n\nas.numeric()\n\n\nas.factor()\n.\n\n\nas.character()\n.\n\n\nand many more.\n\n\n\n\n3, Objects, their Modes and Attributes\n\u00b6\n\n\nObject type\n\u00b6\n\n\nobj <- 1\n\nobj1 <- numeric(obj)\nmode(obj1)\n[1] \"numeric\"\nobj2 <- character(obj)\nmode(obj2)\n[1] \"character\"\n\nx <- 1\n\n# class() != mode(), but almost\nmode(x)\n[1] \"numeric\"\n\nx <- factor(x)\nx\n[1] 1\nLevels: 1\n\nx <- numeric(x)\nx\n[1] 0\n\n# load key:value\nobj[3] <- 17\n\nobj\n[1]  1 NA 17\n\n\n\n\nClasses\n\u00b6\n\n\nclass\n\n\n\n\n\u201cnumeric\u201d.\n\n\n\u201clogical\u201d.\n\n\n\u201ccharacter\u201d.\n\n\n\u201clist\u201d.\n\n\n\u201cmatrix\u201d.\n\n\n\u201carray\u201d.\n\n\n\u201cfactor\u201d.\n\n\n\u201cdata.frame\u201d.\n\n\n\n\nclass(obj)\n[1] \"numeric\"\n\n# print the object as ordinary\nunclass(obj)\n[1]  1 NA 17\n\n\n\n\n4, Ordered and Unordered Factors\n\u00b6\n\n\nstate <- c(\"tas\",\"qld\",\"sa\",\"sa\",\"sa\",\"vic\",\"nt\",\"act\",\"qld\",\"nsw\",\"wa\",\"nsw\",\"nsw\",\"vic\",\"vic\",\"vic\",\"nsw\",\"qld\",\"qld\",\"vic\",\"nt\",\"wa\",\"wa\",\"qld\",\"sa\", \"tas\",\"nsw\", \"nsw\", \"wa\",\"act\")\n\nstatef <- factor(state)\n\n# class != mode()\nclass(statef)\n[1] \"factor\"\nmode(statef)\n[1] \"numeric\"\n\nlevels(statef)\n[1] \"act\" \"nsw\" \"nt\"  \"qld\" \"sa\"  \"tas\" \"vic\" \"wa\" \n\nincomes <- c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\n\nincmeans <- tapply(incomes, statef, mean)\n\nincmeans\n  act   nsw    nt   qld    sa   tas   vic    wa \n48.50 55.00 54.00 51.60 54.25 53.00 61.60 54.50 \n\nstderr <- function(x) { sqrt(var(x) / length(x)) }\n\nstderr(incomes)\n[1] 1.524462\n\n# alternatively\nincster <- tapply(incomes, statef, stderr)\n\nincster\n      act       nsw        nt       qld        sa       tas       vic        wa \n5.5000000 3.9665266 5.0000000 2.6570661 5.3909647 7.0000000 0.8717798 6.2249498  \n\n\n\n\n# a vector of characters\nstateff <- c(\"a\", \"b\", \"c\")\n[1] \"a\" \"b\" \"c\"\n\nas.factor(stateff)\n[1] a b c\nLevels: a b c\n\n\n\n\n# Create a factor with the wrong order of levels\nsizes <- factor(c(\"small\", \"large\", \"large\", \"small\", \"medium\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# levels can be specified explicitly\nsizes <- factor(sizes, levels = c(\"small\", \"medium\", \"large\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# do the same with an ordered factor\nsizes <- ordered(c(\"small\", \"large\", \"large\", \"small\", \"medium\"))\n\nsizes <- ordered(sizes, levels = c(\"small\", \"medium\", \"large\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small < medium < large\n\n# use relevel() to make a particular level first in the list. (This will not work for ordered factors.)\n\n# Create a factor with the wrong order\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# make medium first\nsizes <- relevel(sizes, \"medium\")\n\nsizes\n[1] small  large  large  small  medium\nLevels: medium large small\n\n# make small first\nsizes <- relevel(sizes, \"small\")\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# specify the proper order when the factor is created\nsizes <- factor(c(\"small\", \"large\", \"large\", \"small\", \"medium\"),\n                  levels = c(\"small\", \"medium\", \"large\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# Create a factor with the wrong order of levels\nsizes <- factor(c(\"small\", \"large\", \"large\", \"small\", \"medium\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# reverse the order of levels in a factor\nsizes <- factor(sizes, levels=rev(levels(sizes)))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n\n\n\nConvert (\n.as\n)\n\u00b6\n\n\n\n\nas.factor()\n.\n\n\n\n\n5, Arrays and Matrices\n\u00b6\n\n\nDimension\n\u00b6\n\n\nz <- 1:1500\ndim(z) <- c(3, 5, 100)\n\n# gives 100 arrays of 3 lines by 5 columns\n\n\n\n\nCreate a matrix, an array\n\n\nx <- array(1:20, dim=c(4, 5))\n\nx\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20\n\nx <- array(1:20, dim=c(2, 5, 2))\n\nx\n, , 1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n, , 2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   11   13   15   17   19\n[2,]   12   14   16   18   20\n\n\ni <- array(c(1:3, 3:1), dim = c(3, 2))\n\ni\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    2\n[3,]    3    1\n\nj <- array(c(1:8), dim = c(2, 2, 2))\n\nj\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nk <- array(1:27, c(3, 3, 3))\n\n> k\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   19   22   25\n[2,]   20   23   26\n[3,]   21   24   27\n\n\na <- matrix(1, 2, 2)\n\na\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n\nb <- matrix(2, 2, 2)\n\nb\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n\n\n\nExtract, subset (matrix)\n\u00b6\n\n\n\n\na[2, 1]\n; rows, columns.\n\n\n\n\nExtract, subset (array)\n\n\n\n\na[2, 1, 1]\n; rows, columns, matrix.\n\n\n\n\nCross product vs multiplication\n\u00b6\n\n\na <- 1:5\nb <- seq(10, 6, -1)\n\na\n[1] 1 2 3 4 5\nb\n[1] 10  9  8  7  6\n\na * b\n[1] 10 18 24 28 30\n\ncrossprod(a, b)\n     [,1]\n[1,]  110\n\nab <- a %o% b\nab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   10    9    8    7    6\n[2,]   20   18   16   14   12\n[3,]   30   27   24   21   18\n[4,]   40   36   32   28   24\n[5,]   50   45   40   35   30\n\n\n\n\nMatrix operation\n\u00b6\n\n\nA et B are 2x2 matrices:\n\n\n\n\nA * B\n; scalar multiplication.\n\n\nA %*% B\n; matrix multiplication\n\n\nx %*% A %*% y\n; matrix multiplication.\n\n\ncrossproduct(A, B)\n; cross multiplication.\n\n\nab <- outer(A,B,\"*\")\n; \na * b\n.\n\n\nab <- outer(A,B,\"+\")\n; \na + b\n.\n\n\nab <- outer(A,B,\"-\")\n; \na - b\n.\n\n\nand many more.\n\n\n\n\nDiagonal and triangle\n\u00b6\n\n\nab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   11   10    9    8    7\n[2,]   12   11   10    9    8\n[3,]   13   12   11   10    9\n[4,]   14   13   12   11   10\n[5,]   15   14   13   12   11\n\ndiag(ab)\n[1] 11 11 11 11 11\n\nlower.tri(ab)\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE FALSE FALSE FALSE FALSE\n[2,]  TRUE FALSE FALSE FALSE FALSE\n[3,]  TRUE  TRUE FALSE FALSE FALSE\n[4,]  TRUE  TRUE  TRUE FALSE FALSE\n[5,]  TRUE  TRUE  TRUE  TRUE FALSE\n\nlower.tri(ab) * ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]   12    0    0    0    0\n[3,]   13   12    0    0    0\n[4,]   14   13   12    0    0\n[5,]   15   14   13   12    0\n\nupper.tri(ab)\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE  TRUE  TRUE  TRUE  TRUE\n[2,] FALSE FALSE  TRUE  TRUE  TRUE\n[3,] FALSE FALSE FALSE  TRUE  TRUE\n[4,] FALSE FALSE FALSE FALSE  TRUE\n[5,] FALSE FALSE FALSE FALSE FALSE\n\nupper.tri(ab) * ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0   10    9    8    7\n[2,]    0    0   10    9    8\n[3,]    0    0    0   10    9\n[4,]    0    0    0    0   10\n[5,]    0    0    0    0    0\n\n\n\n\nSolving a matrix system (and more matrix algebra)\n\u00b6\n\n\nb <- A %*% x\n\n# or\nsolve(A, b)\n\n\n\n\n\n\nsolve(A)\n; inverse the matrix.\n\n\n\n\nSymmetrical matrix and eigenvalue\n\n\nb <- matrix(2, 2, 2)\n\nb\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\ne <- eigen(b, only.values = TRUE)\ne\n$values\n[1] 4 0\n\n$vectors\nNULL\n\n\n\n\nSingular value decomposition (matrix)\n\n\nsvd(b)\n$d\n[1] 4 0\n\n$u\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068\n\n$v\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068\n\n\n\n\nDeterminant (matrix)\n\n\ndet(ab)\n[1] 0\n\n\n\n\nLeast square fitting (matrix)\n\u00b6\n\n\n\n\nlsfit()\n or least squares estimate of \nb\n in the model: \ny = X b + e\n.\n\n\n\n\nQR decomposition (matrix)\n\n\nqr(ab)\n$qr\n            [,1]        [,2]          [,3]          [,4]          [,5]\n[1,] -29.2403830 -27.0174299 -2.479448e+01 -2.257152e+01 -2.034857e+01\n[2,]   0.4103913   0.2418254  4.836508e-01  7.254763e-01  9.673017e-01\n[3,]   0.4445906  -0.1703815  3.717512e-15  1.134003e-14  1.749210e-14\n[4,]   0.4787899  -0.5015812  3.957070e-01  1.553722e-15  1.295516e-15\n[5,]   0.5129892  -0.8327809  5.076995e-01  6.936403e-01 -1.685698e-16\n\n$rank\n[1] 2\n\n$qraux\n[1] 1.376192e+00 1.160818e+00 1.765282e+00 1.720322e+00 1.685698e-16\n\n$pivot\n[1] 1 2 3 4 5\n\nattr(,\"class\")\n[1] \"qr\"\n\n\n\n\nAlso:\n\n\n\n\nqr.coef()\n.\n\n\nqr.fitted()\n.\n\n\nqr.resid()\n.\n\n\n\n\nConvert (\n.as\n)\n\u00b6\n\n\n\n\nas.array()\n.\n\n\nas.matrix()\n.\n\n\n\n\n6, Lists and Data Frames\n\u00b6\n\n\nCreate a list\n\u00b6\n\n\nLst <- list(name = \"Fred\", wife = \"Mary\", no.children = 3, child.ages = c(4,7,9))\n\nLst\n$name\n[1] \"Fred\"\n\n$wife\n[1] \"Mary\"\n\n$no.children\n[1] 3\n\n$child.ages\n[1] 4 7 9\n\n\n\n\nExtract, subset (list)\n\u00b6\n\n\nLst$name\n[1] \"Fred\"\n\nLst[[\"name\"]] == Lst[[1]]\n[1] TRUE\n\n\nLst[5] <- list(\"\") # or list()  \nLst[\"new\"] <- list()  # key, value\n\nLst[1]\n$name\n[1] \"Fred\"\n\nLst$child.ages[1]\n[1] 4\nLst[[4]][1]\n[1] 4\n\n\n\n\nConcatenate, paste\n\u00b6\n\n\nx <- c(1,2)\ny <- c(3,4)\n\nc(x, y)\n[1] 1 2 3 4\n\npaste(x, y)\n[1] \"1 3\" \"2 4\"\n\ndata.frame(x, y)\n  x y\n1 1 3\n2 2 4\n\na <- list(1, 2)\nb <- list(\"a\", \"b\")\n\nlist(a, b)\n[[1]]\n[[1]][[1]]\n[1] 1\n\n[[1]][[2]]\n[1] 2\n\n\n[[2]]\n[[2]][[1]]\n[1] \"a\"\n\n[[2]][[2]]\n[1] \"b\"\n\nh <- matrix(2, 2, 2)\ng <- matrix(1, 2, 2)\n\nc(h, g)\n[1] 2 2 2 2 1 1 1 1\n\npaste(h, g)\n[1] \"2 1\" \"2 1\" \"2 1\" \"2 1\"\n\nlist(h, g)\n[[1]]\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n[[2]]\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n\n\n\n\nConvert (\nas.\n)\n\u00b6\n\n\n\n\nas.matrix()\n\n\n\n\nData frame\n\u00b6\n\n\n\n\nA data frame can hold other data frames.\n\n\nA list can hold other lists.\n\n\nA vector can hold other vectors.\n\n\n\n\n\n\nEach variable can be numeric, character, logical, factor, numeric matrix, list, data.frame.\n\n\n\n\nstate <- c(\"tas\",\"qld\",\"sa\",\"sa\",\"sa\",\"vic\",\"nt\",\"act\",\"qld\",\"nsw\",\"wa\",\"nsw\",\"nsw\",\"vic\",\"vic\",\"vic\",\"nsw\",\"qld\",\"qld\",\"vic\",\"nt\",\"wa\",\"wa\",\"qld\",\"sa\", \"tas\",\"nsw\", \"nsw\", \"wa\",\"act\")\n\nstatef <- factor(state)\n\nincomes <- c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\n\nincomef <- factor(incomes)\n\naccountants <- data.frame(home=statef, loot=incomes, shot=incomef)\n\nhead(accountants, 10)\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n\n# class() != mode()\nclass(accountants)\n[1] \"data.frame\"\n\nmode(accountants)\n[1] \"list\"\n\n\n\n\nConcatenate, paste (data frame)\n\u00b6\n\n\n# accountants == acc\n\nc(accountants, acc)\n$home\n [1] tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n[14] vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n[27] nsw nsw wa  act\nLevels: act nsw nt qld sa tas vic wa\n\n$loot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n\n$shot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n20 Levels: 40 41 42 43 46 48 49 51 52 54 56 58 59 ... 70\n\n$home\n [1] tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n[14] vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n[27] nsw nsw wa  act\nLevels: act nsw nt qld sa tas vic wa\n\n$loot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n\n$shot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n20 Levels: 40 41 42 43 46 48 49 51 52 54 56 58 59 ... 70\n\n\npaste(accountants, acc)\n[1] \"c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1) c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1)\"                                                            \n[2] \"c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43) c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\"\n[3] \"c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4) c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4)\"                          \n\nlist(accountants, acc)\n[[1]]\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n...\n\n[[2]]\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n...\n\naccountants + acc\n   home loot shot\n1    NA  120   NA\n2    NA   98   NA\n3    NA   80   NA\n4    NA  122   NA\n5    NA  128   NA\n6    NA  120   NA\n7    NA  118   NA\n8    NA  108   NA\n9    NA  124   NA\n10   NA  138   NA\n...\n\n\n\n\nConvert (\nas.\n)\n\u00b6\n\n\n\n\nas.data.frame()\n\n\n\n\nLoad data into R\n\u00b6\n\n\n\n\nread.table()\n; produce a data frame with inputs.\n\n\n\n\nA note on environment objects\n\n\n\n\nsearch()\n; objects in .GlobalEnv; including packages.\n\n\nls()\n; list these objects.\n\n\n\n\n\n\nattach()\n; attach an object to .GlobalEnv.\n\n\ndetach()\n.\n\n\n\n\n7, Reading/Writing Data from/to Files (Input/Output)\n\u00b6\n\n\nA lot more can be found on I/O: depending on the file type, the data format, the desired R object, many commands are available. Depending on a file format, look for the dedicated package on CRAN.\n\n\nread, reading, write, writing, input, output, i/o\n\n\nExamples\n\n\n# read from a file\nHousePrice <- read.table(\"house.data\", header = TRUE, sep = \";\")\n\n# read a 3-variable list\ninput <- scan(\"input.dat\", list(\"\", 0, 0))  \n\n# read a3-variable list\ninput <- scan(\"input.dat\", list(id=\"\", x = 0, y = 0))\n\n# make 3 separate vectors\nlabel <- input[[1]]\nx <- input[[2]]\ny <- input[[3]]\n\n# label them\nlabel <- inp$id\nx <- inp$x\ny <- inp$y\n\n# read a 5-variable list and transform it into a matrix\nX <- matrix(scan(\"light.dat\", 0), ncol = 5, byrow = TRUE)\n\n\n\n\nInputing from file types\u2026\n\n\n\n\n.txt.\n\n\n.csv.\n\n\n.tsv.\n\n\n.hdf5.\n\n\n.bmp.\n\n\n.jpeg.\n\n\n.png.\n\n\n.tiff.\n\n\n.zip.\n\n\n.xls, spreadsheet.\n\n\ndatabases.\n\n\nstatistical programs.\n\n\nbinary files.\n\n\nand many more (some are up and coming such as julia files).\n\n\n\n\nCleaning parameter\n\u00b6\n\n\n\n\nstrip.white = TRUE\n; remove unnecessary spaces.\n\n\n\n\nChange the data frame or matrix format\n\u00b6\n\n\n\n\nstack()\n; for example, take a 6-column, 4-variable data frame and stack the 4 variables into one long column.\n\n\nunstack()\n; vice-versa.\n\n\n\n\n# 6 columns or variables\nStatus Age  V1    V2    V3    V4\n P 23646 45190 50333 55166 56271\nCC 26174 35535 38227 37911 41184\nCC 27723 25691 25712 26144 26398\nCC 27193 30949 29693 29754 30772\nCC 24370 50542 51966 54341 54273\nCC 28359 58591 58803 59435 61292\nCC 25136 45801 45389 47197 47126\n...\n\n# 4 columns, more rows\nStatus Age values ind\nX1  P 23646 45190 V1\nX2 CC 26174 35535 V1\nX3 CC 27723 25691 V1\nX4 CC 27193 30949 V1\nX5 CC 24370 50542 V1\nX6 CC 28359 58591 V1\nX7 CC 25136 45801 V1\nX11 P 23646 50333 V2\n...\n\n\n\n\n\n\nreshape()\n; stack and unstack variables, according to parameters.\n\n\n\n\nreshape(zz, idvar = \"id\", timevar = \"var\", varying = list(c(\"V1\", \"V2\", \"V3\", \"V4\")), direction = \"long\")\n\n\n\n\n\n\nftable()\n; flatten multidimensional matrices (arrays);\n\n\nread.ftable()\n.\n\n\ndata()\n; list of datasets.\n\n\ndata(package = \"rpart\")\n; load package.\n\n\nedit(input)\n; open a mini-spreadsheet.\n\n\nxnew <- edit(input)\n; open and save it as a new dataset.\n\n\nand many more.\n\n\n\n\nEncoding\n\u00b6\n\n\n\n\nutf-8 for Linux and OS X.\n\n\nUCS-2LE and UTF-16 for Windows.\n\n\n\n\n# examples\nreadLines(\"filename.txt\", encoding = \"UTF-8\")\nreadLines(\"filename.txt\", encoding = \"UCS2LE\")\nread.delim(\"clipboard\", fileEncoding=\"UTF-16\")\n\n\n\n\nOutput\n\u00b6\n\n\n\n\nwrite.table()\n; many parameters.\n\n\nwrite.matrix()\n.\n\n\nwrite.foreign()\n.\n\n\n\n\nCheck a file\n\u00b6\n\n\n\n\nreadLines(\"aab.txt\")\n.\n\n\nreadLines(\"aab.txt\", 1)\n.\n\n\nunlink(\"aab.txt\")\n; delete the file on the working directory.\n\n\n\n\nDirectory management\n\u00b6\n\n\n\n\ngetwd()\n; get the current working directory.\n\n\nsetwd()\n; set the current working directory.\n\n\nfile.show(filepath)\n.\n\n\nsystem.file()\n\n\n\n\nSpreadsheet editor and edition in R\n\u00b6\n\n\n\n\nfix(c)\n; edit object c; a vetor, list, data frame, etc. in a mini-spreadsheet.\n\n\ndata.entry(c, mode=NULL, Names=NULL), dataentry(c), de(c, Modes=list(), Names=NULL)\n; idem.\n\n\nvi(file= )\n; invoke a text editor.\n\n\nemacs(file= )\n; idem.\n\n\npico(file= )\n; idem.\n\n\nxemacs(file= )\n; idem.\n\n\nxedit(file=)\n; idem.\n\n\n\n\n8, Probability Distributions\n\u00b6\n\n\nDistribution\n\u00b6\n\n\n\n\nbeta\n.\n\n\nbinom\n.\n\n\ncauchy\n.\n\n\nchisq\n.\n\n\nexp\n.\n\n\nf\n.\n\n\ngamma\n.\n\n\ngeom\n.\n\n\nhyper\n.\n\n\nlnorm\n.\n\n\nlogis\n.\n\n\nnbinom\n.\n\n\nnorm\n.\n\n\npois\n.\n\n\nsignrank\n.\n\n\nt\n.\n\n\nunif\n.\n\n\nweibull\n.\n\n\nwilcox\n.\n\n\nand many more.\n\n\n\n\nPrefixes\n\n\n\n\nd\n; density.\n\n\np\n; CDF or cumulative density function.\n\n\nq\n; quantile.\n\n\nr\n; random deviates or simulation or number generation.\n\n\n\n\nDistribution operation\n\u00b6\n\n\nCommands, prefix + distribution, examples:\n\n\n\n\ndbeta(x= )\n.\n\n\npbinom(q= , lower.tail= , log.p= )\n.\n\n\nqcauchy(p= , lower.tail= , log.p= )\n.\n\n\nrchisq(n or r= )\n.\n\n\nptukey()\n; studentized (t) distribution\n\n\nqtukey()\n.\n\n\ndmultinom()\n.\n\n\nltinomial\n.\n\n\nrmultinom()\n.\n\n\netc.\n\n\n\n\nDescriptive statistics\n\u00b6\n\n\n# overview\nsummary(cars)\nstr(cars)\nboxplot(cars$speed)\nboxplot(cars$dist)\n\n# Tukey five-number summaries\nfivenum(cars$speed)\n\n# histograms and bar charts\nstem(cars$speed)\nhist(cars$speed)\nbarplot(cars$speed)\ndotchart(cars$speed)\n\n# scatter plot\nplot(cars$speed, cars$dist)\nlines(cars$speed, cars$dist)\n\n# add a (1-d) representation\nplot(cars$speed, cars$dist)\nrug(cars$speed, ticksize = 0.03)\nplot(cars$speed, cars$dist)\nrug(cars$dist, ticksize = 0.03)\n\n# normality of the residuals, linearity\nqqnorm(cars$speed)\nqqline(cars$speed)\n\nqqnorm(cars$dist)\nqqline(cars$dist)\n\nqqplot(cars$speed, cars$dist)\n\n# normality test\nshapiro.test(cars$speed)\n\n# Kolmogorov-Smirnov test on a sample\nks.test(cars$speed[10], \"pnorm\", mean = mean(cars$speed), sd = sqrt(var(cars$speed)))\nks.test(cars$speed[40], \"pnorm\", mean = mean(cars$speed), sd = sqrt(var(cars$speed)))\n\n\n\n\nTest\n\u00b6\n\n\n\n\nt.test(A,B)\n; true difference of means is not 0, difference means.\n\n\nvar.test(A,B)\n; true ratio of variances is not 1, difference variances.\n\n\nt.test(A,B, var.equal=TRUE)\n; true difference of means is not 0.\n\n\nwilcox.test(A,B)\n; rank sum with continuity correction, continuous distribution.\n\n\nand many more.\n\n\n\n\nNormality\n\u00b6\n\n\n\n\nplot(ecdf(A), do.points=FALSE, verticals=TRUE, xlim=range(A, B))\n\n\nplot(ecdf(B), do.points=FALSE, verticals=TRUE, add=TRUE)\n\n\nks.test(A,B); maximal vertical distance between the two ecdf\n\n\n\n\n9, Grouping, Loops and Conditional Execution\n\u00b6\n\n\nControl flow\n\u00b6\n\n\n\n\n&\n; AND.\n\n\n&&\n; AND; evaluates left to right, examining only the first element of each vector.\n\n\n|\n; OR.\n\n\n||\n; OR; evaluates left to right, examining only the first element of each vector.\n\n\nxor()\n; elementwise exclusive OR.\n\n\nisTRUE(x)\n; is true if and only if x is a length-one logical vector whose only element is TRUE and which has no attributes.\n\n\nifelse(condition, a, b)\n; if \ncondition\n is proven true, return \na\n, or else, return \nb\n.\n\n\n\n\nLoops\n\u00b6\n\n\nlooping\n\n\n\n\nfor(var in seq) expr\n.\n\n\nwhile(condition is true) expr\n.\n\n\nrepeat expr\n.\n\n\nbreak\n; break out of a for, while or repeat loop; control is transferred to the first statement outside the inner-most loop.\n\n\nnext\n; halt the processing of the current iteration and advance the looping index.\n\n\n\n\n10, Writing you own Functions (with examples)\n\u00b6\n\n\nSimple custom function\n\n\n# define\nmyfunction <- function(arg1, arg2, ...){\nstatements\nreturn(object)\n}\n\n# call\nmyfunction(arg1, arg2, ...)\n\n\n\n\nIt is possible to remplace function arguments with variables and objects.\n\n\nComplex custom function\n\n\nInvolve conditions and loops.\n\n\ntwosam <- function(y1, y2) {\n    n1 <- length(y1); n2 <- length(y2)\n    yb1 <- mean(y1);\n    yb2 <- mean(y2)\n    s1 <- var(y1);\n    s2 <- var(y2)\n    s <- ((n1 - 1) * s1 + (n2-1) * s2) / (n1 + n2 - 2)\n    tst <- (yb1 - yb2) / sqrt(s * (1 / n1 + 1 / n2))\n    tst\n}\n\n\n\n\nDefine and call on the same line\n\n\n\n\ntstat <- twosam(data$male, data$female); tstat\n\n\n\n\nSeveral commands and functions can be called on the same line with:\n\n\n\n\ncommand1; command2; function1; function2\n \n\n\n\n\nTwo ways of defining and calling a function\n\n\nmulty <- function(x, y) {\n    x * y\n    }\n\n# with a binary operator\n\"%!%\" <- function(x, y) {\n    x * y\n    }\n\nx <- 2\ny <- 2\n\nmulty(x, y)\n[1] 4\n\nx %!% y\n[1] 4\n\n\n\n\n\nMatrix multiplication operator, \n%*%\n, and the outer product matrix operator, \n%o%\n, are other examples of binary operators.\n\n\nFunction in a function\n\u00b6\n\n\n# case 1\narea <- function(f, a, b, eps = 1.0e-06, lim = 10) {\n    fun1 <- function(f, a, b, fa, fb, a0, eps, lim, fun) {\n        d <- (a + b)/2\n        h <- (b - a)/4\n        fd <- f(d)\n        a1 <- h * (fa + fd)\n        a2 <- h * (fd + fb)\n        if(abs(a0 - a1 - a2) < eps || lim == 0)\n            return(a1 + a2)\n        else {\n            return(fun(f, a, d, fa, fd, a1, eps, lim - 1, fun) \n                fun(f, d, b, fd, fb, a2, eps, lim - 1, fun))\n        }\n    }\n    fa <- f(a)\n    fb <- f(b)\n    a0 <- ((fa + fb) * (b - a))/2\n    fun1(f, a, b, fa, fb, a0, eps, lim, fun1)\n}\n\n# case 2\nf <- function(x) {\n    y <- 2*x\n    print(x)\n    print(y)\n    print(z)\n}\n\n# case 3\ncube <- function(n) {\n    sq <- function() n*n\n    n*sq()\n}\n\n# case 4\nopen.account <- function(total) {\n    list(\n        deposit = function(amount) {\n            if(amount <= 0)\n                stop(\"Deposits must be positive!\\n\")\n            total <<- total + amount\n            cat(amount, \"deposited. Your balance is\", total, \"\\n\\n\")\n        },\n        withdraw = function(amount) {\n            if(amount > total)\n                stop(\"You don\u2019t have that much money!\\n\")\n            total <<- total - amount\n            cat(amount, \"withdrawn. Your balance is\", total, \"\\n\\n\")\n        },\n        balance = function() {\n            cat(\"Your balance is\", total, \"\\n\\n\")\n        }\n    )\n}\n\nross <- open.account(100)\nrobert <- open.account(200)\n\nross$withdraw(30)\nross$balance()\nrobert$balance()\nross$deposit(50)\nross$balance()\nross$withdraw(500)\n\n\n\n\nName\n\n\nAdd, modify, and remove (with \nnames(x) <- NA or 0\n) names.\n\n\n\n\nnames()\n.\n\n\nrownames()\n.\n\n\ncolnames()\n.\n\n\ndimnames()\n.\n\n\n\n\nCustomizing startup\n\u00b6\n\n\nCustomize the R environment through a directory initialization file; commands that you want to execute every time R is started under your system.\n\n\nR will always source the Rprofile.site file first. \n\n\nOn Windows, the file is in C:\\Program Files\\R\\R-n.n.n\\etc. You can also place a .Rprofile file in any directory that you are going to run R from or in the user home directory. \n\n\nIndividual users control over their workspace and allows for different startup procedures in different working directories.\n\n\nIf no .Rprofile file is found in the startup directory, then R looks for a .Rprofile file in the user\u2019s home directory and uses that (if it exists) environment variable R_PROFILE_USER is set. The file it points to is used instead of the .Rprofile files. \n\n\nFunction named .First() in either of the two profile files or in the .RData image has a special status: initialize the environment\n\n\nSequence in which files are executed is:\n\n\n\n\nRprofile.site\n\n\nthe user profile\n\n\n.RData\n\n\n.First()\n\n\n.Last(), if defined, is (normally) executed at the very end of the session.\n\n\n\n\nList function and method\n\u00b6\n\n\n\n\nmethods(class = \"data.frame\")\n; list methods associated with the class.\n\n\nmethods(plot)\n; list methods specific to the object.\n\n\n\n\nmethods(class = \"data.frame\")\n [1] $             $<-           [             [[            [[<-         \n [6] [<-           aggregate     anyDuplicated as.data.frame as.list      \n[11] as.matrix     by            cbind         coerce        dim          \n[16] dimnames      dimnames<-    droplevels    duplicated    edit         \n[21] format        formula       head          initialize    is.na        \n[26] Math          merge         na.exclude    na.omit       Ops          \n[31] plot          print         prompt        rbind         row.names    \n[36] row.names<-   rowsum        show          slotsFromS3   split        \n[41] split<-       stack         str           subset        summary      \n[46] Summary       t             tail          transform     unique       \n[51] unstack       within       \nsee '?methods' for accessing help and source code\n\nmethods(plot)\n [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*\n [4] plot.default        plot.dendrogram*    plot.density*      \n [7] plot.ecdf           plot.factor*        plot.formula*      \n[10] plot.function       plot.hclust*        plot.histogram*    \n[13] plot.HoltWinters*   plot.isoreg*        plot.lm*           \n[16] plot.medpolish*     plot.mlm*           plot.ppr*          \n[19] plot.prcomp*        plot.princomp*      plot.profile.nls*  \n[22] plot.R6*            plot.raster*        plot.spec*         \n[25] plot.stepfun        plot.stl*           plot.table*        \n[28] plot.ts             plot.tskernel*      plot.TukeyHSD*     \nsee '?methods' for accessing help and source code\n\n\n\n\nDifference:\n\n\n\n\nplot()\n; a generic method.\n\n\nplot.\n ; a specific method such as \nplot.ts()\n for example.\n\n\n\n\n11, Statistical models in R\n\u00b6\n\n\nRegression\n\u00b6\n\n\ny <- 1:10\nx <- 1:10\n\na <- lm(y <sub>x)\na\nCall:\nlm(formula = y <sub>x)\n\nCoefficients:\n(Intercept)            x  \n  1.123e-15    1.000e+00  \n\n\n# no intercept, through the origin\nb <- lm(y <sub>0 + x)\nb\nCall:\nlm(formula = y <sub>0 + x)\n\nCoefficients:\nx  \n1  \n\n\n# no intercept\nc <- lm(y <sub>-1 + x)\nc\nCall:\nlm(formula = y <sub>-1 + x)\n\nCoefficients:\nx  \n1  \n\n\nd <- lm(y <sub>x - 1)\nd\nCall:\nlm(formula = y <sub>x - 1)\n\nCoefficients:\nx  \n1  \n\n\n# log\ne <- lm(log(y) <sub>x)\ne\nCall:\nlm(formula = log(y) <sub>x)\n\nCoefficients:\n(Intercept)            x  \n     0.2432       0.2304  \n\n\n# quadratic\nf <- lm(y <sub>1 + x + I(x^2))\nf\nCall:\nlm(formula = y <sub>1 + x + I(x^2))\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n  1.123e-15    1.000e+00    5.699e-18  \n\n\n# polynomial\ng <- lm(y <sub>X + poly(x, 2))\n\n\n# weighted regression\nfm1 <- lm(y <sub>x, data = dummy, weight = 1 / w^2)\n\n\n# and more\nnew.model <- update(old.model, new.formula)\n\nfm05 <- lm(y <sub>x1 + x2 + x3 + x4 + x5, data = production)\n\nfm6 <- update(fm06, . <sub>. + x6)\nsmf6 <- update(fm6, sqrt(,) <sub>.)\n\n\n\n\nExplore the results\n\n\n\n\nsummary(regression results)\n.\n\n\nvcov()\n; variance-covariance matrix.\n\n\naov(formula, data = data.frame)\n.\n\n\nanova(fitted.model.1, fitted.model.2, ...)\n\n\nand many more.\n\n\n\n\nanova, coeficient, coef, deviance, residuals, effects, formula, model, kappa, labels, plot, predict, proj, projection\n\n\nStepwise Regression\n\n\nSelect a suitable model by adding or dropping variables and preserving hierarchies. The best model with the smallest AIC (Akaike\u2019s Information Criterion) is discovered with the search.\n\n\nGeneralized least squares\n\u00b6\n\n\ngls, binomial, logit, probit, log, cloglog, gaussian, identity, log, inverse, gamma, identity, inverse, log, inverse.gaussian, 1/mu^2, identity, inverse, log, poisson, identity, log, sqrt, quasi-likelihood, logit, probit\n\n\nfitted.model <- glm(formula, famili=family.generator, data = data.frame)\n\n\n\n\nNonlinear least squares (MLE, Mixed, Local, Robust, Additive, Tree-based)\n\u00b6\n\n\nnls\n\n\n\n\nnlm(function)\n\n\n\n\nMaximum likehood (MLE)\n\n\nml\n\n\nWhen errors are not normal.\n\n\n\n\nml(function)\n\n\n\n\nMixed model\n\n\n\n\nnlme\n package.\n\n\n\n\nLocal Approximating Regression\n\n\nNonparametric local regression function.\n\n\n\n\nlrf <- lowess(x, y)\n.\n\n\n\n\nRobust regression\n\n\nMASS\n package.\n\n\nAdditive model\n\n\n\n\nacepack\n package.\n\n\nmda\n package.\n\n\ngam\n package.\n\n\nmgcv\n package.\n\n\n\n\nTree-based model\n\n\ndecision, classification tree, random forest<\\sub>\n\n\n\n\nrpart\n package.\n\n\ntree\n package.\n\n\n\n\n12, Graphical Procedures\n\u00b6\n\n\nGraphic, packages\n\u00b6\n\n\n\n\nlattice\n package.\n\n\nggplot2\n package.\n\n\ngrid\n package.\n\n\nggobi\n, \nrgl\n packages; for interactive graphics, 3D, and surfaces.\n\n\nand many more.\n\n\n\n\nBasic plot\n\u00b6\n\n\n\n\nplot()\n.\n\n\nboxplot()\n..\n\n\nlines(x,y)\n; add a line to a basic plot.\n\n\npairs()\n; multivariate, pairwise scatterplot matrix. \n\n\ncoplot(a <sub>b | c)\n; scatter plot of a \nb given c, a factor vector (levels)\n\n\ncoplot(a <sub>v | c + d)\n.\n\n\npie()\n.\n\n\nhist(x)\n; where \nnclass = n\n and \nbreaks = b\n.\n\n\nbarplot()\n; can be horizontal or vertical.\n\n\ndotchart(x, ...)\n; a case of bar chart.\n\n\nand many more with lots of options.\n\n\n\n\nQ-Q plot\n\u00b6\n\n\n\n\nqqnorm(x)\n.\n\n\nqqline(x)\n.\n\n\nqqplot(x, y)\n; comparison.\n\n\n\n\nPictures\n\u00b6\n\n\n\n\nimage(x, y, z, ...)\n; grid of rectangles with colors corresponding to the values in z.\n\n\ncontour(x, y, z, ...)\n; z add contour lines (even to an existing plot).\n\n\npersp(x, y, z, ...)\n; perspective plots of a surface over the x\u2013y plane.\n\n\n\n\nGraphic arguments and parameters\n\u00b6\n\n\nx1 <- rnorm(1000, 0.4, 0.8)\nx2 <- rnorm(1000, 0.0, 1.0)\nx3 <- rnorm(1000, -1.0, 1.0)\nhist(x1, width = 0.33, offset = 0.00, col = \"blue\", xlim = c(-4,4),\n     main = \"Histogram of x1, x2 & x3\",\n     xlab = \"x1 - blue, x2 - red, x3 - green\")\nhist(x2, width = 0.33, offset = 0.33, col = \"red\", add = TRUE)\nhist(x3, width = 0.33, offset = 0.66, col = \"green\", add = TRUE)\n\n\n\n\n\n\nadd = TRUE\n; superimpose a plot on another plot.\n\n\naxes = FALSE\n; no axes.\n\n\naxis(side,...)\n; 1 to 4, bottom, left, top, right.\n\n\nlog = \"x\", \"y\", \"xy\"\n; difference scale.\n\n\ntype = \"p\"\n (points); \n\"l\"\n (lines), \n\"b\"\n (p+l), \n\"o\"\n (l+p), \n\"h\"\n (vertical lines from points to the zero axis), \n\"s\"\n (step-function), \n\"n\"\n (not plotting).\n\n\nxlab = \"bla\"\n.\n\n\nylab = \"bla\"\n.\n\n\nmain = \"bla\"\n.\n\n\nsub = \"bla\"\n.\n\n\ntitle(main, sub)\n.\n\n\npoints(x, y)\n; add points on top of \nplot()\n.\n\n\ntext(x, y, labels,...)\n; add text to each point.\n\n\nplot(x, t, type = \"n\"); text(x, y, names)\n; replace the dot with text.\n\n\nabline(a, b)\n; add a line.\n\n\nabline(0, 1, lty = 3)\n.\n\n\nabline(coef(fm))\n.\n\n\nabline(coef(fm1), col = \"red\")\n.\n\n\nabline(h = y)\n; or \nh = value\n; add a horizontal line.\n\n\nabline(v = x)\n; or \nv = value\n; add a vertical line.\n\n\nabline(lm.obj)\n.\n\n\nlegend(x, y, legend, ...)\n; add a legend at a specified position.\n\n\nfill = v\n; add a vector of the same length as a legend.\n\n\nlty = 2\n; line style.\n\n\nlwd = 1.5\n; line width.\n\n\npch = 0\n; dot style.\n\n\npar()\n; list of permanent graphic parameters.\n\n\npar(c(\"col\", \"lty\"))\n; limit the list of parameters.\n\n\npar(col = 4, lty = 2)\n; set the parameters for all plots.\n\n\nplot(x, y, pch = \"+\")\n; will set a temporary parameter inside a plot.\n\n\npch = \"+\"\n, \npch = 4\n.\n\n\ncol = \"red\"\n; dot color.\n\n\ncol =  \"red\"\n.\n\n\ncol.axis = \"red\"\n.\n\n\ncol.lab = \"red\"\n.\n\n\ncol.main = \"red\"\n.\n\n\ncol.sub = \"red\"\n.\n\n\nfont = \"red\"\n.\n\n\nfont.axis = \"red\"\n.\n\n\nfont.lab = \"red\"\n.\n\n\nfont.main = \"red\"\n.\n\n\nfont.sub = \"red\"\n.\n\n\nadj = -0.1\n; adjust the text to the plotting position (-1\u2026-0.5\u20260\u20260.5\u20261), from left to right, 0 being the center.\n\n\ncex = 1.5\n; character 50% larger.\n\n\ncex.axis = 1.5\n.\n\n\ncex.lab = 1.5\n.\n\n\ncex.main = 1.5\n.\n\n\ncex.sub = 1.5\n.\n\n\nlab = c(5, 7, 12)\n; the first two numbers are the desired number of tick intervals on the x and y axes respectively. The third number is the desired length of axis labels.\n\n\nlas = 1\n; orientation of axis labels (text and numbers). 0 means always parallel to axis, 1 means always horizontal, and 2 means always perpendicular to the axis.\n\n\nmgp = c(3, 1, 0)\n; position of the axis components.\n\n\ntck = 0.01\n; length of tick marks.\n\n\nxaxs = \"r\"\n; axis style.\n\n\nxaxs = \"i\"\n; inside.\n\n\nmai = c(1, 0.5, 0.5, 0)\n; margins in inches around the plot.\n\n\nmar = c(4, 2, 2, 1)\n; in text lines.\n\n\nand many more.\n\n\n\n\nR allows you to create an n by m array of plots on a single page:\n\n\n\n\nmfcol = c(3.2)\n; 3 rows, 2 columns, 6 plotting areas.\n\n\nmfrow = c(3,2)\n; idem but filled by rows.\n\n\nomi = c(1, 0.5, 0.5, 0)\n; margins between plots.\n\n\noma = c(1, 0.5, 0.5, 0)\n; margins outside plots.\n\n\nmfg = c(2, 2, 3, 2)\n; position of the current figure in a multiple figure environment. The first two numbers are the row and column of the current figure; the last two are the total number of rows and columns in the multiple figure array; in other words, 2nd row and 2nd column in a 3x2 panel.\n\n\nfig = c(4, 9, 1, 4) / 10\n; position of the current figure on the page. Values are the positions of the left, right, bottom and top edges, respectively, as a percentage of the page measured from the bottom left corner.\n\n\nand many more.\n\n\n\n\nGeometric shapes\n\u00b6\n\n\n\n\npolygon(x, y, ...)\n; x, y are vectors containing the coordinates of the vertices of the polygon.\n\n\n\n\nGraphic help\n\n\n\n\nhelp(plotmath)\n.\n\n\nexample(plotmath)\n.\n\n\ndemo(plotmath)\n.\n\n\nhelp(Hershey)\n.\n\n\ndemo(Hershey)\n.\n\n\nhelp(Japanese)\n.\n\n\ndemo(Japanese)\n.\n\n\n\n\nGraphic text and mouse\n\u00b6\n\n\nLeave graphic marks and texts.\n\n\n\n\nlocator(n, type)\n; select with mouse a max of n locations to mark on the graph (left click, right click to stop).\n\n\ntext(locator(1), \"Outlier\", adj=0)\n; select with mouse a location to add a string on the graph.\n\n\nidentify(x, y)\n; add a label to a dot with mouse.\n\n\nidentify(x, y, labels)\n.\n\n\nidentify(x, y, \"yes\")\n.\n\n\n\n\nGraphic device\n\u00b6\n\n\n\n\nsplit.screen()\n; FALSE or number of regions within the current device which can, to some extent, be treated as separate graphics devices. It is useful for generating multiple plots on a single device.\n\n\nlayout()\n; divides the device up into as many rows and columns as there are in matrix mat.\n\n\n\n\nGraphic device driver\n\n\nOpen a special graphics window.\n\n\n\n\nX11()\n; under UNIX.\n\n\nwindows()\n; under Windows.\n\n\nwin.printer()\n.\n\n\nwin.metafile()\n.\n\n\n\n\n\n\nquartz()\n; under OS X.\n\n\ndev.new()\n; returns the return value of the device opened, usually invisible NULL.\n\n\ngrid()\n adds an rectangular grid to an existing plot.\n\n\npostscript()\n; starts the graphics device driver for producing PostScript graphics.\n\n\npostscript(\"file.ps\", horizontal=FALSE, height=5, pointsize=10)\n.\n\n\npostscript(\"plot1.eps\", horizontal=FALSE, onefile=FALSE, height=8, width=6, pointsize=10)\n.\n\n\n\n\n\n\npdf()\n; starts the graphics device driver for producing PDF graphics.\n\n\npng()\n; idem.\n\n\njpeg()\n; idem.\n\n\ntiff()\n; idem.\n\n\nbitmap()\n; idem.\n\n\ndev.off()\n; shuts down the specified (by default the current) device.\n\n\ndev.set()\n; dev.set makes the specified device the active device.\n\n\ndev.list()\n; returns the numbers of all open devices.\n\n\ndev.next()\n, \ndev.prev()\n; return the number and name of the next / previous device in the list of devices.\n\n\ngraphics.off(); terminate all devices.\n\n\n\n\n13, Packages\n\u00b6\n\n\n\n\ninstall.package()\n; on the computer.\n\n\nlibrary()\n; load it.\n\n\nsearch()\n; check what is loaded.\n\n\nloadedNamespaces()\n; idem.\n\n\nhelp.start()\n; start the HTML help system, package section.\n\n\n\n\nFind out about packages:\n\n\n\n\nCRAN.R-project.org\n\n\nwww.bioconductor.org\n\n\nwww.omegahat.org\n\n\n\n\n14, OS Facilities\n\u00b6\n\n\nManage files with Linux or Windows or RStudio.",
            "title": "An Introduction to R"
        },
        {
            "location": "/An Introduction to R/#2-simple-manipulations-numbers-and-vectors",
            "text": "",
            "title": "2, Simple Manipulations; Numbers and Vectors"
        },
        {
            "location": "/An Introduction to R/#create-a-vector",
            "text": "x <- c(1, 2) # assignment (universal).\nx\n[1] 1 2\n\nc(2, 1) -> y # alternative assignment.\ny\n[1] 2 1\n\nx = c(1, 2) # alternative assignment (some limitations).\nx <<- c(1, 2) # permanent assignment.\n\n# examples\nab <- 9\nab\n[1] 9\n\nassign(\"ab\", 10)\nab \n[1] 10  Create a sequence (vector)  x <- 2 * 1:15\n\nx\n[1]  2  4  6  8 10 12 14 16 18 20 22 24 26 28 30\n\nn <- 10\nx <- 1:n-1\n\nx\n[1] 1 2 3 4 5 6 7 8 9\n\nx <- 30:1\nx\n[1] 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n[29]  2  1\n\nseq(from = 1, to = 30)\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n[29] 29 30\n\nseq(1, 30)\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n[29] 29 30\n\nseq(30, 1)\n[1] 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n[29]  2  1\n\nseq(1, 30, by = 2)\n[1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29",
            "title": "Create a vector"
        },
        {
            "location": "/An Introduction to R/#repetition",
            "text": "rep(2, times = 5)\n[1] 2 2 2 2 2  x <- c(\"x\", \"y\")[rep(c(1, 2, 2, 1), times = 4)]\n\nx\n[1] \"x\" \"y\" \"y\" \"x\" \"x\" \"y\" \"y\" \"x\" \"x\" \"y\" \"y\" \"x\" \"x\" \"y\" \"y\" \"x\"",
            "title": "Repetition"
        },
        {
            "location": "/An Introduction to R/#length-vector",
            "text": "length(vector)",
            "title": "Length (vector)"
        },
        {
            "location": "/An Introduction to R/#boolean",
            "text": "TRUE  or  T ;  T == 1 .  FALSE  or  F ;  F == 0 .   Some operators   <  >=  <  >=  ==  !=  <>  &  &&  |  ||  and many more.",
            "title": "Boolean"
        },
        {
            "location": "/An Introduction to R/#missing-data-and-more",
            "text": "NA; not available.  NaN; not a number.  Inf - Inf == NaN == 0/0; infinite number.   x <- c(1:3, NA)\n\nx\n[1] 1 2 3 NA   is.na(var) .  var == NA .  is.na(x) .  !is.na(x) .   x <- c(-1:3, NA)\ny <- x[!is.na(x) & x > 0]\n\nx\n[1]  -1  0  1  2  3 NA\n\ny\n[1] 1 2 3",
            "title": "Missing data and more"
        },
        {
            "location": "/An Introduction to R/#extract-subset-vector",
            "text": "x[i] ; index.",
            "title": "Extract, subset (vector)"
        },
        {
            "location": "/An Introduction to R/#backslash-use-for-some-characters",
            "text": "\\\\ ; backslash.  \\n ; new line.  \\t ; tab.  \\b ; backspace.",
            "title": "Backslash use for some characters"
        },
        {
            "location": "/An Introduction to R/#concatenate-paste-vector",
            "text": "labs <- paste(c(\"X\", \"Y\"), 1:10, sep = \"\")\n\nlabs\n[1] \"X1\"  \"Y2\"  \"X3\"  \"Y4\"  \"X5\"  \"Y6\"  \"X7\"  \"Y8\"  \"X9\"  \"Y10\"",
            "title": "Concatenate, paste (vector)"
        },
        {
            "location": "/An Introduction to R/#a-note-of-data-handling-and-manipulations",
            "text": "You can also  split() ,  merge() ,  rbind() ,  cbind()  vectors.   It is also possible with other objects such as factors, lists, arrays, matrices, and data frames.  There are built-in functions to extract, exclude, subset, replace, transform or convert ( .as ), concatenate, paste, group, and bind.  slice, extract, exclude, subset, replace, convert,  concatenate, paste, group, bind",
            "title": "A note of data handling and manipulations"
        },
        {
            "location": "/An Introduction to R/#exclude-remove-vector",
            "text": "z <- 1:20\n\nz\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\nzz <- z[-(1:5)]\n\nzz\n[1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20",
            "title": "Exclude, remove (vector)"
        },
        {
            "location": "/An Introduction to R/#replace-vector",
            "text": "x <- c(1, 1, 1, NA)\n\nx\n[1]  1  1  1 NA\n\nx[is.na(x)] <- 0\n\nx\n[1] 1 1 1 0\n# replaces any missing values in x by zeros",
            "title": "Replace (vector)"
        },
        {
            "location": "/An Introduction to R/#absolute-value",
            "text": "y <- abs(y) .   Convert ( .as )   as.vector() .  as.integer() .  as.numeric()  as.factor() .  as.character() .  and many more.",
            "title": "Absolute value"
        },
        {
            "location": "/An Introduction to R/#3-objects-their-modes-and-attributes",
            "text": "",
            "title": "3, Objects, their Modes and Attributes"
        },
        {
            "location": "/An Introduction to R/#object-type",
            "text": "obj <- 1\n\nobj1 <- numeric(obj)\nmode(obj1)\n[1] \"numeric\"\nobj2 <- character(obj)\nmode(obj2)\n[1] \"character\"\n\nx <- 1\n\n# class() != mode(), but almost\nmode(x)\n[1] \"numeric\"\n\nx <- factor(x)\nx\n[1] 1\nLevels: 1\n\nx <- numeric(x)\nx\n[1] 0\n\n# load key:value\nobj[3] <- 17\n\nobj\n[1]  1 NA 17",
            "title": "Object type"
        },
        {
            "location": "/An Introduction to R/#classes",
            "text": "class   \u201cnumeric\u201d.  \u201clogical\u201d.  \u201ccharacter\u201d.  \u201clist\u201d.  \u201cmatrix\u201d.  \u201carray\u201d.  \u201cfactor\u201d.  \u201cdata.frame\u201d.   class(obj)\n[1] \"numeric\"\n\n# print the object as ordinary\nunclass(obj)\n[1]  1 NA 17",
            "title": "Classes"
        },
        {
            "location": "/An Introduction to R/#4-ordered-and-unordered-factors",
            "text": "state <- c(\"tas\",\"qld\",\"sa\",\"sa\",\"sa\",\"vic\",\"nt\",\"act\",\"qld\",\"nsw\",\"wa\",\"nsw\",\"nsw\",\"vic\",\"vic\",\"vic\",\"nsw\",\"qld\",\"qld\",\"vic\",\"nt\",\"wa\",\"wa\",\"qld\",\"sa\", \"tas\",\"nsw\", \"nsw\", \"wa\",\"act\")\n\nstatef <- factor(state)\n\n# class != mode()\nclass(statef)\n[1] \"factor\"\nmode(statef)\n[1] \"numeric\"\n\nlevels(statef)\n[1] \"act\" \"nsw\" \"nt\"  \"qld\" \"sa\"  \"tas\" \"vic\" \"wa\" \n\nincomes <- c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\n\nincmeans <- tapply(incomes, statef, mean)\n\nincmeans\n  act   nsw    nt   qld    sa   tas   vic    wa \n48.50 55.00 54.00 51.60 54.25 53.00 61.60 54.50 \n\nstderr <- function(x) { sqrt(var(x) / length(x)) }\n\nstderr(incomes)\n[1] 1.524462\n\n# alternatively\nincster <- tapply(incomes, statef, stderr)\n\nincster\n      act       nsw        nt       qld        sa       tas       vic        wa \n5.5000000 3.9665266 5.0000000 2.6570661 5.3909647 7.0000000 0.8717798 6.2249498    # a vector of characters\nstateff <- c(\"a\", \"b\", \"c\")\n[1] \"a\" \"b\" \"c\"\n\nas.factor(stateff)\n[1] a b c\nLevels: a b c  # Create a factor with the wrong order of levels\nsizes <- factor(c(\"small\", \"large\", \"large\", \"small\", \"medium\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# levels can be specified explicitly\nsizes <- factor(sizes, levels = c(\"small\", \"medium\", \"large\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# do the same with an ordered factor\nsizes <- ordered(c(\"small\", \"large\", \"large\", \"small\", \"medium\"))\n\nsizes <- ordered(sizes, levels = c(\"small\", \"medium\", \"large\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small < medium < large\n\n# use relevel() to make a particular level first in the list. (This will not work for ordered factors.)\n\n# Create a factor with the wrong order\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# make medium first\nsizes <- relevel(sizes, \"medium\")\n\nsizes\n[1] small  large  large  small  medium\nLevels: medium large small\n\n# make small first\nsizes <- relevel(sizes, \"small\")\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# specify the proper order when the factor is created\nsizes <- factor(c(\"small\", \"large\", \"large\", \"small\", \"medium\"),\n                  levels = c(\"small\", \"medium\", \"large\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# Create a factor with the wrong order of levels\nsizes <- factor(c(\"small\", \"large\", \"large\", \"small\", \"medium\"))\n\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# reverse the order of levels in a factor\nsizes <- factor(sizes, levels=rev(levels(sizes)))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large",
            "title": "4, Ordered and Unordered Factors"
        },
        {
            "location": "/An Introduction to R/#convert-as",
            "text": "as.factor() .",
            "title": "Convert (.as)"
        },
        {
            "location": "/An Introduction to R/#5-arrays-and-matrices",
            "text": "",
            "title": "5, Arrays and Matrices"
        },
        {
            "location": "/An Introduction to R/#dimension",
            "text": "z <- 1:1500\ndim(z) <- c(3, 5, 100)\n\n# gives 100 arrays of 3 lines by 5 columns  Create a matrix, an array  x <- array(1:20, dim=c(4, 5))\n\nx\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20\n\nx <- array(1:20, dim=c(2, 5, 2))\n\nx\n, , 1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n, , 2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   11   13   15   17   19\n[2,]   12   14   16   18   20\n\n\ni <- array(c(1:3, 3:1), dim = c(3, 2))\n\ni\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    2\n[3,]    3    1\n\nj <- array(c(1:8), dim = c(2, 2, 2))\n\nj\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nk <- array(1:27, c(3, 3, 3))\n\n> k\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   19   22   25\n[2,]   20   23   26\n[3,]   21   24   27\n\n\na <- matrix(1, 2, 2)\n\na\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n\nb <- matrix(2, 2, 2)\n\nb\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2",
            "title": "Dimension"
        },
        {
            "location": "/An Introduction to R/#extract-subset-matrix",
            "text": "a[2, 1] ; rows, columns.   Extract, subset (array)   a[2, 1, 1] ; rows, columns, matrix.",
            "title": "Extract, subset (matrix)"
        },
        {
            "location": "/An Introduction to R/#cross-product-vs-multiplication",
            "text": "a <- 1:5\nb <- seq(10, 6, -1)\n\na\n[1] 1 2 3 4 5\nb\n[1] 10  9  8  7  6\n\na * b\n[1] 10 18 24 28 30\n\ncrossprod(a, b)\n     [,1]\n[1,]  110\n\nab <- a %o% b\nab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   10    9    8    7    6\n[2,]   20   18   16   14   12\n[3,]   30   27   24   21   18\n[4,]   40   36   32   28   24\n[5,]   50   45   40   35   30",
            "title": "Cross product vs multiplication"
        },
        {
            "location": "/An Introduction to R/#matrix-operation",
            "text": "A et B are 2x2 matrices:   A * B ; scalar multiplication.  A %*% B ; matrix multiplication  x %*% A %*% y ; matrix multiplication.  crossproduct(A, B) ; cross multiplication.  ab <- outer(A,B,\"*\") ;  a * b .  ab <- outer(A,B,\"+\") ;  a + b .  ab <- outer(A,B,\"-\") ;  a - b .  and many more.",
            "title": "Matrix operation"
        },
        {
            "location": "/An Introduction to R/#diagonal-and-triangle",
            "text": "ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   11   10    9    8    7\n[2,]   12   11   10    9    8\n[3,]   13   12   11   10    9\n[4,]   14   13   12   11   10\n[5,]   15   14   13   12   11\n\ndiag(ab)\n[1] 11 11 11 11 11\n\nlower.tri(ab)\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE FALSE FALSE FALSE FALSE\n[2,]  TRUE FALSE FALSE FALSE FALSE\n[3,]  TRUE  TRUE FALSE FALSE FALSE\n[4,]  TRUE  TRUE  TRUE FALSE FALSE\n[5,]  TRUE  TRUE  TRUE  TRUE FALSE\n\nlower.tri(ab) * ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]   12    0    0    0    0\n[3,]   13   12    0    0    0\n[4,]   14   13   12    0    0\n[5,]   15   14   13   12    0\n\nupper.tri(ab)\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE  TRUE  TRUE  TRUE  TRUE\n[2,] FALSE FALSE  TRUE  TRUE  TRUE\n[3,] FALSE FALSE FALSE  TRUE  TRUE\n[4,] FALSE FALSE FALSE FALSE  TRUE\n[5,] FALSE FALSE FALSE FALSE FALSE\n\nupper.tri(ab) * ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0   10    9    8    7\n[2,]    0    0   10    9    8\n[3,]    0    0    0   10    9\n[4,]    0    0    0    0   10\n[5,]    0    0    0    0    0",
            "title": "Diagonal and triangle"
        },
        {
            "location": "/An Introduction to R/#solving-a-matrix-system-and-more-matrix-algebra",
            "text": "b <- A %*% x\n\n# or\nsolve(A, b)   solve(A) ; inverse the matrix.   Symmetrical matrix and eigenvalue  b <- matrix(2, 2, 2)\n\nb\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\ne <- eigen(b, only.values = TRUE)\ne\n$values\n[1] 4 0\n\n$vectors\nNULL  Singular value decomposition (matrix)  svd(b)\n$d\n[1] 4 0\n\n$u\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068\n\n$v\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068  Determinant (matrix)  det(ab)\n[1] 0",
            "title": "Solving a matrix system (and more matrix algebra)"
        },
        {
            "location": "/An Introduction to R/#least-square-fitting-matrix",
            "text": "lsfit()  or least squares estimate of  b  in the model:  y = X b + e .   QR decomposition (matrix)  qr(ab)\n$qr\n            [,1]        [,2]          [,3]          [,4]          [,5]\n[1,] -29.2403830 -27.0174299 -2.479448e+01 -2.257152e+01 -2.034857e+01\n[2,]   0.4103913   0.2418254  4.836508e-01  7.254763e-01  9.673017e-01\n[3,]   0.4445906  -0.1703815  3.717512e-15  1.134003e-14  1.749210e-14\n[4,]   0.4787899  -0.5015812  3.957070e-01  1.553722e-15  1.295516e-15\n[5,]   0.5129892  -0.8327809  5.076995e-01  6.936403e-01 -1.685698e-16\n\n$rank\n[1] 2\n\n$qraux\n[1] 1.376192e+00 1.160818e+00 1.765282e+00 1.720322e+00 1.685698e-16\n\n$pivot\n[1] 1 2 3 4 5\n\nattr(,\"class\")\n[1] \"qr\"  Also:   qr.coef() .  qr.fitted() .  qr.resid() .",
            "title": "Least square fitting (matrix)"
        },
        {
            "location": "/An Introduction to R/#convert-as_1",
            "text": "as.array() .  as.matrix() .",
            "title": "Convert (.as)"
        },
        {
            "location": "/An Introduction to R/#6-lists-and-data-frames",
            "text": "",
            "title": "6, Lists and Data Frames"
        },
        {
            "location": "/An Introduction to R/#create-a-list",
            "text": "Lst <- list(name = \"Fred\", wife = \"Mary\", no.children = 3, child.ages = c(4,7,9))\n\nLst\n$name\n[1] \"Fred\"\n\n$wife\n[1] \"Mary\"\n\n$no.children\n[1] 3\n\n$child.ages\n[1] 4 7 9",
            "title": "Create a list"
        },
        {
            "location": "/An Introduction to R/#extract-subset-list",
            "text": "Lst$name\n[1] \"Fred\"\n\nLst[[\"name\"]] == Lst[[1]]\n[1] TRUE\n\n\nLst[5] <- list(\"\") # or list()  \nLst[\"new\"] <- list()  # key, value\n\nLst[1]\n$name\n[1] \"Fred\"\n\nLst$child.ages[1]\n[1] 4\nLst[[4]][1]\n[1] 4",
            "title": "Extract, subset (list)"
        },
        {
            "location": "/An Introduction to R/#concatenate-paste",
            "text": "x <- c(1,2)\ny <- c(3,4)\n\nc(x, y)\n[1] 1 2 3 4\n\npaste(x, y)\n[1] \"1 3\" \"2 4\"\n\ndata.frame(x, y)\n  x y\n1 1 3\n2 2 4\n\na <- list(1, 2)\nb <- list(\"a\", \"b\")\n\nlist(a, b)\n[[1]]\n[[1]][[1]]\n[1] 1\n\n[[1]][[2]]\n[1] 2\n\n\n[[2]]\n[[2]][[1]]\n[1] \"a\"\n\n[[2]][[2]]\n[1] \"b\"\n\nh <- matrix(2, 2, 2)\ng <- matrix(1, 2, 2)\n\nc(h, g)\n[1] 2 2 2 2 1 1 1 1\n\npaste(h, g)\n[1] \"2 1\" \"2 1\" \"2 1\" \"2 1\"\n\nlist(h, g)\n[[1]]\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n[[2]]\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1",
            "title": "Concatenate, paste"
        },
        {
            "location": "/An Introduction to R/#convert-as_2",
            "text": "as.matrix()",
            "title": "Convert (as.)"
        },
        {
            "location": "/An Introduction to R/#data-frame",
            "text": "A data frame can hold other data frames.  A list can hold other lists.  A vector can hold other vectors.    Each variable can be numeric, character, logical, factor, numeric matrix, list, data.frame.   state <- c(\"tas\",\"qld\",\"sa\",\"sa\",\"sa\",\"vic\",\"nt\",\"act\",\"qld\",\"nsw\",\"wa\",\"nsw\",\"nsw\",\"vic\",\"vic\",\"vic\",\"nsw\",\"qld\",\"qld\",\"vic\",\"nt\",\"wa\",\"wa\",\"qld\",\"sa\", \"tas\",\"nsw\", \"nsw\", \"wa\",\"act\")\n\nstatef <- factor(state)\n\nincomes <- c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\n\nincomef <- factor(incomes)\n\naccountants <- data.frame(home=statef, loot=incomes, shot=incomef)\n\nhead(accountants, 10)\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n\n# class() != mode()\nclass(accountants)\n[1] \"data.frame\"\n\nmode(accountants)\n[1] \"list\"",
            "title": "Data frame"
        },
        {
            "location": "/An Introduction to R/#concatenate-paste-data-frame",
            "text": "# accountants == acc\n\nc(accountants, acc)\n$home\n [1] tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n[14] vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n[27] nsw nsw wa  act\nLevels: act nsw nt qld sa tas vic wa\n\n$loot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n\n$shot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n20 Levels: 40 41 42 43 46 48 49 51 52 54 56 58 59 ... 70\n\n$home\n [1] tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n[14] vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n[27] nsw nsw wa  act\nLevels: act nsw nt qld sa tas vic wa\n\n$loot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n\n$shot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n20 Levels: 40 41 42 43 46 48 49 51 52 54 56 58 59 ... 70\n\n\npaste(accountants, acc)\n[1] \"c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1) c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1)\"                                                            \n[2] \"c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43) c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\"\n[3] \"c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4) c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4)\"                          \n\nlist(accountants, acc)\n[[1]]\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n...\n\n[[2]]\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n...\n\naccountants + acc\n   home loot shot\n1    NA  120   NA\n2    NA   98   NA\n3    NA   80   NA\n4    NA  122   NA\n5    NA  128   NA\n6    NA  120   NA\n7    NA  118   NA\n8    NA  108   NA\n9    NA  124   NA\n10   NA  138   NA\n...",
            "title": "Concatenate, paste (data frame)"
        },
        {
            "location": "/An Introduction to R/#convert-as_3",
            "text": "as.data.frame()",
            "title": "Convert (as.)"
        },
        {
            "location": "/An Introduction to R/#load-data-into-r",
            "text": "read.table() ; produce a data frame with inputs.   A note on environment objects   search() ; objects in .GlobalEnv; including packages.  ls() ; list these objects.    attach() ; attach an object to .GlobalEnv.  detach() .",
            "title": "Load data into R"
        },
        {
            "location": "/An Introduction to R/#7-readingwriting-data-fromto-files-inputoutput",
            "text": "A lot more can be found on I/O: depending on the file type, the data format, the desired R object, many commands are available. Depending on a file format, look for the dedicated package on CRAN.  read, reading, write, writing, input, output, i/o  Examples  # read from a file\nHousePrice <- read.table(\"house.data\", header = TRUE, sep = \";\")\n\n# read a 3-variable list\ninput <- scan(\"input.dat\", list(\"\", 0, 0))  \n\n# read a3-variable list\ninput <- scan(\"input.dat\", list(id=\"\", x = 0, y = 0))\n\n# make 3 separate vectors\nlabel <- input[[1]]\nx <- input[[2]]\ny <- input[[3]]\n\n# label them\nlabel <- inp$id\nx <- inp$x\ny <- inp$y\n\n# read a 5-variable list and transform it into a matrix\nX <- matrix(scan(\"light.dat\", 0), ncol = 5, byrow = TRUE)  Inputing from file types\u2026   .txt.  .csv.  .tsv.  .hdf5.  .bmp.  .jpeg.  .png.  .tiff.  .zip.  .xls, spreadsheet.  databases.  statistical programs.  binary files.  and many more (some are up and coming such as julia files).",
            "title": "7, Reading/Writing Data from/to Files (Input/Output)"
        },
        {
            "location": "/An Introduction to R/#cleaning-parameter",
            "text": "strip.white = TRUE ; remove unnecessary spaces.",
            "title": "Cleaning parameter"
        },
        {
            "location": "/An Introduction to R/#change-the-data-frame-or-matrix-format",
            "text": "stack() ; for example, take a 6-column, 4-variable data frame and stack the 4 variables into one long column.  unstack() ; vice-versa.   # 6 columns or variables\nStatus Age  V1    V2    V3    V4\n P 23646 45190 50333 55166 56271\nCC 26174 35535 38227 37911 41184\nCC 27723 25691 25712 26144 26398\nCC 27193 30949 29693 29754 30772\nCC 24370 50542 51966 54341 54273\nCC 28359 58591 58803 59435 61292\nCC 25136 45801 45389 47197 47126\n...\n\n# 4 columns, more rows\nStatus Age values ind\nX1  P 23646 45190 V1\nX2 CC 26174 35535 V1\nX3 CC 27723 25691 V1\nX4 CC 27193 30949 V1\nX5 CC 24370 50542 V1\nX6 CC 28359 58591 V1\nX7 CC 25136 45801 V1\nX11 P 23646 50333 V2\n...   reshape() ; stack and unstack variables, according to parameters.   reshape(zz, idvar = \"id\", timevar = \"var\", varying = list(c(\"V1\", \"V2\", \"V3\", \"V4\")), direction = \"long\")   ftable() ; flatten multidimensional matrices (arrays);  read.ftable() .  data() ; list of datasets.  data(package = \"rpart\") ; load package.  edit(input) ; open a mini-spreadsheet.  xnew <- edit(input) ; open and save it as a new dataset.  and many more.",
            "title": "Change the data frame or matrix format"
        },
        {
            "location": "/An Introduction to R/#encoding",
            "text": "utf-8 for Linux and OS X.  UCS-2LE and UTF-16 for Windows.   # examples\nreadLines(\"filename.txt\", encoding = \"UTF-8\")\nreadLines(\"filename.txt\", encoding = \"UCS2LE\")\nread.delim(\"clipboard\", fileEncoding=\"UTF-16\")",
            "title": "Encoding"
        },
        {
            "location": "/An Introduction to R/#output",
            "text": "write.table() ; many parameters.  write.matrix() .  write.foreign() .",
            "title": "Output"
        },
        {
            "location": "/An Introduction to R/#check-a-file",
            "text": "readLines(\"aab.txt\") .  readLines(\"aab.txt\", 1) .  unlink(\"aab.txt\") ; delete the file on the working directory.",
            "title": "Check a file"
        },
        {
            "location": "/An Introduction to R/#directory-management",
            "text": "getwd() ; get the current working directory.  setwd() ; set the current working directory.  file.show(filepath) .  system.file()",
            "title": "Directory management"
        },
        {
            "location": "/An Introduction to R/#spreadsheet-editor-and-edition-in-r",
            "text": "fix(c) ; edit object c; a vetor, list, data frame, etc. in a mini-spreadsheet.  data.entry(c, mode=NULL, Names=NULL), dataentry(c), de(c, Modes=list(), Names=NULL) ; idem.  vi(file= ) ; invoke a text editor.  emacs(file= ) ; idem.  pico(file= ) ; idem.  xemacs(file= ) ; idem.  xedit(file=) ; idem.",
            "title": "Spreadsheet editor and edition in R"
        },
        {
            "location": "/An Introduction to R/#8-probability-distributions",
            "text": "",
            "title": "8, Probability Distributions"
        },
        {
            "location": "/An Introduction to R/#distribution",
            "text": "beta .  binom .  cauchy .  chisq .  exp .  f .  gamma .  geom .  hyper .  lnorm .  logis .  nbinom .  norm .  pois .  signrank .  t .  unif .  weibull .  wilcox .  and many more.   Prefixes   d ; density.  p ; CDF or cumulative density function.  q ; quantile.  r ; random deviates or simulation or number generation.",
            "title": "Distribution"
        },
        {
            "location": "/An Introduction to R/#distribution-operation",
            "text": "Commands, prefix + distribution, examples:   dbeta(x= ) .  pbinom(q= , lower.tail= , log.p= ) .  qcauchy(p= , lower.tail= , log.p= ) .  rchisq(n or r= ) .  ptukey() ; studentized (t) distribution  qtukey() .  dmultinom() .  ltinomial .  rmultinom() .  etc.",
            "title": "Distribution operation"
        },
        {
            "location": "/An Introduction to R/#descriptive-statistics",
            "text": "# overview\nsummary(cars)\nstr(cars)\nboxplot(cars$speed)\nboxplot(cars$dist)\n\n# Tukey five-number summaries\nfivenum(cars$speed)\n\n# histograms and bar charts\nstem(cars$speed)\nhist(cars$speed)\nbarplot(cars$speed)\ndotchart(cars$speed)\n\n# scatter plot\nplot(cars$speed, cars$dist)\nlines(cars$speed, cars$dist)\n\n# add a (1-d) representation\nplot(cars$speed, cars$dist)\nrug(cars$speed, ticksize = 0.03)\nplot(cars$speed, cars$dist)\nrug(cars$dist, ticksize = 0.03)\n\n# normality of the residuals, linearity\nqqnorm(cars$speed)\nqqline(cars$speed)\n\nqqnorm(cars$dist)\nqqline(cars$dist)\n\nqqplot(cars$speed, cars$dist)\n\n# normality test\nshapiro.test(cars$speed)\n\n# Kolmogorov-Smirnov test on a sample\nks.test(cars$speed[10], \"pnorm\", mean = mean(cars$speed), sd = sqrt(var(cars$speed)))\nks.test(cars$speed[40], \"pnorm\", mean = mean(cars$speed), sd = sqrt(var(cars$speed)))",
            "title": "Descriptive statistics"
        },
        {
            "location": "/An Introduction to R/#test",
            "text": "t.test(A,B) ; true difference of means is not 0, difference means.  var.test(A,B) ; true ratio of variances is not 1, difference variances.  t.test(A,B, var.equal=TRUE) ; true difference of means is not 0.  wilcox.test(A,B) ; rank sum with continuity correction, continuous distribution.  and many more.",
            "title": "Test"
        },
        {
            "location": "/An Introduction to R/#normality",
            "text": "plot(ecdf(A), do.points=FALSE, verticals=TRUE, xlim=range(A, B))  plot(ecdf(B), do.points=FALSE, verticals=TRUE, add=TRUE)  ks.test(A,B); maximal vertical distance between the two ecdf",
            "title": "Normality"
        },
        {
            "location": "/An Introduction to R/#9-grouping-loops-and-conditional-execution",
            "text": "",
            "title": "9, Grouping, Loops and Conditional Execution"
        },
        {
            "location": "/An Introduction to R/#control-flow",
            "text": "& ; AND.  && ; AND; evaluates left to right, examining only the first element of each vector.  | ; OR.  || ; OR; evaluates left to right, examining only the first element of each vector.  xor() ; elementwise exclusive OR.  isTRUE(x) ; is true if and only if x is a length-one logical vector whose only element is TRUE and which has no attributes.  ifelse(condition, a, b) ; if  condition  is proven true, return  a , or else, return  b .",
            "title": "Control flow"
        },
        {
            "location": "/An Introduction to R/#loops",
            "text": "looping   for(var in seq) expr .  while(condition is true) expr .  repeat expr .  break ; break out of a for, while or repeat loop; control is transferred to the first statement outside the inner-most loop.  next ; halt the processing of the current iteration and advance the looping index.",
            "title": "Loops"
        },
        {
            "location": "/An Introduction to R/#10-writing-you-own-functions-with-examples",
            "text": "Simple custom function  # define\nmyfunction <- function(arg1, arg2, ...){\nstatements\nreturn(object)\n}\n\n# call\nmyfunction(arg1, arg2, ...)  It is possible to remplace function arguments with variables and objects.  Complex custom function  Involve conditions and loops.  twosam <- function(y1, y2) {\n    n1 <- length(y1); n2 <- length(y2)\n    yb1 <- mean(y1);\n    yb2 <- mean(y2)\n    s1 <- var(y1);\n    s2 <- var(y2)\n    s <- ((n1 - 1) * s1 + (n2-1) * s2) / (n1 + n2 - 2)\n    tst <- (yb1 - yb2) / sqrt(s * (1 / n1 + 1 / n2))\n    tst\n}  Define and call on the same line   tstat <- twosam(data$male, data$female); tstat   Several commands and functions can be called on the same line with:   command1; command2; function1; function2     Two ways of defining and calling a function  multy <- function(x, y) {\n    x * y\n    }\n\n# with a binary operator\n\"%!%\" <- function(x, y) {\n    x * y\n    }\n\nx <- 2\ny <- 2\n\nmulty(x, y)\n[1] 4\n\nx %!% y\n[1] 4  Matrix multiplication operator,  %*% , and the outer product matrix operator,  %o% , are other examples of binary operators.",
            "title": "10, Writing you own Functions (with examples)"
        },
        {
            "location": "/An Introduction to R/#function-in-a-function",
            "text": "# case 1\narea <- function(f, a, b, eps = 1.0e-06, lim = 10) {\n    fun1 <- function(f, a, b, fa, fb, a0, eps, lim, fun) {\n        d <- (a + b)/2\n        h <- (b - a)/4\n        fd <- f(d)\n        a1 <- h * (fa + fd)\n        a2 <- h * (fd + fb)\n        if(abs(a0 - a1 - a2) < eps || lim == 0)\n            return(a1 + a2)\n        else {\n            return(fun(f, a, d, fa, fd, a1, eps, lim - 1, fun) \n                fun(f, d, b, fd, fb, a2, eps, lim - 1, fun))\n        }\n    }\n    fa <- f(a)\n    fb <- f(b)\n    a0 <- ((fa + fb) * (b - a))/2\n    fun1(f, a, b, fa, fb, a0, eps, lim, fun1)\n}\n\n# case 2\nf <- function(x) {\n    y <- 2*x\n    print(x)\n    print(y)\n    print(z)\n}\n\n# case 3\ncube <- function(n) {\n    sq <- function() n*n\n    n*sq()\n}\n\n# case 4\nopen.account <- function(total) {\n    list(\n        deposit = function(amount) {\n            if(amount <= 0)\n                stop(\"Deposits must be positive!\\n\")\n            total <<- total + amount\n            cat(amount, \"deposited. Your balance is\", total, \"\\n\\n\")\n        },\n        withdraw = function(amount) {\n            if(amount > total)\n                stop(\"You don\u2019t have that much money!\\n\")\n            total <<- total - amount\n            cat(amount, \"withdrawn. Your balance is\", total, \"\\n\\n\")\n        },\n        balance = function() {\n            cat(\"Your balance is\", total, \"\\n\\n\")\n        }\n    )\n}\n\nross <- open.account(100)\nrobert <- open.account(200)\n\nross$withdraw(30)\nross$balance()\nrobert$balance()\nross$deposit(50)\nross$balance()\nross$withdraw(500)  Name  Add, modify, and remove (with  names(x) <- NA or 0 ) names.   names() .  rownames() .  colnames() .  dimnames() .",
            "title": "Function in a function"
        },
        {
            "location": "/An Introduction to R/#customizing-startup",
            "text": "Customize the R environment through a directory initialization file; commands that you want to execute every time R is started under your system.  R will always source the Rprofile.site file first.   On Windows, the file is in C:\\Program Files\\R\\R-n.n.n\\etc. You can also place a .Rprofile file in any directory that you are going to run R from or in the user home directory.   Individual users control over their workspace and allows for different startup procedures in different working directories.  If no .Rprofile file is found in the startup directory, then R looks for a .Rprofile file in the user\u2019s home directory and uses that (if it exists) environment variable R_PROFILE_USER is set. The file it points to is used instead of the .Rprofile files.   Function named .First() in either of the two profile files or in the .RData image has a special status: initialize the environment  Sequence in which files are executed is:   Rprofile.site  the user profile  .RData  .First()  .Last(), if defined, is (normally) executed at the very end of the session.",
            "title": "Customizing startup"
        },
        {
            "location": "/An Introduction to R/#list-function-and-method",
            "text": "methods(class = \"data.frame\") ; list methods associated with the class.  methods(plot) ; list methods specific to the object.   methods(class = \"data.frame\")\n [1] $             $<-           [             [[            [[<-         \n [6] [<-           aggregate     anyDuplicated as.data.frame as.list      \n[11] as.matrix     by            cbind         coerce        dim          \n[16] dimnames      dimnames<-    droplevels    duplicated    edit         \n[21] format        formula       head          initialize    is.na        \n[26] Math          merge         na.exclude    na.omit       Ops          \n[31] plot          print         prompt        rbind         row.names    \n[36] row.names<-   rowsum        show          slotsFromS3   split        \n[41] split<-       stack         str           subset        summary      \n[46] Summary       t             tail          transform     unique       \n[51] unstack       within       \nsee '?methods' for accessing help and source code\n\nmethods(plot)\n [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*\n [4] plot.default        plot.dendrogram*    plot.density*      \n [7] plot.ecdf           plot.factor*        plot.formula*      \n[10] plot.function       plot.hclust*        plot.histogram*    \n[13] plot.HoltWinters*   plot.isoreg*        plot.lm*           \n[16] plot.medpolish*     plot.mlm*           plot.ppr*          \n[19] plot.prcomp*        plot.princomp*      plot.profile.nls*  \n[22] plot.R6*            plot.raster*        plot.spec*         \n[25] plot.stepfun        plot.stl*           plot.table*        \n[28] plot.ts             plot.tskernel*      plot.TukeyHSD*     \nsee '?methods' for accessing help and source code  Difference:   plot() ; a generic method.  plot.  ; a specific method such as  plot.ts()  for example.",
            "title": "List function and method"
        },
        {
            "location": "/An Introduction to R/#11-statistical-models-in-r",
            "text": "",
            "title": "11, Statistical models in R"
        },
        {
            "location": "/An Introduction to R/#regression",
            "text": "y <- 1:10\nx <- 1:10\n\na <- lm(y <sub>x)\na\nCall:\nlm(formula = y <sub>x)\n\nCoefficients:\n(Intercept)            x  \n  1.123e-15    1.000e+00  \n\n\n# no intercept, through the origin\nb <- lm(y <sub>0 + x)\nb\nCall:\nlm(formula = y <sub>0 + x)\n\nCoefficients:\nx  \n1  \n\n\n# no intercept\nc <- lm(y <sub>-1 + x)\nc\nCall:\nlm(formula = y <sub>-1 + x)\n\nCoefficients:\nx  \n1  \n\n\nd <- lm(y <sub>x - 1)\nd\nCall:\nlm(formula = y <sub>x - 1)\n\nCoefficients:\nx  \n1  \n\n\n# log\ne <- lm(log(y) <sub>x)\ne\nCall:\nlm(formula = log(y) <sub>x)\n\nCoefficients:\n(Intercept)            x  \n     0.2432       0.2304  \n\n\n# quadratic\nf <- lm(y <sub>1 + x + I(x^2))\nf\nCall:\nlm(formula = y <sub>1 + x + I(x^2))\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n  1.123e-15    1.000e+00    5.699e-18  \n\n\n# polynomial\ng <- lm(y <sub>X + poly(x, 2))\n\n\n# weighted regression\nfm1 <- lm(y <sub>x, data = dummy, weight = 1 / w^2)\n\n\n# and more\nnew.model <- update(old.model, new.formula)\n\nfm05 <- lm(y <sub>x1 + x2 + x3 + x4 + x5, data = production)\n\nfm6 <- update(fm06, . <sub>. + x6)\nsmf6 <- update(fm6, sqrt(,) <sub>.)  Explore the results   summary(regression results) .  vcov() ; variance-covariance matrix.  aov(formula, data = data.frame) .  anova(fitted.model.1, fitted.model.2, ...)  and many more.   anova, coeficient, coef, deviance, residuals, effects, formula, model, kappa, labels, plot, predict, proj, projection  Stepwise Regression  Select a suitable model by adding or dropping variables and preserving hierarchies. The best model with the smallest AIC (Akaike\u2019s Information Criterion) is discovered with the search.",
            "title": "Regression"
        },
        {
            "location": "/An Introduction to R/#generalized-least-squares",
            "text": "gls, binomial, logit, probit, log, cloglog, gaussian, identity, log, inverse, gamma, identity, inverse, log, inverse.gaussian, 1/mu^2, identity, inverse, log, poisson, identity, log, sqrt, quasi-likelihood, logit, probit  fitted.model <- glm(formula, famili=family.generator, data = data.frame)",
            "title": "Generalized least squares"
        },
        {
            "location": "/An Introduction to R/#nonlinear-least-squares-mle-mixed-local-robust-additive-tree-based",
            "text": "nls   nlm(function)   Maximum likehood (MLE)  ml  When errors are not normal.   ml(function)   Mixed model   nlme  package.   Local Approximating Regression  Nonparametric local regression function.   lrf <- lowess(x, y) .   Robust regression  MASS  package.  Additive model   acepack  package.  mda  package.  gam  package.  mgcv  package.   Tree-based model  decision, classification tree, random forest<\\sub>   rpart  package.  tree  package.",
            "title": "Nonlinear least squares (MLE, Mixed, Local, Robust, Additive, Tree-based)"
        },
        {
            "location": "/An Introduction to R/#12-graphical-procedures",
            "text": "",
            "title": "12, Graphical Procedures"
        },
        {
            "location": "/An Introduction to R/#graphic-packages",
            "text": "lattice  package.  ggplot2  package.  grid  package.  ggobi ,  rgl  packages; for interactive graphics, 3D, and surfaces.  and many more.",
            "title": "Graphic, packages"
        },
        {
            "location": "/An Introduction to R/#basic-plot",
            "text": "plot() .  boxplot() ..  lines(x,y) ; add a line to a basic plot.  pairs() ; multivariate, pairwise scatterplot matrix.   coplot(a <sub>b | c) ; scatter plot of a  b given c, a factor vector (levels)  coplot(a <sub>v | c + d) .  pie() .  hist(x) ; where  nclass = n  and  breaks = b .  barplot() ; can be horizontal or vertical.  dotchart(x, ...) ; a case of bar chart.  and many more with lots of options.",
            "title": "Basic plot"
        },
        {
            "location": "/An Introduction to R/#q-q-plot",
            "text": "qqnorm(x) .  qqline(x) .  qqplot(x, y) ; comparison.",
            "title": "Q-Q plot"
        },
        {
            "location": "/An Introduction to R/#pictures",
            "text": "image(x, y, z, ...) ; grid of rectangles with colors corresponding to the values in z.  contour(x, y, z, ...) ; z add contour lines (even to an existing plot).  persp(x, y, z, ...) ; perspective plots of a surface over the x\u2013y plane.",
            "title": "Pictures"
        },
        {
            "location": "/An Introduction to R/#graphic-arguments-and-parameters",
            "text": "x1 <- rnorm(1000, 0.4, 0.8)\nx2 <- rnorm(1000, 0.0, 1.0)\nx3 <- rnorm(1000, -1.0, 1.0)\nhist(x1, width = 0.33, offset = 0.00, col = \"blue\", xlim = c(-4,4),\n     main = \"Histogram of x1, x2 & x3\",\n     xlab = \"x1 - blue, x2 - red, x3 - green\")\nhist(x2, width = 0.33, offset = 0.33, col = \"red\", add = TRUE)\nhist(x3, width = 0.33, offset = 0.66, col = \"green\", add = TRUE)   add = TRUE ; superimpose a plot on another plot.  axes = FALSE ; no axes.  axis(side,...) ; 1 to 4, bottom, left, top, right.  log = \"x\", \"y\", \"xy\" ; difference scale.  type = \"p\"  (points);  \"l\"  (lines),  \"b\"  (p+l),  \"o\"  (l+p),  \"h\"  (vertical lines from points to the zero axis),  \"s\"  (step-function),  \"n\"  (not plotting).  xlab = \"bla\" .  ylab = \"bla\" .  main = \"bla\" .  sub = \"bla\" .  title(main, sub) .  points(x, y) ; add points on top of  plot() .  text(x, y, labels,...) ; add text to each point.  plot(x, t, type = \"n\"); text(x, y, names) ; replace the dot with text.  abline(a, b) ; add a line.  abline(0, 1, lty = 3) .  abline(coef(fm)) .  abline(coef(fm1), col = \"red\") .  abline(h = y) ; or  h = value ; add a horizontal line.  abline(v = x) ; or  v = value ; add a vertical line.  abline(lm.obj) .  legend(x, y, legend, ...) ; add a legend at a specified position.  fill = v ; add a vector of the same length as a legend.  lty = 2 ; line style.  lwd = 1.5 ; line width.  pch = 0 ; dot style.  par() ; list of permanent graphic parameters.  par(c(\"col\", \"lty\")) ; limit the list of parameters.  par(col = 4, lty = 2) ; set the parameters for all plots.  plot(x, y, pch = \"+\") ; will set a temporary parameter inside a plot.  pch = \"+\" ,  pch = 4 .  col = \"red\" ; dot color.  col =  \"red\" .  col.axis = \"red\" .  col.lab = \"red\" .  col.main = \"red\" .  col.sub = \"red\" .  font = \"red\" .  font.axis = \"red\" .  font.lab = \"red\" .  font.main = \"red\" .  font.sub = \"red\" .  adj = -0.1 ; adjust the text to the plotting position (-1\u2026-0.5\u20260\u20260.5\u20261), from left to right, 0 being the center.  cex = 1.5 ; character 50% larger.  cex.axis = 1.5 .  cex.lab = 1.5 .  cex.main = 1.5 .  cex.sub = 1.5 .  lab = c(5, 7, 12) ; the first two numbers are the desired number of tick intervals on the x and y axes respectively. The third number is the desired length of axis labels.  las = 1 ; orientation of axis labels (text and numbers). 0 means always parallel to axis, 1 means always horizontal, and 2 means always perpendicular to the axis.  mgp = c(3, 1, 0) ; position of the axis components.  tck = 0.01 ; length of tick marks.  xaxs = \"r\" ; axis style.  xaxs = \"i\" ; inside.  mai = c(1, 0.5, 0.5, 0) ; margins in inches around the plot.  mar = c(4, 2, 2, 1) ; in text lines.  and many more.   R allows you to create an n by m array of plots on a single page:   mfcol = c(3.2) ; 3 rows, 2 columns, 6 plotting areas.  mfrow = c(3,2) ; idem but filled by rows.  omi = c(1, 0.5, 0.5, 0) ; margins between plots.  oma = c(1, 0.5, 0.5, 0) ; margins outside plots.  mfg = c(2, 2, 3, 2) ; position of the current figure in a multiple figure environment. The first two numbers are the row and column of the current figure; the last two are the total number of rows and columns in the multiple figure array; in other words, 2nd row and 2nd column in a 3x2 panel.  fig = c(4, 9, 1, 4) / 10 ; position of the current figure on the page. Values are the positions of the left, right, bottom and top edges, respectively, as a percentage of the page measured from the bottom left corner.  and many more.",
            "title": "Graphic arguments and parameters"
        },
        {
            "location": "/An Introduction to R/#geometric-shapes",
            "text": "polygon(x, y, ...) ; x, y are vectors containing the coordinates of the vertices of the polygon.   Graphic help   help(plotmath) .  example(plotmath) .  demo(plotmath) .  help(Hershey) .  demo(Hershey) .  help(Japanese) .  demo(Japanese) .",
            "title": "Geometric shapes"
        },
        {
            "location": "/An Introduction to R/#graphic-text-and-mouse",
            "text": "Leave graphic marks and texts.   locator(n, type) ; select with mouse a max of n locations to mark on the graph (left click, right click to stop).  text(locator(1), \"Outlier\", adj=0) ; select with mouse a location to add a string on the graph.  identify(x, y) ; add a label to a dot with mouse.  identify(x, y, labels) .  identify(x, y, \"yes\") .",
            "title": "Graphic text and mouse"
        },
        {
            "location": "/An Introduction to R/#graphic-device",
            "text": "split.screen() ; FALSE or number of regions within the current device which can, to some extent, be treated as separate graphics devices. It is useful for generating multiple plots on a single device.  layout() ; divides the device up into as many rows and columns as there are in matrix mat.   Graphic device driver  Open a special graphics window.   X11() ; under UNIX.  windows() ; under Windows.  win.printer() .  win.metafile() .    quartz() ; under OS X.  dev.new() ; returns the return value of the device opened, usually invisible NULL.  grid()  adds an rectangular grid to an existing plot.  postscript() ; starts the graphics device driver for producing PostScript graphics.  postscript(\"file.ps\", horizontal=FALSE, height=5, pointsize=10) .  postscript(\"plot1.eps\", horizontal=FALSE, onefile=FALSE, height=8, width=6, pointsize=10) .    pdf() ; starts the graphics device driver for producing PDF graphics.  png() ; idem.  jpeg() ; idem.  tiff() ; idem.  bitmap() ; idem.  dev.off() ; shuts down the specified (by default the current) device.  dev.set() ; dev.set makes the specified device the active device.  dev.list() ; returns the numbers of all open devices.  dev.next() ,  dev.prev() ; return the number and name of the next / previous device in the list of devices.  graphics.off(); terminate all devices.",
            "title": "Graphic device"
        },
        {
            "location": "/An Introduction to R/#13-packages",
            "text": "install.package() ; on the computer.  library() ; load it.  search() ; check what is loaded.  loadedNamespaces() ; idem.  help.start() ; start the HTML help system, package section.   Find out about packages:   CRAN.R-project.org  www.bioconductor.org  www.omegahat.org",
            "title": "13, Packages"
        },
        {
            "location": "/An Introduction to R/#14-os-facilities",
            "text": "Manage files with Linux or Windows or RStudio.",
            "title": "14, OS Facilities"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/",
            "text": "CONTENT\n\n\nA, Presentation\n\n\nGUI\n\n\n\n\n\n\nB, Datasets\n\n\nPart 1, Basics\n\n\nChapter 1, Basic Concept, Organizing Data\n\n\nEditors\n\n\nData entry\n\n\nExponent\n\n\nLogarithm\n\n\nFactorial\n\n\nFind out about an object, a variable (is.)\n\n\nConvert (as.)\n\n\nArray and matrix\n\n\nVector\n\n\nSequence\n\n\nList\n\n\nData Frame\n\n\nTime series\n\n\nClass and mode\n\n\n\n\n\n\nChapter 2, Import-Export and Producing Data\n\n\nInput data from files\n\n\nOuput data and export\n\n\nMeasure computation time\n\n\nProduce by repetition\n\n\nProduce random numbers\n\n\nProduce data by manual input (vector-like)\n\n\nProduce data with a mini-spreadsheet (tabular-like)\n\n\nList of objects\n\n\nRead from and write to a database\n\n\nFile management\n\n\nRead from the clipboard\n\n\n\n\n\n\nChapter 3, Data Manipulation\n\n\nArithmetics\n\n\nBuilt-in functions\n\n\nDimension functions\n\n\nName functions\n\n\nMerge functions\n\n\napply functions and family\n\n\nSweep functions\n\n\nStack functions\n\n\nAggregation functions\n\n\nBoolean & logical functions\n\n\nVenn functions\n\n\nVector functions\n\n\nSearch functions\n\n\nReplace functions\n\n\nExtend a vector\n\n\nMatrix and array\n\n\nLists\n\n\nString\n\n\nText\n\n\nDate and time\n\n\nCustom functions (two examples)\n\n\nLoops structure\n\n\nBinary and decimal\n\n\n\n\n\n\nChapter 4, R Documentation\n\n\nHelp\n\n\nResources\n\n\n\n\n\n\nChapter 5, Techniques for Plotting Graphics\n\n\nGraphic windows\n\n\nMultiple windows\n\n\nDraw on a graphic\n\n\nRandom numbers\n\n\nColors\n\n\nImages\n\n\nWrite and add marks on a graphic\n\n\nGraphic parameters and graphic windows\n\n\nAdvanced graphic package\n\n\n\n\n\n\nChapter 6, Initiation to R Programming\n\n\nUnion\n\n\nClass\n\n\nMethods\n\n\nCombine and permute\n\n\nMore power, speed\n\n\nInvolve the graphical card for more power\n\n\n\n\n\n\nChapter 7, Session Management\n\n\nEnvironment\n\n\nFile manipulation\n\n\nMemory management\n\n\nCreate a package\n\n\n\n\n\n\nPart 2, Mathematics and Basic Statistics\n\n\nChapter 8, Basic Mathematics, matrix algebra, integration, optimization\n\n\nMath functions\n\n\nMatrix calculation\n\n\nIntegral calculus\n\n\nOptimisation\n\n\n\n\n\n\nChapter 9, Descriptive Statistics\n\n\nFactor, levels, labels\n\n\nNames\n\n\nOrder\n\n\nConvert (as.)\n\n\nTable, proportion table\n\n\nDescriptive statistics\n\n\nGraphic descriptive statistics\n\n\n\n\n\n\nChapter 10, Random Variables, Laws, and Simulation\n\n\nRandomness\n\n\n\n\n\n\nChapter 11, Confidence Intervals and Hypothesis Testing\n\n\nConfidence intervals\n\n\nTest\n\n\n\n\n\n\nChapter 12, Simple and Multiple Linear Regression\n\n\nRegression\n\n\nNormality\n\n\nCorrelation\n\n\nModel improvement\n\n\nPolynomial regression\n\n\n\n\n\n\nChapter 13, Elementary Variance Analysis\n\n\nAppendix, Installing the R Software and Packages\n\n\nAnswers to the Exercises\n\n\n\n\n\n\n\n\nForeword\n\n\nNotes, leads, and ideas on what R can do. \nREF: reference(s) to the book.\n From Springer, 2014.\n\n\n\n\nA, Presentation\n\u00b6\n\n\nGUI\n\u00b6\n\n\n\n\n\n\nRcommander; \nRcmdr\n pachage.\n\n\n\n\n\n\nThe R Commander: A Basic-Statistics GUI for R\n\n\n\n\n\n\nREF: p.3-5\n\n\nB, Datasets\n\u00b6\n\n\n\n\nDatasets.\n\n\n\n\nPart 1, Basics\n\u00b6\n\n\nChapter 1, Basic Concept, Organizing Data\n\u00b6\n\n\nEditors\n\u00b6\n\n\n\n\nRStudio.\n\n\nTinn-R.\n\n\nJGR.\n\n\nEmacs/ESS.\n\n\n\n\nData entry\n\u00b6\n\n\n\n\nx <- 2\n.\n\n\n2 -> 2\n.\n\n\n\n\nExponent\n\u00b6\n\n\n\n\nexp(1)\n.\n\n\n\n\nLogarithm\n\u00b6\n\n\n\n\nlog(3)\n.\n\n\nlog(x = 3)\n.\n\n\nlog(x = 3, base(exp(1))\n.\n\n\nlog(3, exp(1))\n.\n\n\n\n\nFactorial\n\u00b6\n\n\n\n\nfactorial(2)\n.\n\n\n\n\nFind out about an object, a variable (\nis.\n)\n\u00b6\n\n\n\n\nis.character()\n.\n\n\nis.vector()\n.\n\n\nis.character()\n.\n\n\nis.character()\n\n\nand many more.\n\n\n\n\nConvert (\nas.\n)\n\u00b6\n\n\n\n\nas.character()\n.\n\n\nas.raw()\n.\n\n\nas.date()\n.\n\n\nand many more.\n\n\n\n\nArray and matrix\n\u00b6\n\n\nmatrix(1:12, nrow = 4, ncol = 3, byrow = FALSE)\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\nmatrix(1:12, nrow = 4, ncol = 3, byrow = TRUE)\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n\narray(1:12, dim = c(2, 2, 3))\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 3\n\n     [,1] [,2]\n[1,]    9   11\n[2,]   10   12\n\n\n\n\nVector\n\u00b6\n\n\n\n\nvec <- c(1.1, 2.2, 3.5)\n.\n\n\nvec <- 1:3\n.\n\n\n\n\nSequence\n\u00b6\n\n\n\n\nseq(1:3)\n.\n\n\n1:3\n.\n\n\n\n\nList\n\u00b6\n\n\nc(1:3) # vector\n[1] 1 2 3\n\n# vs\n\nlist(1:3) # list\n[[1]]\n[1] 1 2 3\n\n\n\n\nData Frame\n\u00b6\n\n\ntable, tabular\n\n\n\n\ndata.frame(name = c(), name = c(), name = c(), etc)\n; each column is a vector with a name.\n\n\n\n\nTime series\n\u00b6\n\n\n> ts(1:10, frequency=4, start=c(1959,2))\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         1    2    3\n1960    4    5    6    7\n1961    8    9   10     \n\n\n\n\nClass and mode\n\u00b6\n\n\ntype, data, variable, object\n\n\n\n\nmode()\n.\n\n\nclass()\n; mode != class.\n\n\ntypeof()\n; type of storage.\n\n\n\n\nChapter 2, Import-Export and Producing Data\n\u00b6\n\n\nimport, export, i/o\n\n\nInput data from files\n\u00b6\n\n\nread.table(file = path/file.txt, header = TRUE, sep= \"\\t\", dec=\".\", row.names = 1)\n\n\n\n\n\n\nattach(data)\n; dataset is attached to .GlobalEnv.\n\n\nsearch()\n; search objects in .GlobalEnv, including attached dataset,\n\n\ndetach(data)\n.\n\n\n\n\nMore about .GlobalEnv in Chapter 7, Session Management.\n\n\nRead .csv and .tsv\n\n\n\n\nread.csv()\n.\n\n\nread.csv2()\n.\n\n\nread.delim()\n.\n\n\nread.delim2()\n.\n\n\nand many more.\n\n\n\n\nRead text files\n\n\n\n\nread.ftable(\"file.txt\", row.var.names = c(...), col.vars = list())\n.\n\n\nscan(\"file.txt\", skip = 9, nlines = 1, what = \"\", dec = \"\")\n.\n\n\nand many more.\n\n\n\n\nRead software files\n\n\n\n\n.sav\n; SPSS.\n\n\nread.spss\n.\n\n\n\n\n\n\n.mtp\n; Minitab.\n\n\nread.mtp\n.\n\n\n\n\n\n\n.xpt\n; SAS en data.frame.\n\n\nread.xport\n.\n\n\n\n\n\n\n.mat\n; Matlab.\n\n\nreadMat()\n.\n\n\n\n\n\n\nand many other formats and commands.\n\n\n\n\nOuput data and export\n\u00b6\n\n\n\n\nlookup.xport()\n; for SAS.\n\n\nwrite.table(data, file=\"file.txt\", sep=\"\\t\")\n.\n\n\nxlsReadWrite()\n.\n\n\nand many more.\n\n\n\n\nMeasure computation time\n\u00b6\n\n\n# start the timer\ntmps <- Sys.time()\n\n# run\ndbsnp <- read.table(\"file\")\n\n# stop the timer\nSys.time() - tmps\n\n\n\n\nProduce by repetition\n\u00b6\n\n\nrepeat\n\n\nrep(1:4, reach = 2, len = 10)\n[1] 1 2 3 4 1 2 3 4 1 2\n\n\n\n\nProduce random numbers\n\u00b6\n\n\n# generate random numbers between 0 and 1\nrunif(5)\n[1] 0.2424283 0.6140730 0.4824881 0.7263319 0.1381030\n\nrunif(5, min = 2, max = 7)\n[1] 4.588744 5.522278 4.307162 6.248397 3.854982\n\n\n\n\nMore on random number in Chapter 10, Random Variables, Laws, and Simulation.\n\n\nProduce random number following a distribution\n\n\n# generate random numbers following the normal distribution\nrnorm(5)\n[1]  0.8752170  1.3869022 -0.4419174 -0.6129075 -1.6987139\n\n\n\n\nGenerate numbers with other distributions.\n\n\nProduce random number by sampling a population\n\n\nurne <- 0:9\n\n# 20 draws from 'urne'\nsample(urne, 20, replace = TRUE)\n[1] 5 4 2 2 9 7 4 6 2 2 7 8 3 3 9 6 6 1 1 0\n\n\n\n\nProduce data by manual input (vector-like)\n\u00b6\n\n\n\n\nFirst,  \nz <- scan()\n.\n\n\nSecond, input data in the prompt.\n\n\n\n\nProduce data with a mini-spreadsheet (tabular-like)\n\u00b6\n\n\n\n\nx <- as.data.frame(de(\"\"))\n; open a spreadsheet.\n\n\ndata.entry(\"\")\n; alternatively.\n\n\nInput data; column are variables like in a data frame.\n\n\n\n\n\n\nfix(x)\n; invokes edit on x, then assigns the new (edited) version of x to the user\u2019s workspace.\n\n\n\n\nList of objects\n\u00b6\n\n\n\n\nls()\n; list of objects.\n\n\nrm(list = ls())\n; remove all the objects.\n\n\n\n\nRead from and write to a database\n\u00b6\n\n\n\n\nRODBC\n package.\n\n\nodbcConnect()\n.\n\n\nsqlQuery()\n.\n\n\nodbcClose()\n.\n\n\n\n\nREF: p.82-84\n\n\nFile management\n\u00b6\n\n\n\n\nfile.choose()\n; open a window.\n\n\n\n\nRead from the clipboard\n\u00b6\n\n\n\n\nFirst, copy from a spreadsheet or a table.\n\n\nSecond, \nread.clipboard()\n.\n\n\n\n\nChapter 3, Data Manipulation\n\u00b6\n\n\nArithmetics\n\u00b6\n\n\nx <- c(1,2,3)\ny <- c(4,5,6)\n\nx + y\n[1] 5 7 9\n\n\n\n\nBuilt-in functions\n\u00b6\n\n\n\n\nlength(vec)\n; length.\n\n\nsort(vec, decreasing = TRUE)\n; sort.\n\n\nrev(vec)\n; inverse sorting.\n\n\norder(vec)\n; sort a vector according to the names or a list of strings.\n\n\nnames(vec) <- 1:9\n; attribute names.\n\n\nrank(vec)\n; rank the elements.\n\n\nunique(vec)\n; remove doubles.\n\n\nduplicated(vec)\n; create a TRUE/FALSE vector indicating doubles.\n\n\nx %% y\n;  modulus (x mod y).\n\n\nx %/% y\n; integer division.\n\n\n\n\nDimension functions\n\u00b6\n\n\nnumber, row, column, dimension\n\n\n\n\ndim(df)\n.\n\n\nnrow(df)\n.\n\n\nncol(df)\n.\n\n\n\n\nName functions\n\u00b6\n\n\n\n\ndimnames(df)\n.\n\n\nnames(df)\n, \ncolnames(df)\n.\n\n\nrownames(df)\n.\n\n\n\n\nMerge functions\n\u00b6\n\n\ncombine\n\n\n\n\ncbind()\n.\n\n\nrbind()\n.\n\n\n\n\nREF: p.98\n\n\ny <- array(1:12, dim = c(4, 3))\n\ny\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\ny <- cbind(y, c(100, 101, 102, 103))\n\ny\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9  100\n[2,]    2    6   10  101\n[3,]    3    7   11  102\n[4,]    4    8   12  103\n\nmerge(x, y)\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n# idem with rows\n\n\n\n\nREF: p.96-98\n\n\n\n\ngtools\n package.\n\n\nsmartbind(x,y)\n; for two data frames, similar to merge.\n\n\n\n\napply\n functions and family\n\u00b6\n\n\nexcel, wrangle\n\n\nAmong the most useful function for \u2018wrangling\u2019 data. Excel-like power. When and how to use them.\n\n\n\n\napply()\n.\n\n\nlapply()\n.\n\n\nsapply()\n.\n\n\nmapply()\n.\n\n\nby()\n.\n\n\nwith()\n.\n\n\nreplicate()\n.\n\n\ntransform()\n.\n\n\nrowSums(df)\n.\n\n\ncolSums(df)\n.\n\n\nrowMeans(df)\n.\n\n\ncolMeans(df)\n.\n\n\nsweep()\n.\n\n\nstack()\n.\n\n\nunstack()\n.\n\n\naggregate()\n\n\n\n\nREF: p.99\n\n\nSweep functions\n\u00b6\n\n\nu\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n# removes pattern '3, 5, 3, 5, etc.'\nsweep(u, MARGIN = 1, STATS = c(3, 5), FUN = \"-\")\n  V1 V2 V3 V4\n1 -2  2  6 97\n2 -3  1  5 96\n3  0  4  8 99\n4 -1  3  7 98\n\n# divide by a vector\nsweep(u, MARGIN = 2, STATS = c(2, 2, 3, 3), FUN = \"/\")\n   V1  V2       V3       V4\n1 0.5 2.5 3.000000 33.33333\n2 1.0 3.0 3.333333 33.66667\n3 1.5 3.5 3.666667 34.00000\n4 2.0 4.0 4.000000 34.33333\n\n\n\n\nREF: p.100-101\n\n\nStack functions\n\u00b6\n\n\nstack, unstack\n\n\nu\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\nv<- stack(u)\nv\n   values ind\n1       1  V1\n2       2  V1\n3       3  V1\n4       4  V1\n5       5  V2\n6       6  V2\n7       7  V2\n8       8  V2\n9       9  V3\n10     10  V3\n11     11  V3\n12     12  V3\n13    100  V4\n14    101  V4\n15    102  V4\n16    103  V4\n\nw <- unstack(v)\nw\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n\n\n\nAggregation functions\n\u00b6\n\n\naggregate\n\n\nw\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\nfac <- c(\"a\", \"b\", \"b\", \"a\")\nx <- cbind(w, fac)\nx\n  V1 V2 V3  V4 fac\n1  1  5  9 100   a\n2  2  6 10 101   b\n3  3  7 11 102   b\n4  4  8 12 103   a\n\naggregate(w, by = list(x$fac), sum)\n  Group.1 V1 V2 V3  V4\n1       a  5 13 21 203\n2       b  5 13 21 203\n\n\n\n\nREF: p.101\n\n\nBoolean & logical functions\n\u00b6\n\n\n\n\nlogical(2)\n; generate two FALSE in a vector; change the length.\n\n\n!logical(2)\n; generate two TRUE.\n\n\nas.logical(vec)\n.\n\n\nis.logical(vec)\n.\n\n\nisTRUE()\n.\n\n\n&\n; AND.\n\n\n&&\n; sequential AND.\n\n\n|\n; OR.\n\n\n||\n; sequential OR.\n\n\nPrefer \n&&\n over \n&\n, and \n||\n over \n|\n. Assessments go from left to right, and keep on going as long as the conditions are TRUE.\n\n\nxor()\n; exclusive OR.\n\n\nif\n, \nelse\n.\n\n\nany()\n; if one or another is TRUE.\n\n\nall()\n; if all are TRUE.\n\n\nidentical()\n; if all are identical.\n\n\nall.equal()\n.\n\n\n==\n or \nall.equal\n (and \n!=\n) can yield a FALSE because decimal are different on large numbers.\n\n\nall.equal(x, y, tolerance = 10^-6)\n fixes the problem.\n\n\n\n\n\n\nifelse(cond, a, b)\n; if \ncond\n is TRUE, \na\n, else, \nb\n.\n\n\nx <- c(3:-2); sqrt(ifelse(x >= 0, x, NA)\n.\n\n\n\n\n\n\n\n\nREF: p.126-127\n\n\nVenn functions\n\u00b6\n\n\nA <- 1:3\nB <- 3:6\n\nis.element(1, A)\n[1] TRUE\nis.element(4, A)\n[1] FALSE\nis.element(4, B)\n[1] TRUE\n\nall(A %in% B)\n[1] FALSE\nall(B %in% A)\n[1] FALSE\n\nintersect(A, B)\n[1] 3\nunion(A, B)\n[1] 1 2 3 4 5 6\n\nsetdiff(A, B)\n[1] 1 2\nsetdiff(B, A)\n[1] 4 5 6\nintersect(A, B)\n[1] 3\n\n\n\n\nVector functions\n\u00b6\n\n\n\n\nvec[2]\n; extract.\n\n\nvec[2:5]\n; extract.\n\n\nvec[c(T, F, T)]\n; extraction with filter.\n\n\nvec[vec > 4]\n; conditional extraction.\n\n\nvec[vec == 3]\n.\n\n\nvec[which.max(z)]\n; extract the maximum value.\n\n\nvec[which.min(z)]\n.\n\n\nvec > 4\n; yield a vector of TRUE or FALSE.\n\n\nvec[-2]\n; exclude.\n\n\nvec[-c(1,5)]\n; exclude.\n\n\n\n\nSearch functions\n\u00b6\n\n\n\n\nmasque <- c(TRUE, FALSE)\n.\n\n\nwhich(masque)\n; return the TRUE indices.\n\n\nwhich.min(x)\n; return the index with minimum value.\n\n\nwhich.max(x)\n.\n\n\n\n\nReplace functions\n\u00b6\n\n\n\n\nz[c(1, 5)] <- 1\n; replace value 1 and 5 by 1.\n\n\nz[which.max(z)] <- 0\n; replace the maximum value.\n\n\nz[z == 0] <- 8\n; replace zeros and FALSE.\n\n\n\n\nExtend a vector\n\u00b6\n\n\n\n\nvecA\n.\n\n\nvecB <- c(vecA, 4, 5)\n.\n\n\nvecC <- c(vecA[1:4], 8, 5, vecA[5:9])\n.\n\n\n\n\nMatrix and array\n\u00b6\n\n\n\n\nmat[r, c]\n; extract.\n\n\nmat[1, 2]\n.\n\n\nmat[,2]\n; all rows, column 2 only.\n\n\nmat[1, ]\n; all columns, row 1 only.\n\n\nmat[c(1, 3), c(4:5)]\n.\n\n\nmat[, 1, drop = FALSE]\n; avoid making a (horizontal) row with a (vertical) column.\n\n\nmat[ind]\n; matrix index.\n\n\narray[r, c, m]\n; extract.\n\n\n\n\nREF: p.110-113\n\n\nLists\n\u00b6\n\n\nchar <- c(\"a\", \"b\", \"c\")\nnumb <- c(1, 2, 3)\ngreek <- c(\"alpha\", \"beta\", \"gamma\")\n\nx <- list(char, numb, greek)\nx\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n[[2]]\n[1] 1 2 3\n\n[[3]]\n[1] \"alpha\" \"beta\"  \"gamma\"\n\nnames(x) <- c(\"char\", \"numb\", \"greek\")\nx\n$char\n[1] \"a\" \"b\" \"c\"\n\n$numb\n[1] 1 2 3\n\n$greek\n[1] \"alpha\" \"beta\"  \"gamma\"\n\nx[2]\n$numb\n[1] 1 2 3\n\nx[[2]][2]\n[1] 2\n\nx$numb[2]\n[1] 2\n\n\n\n\nREF: p.113-115\n\n\nString\n\u00b6\n\n\n\"bla bla bla\"\n[1] \"bla bla bla\"\n\nnoquote(\"bla bla bla\")\n[1] bla bla bla\n\nsQuote(\"bla bla bla\")\n[1] \"\u2018bla bla bla\u2019\"\n\ndQuote(\"bla bla bla\")\n[1] \"\u201cbla bla bla\u201d\"\n\n\n\n\nText\n\u00b6\n\n\nwrangle, text, string, character, natural language processing, nlp\n\n\n\n\n\n\nformat()\n; and arguments:\n\n\n\n\ndigits\n.\n\n\ntrim\n.\n\n\ndigit\n.\n\n\nnsmall\n.\n\n\njustify\n.\n\n\nwidth\n.\n\n\nna.encode\n.\n\n\ndecimal.mark\n.\n\n\ndrop0trailing\n.\n\n\nand many more.\n\n\n\n\n\n\n\n\ncat(\"current working dir: \", wd)\n; print the objects, concatenate the representations.\n\n\n\n\nprintf(\"hello %d\\n\", 56)\n; mix text and data; pythonic print.\n\n\nprint(paste0(\"current working dir: \", wd))\n.\n\n\nnchar()\n; number of characters.\n\n\nx[nchar(x) > 2]\n.\n\n\nx[x %n% c(letters, LETTERS)]\n; retrieve letters, patterns or strings in a text object; alike Venn.\n\n\npaste(ch1, ch2, sep = \"-\")\n; concatenate.\n\n\npaste0(ch1, ch2)\n; concatenate.\n\n\nsubstring(\"abcdef\", first = 1:3, last = 2:4)\n; create subsets \nab\n, \nbc\n, \ncd\n.\n\n\nstrsplit(c(\"\",\"\"), split=\" \")\n; break down a string.\n\n\ngrep(\"i\", c())\n; extract an object index.\n\n\ngsub(\"i\", \"L\", c())\n; substitute.\n\n\nsub()\n; substitute the first occurrence.\n\n\ntolower()\n.\n\n\ntoupper()\n.\n\n\n\n\nDate and time\n\u00b6\n\n\nconvert, extract\n\n\n\n\nSys.time()\n.\n\n\ndate()\n.\n\n\nSys.setlocale()\n.\n\n\nas.numeric()\n.\n\n\nstrptime()\n; extract time.\n\n\nas.POSIXlt()\n.\n\n\n\n\nREF: p.120-123\n\n\nCustom functions (two examples)\n\u00b6\n\n\na <- 2\nb <- -3\n\n# quadratic\nf <- function (x) { x**2 - 2*x - 2 }\n\nf(a)\n[1] -2\n\nf(b)\n[1] 13\n\n# test\ng <- function (x, y) {\n  if (x <= y) {\n    z <- y - x\n    print(\"x smaller\")\n  } else {\n    z <- x - y\n    print(\"x larger\")\n  }\n} \n\ng(a, b)\n[1] \"x larger\"\n\ng(a, abs(b))\n[1] \"x smaller\"\n\n\n\n\nMore about custom functions in \nChapter 6, Initiation to R Programming\n.\n\n\nLoops structure\n\u00b6\n\n\n\n\nfor\n.\n\n\nwhile\n.\n\n\nrepeat\n.\n\n\n\n\n# while\nwhile(x + y < 7) { x <- x + y }\n\n# for\nfor (i in 1:4) {\n    if (i == 3) break\n    for (j in 6:8) {\n        if (j == 7) next\n        j <- i + j\n    }\n}\n\n# repeat\ni <- 0\nrepeat {\n    i <- i + 1\n    if (i == 4) break\n}\n\n\n\n\nLoop example\n\n\nIMC <- function (poids, taille) {\n  imc <- poids / taille^2\n  names(imc) <- \"IMC\"\n  return(imc)\n}\n\nIMC(100, 1.90)\n     IMC \n27.70083 \n\np <- c(100, 101, 95, 97)\nt <- c(1.90, 1.75, 1.68, 1.92)\n\nIMC(p, t)\n     IMC     <NA>     <NA>     <NA> \n27.70083 32.97959 33.65930 26.31293 \n\nfor (i in 1:4) {\n  print(IMC(data[i,1], data[i,2]))\n}\n     IMC \n27.70083 \n     IMC \n32.97959 \n    IMC \n33.6593 \n     IMC \n26.31293 \n\n\n\n\nLoops increase computation time\n\n\nsystem.time(for (i in 1:1000000) sqrt(i))\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       0.19        0.01        0.22 \n\nsystem.time(sqrt(1:1000000))\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       0.07        0.00        0.07 \n\n\n\n\nREF: p.133-136\n\n\nBinary and decimal\n\u00b6\n\n\nconvert\n\n\n\n\nbin2dec()\n.\n\n\ndec2bin()\n.\n\n\n\n\nREF: p.136-144, 147-152\n\n\nChapter 4, R Documentation\n\u00b6\n\n\nHelp\n\u00b6\n\n\n\n\n?functionname\n.\n\n\nhelp(functionname)\n.\n\n\nhelp.start()\n; user manual in the browser.\n\n\napropos('mean')\n; object with the name \u2018mean\u2019.\n\n\n\n\nLibrary\n\n\n\n\ninstall.packages('package')\n; should be done as an administrator or superuser directly in R, not an editor. Once the package is installed, it can be loaded on the PC by any user in R or an editor (such as RStudio).\n\n\nlibrary(package = 'package')\n; load a package.\n\n\nlibrary(help = package)\n; help on a specific package.\n\n\nlibrary(help = base)\n; for example.\n\n\nlibrary(help = utils)\n; for example.\n\n\nlibrary(help = datasets)\n; for example.\n\n\nlibrary(help = graphics)\n; for example.\n\n\nlibrary(help = grDevices)\n; for example.\n\n\n\n\n\n\nlibrary(lib.loc = .Library)\n; find which package (system packages, but not user packages).\n\n\nfind('function')\n; find which package (system).\n\n\ndata()\n; datasets available.\n\n\ndemo()\n; available demos by package.\n\n\ndemo(graphics)\n; give examples.\n\n\nexample(mean)\n; give examples.\n\n\n\n\nResources\n\u00b6\n\n\n\n\nnmz\n; search the content of the R functions, package vignette, and task views.\n\n\nR-Project Mailing Lists\n; mailing lists.\n\n\nRSeek\n; search engine.\n\n\nStackOverflow\n; forum.\n\n\nAbcd\u2019R\n; scripts.\n\n\nCIRAD\n; forum.\n\n\nDeveloppez.net\n; forum.\n\n\nETHZ\n; mailing list.\n\n\nETHZ Manual\n.\n\n\nhttp://a\n; manuals and resources.\n\nstat.ethz.ch/mailman/listinfo/r-annonce\n\n\nETHZ List Info\n.\n\n\nR Programming Wikibooks\n; wiki.\n\n\nCRAN-R\n.\n\n\nCRAN-R Doc\n; PDF and documentation.\n\n\nCRAN-R Views\n; projects.\n\n\nR Journal\n; publication.\n\n\nJournal of Statistical Software\n; publication.\n\n\n\n\nChapter 5, Techniques for Plotting Graphics\n\u00b6\n\n\nGraphic windows\n\u00b6\n\n\nOf course, these commands are automated in an editor (RStudio).\n\n\n\n\ndev.new()\n; graphic window on Windows.\n\n\nwindows()\n; graphic window on Windows.\n\n\nwin.graph()\n; graphic window on Windows.\n\n\nX11()\n; graphic window on Linux.\n\n\nAdd parameters in the command:\n\n\nwidth = , height =\n.\n\n\npointsize =\n.\n\n\nxpinch = , ypinch =\n; pixels per inch.\n\n\nxpos = , ypos =\n; position of the upper left corner, in pixel.\n\n\n\n\n\n\ndev.set(num)\n; activate window number \u2018num\u2019; there can be several graphic windows.\n\n\ndev.off(num)\n; close a window.\n\n\ngraphics.off()\n; close all windows.\n\n\ndev.list()\n; return window numbers.\n\n\ndev.cur()\n; return the current number, active window.\n\n\ndev.print(png, file = , width = , height = )\n; print.\n\n\nsavePlot(filename = , type = \"png\")\n; save.\n\n\npng(file=\" \", width = , height = )\n; save directly in .png.\n\n\njpeg()\n.\n\n\nbitmap()\n.\n\n\npostscript()\n.\n\n\npdf()\n.\n\n\n\n\n# examples\n\ndev.new(width = 200, height = 200, xpos = 100, ypos = 100)\nNULL\n\ndev.list()\nRStudioGD       png   windows \n        2         3         4 \n\ndev.cur()\nwindows \n      4\n\nhist(runif(100)) # create a graphic\n\n# in the current working directory\ndev.print(png, file = \"mygraph.png\", width = 480, height = 480)\nsavePlot(filename = \"mygraph.png\", type = \"png\")\npdf(file = \"mygraph.pdf\")\n\ndev.off()\n\n\n\n\nMultiple windows\n\u00b6\n\n\n\n\npar(mfrow = c(3, 2))\n; create 6 graphic boxes, 3 rows X 2 columns.\n\n\n\n\nREF: p.166-167\n\n\ndev.new()\nmat <- matrix(c(2, 3, 0, 1, 0, 0, 0, 4, 0, 0, 0, 5), 4, 3, byrow = TRUE)\nlayout(mat)\n\n\ndev.new()\nlayout(mat, widths = c(1, 5, 14), heights = c(1,2,4,1))\nlayout.show(5)\n\n\n\n\nREF: p.168\n\n\nDraw on a graphic\n\u00b6\n\n\n\n\nsegments(x0 = 0, y0 = 0, x1 = 1, y1 = 1)\n; draw lines on a plot.\n\n\nlines(x = c(1,0), y = c(0,1))\n.\n\n\nabline(h = 0, v = 0)\n; add a line to a plot.\n\n\nabline(a = 1, b = 1)\n.\n\n\n\n\nRandom numbers\n\u00b6\n\n\nx <- runif(12)\nx\n[1] 0.03344539 0.22711659 0.66696650 0.02840671 0.61067995 0.92957527\n[7] 0.26962190 0.60013387 0.04831111 0.12603905 0.41913598 0.13142315\n\n# ranking\ni <- order(x)\ni\n[1]  4  1  9 10 12  2  7 11  8  5  3  6\n\n# reorder\nx <- x[i]\nx\n[1] 0.02840671 0.03344539 0.04831111 0.12603905 0.13142315 0.22711659\n[7] 0.26962190 0.41913598 0.60013387 0.61067995 0.66696650 0.92957527\n\n\n\n\nMore on random number in Chapter 10, Random Variables, Laws, and Simulation.\n\n\nExamples\n\n\n\n\nexample(polygon)\n; see examples.\n\n\ncurve(x**3 - 3*x, from = -2, to = 2)\n; trace a curve according to a function.\n\n\nhist(rnorm(10000), prob = TRUE, breaks = 100)\n.\n\n\nplot(runif(7), type = \"h\", axes = F); box(lty = \"1373\")\n\n\nplot(1:10, runif(10), type = \"l\", col = \"orangered\")\n\n\n\n\nHelp\n\n\n\n\ncolors()[grep(\"orange\", colors())]\n; list of tones.\n\n\n\n\nColors\n\u00b6\n\n\n\n\nrgb(red = 26, green = 204, blue = 76, maxColorValue = 255)\n; return the code.\n\n\nrgb(red = 0.1, green = 0.8, blue = 0.3)\n.\n\n\ncol2rgb(\"#1AVV4C\")\n; inversely.\n\n\nR generates 256\u00b3 colors, 15M.\n\n\nrainbow(#)\n; show 1, 8, 17 or more colors; R can generate 256\u00b3 colors (over 15M),\n\n\npie(rep(1,200), labels = \"\", col = rainbow(200), border = NA)\n; show the colors.\n\n\nplot(1:40, col = rainbow(n), pch = 19, cex = 2),     grid(col = \"grey50\")\n; show.\n\n\nRColorBrewer\n package.\n\n\n\n\nlibrary('RColorBrewer')\ndisplay.brewer.all()\n\n\n\n\nImages\n\u00b6\n\n\n\n\ncaTools\n package to manage images.\n\n\nread.gif()\n.\n\n\nimage()\n.\n\n\n\n\nWrite and add marks on a graphic\n\u00b6\n\n\n\n\ntext(coord x, coord y, 'text')\n; write.\n\n\ndemo(plotmath)\n; see examples.\n\n\nmtext('bas', side = 1)\n; write \u2018bas\u2019 on the graphic box; 4 sides of the box (bottom, left, top, right).\n\n\nlocator()\n; point with the mouse on a graphic to record coordinates; \nesc\n to show the coordinates.\n\nplot(1,1); locate area to be annotated with \ntext()\n.\n\n\ntext(locator(1), label=\"ici\")\n; add a label, one or several occurrences, by pointing the location with the mouse.\n\n\nidentify(occurrence, label)\n; add one or more labels by pointing the location with the mouse. \n\n\n\n\nGraphic parameters and graphic windows\n\u00b6\n\n\nmargin, border, box, square, space, height, width, color, text, title, label, mark, font, police, character, language, size, length, size, tick, coordinate, x, y, log, scale, axis, axes, format, alignment, point\n\n\nAdvanced graphic package\n\u00b6\n\n\n\n\nrgl\n.\n\n\nlattice\n.\n\n\nggplot2\n.\n\n\n\n\nREF: p.190-201\n\n\nChapter 6, Initiation to R Programming\n\u00b6\n\n\nfunction (<parameters>) {\n    <body>\n}\n\n\n\n\nlance <- function (nom) {\n  cat(\"Bonjour\", nom, \"!\")\n}\n\nlance('allo')\nBonjour Alex !\n\nbonjour <- function (nom=\"Pierre\", langue=\"fr\") {\n  cat(switch(langue, fr=\"Bonjour\", esp=\"Hola\", ang=\"Hi\"), nom, \"!\")\n}\n\nbonjour()\nBonjour Pierre !\nbonjour(nom=\"Ben\") # replace the default value.\nBonjour Ben !\nbonjour(langue=\"ang\")\nHi Pierre !\nbonjour(lang = \"ang\") # partial call\nHi Pierre !\nbonjour(l=\"ang\")\nHi Pierre !\nbonjour(l=\"a\")\n Pierre !\n\n\n\n\nREF: p.218-219\n\n\nUnion\n\u00b6\n\n\n\"%union%\" <- function (A,B) { union(A,B) }\nA <- c(4,5,2,7)\nB <- c(2,1,7,3)\n\nA %union% B\n[1] 4 5 2 7 1 3\n\n\n\n\nClass\n\u00b6\n\n\nobj <- 1:10\n\nclass(obj)\n[1] \"integer\"\n\nclass(obj) <- \"TheClass\"\n\nclass(obj)\n[1] \"TheClass\"\n\ninherits(obj, \"TheClass\")\n[1] TRUE\n\n\n\n\nMethods\n\u00b6\n\n\nx <- 1:10\n\nprint.default(x)\n[1] 1 2 3 4 5 6 7 8 9 10\n\n\n\n\nREF: p.227-231\n\n\nCombine and permute\n\u00b6\n\n\n\n\ncombinat\n package.\n\n\ncombn(5,3)\n; combine 3 numbers from 1:5.\n\n\nchoose(200,3)\n; choose 3 numbers from 1:200.\n\n\npermn(n,m)\n.\n\n\n\n\nVery time consuming!\n\n\nMore power, speed\n\u00b6\n\n\n\n\nThe core of R in programmed in C. Converting a R function into C is easy. C is faster.\n\n\nCall it through API C.\n\n\nWith R graphic interface and C computation speed, it is the best of both world.\n\n\nEasier to set up on Linux or OS X (the OS has default compilers. Use the \nRcpp\n package. \n\n\nOn Windows, there is a need for \nRtools\n. \n\n\nR can be compiled (byte compiler) with the \nRevoScaleR\n package (parallel computing).\n\n\nbigmemory\n, \nff\n, packages; split a matrix into sub-matrices, perform computation and combine the results (parallel computing). \n\n\nUse a multi-core architecture with the \nparallel\n package.\n\n\nHighPerformanceComputing list of packages\n.\n\n\n\n\nTry parallel computing with a Monte Carlo with the \nparallel\n package\n\n\nmyfunc <- function(M=1000) {\n    decision <- 0\n    for (i in 1:M) {\n      x <- rnorm(100)\n      if (shapiro.test(x)$p < 0.05) decision <- decision +1\n    }\nreturn(decision)\n}\n\nsystem.time({\n    M <- 60000\n    decision <- myfunc(M)\n    print(decision/M)\n})\n\n\n\n\n\n\nFor a parallel execution.\n\n\nStart menu.\n\n\nType devmgmt.msc\n\n\nUnder Processors in Linux, type \ntop\n in the terminal.\n\n\nThen, type \n1\n in R.\n\n\nEnter \ndetectCores()\n from the \nparallel\n package.\n\n\n\n\nrequire(\"parallel\")\nsystem.time({\n    nbcores <- 6 # less than detectCores() - 1\n    M <- 60000\n    cl <- makeCluster(nbcores, type = \"PSOCK\")\n    out <- clusterCall(cl, myfunc, round(M/nbcores))\n    stopCluster(cl)\n    decision <- 0\n    for (clus in 1:nbcores) {\n        decision <- decision + out[[clus]]\n    }\n    print(decision/(round(M/nbcores)*nbcores))\n})\n\n\n\n\n\n\nThe process number (PID) of each computation node (core) in the cluster.\n\n\n\n\nrequire(\"parallel\")\nSys.getpid()\ncl <- makeCluster(4,type=\"PSOCK\")\nout <- clusterCall(cl, Sys.getpid))\n\n\n\n\nInvolve the graphical card for more power\n\u00b6\n\n\n\n\nRun computations with the graphical card, the GPU.\n\n\ngputools\n package.\n\n\n\n\n\n\n\n\nREF: p.303-308\n\n\nChapter 7, Session Management\n\u00b6\n\n\nwork, session, save, object, instruction, graphic, create, package\n\n\nEnvironment\n\u00b6\n\n\n\n\nglobalenv()\n; .GlobalEnv.\n\n\nnew.env()\n; new environment with its own functions, variable, etc.\n\n\nls()\n; list of objects in the environment.\n\n\nobjects()\n; idem.\n\n\nrm()\n; remove one or more objects.\n\n\nrm(list = ls())\n; remove all.\n\n\n.RData; workspace file extension.\n\n\nfile.RData; R file.\n\n\nsave.image(\"file.RData\")\n; save a R workspace to the current working directory. Several workspace can have their own objects.\n\n\nload(\"file.RData\")\n; load a R workspace from the current working directory.\n\n\nload(file.choose())\n; open the current working directory.\n\n\ngetwd()\n; get the current working directory.\n\n\nsetwd()\n; set the current working directory.\n\n\n.Rhistory; history file extension.\n\n\nhistory()\n; consult the R log.\n\n\nsavehistory()\n; save the log to a file.\n\n\nloadhistory()\n; load the log from a file.\n\n\nsearch()\n; list of attached packages.\n\n\nsearchpaths()\n;  list of paths.\n\n\nlibrary()\n; list of packages in memory.\n\n\nrequire(\"packages\")\n, \nlibrary(\"packages\")\n; load a package.\n\n\nattach(data)\n; attach a dataset to .GlobalEnv..\n\n\ndetach(data)\n.\n\n\nls(pos = 1)\n, \nls()\n; object list in database 1.\n\n\nls(pos = 2)\n; object list in database 2.\n\n\nls(pos = match(\"package:datasets\", search()))\n; list datasets.\n\n\nls(data)\n; list object related to the dataset.\n\n\nfix(data)\n; open a spreadsheet with the data.\n\n\nsink(file = \"sortie.txt\")\n; save a file on the current working directory.\n\n\nsink()\n; stop the recording.\n\n\n\n\nFile manipulation\n\u00b6\n\n\n\n\nLow-level interface to the computer\u2019s file system.\n\n\nCreate a file.\n\n\nExecute the command.\n\n\nCheck out the result in the text files themselves.\n\n\nfile.create(\"sorty.txt\", showWarnings = TRUE)\n.\n\n\nfile.exists(\"sorty.txt\")\n.\n\n\nfile.remove(\"sorty.txt\")\n.\n\n\nfile.rename(\"sorty.txt\", \"sorti.txt\")\n.\n\n\nfile.append(\"sorty.txt\", \"sorti2.txt\")\n.\n\n\nfile.copy(\"sorty.txt\", \"sorti2.txt\")\n.\n\n\nfile.symlink(\"sorty.txt\", \"sorti2.txt\")\n.\n\n\nfile.path(\"sorty.txt\")\n.\n\n\nfile.show()\n.\n\n\nlist.files()\n.\n\n\nunlink()\n.\n\n\nbasename()\n.\n\n\npath.expand()\n.\n\n\nlist.files()\n.\n\n\nfile.exists()\n.\n\n\nmemory.size()\n.\n\n\nmemory.limit()\n.\n\n\n\n\nREF: p.320-330\n\n\nMemory management\n\u00b6\n\n\n\n\nThe KSysGuard software analyzes memory usage in real time. It is a task manager and performance monitor for UNIX-like OS.\n\n\n\n\nCreate a package\n\u00b6\n\n\n\n\nHow to: build a list of R instruction, load a dataset, create function, and output the results.\n\n\nRun a series of scripts in batch, run a script at a distance.\n\n\nSet the PATH to an executable Rgui.exe.\n\n\nOpen a R script without opening R.\n\n\nCreate a runthis script, apply \nchmod\n to use and execute it. The bash script launch R commands (it must begin with #!/in/bash).\n\n\nand much more.\n\n\n\n\nREF: p.332-338\n\n\nPart 2, Mathematics and Basic Statistics\n\u00b6\n\n\nChapter 8, Basic Mathematics, matrix algebra, integration, optimization\n\u00b6\n\n\nMath functions\n\u00b6\n\n\nREF: p.342\n\n\nMatrix calculation\n\u00b6\n\n\nA <- matrix(c(2,3,5,4), nrow = 2, ncol = 2)\nB <- matrix(c(1,2,2,7), nrow = 2, ncol = 2)\n\nA\n     [,1] [,2]\n[1,]    2    5\n[2,]    3    4\n\nB\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    7\n\nA + B\n     [,1] [,2]\n[1,]    3    7\n[2,]    5   11\n\nA - B\n     [,1] [,2]\n[1,]    1    3\n[2,]    1   -3\n\n# scalar multiplication\nA * B\n     [,1] [,2]\n[1,]    2   10\n[2,]    6   28\n\n# matrix multiplication\nA %*% B\n     [,1] [,2]\n[1,]   12   39\n[2,]   11   34\n\n# scalar multiplication\na <- 10\na * A\n     [,1] [,2]\n[1,]   20   50\n[2,]   30   40\n\n# transpose\nt(A)\n     [,1] [,2]\n[1,]    2    3\n[2,]    5    4\n\n# inverse\nsolve(B)\n           [,1]       [,2]\n[1,]  2.3333333 -0.6666667\n[2,] -0.6666667  0.3333333\n\nsolve(A) %*% B\n           [,1]      [,2]\n[1,]  0.8571429  3.857143\n[2,] -0.1428571 -1.142857\n\nt(A) * B\n     [,1] [,2]\n[1,]    2    6\n[2,]   10   28\n\nx <- seq(1,4)\ny <- seq(4,7)\n\nx\n[1] 1 2 3 4\ny\n[1] 4 5 6 7\n\nouter(x, y, FUN = \"*\")\n     [,1] [,2] [,3] [,4]\n[1,]    4    5    6    7\n[2,]    8   10   12   14\n[3,]   12   15   18   21\n[4,]   16   20   24   28\n\n# Kronecker\nkronecker(A, B)\n     [,1] [,2] [,3] [,4]\n[1,]    2    4    5   10\n[2,]    4   14   10   35\n[3,]    3    6    4    8\n[4,]    6   21    8   28\n\n# triangle\nlower.tri(A)\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,]  TRUE FALSE\n\nlower.tri(A, diag = TRUE)\n     [,1]  [,2]\n[1,] TRUE FALSE\n[2,] TRUE  TRUE\n\nlower.tri(A) * A\n     [,1] [,2]\n[1,]    0    0\n[2,]    3    0\n\n# diagonal\ndiag(A)\n[1] 2 4\n\nsum(diag(A))\n[1] 6\n\n# kappa\nkappa(A, exact = TRUE)\n[1] 7.582401\n\n# matrix reduction\nscale(A, scale = FALSE)\n     [,1] [,2]\n[1,] -0.5  0.5\n[2,]  0.5 -0.5\nattr(,\"scaled:center\")\n[1] 2.5 4.5\n\nscale(A, center = FALSE, scale = apply(A, 2, sd))\n         [,1]     [,2]\n[1,] 2.828427 7.071068\n[2,] 4.242641 5.656854\nattr(,\"scaled:scale\")\n[1] 0.7071068 0.7071068\n\n# eigenvalue\neigen(A)\n$values\n[1]  7 -1\n\n$vectors\n           [,1]       [,2]\n[1,] -0.7071068 -0.8574929\n[2,] -0.7071068  0.5144958\n\n# singular value vector\nsvd(A)\n$d\n[1] 7.285383 0.960828\n\n$u\n           [,1]       [,2]\n[1,] -0.7337222 -0.6794496\n[2,] -0.6794496  0.7337222\n\n$v\n           [,1]       [,2]\n[1,] -0.4812092  0.8766058\n[2,] -0.8766058 -0.4812092\n\n# Cholesky\nchol2inv(A) \n          [,1]     [,2]\n[1,]  0.640625 -0.15625\n[2,] -0.156250  0.06250\n\n# QR\nqr(A)\n$qr\n           [,1]      [,2]\n[1,] -3.6055513 -6.101702\n[2,]  0.8320503 -1.941451\n\n$rank\n[1] 2\n\n$qraux\n[1] 1.554700 1.941451\n\n$pivot\n[1] 1 2\n\nattr(,\"class\")\n[1] \"qr\"\n\n\n\n\nIntegral calculus\n\u00b6\n\n\nintegration\n\n\nmyf <- function(x) { exp(-x^2 / 2) / sqrt(2  *pi) }\n\nintegrate(myf, lower = -Inf, upper = Inf)$value\n[1] 1\n\n\n\n\nDifferential calculus\n\n\nderivative\n\n\nD(expression(sin(cos(x + y^2))), \"x\")\n-(cos(cos(x + y^2)) * sin(x + y^2))\n\nf <- deriv(<sub>x^2, \"x\", TRUE)\nf(3)\n[1] 9\nattr(,\"gradient\")\n     x\n[1,] 6\n\n\n\n\n\n\nnumDeriv\n package.\n\n\ngrad()\n; first-degree derivative.\n\n\nhessian()\n; second-degree derivative.\n\n\nand many more.\n\n\n\n\n\n\n\n\nOptimisation\n\u00b6\n\n\nlinear, programming, constraints, min-max\n\n\n# compute a 1-variable min, max (y <sub>x)\n\noptimize(function(x) { cos(x^2) }, lower = 0, upper = 2, maximum = FALSE)\n\nsource('</sub>/.active-rstudio-document', echo = TRUE)\n\n\n\n\n\n\nnlm()\n; compute a 2-variable min, max (z \nx + y).\n\n\nnlminb()\n; add contraints on x and y, add parameters \nupper\n and \nlower\n.\n\n\noptim()\n.\n\n\nconstrOptim()\n.\n\n\nand many more.\n\n\n\n\nUnit root\n\n\nunitroot(f=function(x) { cos(x^2) }, lower = 0,upper = 2,tol = 0.00001)$root\n\npolyroot(x(3, -8, 1)) # for p(x) = 3 - 8x + x\u00b2\n\n\n\n\n\n\ncummax()\n.\n\n\ncummin()\n.\n\n\ncumprod()\n.\n\n\ncumsum()\n.\n\n\nand many more.\n\n\n\n\nChapter 9, Descriptive Statistics\n\u00b6\n\n\nFactor, levels, labels\n\u00b6\n\n\n\n\nfactor(c())\n\n\nas.factor()\n.\n\n\nis.factor()\n.\n\n\nlevels(var) <- c()\n.\n\n\nlabels(var) <- c()\n.\n\n\nlevels(var)\n; output the levels.\n\n\nlabels(var)\n; output the labels.\n\n\nnlevels(var)\n; output the number of levels.\n\n\n\n\nmydata <- factor(mydata,\n    levels = c(1,2,3),\n    labels = c(\"red\", \"blue\", \"green\")\n    ) \n\nmydata <- ordered(mydata,\n    levels = c(1,3, 5),\n    labels = c(\"Low\", \"Medium\", \"High\")\n    ) \n\n\n\n\nNames\n\u00b6\n\n\n\n\nnames(var) <- c()\n; add names to a vector, data frame, list.\n\n\ncolnames(var) <- c()\n; idem.\n\n\nrownames()\n; left-most column.\n\n\ndimnames()\n; add names to an array.\n\n\n\n\nOrder\n\u00b6\n\n\n\n\nsort(vec, decreasing = TRUE)\n.\n\n\nrev(vec)\n; inverse sorting.\n\n\norder(vec)\n; sort a vector with names or a list of strings.\n\n\nordered(vec)\n.\n\n\nas.ordered()\n.\n\n\nis.ordered()\n.\n\n\nand many more.\n\n\n\n\nConsult keywords \u2018arithmetics\u2019 and \u2018random numbers\u2019, where ordering data is commonly used.\n\n\nConvert (\nas.\n)\n\u00b6\n\n\n\n\nis.integer()\n.\n\n\nas.integer()\n.\n\n\nas.double()\n.\n\n\nis.double()\n.\n\n\nis.numeric()\n.\n\n\nas.numeric()\n.\n\n\nis.character()\n.\n\n\nas.character()\n.\n\n\nand many more.\n\n\n\n\nTable, proportion table\n\u00b6\n\n\ntabular, comparison, 2-dimensional, 2, two, dimensions\n\n\n\n\ntable(var1, var2)\n; 2-dimensional view; cross table.\n\n\nas.table(var)\n; convert.\n\n\ncut()\n; divide the range into intervals.\n\n\ntable(cut(x, res$breaks, include.lowest = TRUE))\n.\n\n\n\n\n\n\naddmargins(x, FUN = sum, quiet = TRUE)\n; add a column or row with sums, means, etc.\n\n\nread.ftable()\n; frequencies.\n\n\ntablefreq <- mytable / sum(mytable)\n.\n\n\nmargin.table(tablefreq, 1)\n; margin, right and bottom.\n\n\ntablefreq[ ,ncol()]\n; extract the total.\n\n\ntablefreq[nrow(), ]\n; extract the total.\n\n\n\n\n\n\nprop.table(mytable, 1)\n; percentage view; 1, row sum = 100%.\n\n\nprop.table(mytable, 2)\n; percentage view; 2, row sum = 100%.\n\n\nwhich.max(table)\n; find the max, min, mean, etc.\n\n\n\n\nDescriptive statistics\n\u00b6\n\n\n\n\nmean(x)\n.\n\n\nmedian(x)\n.\n\n\nquantile(x, probs = c(0.1, 0.9))\n.\n\n\nprobs = 1:10 / 10\n.\n\n\n\n\n\n\nmax(x)\n.\n\n\nmin(x)\n.\n\n\ndiff(range(x))\n.\n\n\nIQR(x)\n.\n\n\nvar.pop(x)\n, \nvar(x)\n.\n\n\nsd.pop(x)\n, \nsd(x)\n.\n\n\nco.var(x)\n.\n\n\nmad(x)\n; absolute deviation from the median.\n\n\nmean(abs(x - mean(x)))\n.\n\n\n\n\n\n\nskew(x)\n.\n\n\nkurt(x)\n.\n\n\nchisq.test()\n.\n\n\nround()\n.\n\n\nsum()\n.\n\n\nnrow()\n.\n\n\nncol()\n.\n\n\ncor(var1, var2)\n.\n\n\nmethod = \"kendall\", \"spearman\"\n\n\n\n\n\n\nrank()\n.\n\n\nrgrs\n package .\n\n\ncramer.v()\n.\n\n\n\n\n\n\n\n\nREF: p.378-379\n\n\nGraphic descriptive statistics\n\u00b6\n\n\n\n\nplot()\n.\n\n\ndotchart(table(), col = c(\"\", \"\", \"\" ,\"\") ,pch = , main = , lcolor = )\n.\n\n\nbarplot()\n.\n\n\nbarplot(var, col = , pareto = TRUE)\n.\n\n\nbarplot(sort(table(var)), TRUE))\n.\n\n\nbarplot(xlim = , width = , space = , names.arg = , legend = , density = , ylab = , lwd = )\n.\n\n\npie()\n.\n\n\npoints(barplot(), cumsum(var), type=\"l\")\n.\n\n\nboxplot()\n.\n\n\nstem()\n.\n\n\naplpack\n package.\n\n\nstem.left()\n\n\n\n\n\n\nhist()\n.\n\n\nsegments()\n.\n\n\n\n\nREF: p.392-394, 397-398, 400-410\n\n\nChapter 10, Random Variables, Laws, and Simulation\n\u00b6\n\n\n\n\nx %% m\n; modulo or modulus.\n\n\n\n\nRandomness\n\u00b6\n\n\n\n\nrunif(1)\n; generate a pseudo-random number between 0 and 1.\n\n\nset.seed()\n; \u2018shuffle the dice!\u2019.\n\n\nx <- function() { runif(1) }\n; generate random numbers, following the uniform distribution.\n\n\nrnorm(1)\n; generate a random numbers, following the normal distribution.\n\n\ngenerate random numbers with a randomness function and \ncurve()\n.\n\n\n\n\nx <- function() { rnorm(1, 7, 1) }\n# avg = 7, sd = 1, 1 obs\n\ncurve(rnorm(x, 7, 1), xlim = c(-1,10))\n\nplot(density(rnorm(1000, 7, 1)),xlim = c(-1, 10), main=\"Density Curve\")\n\n\n\n\nREF: p.422-423\n\n\nmean(runif(1))\n[1] 0.6586903\n\nmean(runif(10))\n[1] 0.5196868\n\nmean(runif(100))\n[1] 0.5345603\n\nmean(runif(1000))\n[1] 0.5042301\n\nmean(runif(10000))\n[1] 0.5021896\n\n# getting closer to 0.5 as the sample increases\n\n\n\n\n\n\nConvergenceConcepts\n package to view the law of great numbers.\n\n\n\n\nDice\n\n\nREF: p.430-431\n\n\nBootstrap\n\n\nREF: p.436\n\n\nLaws\n\n\nREF: p.437-447\n\n\nChapter 11, Confidence Intervals and Hypothesis Testing\n\u00b6\n\n\nConfidence intervals\n\u00b6\n\n\n\n\nUse t-values with at least 30 observation in the sample.\n\n\nWith smaller samples (or larger too), use bootstraps to simulate populations from the \nboot\n package (\nboot()\n and \nboot.ci()\n).\n\n\nFor proportion with large samples, use the \nepitools\n package and \nbinom.approx()\n. With smaller samples, go with \nbinom.test()\n.\n\n\nVariance confidence intervals; test for normality with \nsigma2.test()\n.\n\n\nFor non-parametric sample, again, simulate populations with \nboot()\n and \nboot.ci()\n.\n\n\nFor median, use \nqbinom()\n.\n\n\nFor correlation, use \ncor.test()\n.\n\n\n\n\nREF: p.450-456\n\n\nTest\n\u00b6\n\n\n\n\nIn a test, \n\\alpha\n is the signification threshold, \nH_0\n is the tested hypothesis.\n\n\nAverage tests;  compare the theoretical average to a reference value with the \nt.test()\n.\n\n\nCompare two theoretical averages with a \nt.test()\n.\n\n\nCompare pair samples with \nt.test(paired=TRUE)\n.\n\n\nTest variance(s) with ANOVA.\n\n\nCompare a theoretical variance with a reference value with \nsigma2.test()\n.\n\n\nCompare two theoretical variances with \nvar.test()\n.\n\n\nCompare a theoretical proportion with a reference value with \nprop.test()\n.\n\n\nCompare two theoretical proportions with \nprop.test(\n).\n\n\nTest the theoretical correlation coefficient vs a reference value with \ncor.test()\n and \ncor-.test()\n.\n\n\nTest two theoretical correlation coefficients vs a reference value with \ncor.test.2.sample()\n.\n\n\nTest of independence or chi\u00b2 with \nchisq.test()\n.\n\n\nYates chi\u00b2, adjustment chi\u00b2 with \nchisq.test()\n.\n\n\nFisher test with \nfisher.test()\n.\n\n\nAdequacy test or Shapiro-Walk test with \nshapiro.test()\n.\n\n\nPositional test or sign test or, median sign test with \nprop.test()\n and \nbinom.test()\n.\n\n\nMedian sign test for two independent samples with \nchisq.test()\n and \nfisher.test()\n.\n\n\nSign test for two matching samples with \nprop.test()\n and \nbinom.test()\n.\n\n\nWilcoxon rank test or Mann-Whitney test for two independent samples with \nwilcox.test()\n.\n\n\nWilcoxon test for two matching samples with \nwilcox.test()\n.\n\n\n\n\nREF: p.459-488\n\n\nChapter 12, Simple and Multiple Linear Regression\n\u00b6\n\n\nRegression\n\u00b6\n\n\n\n\nlm(y <sub>x\n.\n\n\nlm(y <sub>0 + x)\n; no intercept.\n\n\nmodel <- lm(y <sub>x)\n; run the regression.\n\n\nsummary(model)\n; extract the results.\n\n\nplot(y <sub>x)\n; plot the results.\n\n\nabline(model)\n; add a line on the observations.\n\n\nconfint(model)\n; confidence intervals; 95% or 2.5% on both sides.\n\n\ncoefficients(model)\n; extract one or several coefficient.\n\n\nmodel$coefficients\n.\n\n\nmodel$call\n.\n\n\nmodel$residuals\n.\n\n\nanova(model)\n.\n\n\npredict(model, data.frame(LWT = prediction), interval = \"prediction\")\n\n\nand many more.\n\n\n\n\nprediction, result, extraction, residual\n\n\nREF: p.498-499\n\n\nNormality\n\u00b6\n\n\nhistogram, test, residual, quantile-quantile, quantile, qq\n \n\n\npar(mfrow=c(1,2))\nhist(residuals(model), main = \"Histogram\")\n\n\n\n\n\n\nqqnorm(resid(model), datax = TRUE)\n; quantile-quantile.\n\n\nqqplot()\n.\n\n\nqqline()\n.\n\n\nplot(model, 1:6, col.smooth = \"red\")\n; 6 graphics.\n\n\njarque.bera.test(residuals(model))\n; from the \ntseries\n package.\n\n\ndwtest()\n; Durbin-Watson test from the \nlmtest\n package.\n\n\n\n\nREF: p.502-503\n\n\nCorrelation\n\u00b6\n\n\ntest, explanatory variable interaction, colinearity,  best subset\n\n\n\n\npairs(newdata, lower.panel = panel.smooth, upper.panel = add.cor)\n.\n\n\n\n\nREF: p.506\n\n\nModel improvement\n\u00b6\n\n\n\n\nVariables selection.\n\n\nBest subset, leaps and bounds.\n\n\nForward selection.\n\n\nBackward selection.\n\n\nStepwise selection.\n\n\nResidual analysis.\n\n\nand many more.\n\n\n\n\nREF: p.511-535\n\n\nPolynomial regression\n\u00b6\n\n\nREF: p.535-540\n\n\nChapter 13, Elementary Variance Analysis\n\u00b6\n\n\nanova, repeated measure, between, within, inspection, hypothesis, comparison, factor, table, parameter, repeated\n\n\nREF: p.542-571\n\n\nAppendix, Installing the R Software and Packages\n\u00b6\n\n\ninstallation, package\n\n\ninstall.packages('package')\n.\n\n\n\n\nBe sure to launch RStudio as an administrator or a superuser to install a package in R (not in RStudio); the package is accessible to all users. Then load the package in R or RStudio.\n\n\nThen attach the package to the work session with \nlibrary('package')\n or \nrequire('packages')\n.\n\n\n\n\nAnswers to the Exercises\n\u00b6\n\n\nREF:  p.625-674",
            "title": "Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#gui",
            "text": "Rcommander;  Rcmdr  pachage.    The R Commander: A Basic-Statistics GUI for R    REF: p.3-5",
            "title": "GUI"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#b-datasets",
            "text": "Datasets.",
            "title": "B, Datasets"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#part-1-basics",
            "text": "",
            "title": "Part 1, Basics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-1-basic-concept-organizing-data",
            "text": "",
            "title": "Chapter 1, Basic Concept, Organizing Data"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#editors",
            "text": "RStudio.  Tinn-R.  JGR.  Emacs/ESS.",
            "title": "Editors"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#data-entry",
            "text": "x <- 2 .  2 -> 2 .",
            "title": "Data entry"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#exponent",
            "text": "exp(1) .",
            "title": "Exponent"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#logarithm",
            "text": "log(3) .  log(x = 3) .  log(x = 3, base(exp(1)) .  log(3, exp(1)) .",
            "title": "Logarithm"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#factorial",
            "text": "factorial(2) .",
            "title": "Factorial"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#find-out-about-an-object-a-variable-is",
            "text": "is.character() .  is.vector() .  is.character() .  is.character()  and many more.",
            "title": "Find out about an object, a variable (is.)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#convert-as",
            "text": "as.character() .  as.raw() .  as.date() .  and many more.",
            "title": "Convert (as.)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#array-and-matrix",
            "text": "matrix(1:12, nrow = 4, ncol = 3, byrow = FALSE)\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\nmatrix(1:12, nrow = 4, ncol = 3, byrow = TRUE)\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n\narray(1:12, dim = c(2, 2, 3))\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 3\n\n     [,1] [,2]\n[1,]    9   11\n[2,]   10   12",
            "title": "Array and matrix"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#vector",
            "text": "vec <- c(1.1, 2.2, 3.5) .  vec <- 1:3 .",
            "title": "Vector"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#sequence",
            "text": "seq(1:3) .  1:3 .",
            "title": "Sequence"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#list",
            "text": "c(1:3) # vector\n[1] 1 2 3\n\n# vs\n\nlist(1:3) # list\n[[1]]\n[1] 1 2 3",
            "title": "List"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#data-frame",
            "text": "table, tabular   data.frame(name = c(), name = c(), name = c(), etc) ; each column is a vector with a name.",
            "title": "Data Frame"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#time-series",
            "text": "> ts(1:10, frequency=4, start=c(1959,2))\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         1    2    3\n1960    4    5    6    7\n1961    8    9   10",
            "title": "Time series"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#class-and-mode",
            "text": "type, data, variable, object   mode() .  class() ; mode != class.  typeof() ; type of storage.",
            "title": "Class and mode"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-2-import-export-and-producing-data",
            "text": "import, export, i/o",
            "title": "Chapter 2, Import-Export and Producing Data"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#input-data-from-files",
            "text": "read.table(file = path/file.txt, header = TRUE, sep= \"\\t\", dec=\".\", row.names = 1)   attach(data) ; dataset is attached to .GlobalEnv.  search() ; search objects in .GlobalEnv, including attached dataset,  detach(data) .   More about .GlobalEnv in Chapter 7, Session Management.  Read .csv and .tsv   read.csv() .  read.csv2() .  read.delim() .  read.delim2() .  and many more.   Read text files   read.ftable(\"file.txt\", row.var.names = c(...), col.vars = list()) .  scan(\"file.txt\", skip = 9, nlines = 1, what = \"\", dec = \"\") .  and many more.   Read software files   .sav ; SPSS.  read.spss .    .mtp ; Minitab.  read.mtp .    .xpt ; SAS en data.frame.  read.xport .    .mat ; Matlab.  readMat() .    and many other formats and commands.",
            "title": "Input data from files"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#ouput-data-and-export",
            "text": "lookup.xport() ; for SAS.  write.table(data, file=\"file.txt\", sep=\"\\t\") .  xlsReadWrite() .  and many more.",
            "title": "Ouput data and export"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#measure-computation-time",
            "text": "# start the timer\ntmps <- Sys.time()\n\n# run\ndbsnp <- read.table(\"file\")\n\n# stop the timer\nSys.time() - tmps",
            "title": "Measure computation time"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#produce-by-repetition",
            "text": "repeat  rep(1:4, reach = 2, len = 10)\n[1] 1 2 3 4 1 2 3 4 1 2",
            "title": "Produce by repetition"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#produce-random-numbers",
            "text": "# generate random numbers between 0 and 1\nrunif(5)\n[1] 0.2424283 0.6140730 0.4824881 0.7263319 0.1381030\n\nrunif(5, min = 2, max = 7)\n[1] 4.588744 5.522278 4.307162 6.248397 3.854982  More on random number in Chapter 10, Random Variables, Laws, and Simulation.  Produce random number following a distribution  # generate random numbers following the normal distribution\nrnorm(5)\n[1]  0.8752170  1.3869022 -0.4419174 -0.6129075 -1.6987139  Generate numbers with other distributions.  Produce random number by sampling a population  urne <- 0:9\n\n# 20 draws from 'urne'\nsample(urne, 20, replace = TRUE)\n[1] 5 4 2 2 9 7 4 6 2 2 7 8 3 3 9 6 6 1 1 0",
            "title": "Produce random numbers"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#produce-data-by-manual-input-vector-like",
            "text": "First,   z <- scan() .  Second, input data in the prompt.",
            "title": "Produce data by manual input (vector-like)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#produce-data-with-a-mini-spreadsheet-tabular-like",
            "text": "x <- as.data.frame(de(\"\")) ; open a spreadsheet.  data.entry(\"\") ; alternatively.  Input data; column are variables like in a data frame.    fix(x) ; invokes edit on x, then assigns the new (edited) version of x to the user\u2019s workspace.",
            "title": "Produce data with a mini-spreadsheet (tabular-like)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#list-of-objects",
            "text": "ls() ; list of objects.  rm(list = ls()) ; remove all the objects.",
            "title": "List of objects"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#read-from-and-write-to-a-database",
            "text": "RODBC  package.  odbcConnect() .  sqlQuery() .  odbcClose() .   REF: p.82-84",
            "title": "Read from and write to a database"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#file-management",
            "text": "file.choose() ; open a window.",
            "title": "File management"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#read-from-the-clipboard",
            "text": "First, copy from a spreadsheet or a table.  Second,  read.clipboard() .",
            "title": "Read from the clipboard"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-3-data-manipulation",
            "text": "",
            "title": "Chapter 3, Data Manipulation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#arithmetics",
            "text": "x <- c(1,2,3)\ny <- c(4,5,6)\n\nx + y\n[1] 5 7 9",
            "title": "Arithmetics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#built-in-functions",
            "text": "length(vec) ; length.  sort(vec, decreasing = TRUE) ; sort.  rev(vec) ; inverse sorting.  order(vec) ; sort a vector according to the names or a list of strings.  names(vec) <- 1:9 ; attribute names.  rank(vec) ; rank the elements.  unique(vec) ; remove doubles.  duplicated(vec) ; create a TRUE/FALSE vector indicating doubles.  x %% y ;  modulus (x mod y).  x %/% y ; integer division.",
            "title": "Built-in functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#dimension-functions",
            "text": "number, row, column, dimension   dim(df) .  nrow(df) .  ncol(df) .",
            "title": "Dimension functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#name-functions",
            "text": "dimnames(df) .  names(df) ,  colnames(df) .  rownames(df) .",
            "title": "Name functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#merge-functions",
            "text": "combine   cbind() .  rbind() .   REF: p.98  y <- array(1:12, dim = c(4, 3))\n\ny\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\ny <- cbind(y, c(100, 101, 102, 103))\n\ny\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9  100\n[2,]    2    6   10  101\n[3,]    3    7   11  102\n[4,]    4    8   12  103\n\nmerge(x, y)\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n# idem with rows  REF: p.96-98   gtools  package.  smartbind(x,y) ; for two data frames, similar to merge.",
            "title": "Merge functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#apply-functions-and-family",
            "text": "excel, wrangle  Among the most useful function for \u2018wrangling\u2019 data. Excel-like power. When and how to use them.   apply() .  lapply() .  sapply() .  mapply() .  by() .  with() .  replicate() .  transform() .  rowSums(df) .  colSums(df) .  rowMeans(df) .  colMeans(df) .  sweep() .  stack() .  unstack() .  aggregate()   REF: p.99",
            "title": "apply functions and family"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#sweep-functions",
            "text": "u\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n# removes pattern '3, 5, 3, 5, etc.'\nsweep(u, MARGIN = 1, STATS = c(3, 5), FUN = \"-\")\n  V1 V2 V3 V4\n1 -2  2  6 97\n2 -3  1  5 96\n3  0  4  8 99\n4 -1  3  7 98\n\n# divide by a vector\nsweep(u, MARGIN = 2, STATS = c(2, 2, 3, 3), FUN = \"/\")\n   V1  V2       V3       V4\n1 0.5 2.5 3.000000 33.33333\n2 1.0 3.0 3.333333 33.66667\n3 1.5 3.5 3.666667 34.00000\n4 2.0 4.0 4.000000 34.33333  REF: p.100-101",
            "title": "Sweep functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#stack-functions",
            "text": "stack, unstack  u\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\nv<- stack(u)\nv\n   values ind\n1       1  V1\n2       2  V1\n3       3  V1\n4       4  V1\n5       5  V2\n6       6  V2\n7       7  V2\n8       8  V2\n9       9  V3\n10     10  V3\n11     11  V3\n12     12  V3\n13    100  V4\n14    101  V4\n15    102  V4\n16    103  V4\n\nw <- unstack(v)\nw\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103",
            "title": "Stack functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#aggregation-functions",
            "text": "aggregate  w\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\nfac <- c(\"a\", \"b\", \"b\", \"a\")\nx <- cbind(w, fac)\nx\n  V1 V2 V3  V4 fac\n1  1  5  9 100   a\n2  2  6 10 101   b\n3  3  7 11 102   b\n4  4  8 12 103   a\n\naggregate(w, by = list(x$fac), sum)\n  Group.1 V1 V2 V3  V4\n1       a  5 13 21 203\n2       b  5 13 21 203  REF: p.101",
            "title": "Aggregation functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#boolean-logical-functions",
            "text": "logical(2) ; generate two FALSE in a vector; change the length.  !logical(2) ; generate two TRUE.  as.logical(vec) .  is.logical(vec) .  isTRUE() .  & ; AND.  && ; sequential AND.  | ; OR.  || ; sequential OR.  Prefer  &&  over  & , and  ||  over  | . Assessments go from left to right, and keep on going as long as the conditions are TRUE.  xor() ; exclusive OR.  if ,  else .  any() ; if one or another is TRUE.  all() ; if all are TRUE.  identical() ; if all are identical.  all.equal() .  ==  or  all.equal  (and  != ) can yield a FALSE because decimal are different on large numbers.  all.equal(x, y, tolerance = 10^-6)  fixes the problem.    ifelse(cond, a, b) ; if  cond  is TRUE,  a , else,  b .  x <- c(3:-2); sqrt(ifelse(x >= 0, x, NA) .     REF: p.126-127",
            "title": "Boolean &amp; logical functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#venn-functions",
            "text": "A <- 1:3\nB <- 3:6\n\nis.element(1, A)\n[1] TRUE\nis.element(4, A)\n[1] FALSE\nis.element(4, B)\n[1] TRUE\n\nall(A %in% B)\n[1] FALSE\nall(B %in% A)\n[1] FALSE\n\nintersect(A, B)\n[1] 3\nunion(A, B)\n[1] 1 2 3 4 5 6\n\nsetdiff(A, B)\n[1] 1 2\nsetdiff(B, A)\n[1] 4 5 6\nintersect(A, B)\n[1] 3",
            "title": "Venn functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#vector-functions",
            "text": "vec[2] ; extract.  vec[2:5] ; extract.  vec[c(T, F, T)] ; extraction with filter.  vec[vec > 4] ; conditional extraction.  vec[vec == 3] .  vec[which.max(z)] ; extract the maximum value.  vec[which.min(z)] .  vec > 4 ; yield a vector of TRUE or FALSE.  vec[-2] ; exclude.  vec[-c(1,5)] ; exclude.",
            "title": "Vector functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#search-functions",
            "text": "masque <- c(TRUE, FALSE) .  which(masque) ; return the TRUE indices.  which.min(x) ; return the index with minimum value.  which.max(x) .",
            "title": "Search functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#replace-functions",
            "text": "z[c(1, 5)] <- 1 ; replace value 1 and 5 by 1.  z[which.max(z)] <- 0 ; replace the maximum value.  z[z == 0] <- 8 ; replace zeros and FALSE.",
            "title": "Replace functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#extend-a-vector",
            "text": "vecA .  vecB <- c(vecA, 4, 5) .  vecC <- c(vecA[1:4], 8, 5, vecA[5:9]) .",
            "title": "Extend a vector"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#matrix-and-array",
            "text": "mat[r, c] ; extract.  mat[1, 2] .  mat[,2] ; all rows, column 2 only.  mat[1, ] ; all columns, row 1 only.  mat[c(1, 3), c(4:5)] .  mat[, 1, drop = FALSE] ; avoid making a (horizontal) row with a (vertical) column.  mat[ind] ; matrix index.  array[r, c, m] ; extract.   REF: p.110-113",
            "title": "Matrix and array"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#lists",
            "text": "char <- c(\"a\", \"b\", \"c\")\nnumb <- c(1, 2, 3)\ngreek <- c(\"alpha\", \"beta\", \"gamma\")\n\nx <- list(char, numb, greek)\nx\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n[[2]]\n[1] 1 2 3\n\n[[3]]\n[1] \"alpha\" \"beta\"  \"gamma\"\n\nnames(x) <- c(\"char\", \"numb\", \"greek\")\nx\n$char\n[1] \"a\" \"b\" \"c\"\n\n$numb\n[1] 1 2 3\n\n$greek\n[1] \"alpha\" \"beta\"  \"gamma\"\n\nx[2]\n$numb\n[1] 1 2 3\n\nx[[2]][2]\n[1] 2\n\nx$numb[2]\n[1] 2  REF: p.113-115",
            "title": "Lists"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#string",
            "text": "\"bla bla bla\"\n[1] \"bla bla bla\"\n\nnoquote(\"bla bla bla\")\n[1] bla bla bla\n\nsQuote(\"bla bla bla\")\n[1] \"\u2018bla bla bla\u2019\"\n\ndQuote(\"bla bla bla\")\n[1] \"\u201cbla bla bla\u201d\"",
            "title": "String"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#text",
            "text": "wrangle, text, string, character, natural language processing, nlp    format() ; and arguments:   digits .  trim .  digit .  nsmall .  justify .  width .  na.encode .  decimal.mark .  drop0trailing .  and many more.     cat(\"current working dir: \", wd) ; print the objects, concatenate the representations.   printf(\"hello %d\\n\", 56) ; mix text and data; pythonic print.  print(paste0(\"current working dir: \", wd)) .  nchar() ; number of characters.  x[nchar(x) > 2] .  x[x %n% c(letters, LETTERS)] ; retrieve letters, patterns or strings in a text object; alike Venn.  paste(ch1, ch2, sep = \"-\") ; concatenate.  paste0(ch1, ch2) ; concatenate.  substring(\"abcdef\", first = 1:3, last = 2:4) ; create subsets  ab ,  bc ,  cd .  strsplit(c(\"\",\"\"), split=\" \") ; break down a string.  grep(\"i\", c()) ; extract an object index.  gsub(\"i\", \"L\", c()) ; substitute.  sub() ; substitute the first occurrence.  tolower() .  toupper() .",
            "title": "Text"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#date-and-time",
            "text": "convert, extract   Sys.time() .  date() .  Sys.setlocale() .  as.numeric() .  strptime() ; extract time.  as.POSIXlt() .   REF: p.120-123",
            "title": "Date and time"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#custom-functions-two-examples",
            "text": "a <- 2\nb <- -3\n\n# quadratic\nf <- function (x) { x**2 - 2*x - 2 }\n\nf(a)\n[1] -2\n\nf(b)\n[1] 13\n\n# test\ng <- function (x, y) {\n  if (x <= y) {\n    z <- y - x\n    print(\"x smaller\")\n  } else {\n    z <- x - y\n    print(\"x larger\")\n  }\n} \n\ng(a, b)\n[1] \"x larger\"\n\ng(a, abs(b))\n[1] \"x smaller\"  More about custom functions in  Chapter 6, Initiation to R Programming .",
            "title": "Custom functions (two examples)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#loops-structure",
            "text": "for .  while .  repeat .   # while\nwhile(x + y < 7) { x <- x + y }\n\n# for\nfor (i in 1:4) {\n    if (i == 3) break\n    for (j in 6:8) {\n        if (j == 7) next\n        j <- i + j\n    }\n}\n\n# repeat\ni <- 0\nrepeat {\n    i <- i + 1\n    if (i == 4) break\n}  Loop example  IMC <- function (poids, taille) {\n  imc <- poids / taille^2\n  names(imc) <- \"IMC\"\n  return(imc)\n}\n\nIMC(100, 1.90)\n     IMC \n27.70083 \n\np <- c(100, 101, 95, 97)\nt <- c(1.90, 1.75, 1.68, 1.92)\n\nIMC(p, t)\n     IMC     <NA>     <NA>     <NA> \n27.70083 32.97959 33.65930 26.31293 \n\nfor (i in 1:4) {\n  print(IMC(data[i,1], data[i,2]))\n}\n     IMC \n27.70083 \n     IMC \n32.97959 \n    IMC \n33.6593 \n     IMC \n26.31293   Loops increase computation time  system.time(for (i in 1:1000000) sqrt(i))\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       0.19        0.01        0.22 \n\nsystem.time(sqrt(1:1000000))\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       0.07        0.00        0.07   REF: p.133-136",
            "title": "Loops structure"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#binary-and-decimal",
            "text": "convert   bin2dec() .  dec2bin() .   REF: p.136-144, 147-152",
            "title": "Binary and decimal"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-4-r-documentation",
            "text": "",
            "title": "Chapter 4, R Documentation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#help",
            "text": "?functionname .  help(functionname) .  help.start() ; user manual in the browser.  apropos('mean') ; object with the name \u2018mean\u2019.   Library   install.packages('package') ; should be done as an administrator or superuser directly in R, not an editor. Once the package is installed, it can be loaded on the PC by any user in R or an editor (such as RStudio).  library(package = 'package') ; load a package.  library(help = package) ; help on a specific package.  library(help = base) ; for example.  library(help = utils) ; for example.  library(help = datasets) ; for example.  library(help = graphics) ; for example.  library(help = grDevices) ; for example.    library(lib.loc = .Library) ; find which package (system packages, but not user packages).  find('function') ; find which package (system).  data() ; datasets available.  demo() ; available demos by package.  demo(graphics) ; give examples.  example(mean) ; give examples.",
            "title": "Help"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#resources",
            "text": "nmz ; search the content of the R functions, package vignette, and task views.  R-Project Mailing Lists ; mailing lists.  RSeek ; search engine.  StackOverflow ; forum.  Abcd\u2019R ; scripts.  CIRAD ; forum.  Developpez.net ; forum.  ETHZ ; mailing list.  ETHZ Manual .  http://a ; manuals and resources. \nstat.ethz.ch/mailman/listinfo/r-annonce  ETHZ List Info .  R Programming Wikibooks ; wiki.  CRAN-R .  CRAN-R Doc ; PDF and documentation.  CRAN-R Views ; projects.  R Journal ; publication.  Journal of Statistical Software ; publication.",
            "title": "Resources"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-5-techniques-for-plotting-graphics",
            "text": "",
            "title": "Chapter 5, Techniques for Plotting Graphics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#graphic-windows",
            "text": "Of course, these commands are automated in an editor (RStudio).   dev.new() ; graphic window on Windows.  windows() ; graphic window on Windows.  win.graph() ; graphic window on Windows.  X11() ; graphic window on Linux.  Add parameters in the command:  width = , height = .  pointsize = .  xpinch = , ypinch = ; pixels per inch.  xpos = , ypos = ; position of the upper left corner, in pixel.    dev.set(num) ; activate window number \u2018num\u2019; there can be several graphic windows.  dev.off(num) ; close a window.  graphics.off() ; close all windows.  dev.list() ; return window numbers.  dev.cur() ; return the current number, active window.  dev.print(png, file = , width = , height = ) ; print.  savePlot(filename = , type = \"png\") ; save.  png(file=\" \", width = , height = ) ; save directly in .png.  jpeg() .  bitmap() .  postscript() .  pdf() .   # examples\n\ndev.new(width = 200, height = 200, xpos = 100, ypos = 100)\nNULL\n\ndev.list()\nRStudioGD       png   windows \n        2         3         4 \n\ndev.cur()\nwindows \n      4\n\nhist(runif(100)) # create a graphic\n\n# in the current working directory\ndev.print(png, file = \"mygraph.png\", width = 480, height = 480)\nsavePlot(filename = \"mygraph.png\", type = \"png\")\npdf(file = \"mygraph.pdf\")\n\ndev.off()",
            "title": "Graphic windows"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#multiple-windows",
            "text": "par(mfrow = c(3, 2)) ; create 6 graphic boxes, 3 rows X 2 columns.   REF: p.166-167  dev.new()\nmat <- matrix(c(2, 3, 0, 1, 0, 0, 0, 4, 0, 0, 0, 5), 4, 3, byrow = TRUE)\nlayout(mat)\n\n\ndev.new()\nlayout(mat, widths = c(1, 5, 14), heights = c(1,2,4,1))\nlayout.show(5)  REF: p.168",
            "title": "Multiple windows"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#draw-on-a-graphic",
            "text": "segments(x0 = 0, y0 = 0, x1 = 1, y1 = 1) ; draw lines on a plot.  lines(x = c(1,0), y = c(0,1)) .  abline(h = 0, v = 0) ; add a line to a plot.  abline(a = 1, b = 1) .",
            "title": "Draw on a graphic"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#random-numbers",
            "text": "x <- runif(12)\nx\n[1] 0.03344539 0.22711659 0.66696650 0.02840671 0.61067995 0.92957527\n[7] 0.26962190 0.60013387 0.04831111 0.12603905 0.41913598 0.13142315\n\n# ranking\ni <- order(x)\ni\n[1]  4  1  9 10 12  2  7 11  8  5  3  6\n\n# reorder\nx <- x[i]\nx\n[1] 0.02840671 0.03344539 0.04831111 0.12603905 0.13142315 0.22711659\n[7] 0.26962190 0.41913598 0.60013387 0.61067995 0.66696650 0.92957527  More on random number in Chapter 10, Random Variables, Laws, and Simulation.  Examples   example(polygon) ; see examples.  curve(x**3 - 3*x, from = -2, to = 2) ; trace a curve according to a function.  hist(rnorm(10000), prob = TRUE, breaks = 100) .  plot(runif(7), type = \"h\", axes = F); box(lty = \"1373\")  plot(1:10, runif(10), type = \"l\", col = \"orangered\")   Help   colors()[grep(\"orange\", colors())] ; list of tones.",
            "title": "Random numbers"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#colors",
            "text": "rgb(red = 26, green = 204, blue = 76, maxColorValue = 255) ; return the code.  rgb(red = 0.1, green = 0.8, blue = 0.3) .  col2rgb(\"#1AVV4C\") ; inversely.  R generates 256\u00b3 colors, 15M.  rainbow(#) ; show 1, 8, 17 or more colors; R can generate 256\u00b3 colors (over 15M),  pie(rep(1,200), labels = \"\", col = rainbow(200), border = NA) ; show the colors.  plot(1:40, col = rainbow(n), pch = 19, cex = 2),     grid(col = \"grey50\") ; show.  RColorBrewer  package.   library('RColorBrewer')\ndisplay.brewer.all()",
            "title": "Colors"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#images",
            "text": "caTools  package to manage images.  read.gif() .  image() .",
            "title": "Images"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#write-and-add-marks-on-a-graphic",
            "text": "text(coord x, coord y, 'text') ; write.  demo(plotmath) ; see examples.  mtext('bas', side = 1) ; write \u2018bas\u2019 on the graphic box; 4 sides of the box (bottom, left, top, right).  locator() ; point with the mouse on a graphic to record coordinates;  esc  to show the coordinates. \nplot(1,1); locate area to be annotated with  text() .  text(locator(1), label=\"ici\") ; add a label, one or several occurrences, by pointing the location with the mouse.  identify(occurrence, label) ; add one or more labels by pointing the location with the mouse.",
            "title": "Write and add marks on a graphic"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#graphic-parameters-and-graphic-windows",
            "text": "margin, border, box, square, space, height, width, color, text, title, label, mark, font, police, character, language, size, length, size, tick, coordinate, x, y, log, scale, axis, axes, format, alignment, point",
            "title": "Graphic parameters and graphic windows"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#advanced-graphic-package",
            "text": "rgl .  lattice .  ggplot2 .   REF: p.190-201",
            "title": "Advanced graphic package"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-6-initiation-to-r-programming",
            "text": "function (<parameters>) {\n    <body>\n}  lance <- function (nom) {\n  cat(\"Bonjour\", nom, \"!\")\n}\n\nlance('allo')\nBonjour Alex !\n\nbonjour <- function (nom=\"Pierre\", langue=\"fr\") {\n  cat(switch(langue, fr=\"Bonjour\", esp=\"Hola\", ang=\"Hi\"), nom, \"!\")\n}\n\nbonjour()\nBonjour Pierre !\nbonjour(nom=\"Ben\") # replace the default value.\nBonjour Ben !\nbonjour(langue=\"ang\")\nHi Pierre !\nbonjour(lang = \"ang\") # partial call\nHi Pierre !\nbonjour(l=\"ang\")\nHi Pierre !\nbonjour(l=\"a\")\n Pierre !  REF: p.218-219",
            "title": "Chapter 6, Initiation to R Programming"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#union",
            "text": "\"%union%\" <- function (A,B) { union(A,B) }\nA <- c(4,5,2,7)\nB <- c(2,1,7,3)\n\nA %union% B\n[1] 4 5 2 7 1 3",
            "title": "Union"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#class",
            "text": "obj <- 1:10\n\nclass(obj)\n[1] \"integer\"\n\nclass(obj) <- \"TheClass\"\n\nclass(obj)\n[1] \"TheClass\"\n\ninherits(obj, \"TheClass\")\n[1] TRUE",
            "title": "Class"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#methods",
            "text": "x <- 1:10\n\nprint.default(x)\n[1] 1 2 3 4 5 6 7 8 9 10  REF: p.227-231",
            "title": "Methods"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#combine-and-permute",
            "text": "combinat  package.  combn(5,3) ; combine 3 numbers from 1:5.  choose(200,3) ; choose 3 numbers from 1:200.  permn(n,m) .   Very time consuming!",
            "title": "Combine and permute"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#more-power-speed",
            "text": "The core of R in programmed in C. Converting a R function into C is easy. C is faster.  Call it through API C.  With R graphic interface and C computation speed, it is the best of both world.  Easier to set up on Linux or OS X (the OS has default compilers. Use the  Rcpp  package.   On Windows, there is a need for  Rtools .   R can be compiled (byte compiler) with the  RevoScaleR  package (parallel computing).  bigmemory ,  ff , packages; split a matrix into sub-matrices, perform computation and combine the results (parallel computing).   Use a multi-core architecture with the  parallel  package.  HighPerformanceComputing list of packages .   Try parallel computing with a Monte Carlo with the  parallel  package  myfunc <- function(M=1000) {\n    decision <- 0\n    for (i in 1:M) {\n      x <- rnorm(100)\n      if (shapiro.test(x)$p < 0.05) decision <- decision +1\n    }\nreturn(decision)\n}\n\nsystem.time({\n    M <- 60000\n    decision <- myfunc(M)\n    print(decision/M)\n})   For a parallel execution.  Start menu.  Type devmgmt.msc  Under Processors in Linux, type  top  in the terminal.  Then, type  1  in R.  Enter  detectCores()  from the  parallel  package.   require(\"parallel\")\nsystem.time({\n    nbcores <- 6 # less than detectCores() - 1\n    M <- 60000\n    cl <- makeCluster(nbcores, type = \"PSOCK\")\n    out <- clusterCall(cl, myfunc, round(M/nbcores))\n    stopCluster(cl)\n    decision <- 0\n    for (clus in 1:nbcores) {\n        decision <- decision + out[[clus]]\n    }\n    print(decision/(round(M/nbcores)*nbcores))\n})   The process number (PID) of each computation node (core) in the cluster.   require(\"parallel\")\nSys.getpid()\ncl <- makeCluster(4,type=\"PSOCK\")\nout <- clusterCall(cl, Sys.getpid))",
            "title": "More power, speed"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#involve-the-graphical-card-for-more-power",
            "text": "Run computations with the graphical card, the GPU.  gputools  package.     REF: p.303-308",
            "title": "Involve the graphical card for more power"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-7-session-management",
            "text": "work, session, save, object, instruction, graphic, create, package",
            "title": "Chapter 7, Session Management"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#environment",
            "text": "globalenv() ; .GlobalEnv.  new.env() ; new environment with its own functions, variable, etc.  ls() ; list of objects in the environment.  objects() ; idem.  rm() ; remove one or more objects.  rm(list = ls()) ; remove all.  .RData; workspace file extension.  file.RData; R file.  save.image(\"file.RData\") ; save a R workspace to the current working directory. Several workspace can have their own objects.  load(\"file.RData\") ; load a R workspace from the current working directory.  load(file.choose()) ; open the current working directory.  getwd() ; get the current working directory.  setwd() ; set the current working directory.  .Rhistory; history file extension.  history() ; consult the R log.  savehistory() ; save the log to a file.  loadhistory() ; load the log from a file.  search() ; list of attached packages.  searchpaths() ;  list of paths.  library() ; list of packages in memory.  require(\"packages\") ,  library(\"packages\") ; load a package.  attach(data) ; attach a dataset to .GlobalEnv..  detach(data) .  ls(pos = 1) ,  ls() ; object list in database 1.  ls(pos = 2) ; object list in database 2.  ls(pos = match(\"package:datasets\", search())) ; list datasets.  ls(data) ; list object related to the dataset.  fix(data) ; open a spreadsheet with the data.  sink(file = \"sortie.txt\") ; save a file on the current working directory.  sink() ; stop the recording.",
            "title": "Environment"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#file-manipulation",
            "text": "Low-level interface to the computer\u2019s file system.  Create a file.  Execute the command.  Check out the result in the text files themselves.  file.create(\"sorty.txt\", showWarnings = TRUE) .  file.exists(\"sorty.txt\") .  file.remove(\"sorty.txt\") .  file.rename(\"sorty.txt\", \"sorti.txt\") .  file.append(\"sorty.txt\", \"sorti2.txt\") .  file.copy(\"sorty.txt\", \"sorti2.txt\") .  file.symlink(\"sorty.txt\", \"sorti2.txt\") .  file.path(\"sorty.txt\") .  file.show() .  list.files() .  unlink() .  basename() .  path.expand() .  list.files() .  file.exists() .  memory.size() .  memory.limit() .   REF: p.320-330",
            "title": "File manipulation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#memory-management",
            "text": "The KSysGuard software analyzes memory usage in real time. It is a task manager and performance monitor for UNIX-like OS.",
            "title": "Memory management"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#create-a-package",
            "text": "How to: build a list of R instruction, load a dataset, create function, and output the results.  Run a series of scripts in batch, run a script at a distance.  Set the PATH to an executable Rgui.exe.  Open a R script without opening R.  Create a runthis script, apply  chmod  to use and execute it. The bash script launch R commands (it must begin with #!/in/bash).  and much more.   REF: p.332-338",
            "title": "Create a package"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#part-2-mathematics-and-basic-statistics",
            "text": "",
            "title": "Part 2, Mathematics and Basic Statistics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-8-basic-mathematics-matrix-algebra-integration-optimization",
            "text": "",
            "title": "Chapter 8, Basic Mathematics, matrix algebra, integration, optimization"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#math-functions",
            "text": "REF: p.342",
            "title": "Math functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#matrix-calculation",
            "text": "A <- matrix(c(2,3,5,4), nrow = 2, ncol = 2)\nB <- matrix(c(1,2,2,7), nrow = 2, ncol = 2)\n\nA\n     [,1] [,2]\n[1,]    2    5\n[2,]    3    4\n\nB\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    7\n\nA + B\n     [,1] [,2]\n[1,]    3    7\n[2,]    5   11\n\nA - B\n     [,1] [,2]\n[1,]    1    3\n[2,]    1   -3\n\n# scalar multiplication\nA * B\n     [,1] [,2]\n[1,]    2   10\n[2,]    6   28\n\n# matrix multiplication\nA %*% B\n     [,1] [,2]\n[1,]   12   39\n[2,]   11   34\n\n# scalar multiplication\na <- 10\na * A\n     [,1] [,2]\n[1,]   20   50\n[2,]   30   40\n\n# transpose\nt(A)\n     [,1] [,2]\n[1,]    2    3\n[2,]    5    4\n\n# inverse\nsolve(B)\n           [,1]       [,2]\n[1,]  2.3333333 -0.6666667\n[2,] -0.6666667  0.3333333\n\nsolve(A) %*% B\n           [,1]      [,2]\n[1,]  0.8571429  3.857143\n[2,] -0.1428571 -1.142857\n\nt(A) * B\n     [,1] [,2]\n[1,]    2    6\n[2,]   10   28\n\nx <- seq(1,4)\ny <- seq(4,7)\n\nx\n[1] 1 2 3 4\ny\n[1] 4 5 6 7\n\nouter(x, y, FUN = \"*\")\n     [,1] [,2] [,3] [,4]\n[1,]    4    5    6    7\n[2,]    8   10   12   14\n[3,]   12   15   18   21\n[4,]   16   20   24   28\n\n# Kronecker\nkronecker(A, B)\n     [,1] [,2] [,3] [,4]\n[1,]    2    4    5   10\n[2,]    4   14   10   35\n[3,]    3    6    4    8\n[4,]    6   21    8   28\n\n# triangle\nlower.tri(A)\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,]  TRUE FALSE\n\nlower.tri(A, diag = TRUE)\n     [,1]  [,2]\n[1,] TRUE FALSE\n[2,] TRUE  TRUE\n\nlower.tri(A) * A\n     [,1] [,2]\n[1,]    0    0\n[2,]    3    0\n\n# diagonal\ndiag(A)\n[1] 2 4\n\nsum(diag(A))\n[1] 6\n\n# kappa\nkappa(A, exact = TRUE)\n[1] 7.582401\n\n# matrix reduction\nscale(A, scale = FALSE)\n     [,1] [,2]\n[1,] -0.5  0.5\n[2,]  0.5 -0.5\nattr(,\"scaled:center\")\n[1] 2.5 4.5\n\nscale(A, center = FALSE, scale = apply(A, 2, sd))\n         [,1]     [,2]\n[1,] 2.828427 7.071068\n[2,] 4.242641 5.656854\nattr(,\"scaled:scale\")\n[1] 0.7071068 0.7071068\n\n# eigenvalue\neigen(A)\n$values\n[1]  7 -1\n\n$vectors\n           [,1]       [,2]\n[1,] -0.7071068 -0.8574929\n[2,] -0.7071068  0.5144958\n\n# singular value vector\nsvd(A)\n$d\n[1] 7.285383 0.960828\n\n$u\n           [,1]       [,2]\n[1,] -0.7337222 -0.6794496\n[2,] -0.6794496  0.7337222\n\n$v\n           [,1]       [,2]\n[1,] -0.4812092  0.8766058\n[2,] -0.8766058 -0.4812092\n\n# Cholesky\nchol2inv(A) \n          [,1]     [,2]\n[1,]  0.640625 -0.15625\n[2,] -0.156250  0.06250\n\n# QR\nqr(A)\n$qr\n           [,1]      [,2]\n[1,] -3.6055513 -6.101702\n[2,]  0.8320503 -1.941451\n\n$rank\n[1] 2\n\n$qraux\n[1] 1.554700 1.941451\n\n$pivot\n[1] 1 2\n\nattr(,\"class\")\n[1] \"qr\"",
            "title": "Matrix calculation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#integral-calculus",
            "text": "integration  myf <- function(x) { exp(-x^2 / 2) / sqrt(2  *pi) }\n\nintegrate(myf, lower = -Inf, upper = Inf)$value\n[1] 1  Differential calculus  derivative  D(expression(sin(cos(x + y^2))), \"x\")\n-(cos(cos(x + y^2)) * sin(x + y^2))\n\nf <- deriv(<sub>x^2, \"x\", TRUE)\nf(3)\n[1] 9\nattr(,\"gradient\")\n     x\n[1,] 6   numDeriv  package.  grad() ; first-degree derivative.  hessian() ; second-degree derivative.  and many more.",
            "title": "Integral calculus"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#optimisation",
            "text": "linear, programming, constraints, min-max  # compute a 1-variable min, max (y <sub>x)\n\noptimize(function(x) { cos(x^2) }, lower = 0, upper = 2, maximum = FALSE)\n\nsource('</sub>/.active-rstudio-document', echo = TRUE)   nlm() ; compute a 2-variable min, max (z  x + y).  nlminb() ; add contraints on x and y, add parameters  upper  and  lower .  optim() .  constrOptim() .  and many more.   Unit root  unitroot(f=function(x) { cos(x^2) }, lower = 0,upper = 2,tol = 0.00001)$root\n\npolyroot(x(3, -8, 1)) # for p(x) = 3 - 8x + x\u00b2   cummax() .  cummin() .  cumprod() .  cumsum() .  and many more.",
            "title": "Optimisation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-9-descriptive-statistics",
            "text": "",
            "title": "Chapter 9, Descriptive Statistics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#factor-levels-labels",
            "text": "factor(c())  as.factor() .  is.factor() .  levels(var) <- c() .  labels(var) <- c() .  levels(var) ; output the levels.  labels(var) ; output the labels.  nlevels(var) ; output the number of levels.   mydata <- factor(mydata,\n    levels = c(1,2,3),\n    labels = c(\"red\", \"blue\", \"green\")\n    ) \n\nmydata <- ordered(mydata,\n    levels = c(1,3, 5),\n    labels = c(\"Low\", \"Medium\", \"High\")\n    )",
            "title": "Factor, levels, labels"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#names",
            "text": "names(var) <- c() ; add names to a vector, data frame, list.  colnames(var) <- c() ; idem.  rownames() ; left-most column.  dimnames() ; add names to an array.",
            "title": "Names"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#order",
            "text": "sort(vec, decreasing = TRUE) .  rev(vec) ; inverse sorting.  order(vec) ; sort a vector with names or a list of strings.  ordered(vec) .  as.ordered() .  is.ordered() .  and many more.   Consult keywords \u2018arithmetics\u2019 and \u2018random numbers\u2019, where ordering data is commonly used.",
            "title": "Order"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#convert-as_1",
            "text": "is.integer() .  as.integer() .  as.double() .  is.double() .  is.numeric() .  as.numeric() .  is.character() .  as.character() .  and many more.",
            "title": "Convert (as.)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#table-proportion-table",
            "text": "tabular, comparison, 2-dimensional, 2, two, dimensions   table(var1, var2) ; 2-dimensional view; cross table.  as.table(var) ; convert.  cut() ; divide the range into intervals.  table(cut(x, res$breaks, include.lowest = TRUE)) .    addmargins(x, FUN = sum, quiet = TRUE) ; add a column or row with sums, means, etc.  read.ftable() ; frequencies.  tablefreq <- mytable / sum(mytable) .  margin.table(tablefreq, 1) ; margin, right and bottom.  tablefreq[ ,ncol()] ; extract the total.  tablefreq[nrow(), ] ; extract the total.    prop.table(mytable, 1) ; percentage view; 1, row sum = 100%.  prop.table(mytable, 2) ; percentage view; 2, row sum = 100%.  which.max(table) ; find the max, min, mean, etc.",
            "title": "Table, proportion table"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#descriptive-statistics",
            "text": "mean(x) .  median(x) .  quantile(x, probs = c(0.1, 0.9)) .  probs = 1:10 / 10 .    max(x) .  min(x) .  diff(range(x)) .  IQR(x) .  var.pop(x) ,  var(x) .  sd.pop(x) ,  sd(x) .  co.var(x) .  mad(x) ; absolute deviation from the median.  mean(abs(x - mean(x))) .    skew(x) .  kurt(x) .  chisq.test() .  round() .  sum() .  nrow() .  ncol() .  cor(var1, var2) .  method = \"kendall\", \"spearman\"    rank() .  rgrs  package .  cramer.v() .     REF: p.378-379",
            "title": "Descriptive statistics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#graphic-descriptive-statistics",
            "text": "plot() .  dotchart(table(), col = c(\"\", \"\", \"\" ,\"\") ,pch = , main = , lcolor = ) .  barplot() .  barplot(var, col = , pareto = TRUE) .  barplot(sort(table(var)), TRUE)) .  barplot(xlim = , width = , space = , names.arg = , legend = , density = , ylab = , lwd = ) .  pie() .  points(barplot(), cumsum(var), type=\"l\") .  boxplot() .  stem() .  aplpack  package.  stem.left()    hist() .  segments() .   REF: p.392-394, 397-398, 400-410",
            "title": "Graphic descriptive statistics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-10-random-variables-laws-and-simulation",
            "text": "x %% m ; modulo or modulus.",
            "title": "Chapter 10, Random Variables, Laws, and Simulation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#randomness",
            "text": "runif(1) ; generate a pseudo-random number between 0 and 1.  set.seed() ; \u2018shuffle the dice!\u2019.  x <- function() { runif(1) } ; generate random numbers, following the uniform distribution.  rnorm(1) ; generate a random numbers, following the normal distribution.  generate random numbers with a randomness function and  curve() .   x <- function() { rnorm(1, 7, 1) }\n# avg = 7, sd = 1, 1 obs\n\ncurve(rnorm(x, 7, 1), xlim = c(-1,10))\n\nplot(density(rnorm(1000, 7, 1)),xlim = c(-1, 10), main=\"Density Curve\")  REF: p.422-423  mean(runif(1))\n[1] 0.6586903\n\nmean(runif(10))\n[1] 0.5196868\n\nmean(runif(100))\n[1] 0.5345603\n\nmean(runif(1000))\n[1] 0.5042301\n\nmean(runif(10000))\n[1] 0.5021896\n\n# getting closer to 0.5 as the sample increases   ConvergenceConcepts  package to view the law of great numbers.   Dice  REF: p.430-431  Bootstrap  REF: p.436  Laws  REF: p.437-447",
            "title": "Randomness"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-11-confidence-intervals-and-hypothesis-testing",
            "text": "",
            "title": "Chapter 11, Confidence Intervals and Hypothesis Testing"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#confidence-intervals",
            "text": "Use t-values with at least 30 observation in the sample.  With smaller samples (or larger too), use bootstraps to simulate populations from the  boot  package ( boot()  and  boot.ci() ).  For proportion with large samples, use the  epitools  package and  binom.approx() . With smaller samples, go with  binom.test() .  Variance confidence intervals; test for normality with  sigma2.test() .  For non-parametric sample, again, simulate populations with  boot()  and  boot.ci() .  For median, use  qbinom() .  For correlation, use  cor.test() .   REF: p.450-456",
            "title": "Confidence intervals"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#test",
            "text": "In a test,  \\alpha  is the signification threshold,  H_0  is the tested hypothesis.  Average tests;  compare the theoretical average to a reference value with the  t.test() .  Compare two theoretical averages with a  t.test() .  Compare pair samples with  t.test(paired=TRUE) .  Test variance(s) with ANOVA.  Compare a theoretical variance with a reference value with  sigma2.test() .  Compare two theoretical variances with  var.test() .  Compare a theoretical proportion with a reference value with  prop.test() .  Compare two theoretical proportions with  prop.test( ).  Test the theoretical correlation coefficient vs a reference value with  cor.test()  and  cor-.test() .  Test two theoretical correlation coefficients vs a reference value with  cor.test.2.sample() .  Test of independence or chi\u00b2 with  chisq.test() .  Yates chi\u00b2, adjustment chi\u00b2 with  chisq.test() .  Fisher test with  fisher.test() .  Adequacy test or Shapiro-Walk test with  shapiro.test() .  Positional test or sign test or, median sign test with  prop.test()  and  binom.test() .  Median sign test for two independent samples with  chisq.test()  and  fisher.test() .  Sign test for two matching samples with  prop.test()  and  binom.test() .  Wilcoxon rank test or Mann-Whitney test for two independent samples with  wilcox.test() .  Wilcoxon test for two matching samples with  wilcox.test() .   REF: p.459-488",
            "title": "Test"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-12-simple-and-multiple-linear-regression",
            "text": "",
            "title": "Chapter 12, Simple and Multiple Linear Regression"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#regression",
            "text": "lm(y <sub>x .  lm(y <sub>0 + x) ; no intercept.  model <- lm(y <sub>x) ; run the regression.  summary(model) ; extract the results.  plot(y <sub>x) ; plot the results.  abline(model) ; add a line on the observations.  confint(model) ; confidence intervals; 95% or 2.5% on both sides.  coefficients(model) ; extract one or several coefficient.  model$coefficients .  model$call .  model$residuals .  anova(model) .  predict(model, data.frame(LWT = prediction), interval = \"prediction\")  and many more.   prediction, result, extraction, residual  REF: p.498-499",
            "title": "Regression"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#normality",
            "text": "histogram, test, residual, quantile-quantile, quantile, qq    par(mfrow=c(1,2))\nhist(residuals(model), main = \"Histogram\")   qqnorm(resid(model), datax = TRUE) ; quantile-quantile.  qqplot() .  qqline() .  plot(model, 1:6, col.smooth = \"red\") ; 6 graphics.  jarque.bera.test(residuals(model)) ; from the  tseries  package.  dwtest() ; Durbin-Watson test from the  lmtest  package.   REF: p.502-503",
            "title": "Normality"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#correlation",
            "text": "test, explanatory variable interaction, colinearity,  best subset   pairs(newdata, lower.panel = panel.smooth, upper.panel = add.cor) .   REF: p.506",
            "title": "Correlation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#model-improvement",
            "text": "Variables selection.  Best subset, leaps and bounds.  Forward selection.  Backward selection.  Stepwise selection.  Residual analysis.  and many more.   REF: p.511-535",
            "title": "Model improvement"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#polynomial-regression",
            "text": "REF: p.535-540",
            "title": "Polynomial regression"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-13-elementary-variance-analysis",
            "text": "anova, repeated measure, between, within, inspection, hypothesis, comparison, factor, table, parameter, repeated  REF: p.542-571",
            "title": "Chapter 13, Elementary Variance Analysis"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#appendix-installing-the-r-software-and-packages",
            "text": "installation, package  install.packages('package') .   Be sure to launch RStudio as an administrator or a superuser to install a package in R (not in RStudio); the package is accessible to all users. Then load the package in R or RStudio.  Then attach the package to the work session with  library('package')  or  require('packages') .",
            "title": "Appendix, Installing the R Software and Packages"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#answers-to-the-exercises",
            "text": "REF:  p.625-674",
            "title": "Answers to the Exercises"
        },
        {
            "location": "/R_CS/",
            "text": "CONTENT\n\n\nBasics\n\n\nAdvanced\n\n\nRStudio & RMarkdown\n\n\nSpecialization\n\n\ndplyr & tidyr\n\n\ndata.table\n\n\nRegular Expressions\n\n\nR-Quandl\n\n\nSpark\n\n\nTime series\n\n\nSurvival, regression, data mining, machine learning\n\n\n\n\n\n\nVisualization (ggplot2, ggmap, ggvis, shiny)\n\n\nCharts\n\n\nParameters\n\n\nGraphs\n\n\nColors\n\n\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\nCheat sheets.\n\n\n\n\nBasics\n\u00b6\n\n\n\n\nBase R\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data\n. PDF only.\n\n\n\n\n\n\nData Import\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControl Structures\n. PDF.\n\n\n\n\n\n\n\n\n\n\nAdvanced\n\u00b6\n\n\n\n\nAdvanced R\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio & RMarkdown\n\u00b6\n\n\n\n\nR Markdown Reference Guide\n. PDF only.\n\n\nsyntax, chunk options, pandoc options, slide formats, LaTeX options\n\n\n\n\n\n\nR Markdown\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nSpecialization\n\u00b6\n\n\ndplyr & tidyr\n\u00b6\n\n\n\n\nGrammar of dplyr\n. PDF only (explanatory slides).\n\n\ndplyr\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndplyr & tidyr\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\ndata.table\n\u00b6\n\n\n\n\ndata.table Intro\n. PDF only (explanatory article).\n\n\ndata.table\n. PDF.\n\n\n\n\n\n\n\n\n\n\nRegular Expressions\n\u00b6\n\n\n\n\nBasic Regular Expressions\n. PDF.\n\n\n\n\n\n\n\n\n\n\nR-Quandl\n\u00b6\n\n\n\n\nQuandl\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpark\n\u00b6\n\n\n\n\nsparklyr\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nTime series\n\u00b6\n\n\n\n\nTime Series\n. PDF only.\n\n\ninput, decomposition, tests, stochastic, graphics, miscellaneous\n\n\n\n\n\n\nxts\n. PDF only.\n\n\nTime Series in R \u2013 The Power of \nxts\n and \nzoo\n. HTML.\n\n\n\n\nSurvival, regression, data mining, machine learning\n\u00b6\n\n\n\n\nsurvminer\n. PDF only.\n\n\ncurve, ggplot2, cox model\n\n\n\n\n\n\nRegressions\n. PDF only.\n\n\nlinear model, variable selection, diagnostics, graphics, tests, variable transformation, ridge, segmented, gls, glm, nls, gnls, loess, splines, robust, structural equation, simultaneous equation, pls, principal components, quantile, linear and nonlinear mixed effects, generalized additive, survival analysis, classification & regression trees, beta\n\n\n\n\n\n\nData Mining\n. PDF only.\n\n\nassociation rules, sequential patterns, classification & prediction, regression, clustering, outliers, time series, text mining, socila networks, graph mining, spatial data, statistics, graphics, data manipulation, data access, big data, parallel computing, reports, weka, editors, guis\n\n\n\n\n\n\nBig Data Machine Learning\n. PDF only.\n\n\nlinear regression, logistic regression, regularization (ridge, lasso), neural network, support vector machine, nayesian network and na\u00efve bayes, k-nearest neighbors, decision tree, tree ensembles (bagging or random forest, boosting)\n\n\n\n\n\n\n\n\nVisualization (ggplot2, ggmap, ggvis, shiny)\n\u00b6\n\n\n\n\nggplot2\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggmap\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggvis\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggvis\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshiny\n.\n\n\n\n\n\n\n\n\n\n\n\n\nCharts\n\u00b6\n\n\n\n\n\n\n\n\nParameters\n\u00b6\n\n\nLines\n\n\n\n\n\n\n\n\nPoints\n\n\n\n\n\n\n\n\nGraphs\n\u00b6\n\n\n\n\nGraphs\n.\n\n\n\n\n\n\n\n\n\n\nColors\n\u00b6",
            "title": "R Cheat Sheets"
        },
        {
            "location": "/R_CS/#advanced",
            "text": "Advanced R . PDF.",
            "title": "Advanced"
        },
        {
            "location": "/R_CS/#rstudio-rmarkdown",
            "text": "R Markdown Reference Guide . PDF only.  syntax, chunk options, pandoc options, slide formats, LaTeX options    R Markdown . PDF.         RStudio . PDF.",
            "title": "RStudio &amp; RMarkdown"
        },
        {
            "location": "/R_CS/#specialization",
            "text": "",
            "title": "Specialization"
        },
        {
            "location": "/R_CS/#dplyr-tidyr",
            "text": "Grammar of dplyr . PDF only (explanatory slides).  dplyr . PDF.         dplyr & tidyr . PDF.",
            "title": "dplyr &amp; tidyr"
        },
        {
            "location": "/R_CS/#datatable",
            "text": "data.table Intro . PDF only (explanatory article).  data.table . PDF.",
            "title": "data.table"
        },
        {
            "location": "/R_CS/#regular-expressions",
            "text": "Basic Regular Expressions . PDF.",
            "title": "Regular Expressions"
        },
        {
            "location": "/R_CS/#r-quandl",
            "text": "Quandl .",
            "title": "R-Quandl"
        },
        {
            "location": "/R_CS/#spark",
            "text": "sparklyr . PDF.",
            "title": "Spark"
        },
        {
            "location": "/R_CS/#time-series",
            "text": "Time Series . PDF only.  input, decomposition, tests, stochastic, graphics, miscellaneous    xts . PDF only.  Time Series in R \u2013 The Power of  xts  and  zoo . HTML.",
            "title": "Time series"
        },
        {
            "location": "/R_CS/#survival-regression-data-mining-machine-learning",
            "text": "survminer . PDF only.  curve, ggplot2, cox model    Regressions . PDF only.  linear model, variable selection, diagnostics, graphics, tests, variable transformation, ridge, segmented, gls, glm, nls, gnls, loess, splines, robust, structural equation, simultaneous equation, pls, principal components, quantile, linear and nonlinear mixed effects, generalized additive, survival analysis, classification & regression trees, beta    Data Mining . PDF only.  association rules, sequential patterns, classification & prediction, regression, clustering, outliers, time series, text mining, socila networks, graph mining, spatial data, statistics, graphics, data manipulation, data access, big data, parallel computing, reports, weka, editors, guis    Big Data Machine Learning . PDF only.  linear regression, logistic regression, regularization (ridge, lasso), neural network, support vector machine, nayesian network and na\u00efve bayes, k-nearest neighbors, decision tree, tree ensembles (bagging or random forest, boosting)",
            "title": "Survival, regression, data mining, machine learning"
        },
        {
            "location": "/R_CS/#visualization-ggplot2-ggmap-ggvis-shiny",
            "text": "ggplot2 . PDF.         ggmap .         ggvis . PDF.        ggvis .         shiny .",
            "title": "Visualization (ggplot2, ggmap, ggvis, shiny)"
        },
        {
            "location": "/R_CS/#charts",
            "text": "",
            "title": "Charts"
        },
        {
            "location": "/R_CS/#parameters",
            "text": "Lines     Points",
            "title": "Parameters"
        },
        {
            "location": "/R_CS/#graphs",
            "text": "Graphs .",
            "title": "Graphs"
        },
        {
            "location": "/R_CS/#colors",
            "text": "",
            "title": "Colors"
        },
        {
            "location": "/Intermediate_R/",
            "text": "1, Conditionals and Control Flow\n\n\nEquality (or not)\n\n\n&\n and \n|\n\n\nThe \nif\n statement (and more)\n\n\n\n\n\n\n2, Loops\n\n\nWrite a \nwhile\n loop\n\n\nWrite a \nfor\n loop\n\n\nMix up loops with control flow\n\n\n\n\n\n\n3, Functions\n\n\nFunction documentation\n\n\nUse a function\n\n\nFunctions inside functions\n\n\nWrite your own function\n\n\nR you functional?\n\n\nLoad an R package\n\n\n\n\n\n\n4, The \napply\n Family\n\n\nUse \nlapply\n (with a built-in R function)\n\n\nUse \nsapply\n\n\nUse \nvapply\n\n\nApply your knowledge. Or better yet: \nsapply\n it?\n\n\n\n\n\n\n5, Utilities\n\n\nMathematical utilities\n\n\nFind the error\n\n\nData utilities\n\n\nBeat Gauss using R\n\n\ngrepl\n & \ngrep\n (and the likes)\n\n\nTime is of the essence\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, Conditionals and Control Flow\n\u00b6\n\n\nEquality (or not)\n\u00b6\n\n\n# Comparison of logicals\nTRUE == FALSE\n\n\n\n\n## [1] FALSE\n\n\n\n# Comparison of numerics\n(-6 * 14) != (17 - 101)\n\n\n\n\n## [1] FALSE\n\n\n\n# Comparison of character strings\n'useR' == 'user'\n\n\n\n\n## [1] FALSE\n\n\n\n# Compare a logical with a numeric\nTRUE == 1\n\n\n\n\n## [1] TRUE\n\n\n\nGreater and less than\n\n\n# Comparison of numerics\n(-6*5 + 2) >= (-10 + 1)\n\n\n\n\n## [1] FALSE\n\n\n\n# Comparison of character strings\n'raining' <= 'raining dogs'\n\n\n\n\n## [1] TRUE\n\n\n\n# Comparison of logicals\nTRUE > FALSE\n\n\n\n\n## [1] TRUE\n\n\n\nCompare vectors\n\n\n# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Popular days\nlinkedin > 15\n\n\n\n\n## [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n# Quiet days\nlinkedin <= 5\n\n\n\n\n## [1] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\n\n\n# LinkedIn more popular than Facebook\nlinkedin > facebook\n\n\n\n\n## [1] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n\n\n\nCompare matrices\n\n\nviews <- matrix(c(linkedin, facebook), nrow = 2, byrow = TRUE)\n\n# When does views equal 13?\nviews == 13\n\n\n\n\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n# When is views less than or equal to 14?\nviews <= 14\n\n\n\n\n##       [,1] [,2] [,3]  [,4] [,5]  [,6] [,7]\n## [1,] FALSE TRUE TRUE  TRUE TRUE FALSE TRUE\n## [2,] FALSE TRUE TRUE FALSE TRUE  TRUE TRUE\n\n\n\n# How often does facebook equal or exceed linkedin times two?\nsum(facebook >= linkedin * 2)\n\n\n\n\n## [1] 2\n\n\n\n&\n and \n|\n\u00b6\n\n\n# The linkedin and last variable\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nlast <- tail(linkedin, 1)\n\n# Is last under 5 or above 10?\nlast < 5 | last > 10\n\n\n\n\n## [1] TRUE\n\n\n\n# Is last between 15 (exclusive) and 20 (inclusive)?\nlast > 15 & last <= 20\n\n\n\n\n## [1] FALSE\n\n\n\n# Is last between 0 and 5 or between 10 and 15?\n(last > 0 & last < 5) | (last > 10 & last < 15)\n\n\n\n\n## [1] TRUE\n\n\n\n&\n and \n|\n (2)\n\n\n# linkedin exceeds 10 but facebook below 10\nlinkedin > 10 & facebook < 10\n\n\n\n\n## [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n# When were one or both visited at least 12 times?\nlinkedin >= 12 | facebook >= 12\n\n\n\n\n## [1]  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n\n\n\n# When is views between 11 (exclusive) and 14 (inclusive)?\nviews > 11 & views <= 14\n\n\n\n\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6] [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE TRUE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE TRUE\n\n\n\nBlend it all together\n\n\n# Select the second column, named day2, from li_df: second\nsecond <- li_df$day2\n\n# Build a logical vector, TRUE if value in second is extreme: extremes\nextremes <- (second > 25 | second < 5)\n\n# Count the number of TRUEs in extremes\nsum(extremes)\n\n\n\n\n## [1] 16\n\n\n\nThe \nif\n statement (and more)\n\u00b6\n\n\n# Variables related to your last day of recordings\nmedium <- 'LinkedIn'\nnum_views <- 14\n\n# Examine the if statement for medium\nif (medium == 'LinkedIn') {\n  print('Showing LinkedIn information')\n}\n\n\n\n\n## [1] \"Showing LinkedIn information\"\n\n\n\n# Write the if statement for num_views\nif (num_views > 15) {\n    print('You\\'re popular!')\n}\n\n\n\n\nAdd an \nelse\n\n\n# Variables related to your last day of recordings\nmedium <- 'LinkedIn'\nnum_views <- 14\n\n# Control structure for medium\nif (medium == 'LinkedIn') {\n  print('Showing LinkedIn information')\n} else {\n    print('Unknown medium')\n}\n\n\n\n\n## [1] \"Showing LinkedIn information\"\n\n\n\n# Control structure for num_views\nif (num_views > 15) {\n  print('You\\'re popular!')\n} else {\n    print('Try to be more visible!')\n}\n\n\n\n\n## [1] \"Try to be more visible!\"\n\n\n\nCustomize further: \nelse if\n\n\n# Variables related to your last day of recordings\nmedium <- 'LinkedIn'\nnum_views <- 14\n\n# Control structure for medium\nif (medium == 'LinkedIn') {\n  print('Showing LinkedIn information')\n} else if (medium == 'Facebook') {\n  print('Showing Facebook information')\n} else {\n  print('Unknown medium')\n}\n\n\n\n\n## [1] \"Showing LinkedIn information\"\n\n\n\n# Control structure for num_views\nif (num_views > 15) {\n  print('You\\'re popular!')\n} else if (num_views > 10 | num_views <= 15) {\n  print('Your number of views is average')\n} else {\n  print('Try to be more visible!')\n}\n\n\n\n\n## [1] \"Your number of views is average\"\n\n\n\nTake control!\n\n\n# Variables related to your last day of recordings\nli <- 15\nfb <- 9\n\n# Code the control-flow construct\nif (li >= 15 & fb >= 15) {\n    sms <- 2*(li + fb)\n} else if (li < 10 & fb < 10) {\n    sms <- (li + fb)/2\n} else {\n    sms <- li + fb\n}\n\n# Print the resulting sms to the console\nsms\n\n\n\n\n## [1] 24\n\n\n\n2, Loops\n\u00b6\n\n\nWrite a \nwhile\n loop\n\u00b6\n\n\n# Initialize the speed variable\nspeed <- 64\n\n# Code the while loop\nwhile (speed > 30) {\n  print('Slow down!')\n  speed <- speed - 7\n}\n\n\n\n\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n\n\n\n# Print out the speed variable\nspeed\n\n\n\n\n## [1] 29\n\n\n\nThrow in more conditionals\n\n\n# Initialize the speed variable\nspeed <- 64\n\n# Extend/adapt the while loop\nwhile (speed > 30) {\n  print(paste('Your speed is ', speed))\n  if (speed > 48) {\n    print('Slow down big time!')\n    speed <- speed - 11\n  } else {\n        print('Slow down!')\n        speed <- speed - 6\n  }\n}\n\n\n\n\n## [1] \"Your speed is  64\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  53\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  42\"\n## [1] \"Slow down!\"\n## [1] \"Your speed is  36\"\n## [1] \"Slow down!\"\n\n\n\nStop the \nwhile\n loop: \nbreak\n\n\n# Initialize the speed variable\nspeed <- 88\nwhile (speed > 30) {\n  print(paste('Your speed is',speed))\n    # Break the while loop when speed exceeds 80\n  if (speed > 80) {\n    break\n  } else if (speed > 48) {\n    print('Slow down big time!')\n    speed <- speed - 11\n  } else {\n    print('Slow down!')\n    speed <- speed - 6\n  }\n}\n\n\n\n\n## [1] \"Your speed is 88\"\n\n\n\nBuild a \nwhile\n loop from scratch\n\n\nstrsplit\n; split up in a vector that contains separate letters.\n\n\n# Initialize i\ni <- 1\n\n# Code the while loop\nwhile (i <= 10) {\n  print(i * 3)\n  if ( (i * 3) %% 8 == 0) {\n    break\n  }\n  i <- i + 1\n}\n\n\n\n\n## [1] 3\n## [1] 6\n## [1] 9\n## [1] 12\n## [1] 15\n## [1] 18\n## [1] 21\n## [1] 24\n\n\n\nWrite a \nfor\n loop\n\u00b6\n\n\nLoop over a vector\n\n\n# The linkedin vector\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\n\n# Loop version 1\nfor (lin in linkedin) {\n    print(lin)\n}\n\n\n\n\n## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14\n\n\n\n# Loop version 2\nfor (i in 1:length(linkedin)) {\n    print(linkedin[i])\n}\n\n\n\n\n## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14\n\n\n\nLoop over a list\n\n\n[[]]\n; list of list.\n\n\n# The nyc list is already specified\nnyc <- list(pop = 8405837, \n            boroughs = c('Manhattan', 'Bronx', 'Brooklyn', 'Queens', 'Staten Island'), \n            capital = FALSE)\n\n# Loop version 1\nfor (item in nyc) {\n    print(item)\n}\n\n\n\n\n## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE\n\n\n\n# Loop version 2\nfor (i in 1:length(nyc)) {\n    print(nyc[[i]])\n}\n\n\n\n\n## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE\n\n\n\nLoop over a matrix\n\n\n# The tic-tac-toe matrix has already been defined for you\nttt <- matrix(c('O', NA, 'X', NA, 'O', NA, 'X', 'O', 'X'), nrow = 3, ncol = 3)\n\n# define the double for loop\nfor (i in 1:nrow(ttt)) {\n    for (j in 1:ncol(ttt)) {\n    print(paste('On row', i,'and column', j,'the board contains ', ttt[i,j]))\n    }\n}\n\n\n\n\n## [1] \"On row 1 and column 1 the board contains  O\"\n## [1] \"On row 1 and column 2 the board contains  NA\"\n## [1] \"On row 1 and column 3 the board contains  X\"\n## [1] \"On row 2 and column 1 the board contains  NA\"\n## [1] \"On row 2 and column 2 the board contains  O\"\n## [1] \"On row 2 and column 3 the board contains  O\"\n## [1] \"On row 3 and column 1 the board contains  X\"\n## [1] \"On row 3 and column 2 the board contains  NA\"\n## [1] \"On row 3 and column 3 the board contains  X\"\n\n\n\nMix up loops with control flow\n\u00b6\n\n\n# The linkedin vector\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\n\n# Code the for loop with conditionals\nfor (i in 1:length(linkedin)) {\n    if (linkedin[i] > 10) {\n        print('You\\'re popular!')\n    } else {\n        print('Be more visible!')\n    }\n    print(linkedin[i])\n}\n\n\n\n\n## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] 2\n## [1] \"You're popular!\"\n## [1] 17\n## [1] \"You're popular!\"\n## [1] 14\n\n\n\nNext, you break it\n\n\n# The linkedin vector\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\n\n# Extend the for loop\nfor (li in linkedin) {\n  if (li > 10) {\n    print('You\\'re popular!')\n  } else {\n    print('Be more visible!')\n  }\n    # Add code to conditionally break iteration\n  if (li > 16) {\n    print('This is ridiculous, I\\'m outta here!')\n    break\n  }\n  # Add code to conditionally skip iteration\n  if (li < 5) {\n    print('This is too embarrassing!')\n    next\n    }\n  print(li)\n}\n\n\n\n\n## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] \"This is too embarrassing!\"\n## [1] \"You're popular!\"\n## [1] \"This is ridiculous, I'm outta here!\"\n\n\n\nBuild a for loop from scratch\n\n\n# Pre-defined variables\nrquote <- 'R\\'s internals are irrefutably intriguing'\n\nchars <- strsplit(rquote, split = '')[[1]]\nrcount <- 0\n\n# Your solution here\nfor (i in 1:length(chars)) {\n    if (chars[i] == 'u') {\n    break\n    }\n    if (chars[i] == 'r' | chars[i] == 'R') {\n        rcount <- rcount + 1\n    }\n}\n\n# Print the resulting rcount variable to the console\nprint(rcount)\n\n\n\n\n## [1] 5\n\n\n\n3, Functions\n\u00b6\n\n\nFunction documentation\n\u00b6\n\n\n# Consult the documentation on the mean() function\n?mean\n\n# Inspect the arguments of the mean() function\nargs(mean)\n\n\n\n\nUse a function\n\u00b6\n\n\n# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Calculate average number of views\navg_li <- mean(linkedin)\navg_fb <- mean(facebook)\n\n# Inspect avg_li and avg_fb\nprint(avg_li)\n\n\n\n\n## [1] 10.85714\n\n\n\nprint(avg_fb)\n\n\n\n\n## [1] 11.42857\n\n\n\navg_li\n\n\n\n\n## [1] 10.85714\n\n\n\n# Calculate the mean of linkedin minus facebook\nprint(mean(linkedin - facebook))\n\n\n\n\n## [1] -0.5714286\n\n\n\nUse a function (2)\n\n\n# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Calculate the mean of the sum\navg_sum <- mean(linkedin + facebook)\n\n# Calculate the trimmed mean of the sum\navg_sum_trimmed <- mean((linkedin + facebook), trim = 0.2)\n\n# Inspect both new variables\navg_sum\n\n\n\n\n## [1] 22.28571\n\n\n\navg_sum_trimmed\n\n\n\n\n## [1] 22.6\n\n\n\nUse a function (3)\n\n\n# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, NA, 17, 14)\nfacebook <- c(17, NA, 5, 16, 8, 13, 14)\n\n# Basic average of linkedin\nprint(mean(linkedin))\n\n\n\n\n## [1] NA\n\n\n\n# Advanced average of facebook\nprint(mean(facebook, na.rm = TRUE))\n\n\n\n\n## [1] 12.16667\n\n\n\nFunctions inside functions\n\u00b6\n\n\n# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, NA, 17, 14)\nfacebook <- c(17, NA, 5, 16, 8, 13, 14)\n\n# Calculate the mean absolute deviation\nmean((abs(linkedin - facebook)), na.rm = TRUE)\n\n\n\n\n## [1] 4.8\n\n\n\nWrite your own function\n\u00b6\n\n\n# Create a function pow_two()\npow_two <- function(arg1) {\n    arg1^2\n}\n\n# Use the function \npow_two(12)\n\n\n\n\n## [1] 144\n\n\n\n# Create a function sum_abs()\nsum_abs <- function(arg2,arg3) {\n    abs(arg2) + abs(arg3)\n}\n\n# Use the function\nsum_abs(-2,3)\n\n\n\n\n## [1] 5\n\n\n\nWrite your own function (2)\n\n\n# Define the function hello()\nhello <- function() {\n    print('Hi there!')\n    return(TRUE)\n}\n\n# Call the function hello()\nhello()\n\n\n\n\n## [1] \"Hi there!\"\n\n## [1] TRUE\n\n\n\n# Define the function my_filter()\nmy_filter <- function(arg1) {\n    if (arg1 > 0) {\n        return(arg1)\n    } else {\n        return(NULL)\n    }\n}\n\n# Call the function my_filter() twice\nmy_filter(5)\n\n\n\n\n## [1] 5\n\n\n\nmy_filter(-5)\n\n\n\n\n## NULL\n\n\n\nWrite your own function (3)\n\n\nVariables inside a function are not in the Global Environment.\n\n\n# Extend the pow_two() function\npow_two <- function(x, print_info = TRUE) {\n  y <- x ^ 2\n  if (print_info) {\n    print(paste(x, 'to the power two equals', y))\n  }\n  return(y)\n}\n\n#pow_two(2)\npow_two(2, FALSE)\n\n\n\n\n## [1] 4\n\n\n\nR you functional?\n\u00b6\n\n\n# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, NA, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Define the interpret function\ninterpret <- function(arg) {\n    if (arg > 15) {\n        print('You\\'re popular!')\n        return(arg)\n    } else {\n        print('Try to be more visible!')\n        return(0)\n    }\n}\n\ninterpret(linkedin[1])\n\n\n\n\n## [1] \"You're popular!\"\n\n## [1] 16\n\n\n\ninterpret(facebook[2])\n\n\n\n\n## [1] \"Try to be more visible!\"\n\n## [1] 0\n\n\n\nR you functional? (2)\n\n\n# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# The interpret() can be used inside interpret_all()\ninterpret <- function(num_views){\n  if (num_views > 15) {\n    print('You\\'re popular!')\n    return(num_views)\n  } else {\n    print('Try to be more visible!')\n    return(0)\n  }\n}\n\n# Define the interpret_all() function\ninterpret_all <- function(data, logi = TRUE){\n  yy <- 0\n  for (i in data) {\n    yy <- yy + interpret(i)\n  }\n  if (logi) {\n    return(yy)\n  } else {\n    return(NULL)\n  }\n}\n\n# Call the interpret_all() function on both linkedin and facebook\ninterpret_all(linkedin)\n\n\n\n\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33\n\n\n\ninterpret_all(facebook)\n\n\n\n\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33\n\n\n\nLoad an R package\n\u00b6\n\n\n# The mtcars vectors have already been prepared for you\nwt <- mtcars$wt\nhp <- mtcars$hp\n\n# Request the currently attached packages\nsearch()\n\n\n\n\n##  [1] \".GlobalEnv\"            \"package:XLConnect\"    \n##  [3] \"package:XLConnectJars\" \"package:stats\"        \n##  [5] \"package:graphics\"      \"package:grDevices\"    \n##  [7] \"package:utils\"         \"package:datasets\"     \n##  [9] \"package:methods\"       \"Autoloads\"            \n## [11] \"package:base\"\n\n\n\n# Try the qplot() function with wt and hp\nplot(wt,hp)\n\n# Load the ggplot2 package\nlibrary('ggplot2')\n\n\n\n\n\n\n# or\nrequire('ggplot2')\n\n# Retry the qplot() function\nqplot(wt,hp)\n\n\n\n\n\n\n# Check out the currently attached packages again\nsearch()\n\n\n\n\n##  [1] \".GlobalEnv\"            \"package:ggplot2\"      \n##  [3] \"package:XLConnect\"     \"package:XLConnectJars\"\n##  [5] \"package:stats\"         \"package:graphics\"     \n##  [7] \"package:grDevices\"     \"package:utils\"        \n##  [9] \"package:datasets\"      \"package:methods\"      \n## [11] \"Autoloads\"             \"package:base\"\n\n\n\n4, The \napply\n Family\n\u00b6\n\n\nUse \nlapply\n (with a built-in R function)\n\u00b6\n\n\n# The vector pioneers\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\n# Split names from birth year: split_math\nsplit_math <- strsplit(pioneers, ':')\n\n# Convert to lowercase strings: split_low\nsplit_low <- lapply(split_math,tolower)\n\n# Take a look at the structure of split_low\nstr(split_low)\n\n\n\n\n## List of 4\n##  $ : chr [1:2] \"gauss\" \"1777\"\n##  $ : chr [1:2] \"bayes\" \"1702\"\n##  $ : chr [1:2] \"pascal\" \"1623\"\n##  $ : chr [1:2] \"pearson\" \"1857\"\n\n\n\nUse \nlapply\n with your own function\n\n\n# Code from previous exercise\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\nsplit <- strsplit(pioneers, split = ':')\nsplit_low <- lapply(split, tolower)\n\n# Write function select_first()\nselect_first <- function(x) {\n    return(x[1])\n}\n\n# Apply select_first() over split_low: names\nnames <- lapply(split_low, select_first)\nprint(names)\n\n\n\n\n## [[1]]\n## [1] \"gauss\"\n## \n## [[2]]\n## [1] \"bayes\"\n## \n## [[3]]\n## [1] \"pascal\"\n## \n## [[4]]\n## [1] \"pearson\"\n\n\n\n# Write function select_second()\nselect_second <- function(x) {\n    return(x[2])\n}\n\n# Apply select_second() over split_low: years\nyears <- lapply(split_low, select_second)\nprint(years)\n\n\n\n\n## [[1]]\n## [1] \"1777\"\n## \n## [[2]]\n## [1] \"1702\"\n## \n## [[3]]\n## [1] \"1623\"\n## \n## [[4]]\n## [1] \"1857\"\n\n\n\nlapply\n and anonymous functions\n\n\nAnonymous function == lambda function.\n\n\n# Definition of split_low\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\nsplit <- strsplit(pioneers, split = ':')\nsplit_low <- lapply(split, tolower)\n\n#select_first <- function(x) {\n#  x[1]\n#}\n\nnames <- lapply(split_low, function(x) { x[1] })\n\n#select_second <- function(x) {\n#  x[2]\n#}\n\nyears <- lapply(split_low, function(x) { x[2] })\n\n\n\n\nUse \nlapply\n with additional arguments\n\n\n# Definition of split_low\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\nsplit <- strsplit(pioneers, split = ':')\nsplit_low <- lapply(split, tolower)\n\n# Replace the select_*() functions by a single function: select_el\nselect_el <- function(x, i) { \n  x[i] \n}\n\n#select_second <- function(x) { \n#  x[2] \n#}\n\n# Call the select_el() function twice on split_low: names and years\nnames <- lapply(split_low, select_el, i=1)\nyears <- lapply(split_low, select_el, 2)\n\n\n\n\nUse \nsapply\n\u00b6\n\n\ntemp1 <- c(3, 7, 9, 6, -1)\ntemp2 <- c(6, 9, 12, 13, 5)\ntemp3 <- c(4, 8, 3, -1, -3)\ntemp4 <- c(1, 4, 7, 2, -2)\ntemp5 <- c(5, 7, 9, 4, 2)\ntemp6 <- c(-3, 5, 8, 9, 4)\ntemp7 <- c(3, 6, 9, 4, 1)\n\ntemp <- list(temp1, temp2, temp3, temp4, temp5, temp6, temp7)\n\n# Use lapply() to find each day's minimum temperature\nlapply(temp, min)\n\n\n\n\n## [[1]]\n## [1] -1\n## \n## [[2]]\n## [1] 5\n## \n## [[3]]\n## [1] -3\n## \n## [[4]]\n## [1] -2\n## \n## [[5]]\n## [1] 2\n## \n## [[6]]\n## [1] -3\n## \n## [[7]]\n## [1] 1\n\n\n\n# Use sapply() to find each day's minimum temperature\nsapply(temp, min)\n\n\n\n\n## [1] -1  5 -3 -2  2 -3  1\n\n\n\n# Use lapply() to find each day's maximum temperature\nlapply(temp, max)\n\n\n\n\n## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 13\n## \n## [[3]]\n## [1] 8\n## \n## [[4]]\n## [1] 7\n## \n## [[5]]\n## [1] 9\n## \n## [[6]]\n## [1] 9\n## \n## [[7]]\n## [1] 9\n\n\n\n# Use sapply() to find each day's maximum temperature\nsapply(temp, max)\n\n\n\n\n## [1]  9 13  8  7  9  9  9\n\n\n\nsapply\n with your own function\n\n\n# temp is already defined in the workspace\n\n# Define a function calculates the average of the min and max of a vector: extremes_avg\nextremes_avg <- function(x) {\n    return((min(x) + max(x))/2)\n}\n\n# Apply extremes_avg() over temp using sapply()\nsapply(temp, extremes_avg)\n\n\n\n\n## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0\n\n\n\n# Apply extremes_avg() over temp using lapply()\nlapply(temp, extremes_avg)\n\n\n\n\n## [[1]]\n## [1] 4\n## \n## [[2]]\n## [1] 9\n## \n## [[3]]\n## [1] 2.5\n## \n## [[4]]\n## [1] 2.5\n## \n## [[5]]\n## [1] 5.5\n## \n## [[6]]\n## [1] 3\n## \n## [[7]]\n## [1] 5\n\n\n\nsapply\n with function returning vector\n\n\n# temp is already available in the workspace\n\n# Create a function that returns min and max of a vector: extremes\nextremes <- function(x) {\n    c(min(x), max(x))\n}\n\n# Apply extremes() over temp with sapply()\nsapply(temp, extremes)\n\n\n\n\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [1,]   -1    5   -3   -2    2   -3    1\n## [2,]    9   13    8    7    9    9    9\n\n\n\n# Apply extremes() over temp with lapply()\nlapply(temp, extremes)\n\n\n\n\n## [[1]]\n## [1] -1  9\n## \n## [[2]]\n## [1]  5 13\n## \n## [[3]]\n## [1] -3  8\n## \n## [[4]]\n## [1] -2  7\n## \n## [[5]]\n## [1] 2 9\n## \n## [[6]]\n## [1] -3  9\n## \n## [[7]]\n## [1] 1 9\n\n\n\nsapply\n can\u2019t simplify, now what?\n\n\n# temp is already prepared for you in the workspace\n\n# Create a function that returns all values below zero: below_zero\nbelow_zero <- function(x) {\n    x[x<0]\n}\n\n#below_zero(temp) alone won't work!!!\n\n# Apply below_zero over temp using sapply(): freezing_s\nfreezing_s <- sapply(temp, below_zero)\n\n# Apply below_zero over temp using lapply(): freezing_l\nfreezing_l <- lapply(temp, below_zero)\n\n# Compare freezing_s to freezing_l using identical()\nidentical(freezing_s, freezing_l)\n\n\n\n\n## [1] TRUE\n\n\n\nsapply\n with functions that return NULL\n\n\n# temp is already available in the workspace\n\n# Write a function that 'cat()s' out the average temperatures: print_info\nprint_info <- function(x) {\n    cat('The average temperature is', mean(x), '\\n')\n}\n\n# Apply print_info() over temp using lapply()\nlapply(temp, print_info)\n\n\n\n\n## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n\n\n\n# Apply print_info() over temp using sapply()\nsapply(temp, print_info)\n\n\n\n\n## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n\n\n\nUse \nvapply\n\u00b6\n\n\n# temp is already available in the workspace\n\n# Code the basics() function\nbasics <- function(x) {\n    c(minimum = min(x), average = mean(x), maximum = max(x))\n}\n\n# Apply basics() over temp using vapply()\nvapply(temp, basics, numeric(3))\n\n\n\n\n##         [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## minimum -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## average  4.8    9  2.2  2.4  5.4  4.6  4.6\n## maximum  9.0   13  8.0  7.0  9.0  9.0  9.0\n\n\n\nUse \nvapply\n (2)\n\n\n# temp is already available in the workspace\n\n# Definition of the basics() function\nbasics <- function(x) {\n  c(min = min(x), mean = mean(x), median = median(x), max = max(x))\n}\n\n# Fix the error:\nvapply(temp, basics, numeric(4))\n\n\n\n\n##        [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## min    -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## mean    4.8    9  2.2  2.4  5.4  4.6  4.6\n## median  6.0    9  3.0  2.0  5.0  5.0  4.0\n## max     9.0   13  8.0  7.0  9.0  9.0  9.0\n\n\n\nFrom \nsapply\n to \nvapply\n\n\n# temp is already defined in the workspace\n\n# Convert to vapply() expression\nvapply(temp, max, numeric(1))\n\n\n\n\n## [1]  9 13  8  7  9  9  9\n\n\n\n# Convert to vapply() expression\nvapply(temp, function(x, y) { mean(x) > y }, y = 5, logical(1))\n\n\n\n\n## [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n\n\n\n# Definition of get_info (don't change)\nget_info <- function(x, y) { \n  if (mean(x) > y) {\n    return('Not too cold!')\n  } else {\n    return('Pretty cold!')\n  }\n}\n\n# Convert to vapply() expression\nvapply(temp, get_info, y = 5, character(1))\n\n\n\n\n## [1] \"Pretty cold!\"  \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\" \n## [5] \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\"\n\n\n\nApply your knowledge. Or better yet: \nsapply\n it?\n\u00b6\n\n\n# work_todos and fun_todos have already been defined\nwork_todos <- c('Schedule call with team', \n                'Fix error in Recommendation System', \n                'Respond to Marc from IT')\n\nfun_todos <- c('Sleep', 'Make arrangements for summer trip')\n\n# Create a list: todos\ntodos <- list(work_todos, fun_todos)\ntodos\n\n\n\n\n## [[1]]\n## [1] \"Schedule call with team\"           \n## [2] \"Fix error in Recommendation System\"\n## [3] \"Respond to Marc from IT\"           \n## \n## [[2]]\n## [1] \"Sleep\"                             \"Make arrangements for summer trip\"\n\n\n\n# Sort the vectors inside todos alphabetically\nlapply(todos, sort)\n\n\n\n\n## [[1]]\n## [1] \"Fix error in Recommendation System\"\n## [2] \"Respond to Marc from IT\"           \n## [3] \"Schedule call with team\"           \n## \n## [[2]]\n## [1] \"Make arrangements for summer trip\" \"Sleep\"\n\n\n\n5, Utilities\n\u00b6\n\n\nMathematical utilities\n\u00b6\n\n\n\n\nabs\n; calculate the absolute value.\n\n\nsum\n; calculate the sum of all the values in a data structure.\n\n\nmean\n; calculate the arithmetic mean.\n\n\nround\n; round the values to 0 decimal places by default. Try out\n\n\n?round\n in the console for variations of \nround\n and ways to change\n\n    the number of digits to round to.\n\n\n\n\n\n\n\n# The errors vector\nerrors <- c(1.9, -2.6, 4.0, -9.5, -3.4, 7.3)\n\n# Sum of absolute rounded values of errors\nsum(abs(round(errors)))\n\n\n\n\n## [1] 29\n\n\n\nFind the error\n\u00b6\n\n\n# Vectors\nvec1 <- c(1.5, 2.5, 8.4, 3.7, 6.3)\nvec2 <- rev(vec1)\n\n# Fix the error\nmean(abs(append(vec1, vec2)))\n\n\n\n\n## [1] 4.48\n\n\n\nData utilities\n\u00b6\n\n\n\n\nseq\n; generate sequences, by specifying the from, to and\n\n    by arguments.\n\n\nrep\n; replicate elements of vectors and lists.\n\n\nsort\n; sort a vector in ascending order. Works on numerics, but\n\n    also on character strings and logicals.\n\n\nrev\n; reverse the elements in a data structures for which reversal\n\n    is defined.\n\n\nstr\n; display the structure of any R object. append; Merge vectors\n\n    or lists.\n\n\nis.*\n; check for the class of an R object.\n\n\nas.*\n; convert an R object from one class to another.\n\n\nunlist\n; flatten (possibly embedded) lists to produce a vector.\n\n\n\n\n\n\n\n# The linkedin and facebook vectors\nlinkedin <- list(16, 9, 13, 5, 2, 17, 14)\nfacebook <- list(17, 7, 5, 16, 8, 13, 14)\n\n# Convert linkedin and facebook to a vector: li_vec and fb_vec\nli_vec <- unlist(as.vector(linkedin))\nfb_vec <- unlist(as.vector(facebook))\n\n# Append fb_vec to li_vec: social_vec\nsocial_vec <- append(li_vec, fb_vec)\n\n# Sort social_vec\nsort(social_vec, decreasing = TRUE)\n\n\n\n\n##  [1] 17 17 16 16 14 14 13 13  9  8  7  5  5  2\n\n\n\nFind the error (2)\n\n\n# Fix me\nround(sum(unlist(list(1.1, 3, 5))))\n\n\n\n\n## [1] 9\n\n\n\n# Fix me\nrep(seq(1, 7, by = 2), times = 7)\n\n\n\n\n##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7\n\n\n\nprint(rep(seq(1, 7, by = 2), times = 7))\n\n\n\n\n##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7\n\n\n\nBeat Gauss using R\n\u00b6\n\n\n# Create first sequence: seq1\nseq1 <- seq(1,500, by = 3)\nprint(seq1)\n\n\n\n\n##   [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49\n##  [18]  52  55  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\n##  [35] 103 106 109 112 115 118 121 124 127 130 133 136 139 142 145 148 151\n##  [52] 154 157 160 163 166 169 172 175 178 181 184 187 190 193 196 199 202\n##  [69] 205 208 211 214 217 220 223 226 229 232 235 238 241 244 247 250 253\n##  [86] 256 259 262 265 268 271 274 277 280 283 286 289 292 295 298 301 304\n## [103] 307 310 313 316 319 322 325 328 331 334 337 340 343 346 349 352 355\n## [120] 358 361 364 367 370 373 376 379 382 385 388 391 394 397 400 403 406\n## [137] 409 412 415 418 421 424 427 430 433 436 439 442 445 448 451 454 457\n## [154] 460 463 466 469 472 475 478 481 484 487 490 493 496 499\n\n\n\n# Create second sequence: seq2\nseq2 <- seq(1200, 900, by = -7)\nprint(seq2)\n\n\n\n\n##  [1] 1200 1193 1186 1179 1172 1165 1158 1151 1144 1137 1130 1123 1116 1109\n## [15] 1102 1095 1088 1081 1074 1067 1060 1053 1046 1039 1032 1025 1018 1011\n## [29] 1004  997  990  983  976  969  962  955  948  941  934  927  920  913\n## [43]  906\n\n\n\n# Calculate total sum of the sequences\nprint(sum(append(seq1, seq2)))\n\n\n\n\n## [1] 87029\n\n\n\ngrepl\n & \ngrep\n (and the likes)\n\u00b6\n\n\n\n\ngrepl\n; return TRUE when a pattern is found in the corresponding\n\n    character string.\n\n\ngrep\n; return a vector of indices of the character strings that\n\n    contains the pattern.\n\n\n\n\n\n\n\n# The emails vector has\nemails <- c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use grepl() to match for 'edu'\nprint(grepl(pattern = 'edu', x = emails))\n\n\n\n\n## [1]  TRUE  TRUE FALSE  TRUE  TRUE FALSE\n\n\n\n# Use grep() to match for 'edu', save result to hits\nhits <- grep(pattern = 'edu', x = emails)\nhits\n\n\n\n\n## [1] 1 2 4 5\n\n\n\n# Subset emails using hits\nprint(emails[hits])\n\n\n\n\n## [1] \"john.doe@ivyleague.edu\"   \"education@world.gov\"     \n## [3] \"invalid.edu\"              \"quant@bigdatacollege.edu\"\n\n\n\ngrepl\n & \ngrep\n (2)\n\n\nConsult a regex character chart for more.\n\n\n# The emails vector\nemails <- c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use grep() to match for .edu addresses more robustly\nprint(grep(pattern = '@.*\\\\.edu$',x = emails))\n\n\n\n\n## [1] 1 5\n\n\n\n# Use grepl() to match for .edu addresses more robustly, save result to hits\nhits <- grepl(pattern = '@.*\\\\.edu$',x = emails)\nhits\n\n\n\n\n## [1]  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n# Subset emails using hits\nprint(emails[hits])\n\n\n\n\n## [1] \"john.doe@ivyleague.edu\"   \"quant@bigdatacollege.edu\"\n\n\n\nsub\n & \ngsub\n\n\n# The emails vector\nemails <- c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use sub() to convert the email domains to datacamp.edu (attempt 1)\nprint(sub(pattern = '@.*\\\\.edu$', replacement = 'datacamp.edu', x = emails))\n\n\n\n\n## [1] \"john.doedatacamp.edu\"     \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quantdatacamp.edu\"        \"cookie.monster@sesame.tv\"\n\n\n\n# Use sub() to convert the email domains to datacamp.edu (attempt 2)\nprint(sub(pattern = '@.*\\\\.edu$', replacement = '@datacamp.edu', x = emails))\n\n\n\n\n## [1] \"john.doe@datacamp.edu\"    \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quant@datacamp.edu\"       \"cookie.monster@sesame.tv\"\n\n\n\nTime is of the essence\n\u00b6\n\n\nRight here, right now\n\n\n# Get the current date: today\ntoday <- Sys.Date()\ntoday\n\n\n\n\n## [1] \"2017-04-14\"\n\n\n\n# See what today looks like under the hood\nprint(unclass(today))\n\n\n\n\n## [1] 17270\n\n\n\n# Get the current time: now\nnow <- Sys.time()\nnow\n\n\n\n\n## [1] \"2017-04-14 08:29:36 EDT\"\n\n\n\n# See what now looks like under the hood\nprint(unclass(now))\n\n\n\n\n## [1] 1492172976\n\n\n\nCreate and format dates\n\n\n\n\n\n\n\n\nSymbol\n\n\nMeaning\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n%d\n\n\nday as a number (0-31)\n\n\n31-janv\n\n\n\n\n\n\n%a\n\n\nabbreviated weekday\n\n\nMon\n\n\n\n\n\n\n%A\n\n\nunabbreviated weekday\n\n\nMonday\n\n\n\n\n\n\n%m\n\n\nmonth (00-12)\n\n\n00-12\n\n\n\n\n\n\n%b\n\n\nabbreviated month\n\n\nJan\n\n\n\n\n\n\n%B\n\n\nunabbreviated month\n\n\nJanuary\n\n\n\n\n\n\n%y\n\n\n2-digit year\n\n\n07\n\n\n\n\n\n\n%Y\n\n\n4-digit year\n\n\n2007\n\n\n\n\n\n\n%H\n\n\nhours as a decimal number\n\n\n23\n\n\n\n\n\n\n%M\n\n\nminutes as a decimal number\n\n\n10\n\n\n\n\n\n\n%S\n\n\nseconds as a decimal number\n\n\n53\n\n\n\n\n\n\n%T\n\n\nshorthand notation for the typical format %H:%M:%S\n\n\n23:10:53\n\n\n\n\n\n\n\n\n\nFind out more with \n?strptime\n.\n\n\nR offer default functions for dealing with time and dates. There are\n\nbetter packages: \ndate\n and \nlubridate\n.\n\n\nlubridate\n enhances time-series packages such as \nzoo\n and \nxts\n, and\n\nworks well with \ndplyr\n for data wrangling.\n\n\nlibrary(date)\n\n# Definition of character strings representing dates\nstr1 <- \"May 23, 96\"\nstr2 <- \"2012-3-15\"\nstr3 <- \"30/January/2006\"\n\n# Convert the strings to dates: date1, date2, date3\ndate1 <- as.date(str1, order = \"mdy\")\ndate1\n\n\n\n\n## [1] 23May96\n\n\n\ndate1 <- as.POSIXct(date1, format = \"%d %m %y\")\ndate1\n\n\n\n\n## [1] \"1996-05-22 20:00:00 EDT\"\n\n\n\ndate2 <- as.date(str2, order = \"ymd\")\ndate2\n\n\n\n\n## [1] 15Mar2012\n\n\n\ndate2 <- as.POSIXct(date2, format = \"%d %m %y\")\ndate2\n\n\n\n\n## [1] \"2012-03-14 20:00:00 EDT\"\n\n\n\ndate3 <- as.date(str3, order = \"dmy\")\ndate3\n\n\n\n\n## [1] 30Jan2006\n\n\n\ndate3 <- as.POSIXct(date3, format = \"%d %m %y\")\ndate3\n\n\n\n\n## [1] \"2006-01-29 19:00:00 EST\"\n\n\n\n# Convert dates to formatted strings\nformat(date1, \"%A\")\n\n\n\n\n## [1] \"mercredi\"\n\n\n\nformat(date2, \"%d\")\n\n\n\n\n## [1] \"14\"\n\n\n\nformat(date3, \"%b %Y\")\n\n\n\n\n## [1] \"janv. 2006\"\n\n\n\n# convert dates to character data\nstrDate2 <- as.character(date2)\nstrDate2\n\n\n\n\n## [1] \"2012-03-14 20:00:00\"\n\n\n\nCreate and format times\n\n\n# Definition of character strings representing times\nstr1 <- \"2012-3-12 14:23:08\"\n\n# Convert the strings to POSIXct objects: time1, time2\ntime1 <- as.POSIXct(str2, format = \"%Y-%m-%d %H:%M:%S\")\n\n# Convert times to formatted strings\n\n# Definition of character strings representing dates\nformat(time1, \"%M\")\n\n\n\n\n## [1] NA\n\n\n\nformat(time1, \"%I:%M %p\")\n\n\n\n\n## [1] NA\n\n\n\nCalculations with dates\n\n\n# day1, day2, day3, day4 and day5\nday1 <- as.Date(\"2016-11-21\")\nday2 <- as.Date(\"2016-11-16\")\nday3 <- as.Date(\"2016-11-27\")\nday4 <- as.Date(\"2016-11-14\")\nday5 <- as.Date(\"2016-12-02\")\n\n# Difference between last and first pizza day\nprint(day5 - day1)\n\n\n\n\n## Time difference of 11 days\n\n\n\n# Create vector pizza\npizza <- c(day1, day2, day3, day4, day5)\n\n# Create differences between consecutive pizza days: day_diff\nday_diff <- diff(pizza, lag = 1, differences = 1)\nday_diff\n\n\n\n\n## Time differences in days\n## [1]  -5  11 -13  18\n\n\n\n# Average period between two consecutive pizza days\nprint(mean(day_diff))\n\n\n\n\n## Time difference of 2.75 days\n\n\n\nCalculus with times\n\n\n# login and logout\nlogin <- as.POSIXct(c(\"2016-11-18 10:18:04 UTC\", \"2016-11-23 09:14:18 UTC\", \"2016-11-23 12:21:51 UTC\", \"2016-11-23 12:37:24 UTC\", \"2016-11-25 21:37:55 UTC\"))\n\nlogout <- as.POSIXct(c(\"2016-11-18 10:56:29 UTC\", \"2016-11-23 09:14:52 UTC\", \"2016-11-23 12:35:48 UTC\", \"2016-11-23 13:17:22 UTC\", \"2016-11-25 22:08:47 UTC\"))\n\n# Calculate the difference between login and logout: time_online\ntime_online <- logout - login\n\n# Inspect the variable time_online\n#class(time_online)\ntime_online\n\n\n\n\n## Time differences in secs\n## [1] 2305   34  837 2398 1852\n\n\n\n# Calculate the total time online\nprint(sum(time_online))\n\n\n\n\n## Time difference of 7426 secs\n\n\n\n# Calculate the average time online\nprint(mean(time_online))\n\n\n\n\n## Time difference of 1485.2 secs",
            "title": "Intermediate R"
        },
        {
            "location": "/Intermediate_R/#equality-or-not",
            "text": "# Comparison of logicals\nTRUE == FALSE  ## [1] FALSE  # Comparison of numerics\n(-6 * 14) != (17 - 101)  ## [1] FALSE  # Comparison of character strings\n'useR' == 'user'  ## [1] FALSE  # Compare a logical with a numeric\nTRUE == 1  ## [1] TRUE  Greater and less than  # Comparison of numerics\n(-6*5 + 2) >= (-10 + 1)  ## [1] FALSE  # Comparison of character strings\n'raining' <= 'raining dogs'  ## [1] TRUE  # Comparison of logicals\nTRUE > FALSE  ## [1] TRUE  Compare vectors  # The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Popular days\nlinkedin > 15  ## [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  # Quiet days\nlinkedin <= 5  ## [1] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  # LinkedIn more popular than Facebook\nlinkedin > facebook  ## [1] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  Compare matrices  views <- matrix(c(linkedin, facebook), nrow = 2, byrow = TRUE)\n\n# When does views equal 13?\nviews == 13  ##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  # When is views less than or equal to 14?\nviews <= 14  ##       [,1] [,2] [,3]  [,4] [,5]  [,6] [,7]\n## [1,] FALSE TRUE TRUE  TRUE TRUE FALSE TRUE\n## [2,] FALSE TRUE TRUE FALSE TRUE  TRUE TRUE  # How often does facebook equal or exceed linkedin times two?\nsum(facebook >= linkedin * 2)  ## [1] 2",
            "title": "Equality (or not)"
        },
        {
            "location": "/Intermediate_R/#and",
            "text": "# The linkedin and last variable\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nlast <- tail(linkedin, 1)\n\n# Is last under 5 or above 10?\nlast < 5 | last > 10  ## [1] TRUE  # Is last between 15 (exclusive) and 20 (inclusive)?\nlast > 15 & last <= 20  ## [1] FALSE  # Is last between 0 and 5 or between 10 and 15?\n(last > 0 & last < 5) | (last > 10 & last < 15)  ## [1] TRUE  &  and  |  (2)  # linkedin exceeds 10 but facebook below 10\nlinkedin > 10 & facebook < 10  ## [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  # When were one or both visited at least 12 times?\nlinkedin >= 12 | facebook >= 12  ## [1]  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  # When is views between 11 (exclusive) and 14 (inclusive)?\nviews > 11 & views <= 14  ##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6] [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE TRUE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE TRUE  Blend it all together  # Select the second column, named day2, from li_df: second\nsecond <- li_df$day2\n\n# Build a logical vector, TRUE if value in second is extreme: extremes\nextremes <- (second > 25 | second < 5)\n\n# Count the number of TRUEs in extremes\nsum(extremes)  ## [1] 16",
            "title": "&amp; and |"
        },
        {
            "location": "/Intermediate_R/#the-if-statement-and-more",
            "text": "# Variables related to your last day of recordings\nmedium <- 'LinkedIn'\nnum_views <- 14\n\n# Examine the if statement for medium\nif (medium == 'LinkedIn') {\n  print('Showing LinkedIn information')\n}  ## [1] \"Showing LinkedIn information\"  # Write the if statement for num_views\nif (num_views > 15) {\n    print('You\\'re popular!')\n}  Add an  else  # Variables related to your last day of recordings\nmedium <- 'LinkedIn'\nnum_views <- 14\n\n# Control structure for medium\nif (medium == 'LinkedIn') {\n  print('Showing LinkedIn information')\n} else {\n    print('Unknown medium')\n}  ## [1] \"Showing LinkedIn information\"  # Control structure for num_views\nif (num_views > 15) {\n  print('You\\'re popular!')\n} else {\n    print('Try to be more visible!')\n}  ## [1] \"Try to be more visible!\"  Customize further:  else if  # Variables related to your last day of recordings\nmedium <- 'LinkedIn'\nnum_views <- 14\n\n# Control structure for medium\nif (medium == 'LinkedIn') {\n  print('Showing LinkedIn information')\n} else if (medium == 'Facebook') {\n  print('Showing Facebook information')\n} else {\n  print('Unknown medium')\n}  ## [1] \"Showing LinkedIn information\"  # Control structure for num_views\nif (num_views > 15) {\n  print('You\\'re popular!')\n} else if (num_views > 10 | num_views <= 15) {\n  print('Your number of views is average')\n} else {\n  print('Try to be more visible!')\n}  ## [1] \"Your number of views is average\"  Take control!  # Variables related to your last day of recordings\nli <- 15\nfb <- 9\n\n# Code the control-flow construct\nif (li >= 15 & fb >= 15) {\n    sms <- 2*(li + fb)\n} else if (li < 10 & fb < 10) {\n    sms <- (li + fb)/2\n} else {\n    sms <- li + fb\n}\n\n# Print the resulting sms to the console\nsms  ## [1] 24",
            "title": "The if statement (and more)"
        },
        {
            "location": "/Intermediate_R/#2-loops",
            "text": "",
            "title": "2, Loops"
        },
        {
            "location": "/Intermediate_R/#write-a-while-loop",
            "text": "# Initialize the speed variable\nspeed <- 64\n\n# Code the while loop\nwhile (speed > 30) {\n  print('Slow down!')\n  speed <- speed - 7\n}  ## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"  # Print out the speed variable\nspeed  ## [1] 29  Throw in more conditionals  # Initialize the speed variable\nspeed <- 64\n\n# Extend/adapt the while loop\nwhile (speed > 30) {\n  print(paste('Your speed is ', speed))\n  if (speed > 48) {\n    print('Slow down big time!')\n    speed <- speed - 11\n  } else {\n        print('Slow down!')\n        speed <- speed - 6\n  }\n}  ## [1] \"Your speed is  64\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  53\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  42\"\n## [1] \"Slow down!\"\n## [1] \"Your speed is  36\"\n## [1] \"Slow down!\"  Stop the  while  loop:  break  # Initialize the speed variable\nspeed <- 88\nwhile (speed > 30) {\n  print(paste('Your speed is',speed))\n    # Break the while loop when speed exceeds 80\n  if (speed > 80) {\n    break\n  } else if (speed > 48) {\n    print('Slow down big time!')\n    speed <- speed - 11\n  } else {\n    print('Slow down!')\n    speed <- speed - 6\n  }\n}  ## [1] \"Your speed is 88\"  Build a  while  loop from scratch  strsplit ; split up in a vector that contains separate letters.  # Initialize i\ni <- 1\n\n# Code the while loop\nwhile (i <= 10) {\n  print(i * 3)\n  if ( (i * 3) %% 8 == 0) {\n    break\n  }\n  i <- i + 1\n}  ## [1] 3\n## [1] 6\n## [1] 9\n## [1] 12\n## [1] 15\n## [1] 18\n## [1] 21\n## [1] 24",
            "title": "Write a while loop"
        },
        {
            "location": "/Intermediate_R/#write-a-for-loop",
            "text": "Loop over a vector  # The linkedin vector\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\n\n# Loop version 1\nfor (lin in linkedin) {\n    print(lin)\n}  ## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14  # Loop version 2\nfor (i in 1:length(linkedin)) {\n    print(linkedin[i])\n}  ## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14  Loop over a list  [[]] ; list of list.  # The nyc list is already specified\nnyc <- list(pop = 8405837, \n            boroughs = c('Manhattan', 'Bronx', 'Brooklyn', 'Queens', 'Staten Island'), \n            capital = FALSE)\n\n# Loop version 1\nfor (item in nyc) {\n    print(item)\n}  ## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE  # Loop version 2\nfor (i in 1:length(nyc)) {\n    print(nyc[[i]])\n}  ## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE  Loop over a matrix  # The tic-tac-toe matrix has already been defined for you\nttt <- matrix(c('O', NA, 'X', NA, 'O', NA, 'X', 'O', 'X'), nrow = 3, ncol = 3)\n\n# define the double for loop\nfor (i in 1:nrow(ttt)) {\n    for (j in 1:ncol(ttt)) {\n    print(paste('On row', i,'and column', j,'the board contains ', ttt[i,j]))\n    }\n}  ## [1] \"On row 1 and column 1 the board contains  O\"\n## [1] \"On row 1 and column 2 the board contains  NA\"\n## [1] \"On row 1 and column 3 the board contains  X\"\n## [1] \"On row 2 and column 1 the board contains  NA\"\n## [1] \"On row 2 and column 2 the board contains  O\"\n## [1] \"On row 2 and column 3 the board contains  O\"\n## [1] \"On row 3 and column 1 the board contains  X\"\n## [1] \"On row 3 and column 2 the board contains  NA\"\n## [1] \"On row 3 and column 3 the board contains  X\"",
            "title": "Write a for loop"
        },
        {
            "location": "/Intermediate_R/#mix-up-loops-with-control-flow",
            "text": "# The linkedin vector\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\n\n# Code the for loop with conditionals\nfor (i in 1:length(linkedin)) {\n    if (linkedin[i] > 10) {\n        print('You\\'re popular!')\n    } else {\n        print('Be more visible!')\n    }\n    print(linkedin[i])\n}  ## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] 2\n## [1] \"You're popular!\"\n## [1] 17\n## [1] \"You're popular!\"\n## [1] 14  Next, you break it  # The linkedin vector\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\n\n# Extend the for loop\nfor (li in linkedin) {\n  if (li > 10) {\n    print('You\\'re popular!')\n  } else {\n    print('Be more visible!')\n  }\n    # Add code to conditionally break iteration\n  if (li > 16) {\n    print('This is ridiculous, I\\'m outta here!')\n    break\n  }\n  # Add code to conditionally skip iteration\n  if (li < 5) {\n    print('This is too embarrassing!')\n    next\n    }\n  print(li)\n}  ## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] \"This is too embarrassing!\"\n## [1] \"You're popular!\"\n## [1] \"This is ridiculous, I'm outta here!\"  Build a for loop from scratch  # Pre-defined variables\nrquote <- 'R\\'s internals are irrefutably intriguing'\n\nchars <- strsplit(rquote, split = '')[[1]]\nrcount <- 0\n\n# Your solution here\nfor (i in 1:length(chars)) {\n    if (chars[i] == 'u') {\n    break\n    }\n    if (chars[i] == 'r' | chars[i] == 'R') {\n        rcount <- rcount + 1\n    }\n}\n\n# Print the resulting rcount variable to the console\nprint(rcount)  ## [1] 5",
            "title": "Mix up loops with control flow"
        },
        {
            "location": "/Intermediate_R/#3-functions",
            "text": "",
            "title": "3, Functions"
        },
        {
            "location": "/Intermediate_R/#function-documentation",
            "text": "# Consult the documentation on the mean() function\n?mean\n\n# Inspect the arguments of the mean() function\nargs(mean)",
            "title": "Function documentation"
        },
        {
            "location": "/Intermediate_R/#use-a-function",
            "text": "# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Calculate average number of views\navg_li <- mean(linkedin)\navg_fb <- mean(facebook)\n\n# Inspect avg_li and avg_fb\nprint(avg_li)  ## [1] 10.85714  print(avg_fb)  ## [1] 11.42857  avg_li  ## [1] 10.85714  # Calculate the mean of linkedin minus facebook\nprint(mean(linkedin - facebook))  ## [1] -0.5714286  Use a function (2)  # The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Calculate the mean of the sum\navg_sum <- mean(linkedin + facebook)\n\n# Calculate the trimmed mean of the sum\navg_sum_trimmed <- mean((linkedin + facebook), trim = 0.2)\n\n# Inspect both new variables\navg_sum  ## [1] 22.28571  avg_sum_trimmed  ## [1] 22.6  Use a function (3)  # The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, NA, 17, 14)\nfacebook <- c(17, NA, 5, 16, 8, 13, 14)\n\n# Basic average of linkedin\nprint(mean(linkedin))  ## [1] NA  # Advanced average of facebook\nprint(mean(facebook, na.rm = TRUE))  ## [1] 12.16667",
            "title": "Use a function"
        },
        {
            "location": "/Intermediate_R/#functions-inside-functions",
            "text": "# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, NA, 17, 14)\nfacebook <- c(17, NA, 5, 16, 8, 13, 14)\n\n# Calculate the mean absolute deviation\nmean((abs(linkedin - facebook)), na.rm = TRUE)  ## [1] 4.8",
            "title": "Functions inside functions"
        },
        {
            "location": "/Intermediate_R/#write-your-own-function",
            "text": "# Create a function pow_two()\npow_two <- function(arg1) {\n    arg1^2\n}\n\n# Use the function \npow_two(12)  ## [1] 144  # Create a function sum_abs()\nsum_abs <- function(arg2,arg3) {\n    abs(arg2) + abs(arg3)\n}\n\n# Use the function\nsum_abs(-2,3)  ## [1] 5  Write your own function (2)  # Define the function hello()\nhello <- function() {\n    print('Hi there!')\n    return(TRUE)\n}\n\n# Call the function hello()\nhello()  ## [1] \"Hi there!\"\n\n## [1] TRUE  # Define the function my_filter()\nmy_filter <- function(arg1) {\n    if (arg1 > 0) {\n        return(arg1)\n    } else {\n        return(NULL)\n    }\n}\n\n# Call the function my_filter() twice\nmy_filter(5)  ## [1] 5  my_filter(-5)  ## NULL  Write your own function (3)  Variables inside a function are not in the Global Environment.  # Extend the pow_two() function\npow_two <- function(x, print_info = TRUE) {\n  y <- x ^ 2\n  if (print_info) {\n    print(paste(x, 'to the power two equals', y))\n  }\n  return(y)\n}\n\n#pow_two(2)\npow_two(2, FALSE)  ## [1] 4",
            "title": "Write your own function"
        },
        {
            "location": "/Intermediate_R/#r-you-functional",
            "text": "# The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, NA, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# Define the interpret function\ninterpret <- function(arg) {\n    if (arg > 15) {\n        print('You\\'re popular!')\n        return(arg)\n    } else {\n        print('Try to be more visible!')\n        return(0)\n    }\n}\n\ninterpret(linkedin[1])  ## [1] \"You're popular!\"\n\n## [1] 16  interpret(facebook[2])  ## [1] \"Try to be more visible!\"\n\n## [1] 0  R you functional? (2)  # The linkedin and facebook vectors\nlinkedin <- c(16, 9, 13, 5, 2, 17, 14)\nfacebook <- c(17, 7, 5, 16, 8, 13, 14)\n\n# The interpret() can be used inside interpret_all()\ninterpret <- function(num_views){\n  if (num_views > 15) {\n    print('You\\'re popular!')\n    return(num_views)\n  } else {\n    print('Try to be more visible!')\n    return(0)\n  }\n}\n\n# Define the interpret_all() function\ninterpret_all <- function(data, logi = TRUE){\n  yy <- 0\n  for (i in data) {\n    yy <- yy + interpret(i)\n  }\n  if (logi) {\n    return(yy)\n  } else {\n    return(NULL)\n  }\n}\n\n# Call the interpret_all() function on both linkedin and facebook\ninterpret_all(linkedin)  ## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33  interpret_all(facebook)  ## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33",
            "title": "R you functional?"
        },
        {
            "location": "/Intermediate_R/#load-an-r-package",
            "text": "# The mtcars vectors have already been prepared for you\nwt <- mtcars$wt\nhp <- mtcars$hp\n\n# Request the currently attached packages\nsearch()  ##  [1] \".GlobalEnv\"            \"package:XLConnect\"    \n##  [3] \"package:XLConnectJars\" \"package:stats\"        \n##  [5] \"package:graphics\"      \"package:grDevices\"    \n##  [7] \"package:utils\"         \"package:datasets\"     \n##  [9] \"package:methods\"       \"Autoloads\"            \n## [11] \"package:base\"  # Try the qplot() function with wt and hp\nplot(wt,hp)\n\n# Load the ggplot2 package\nlibrary('ggplot2')   # or\nrequire('ggplot2')\n\n# Retry the qplot() function\nqplot(wt,hp)   # Check out the currently attached packages again\nsearch()  ##  [1] \".GlobalEnv\"            \"package:ggplot2\"      \n##  [3] \"package:XLConnect\"     \"package:XLConnectJars\"\n##  [5] \"package:stats\"         \"package:graphics\"     \n##  [7] \"package:grDevices\"     \"package:utils\"        \n##  [9] \"package:datasets\"      \"package:methods\"      \n## [11] \"Autoloads\"             \"package:base\"",
            "title": "Load an R package"
        },
        {
            "location": "/Intermediate_R/#4-the-apply-family",
            "text": "",
            "title": "4, The apply Family"
        },
        {
            "location": "/Intermediate_R/#use-lapply-with-a-built-in-r-function",
            "text": "# The vector pioneers\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\n# Split names from birth year: split_math\nsplit_math <- strsplit(pioneers, ':')\n\n# Convert to lowercase strings: split_low\nsplit_low <- lapply(split_math,tolower)\n\n# Take a look at the structure of split_low\nstr(split_low)  ## List of 4\n##  $ : chr [1:2] \"gauss\" \"1777\"\n##  $ : chr [1:2] \"bayes\" \"1702\"\n##  $ : chr [1:2] \"pascal\" \"1623\"\n##  $ : chr [1:2] \"pearson\" \"1857\"  Use  lapply  with your own function  # Code from previous exercise\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\nsplit <- strsplit(pioneers, split = ':')\nsplit_low <- lapply(split, tolower)\n\n# Write function select_first()\nselect_first <- function(x) {\n    return(x[1])\n}\n\n# Apply select_first() over split_low: names\nnames <- lapply(split_low, select_first)\nprint(names)  ## [[1]]\n## [1] \"gauss\"\n## \n## [[2]]\n## [1] \"bayes\"\n## \n## [[3]]\n## [1] \"pascal\"\n## \n## [[4]]\n## [1] \"pearson\"  # Write function select_second()\nselect_second <- function(x) {\n    return(x[2])\n}\n\n# Apply select_second() over split_low: years\nyears <- lapply(split_low, select_second)\nprint(years)  ## [[1]]\n## [1] \"1777\"\n## \n## [[2]]\n## [1] \"1702\"\n## \n## [[3]]\n## [1] \"1623\"\n## \n## [[4]]\n## [1] \"1857\"  lapply  and anonymous functions  Anonymous function == lambda function.  # Definition of split_low\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\nsplit <- strsplit(pioneers, split = ':')\nsplit_low <- lapply(split, tolower)\n\n#select_first <- function(x) {\n#  x[1]\n#}\n\nnames <- lapply(split_low, function(x) { x[1] })\n\n#select_second <- function(x) {\n#  x[2]\n#}\n\nyears <- lapply(split_low, function(x) { x[2] })  Use  lapply  with additional arguments  # Definition of split_low\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\nsplit <- strsplit(pioneers, split = ':')\nsplit_low <- lapply(split, tolower)\n\n# Replace the select_*() functions by a single function: select_el\nselect_el <- function(x, i) { \n  x[i] \n}\n\n#select_second <- function(x) { \n#  x[2] \n#}\n\n# Call the select_el() function twice on split_low: names and years\nnames <- lapply(split_low, select_el, i=1)\nyears <- lapply(split_low, select_el, 2)",
            "title": "Use lapply (with a built-in R function)"
        },
        {
            "location": "/Intermediate_R/#use-sapply",
            "text": "temp1 <- c(3, 7, 9, 6, -1)\ntemp2 <- c(6, 9, 12, 13, 5)\ntemp3 <- c(4, 8, 3, -1, -3)\ntemp4 <- c(1, 4, 7, 2, -2)\ntemp5 <- c(5, 7, 9, 4, 2)\ntemp6 <- c(-3, 5, 8, 9, 4)\ntemp7 <- c(3, 6, 9, 4, 1)\n\ntemp <- list(temp1, temp2, temp3, temp4, temp5, temp6, temp7)\n\n# Use lapply() to find each day's minimum temperature\nlapply(temp, min)  ## [[1]]\n## [1] -1\n## \n## [[2]]\n## [1] 5\n## \n## [[3]]\n## [1] -3\n## \n## [[4]]\n## [1] -2\n## \n## [[5]]\n## [1] 2\n## \n## [[6]]\n## [1] -3\n## \n## [[7]]\n## [1] 1  # Use sapply() to find each day's minimum temperature\nsapply(temp, min)  ## [1] -1  5 -3 -2  2 -3  1  # Use lapply() to find each day's maximum temperature\nlapply(temp, max)  ## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 13\n## \n## [[3]]\n## [1] 8\n## \n## [[4]]\n## [1] 7\n## \n## [[5]]\n## [1] 9\n## \n## [[6]]\n## [1] 9\n## \n## [[7]]\n## [1] 9  # Use sapply() to find each day's maximum temperature\nsapply(temp, max)  ## [1]  9 13  8  7  9  9  9  sapply  with your own function  # temp is already defined in the workspace\n\n# Define a function calculates the average of the min and max of a vector: extremes_avg\nextremes_avg <- function(x) {\n    return((min(x) + max(x))/2)\n}\n\n# Apply extremes_avg() over temp using sapply()\nsapply(temp, extremes_avg)  ## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0  # Apply extremes_avg() over temp using lapply()\nlapply(temp, extremes_avg)  ## [[1]]\n## [1] 4\n## \n## [[2]]\n## [1] 9\n## \n## [[3]]\n## [1] 2.5\n## \n## [[4]]\n## [1] 2.5\n## \n## [[5]]\n## [1] 5.5\n## \n## [[6]]\n## [1] 3\n## \n## [[7]]\n## [1] 5  sapply  with function returning vector  # temp is already available in the workspace\n\n# Create a function that returns min and max of a vector: extremes\nextremes <- function(x) {\n    c(min(x), max(x))\n}\n\n# Apply extremes() over temp with sapply()\nsapply(temp, extremes)  ##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [1,]   -1    5   -3   -2    2   -3    1\n## [2,]    9   13    8    7    9    9    9  # Apply extremes() over temp with lapply()\nlapply(temp, extremes)  ## [[1]]\n## [1] -1  9\n## \n## [[2]]\n## [1]  5 13\n## \n## [[3]]\n## [1] -3  8\n## \n## [[4]]\n## [1] -2  7\n## \n## [[5]]\n## [1] 2 9\n## \n## [[6]]\n## [1] -3  9\n## \n## [[7]]\n## [1] 1 9  sapply  can\u2019t simplify, now what?  # temp is already prepared for you in the workspace\n\n# Create a function that returns all values below zero: below_zero\nbelow_zero <- function(x) {\n    x[x<0]\n}\n\n#below_zero(temp) alone won't work!!!\n\n# Apply below_zero over temp using sapply(): freezing_s\nfreezing_s <- sapply(temp, below_zero)\n\n# Apply below_zero over temp using lapply(): freezing_l\nfreezing_l <- lapply(temp, below_zero)\n\n# Compare freezing_s to freezing_l using identical()\nidentical(freezing_s, freezing_l)  ## [1] TRUE  sapply  with functions that return NULL  # temp is already available in the workspace\n\n# Write a function that 'cat()s' out the average temperatures: print_info\nprint_info <- function(x) {\n    cat('The average temperature is', mean(x), '\\n')\n}\n\n# Apply print_info() over temp using lapply()\nlapply(temp, print_info)  ## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL  # Apply print_info() over temp using sapply()\nsapply(temp, print_info)  ## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL",
            "title": "Use sapply"
        },
        {
            "location": "/Intermediate_R/#use-vapply",
            "text": "# temp is already available in the workspace\n\n# Code the basics() function\nbasics <- function(x) {\n    c(minimum = min(x), average = mean(x), maximum = max(x))\n}\n\n# Apply basics() over temp using vapply()\nvapply(temp, basics, numeric(3))  ##         [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## minimum -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## average  4.8    9  2.2  2.4  5.4  4.6  4.6\n## maximum  9.0   13  8.0  7.0  9.0  9.0  9.0  Use  vapply  (2)  # temp is already available in the workspace\n\n# Definition of the basics() function\nbasics <- function(x) {\n  c(min = min(x), mean = mean(x), median = median(x), max = max(x))\n}\n\n# Fix the error:\nvapply(temp, basics, numeric(4))  ##        [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## min    -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## mean    4.8    9  2.2  2.4  5.4  4.6  4.6\n## median  6.0    9  3.0  2.0  5.0  5.0  4.0\n## max     9.0   13  8.0  7.0  9.0  9.0  9.0  From  sapply  to  vapply  # temp is already defined in the workspace\n\n# Convert to vapply() expression\nvapply(temp, max, numeric(1))  ## [1]  9 13  8  7  9  9  9  # Convert to vapply() expression\nvapply(temp, function(x, y) { mean(x) > y }, y = 5, logical(1))  ## [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  # Definition of get_info (don't change)\nget_info <- function(x, y) { \n  if (mean(x) > y) {\n    return('Not too cold!')\n  } else {\n    return('Pretty cold!')\n  }\n}\n\n# Convert to vapply() expression\nvapply(temp, get_info, y = 5, character(1))  ## [1] \"Pretty cold!\"  \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\" \n## [5] \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\"",
            "title": "Use vapply"
        },
        {
            "location": "/Intermediate_R/#apply-your-knowledge-or-better-yet-sapply-it",
            "text": "# work_todos and fun_todos have already been defined\nwork_todos <- c('Schedule call with team', \n                'Fix error in Recommendation System', \n                'Respond to Marc from IT')\n\nfun_todos <- c('Sleep', 'Make arrangements for summer trip')\n\n# Create a list: todos\ntodos <- list(work_todos, fun_todos)\ntodos  ## [[1]]\n## [1] \"Schedule call with team\"           \n## [2] \"Fix error in Recommendation System\"\n## [3] \"Respond to Marc from IT\"           \n## \n## [[2]]\n## [1] \"Sleep\"                             \"Make arrangements for summer trip\"  # Sort the vectors inside todos alphabetically\nlapply(todos, sort)  ## [[1]]\n## [1] \"Fix error in Recommendation System\"\n## [2] \"Respond to Marc from IT\"           \n## [3] \"Schedule call with team\"           \n## \n## [[2]]\n## [1] \"Make arrangements for summer trip\" \"Sleep\"",
            "title": "Apply your knowledge. Or better yet: sapply it?"
        },
        {
            "location": "/Intermediate_R/#5-utilities",
            "text": "",
            "title": "5, Utilities"
        },
        {
            "location": "/Intermediate_R/#mathematical-utilities",
            "text": "abs ; calculate the absolute value.  sum ; calculate the sum of all the values in a data structure.  mean ; calculate the arithmetic mean.  round ; round the values to 0 decimal places by default. Try out  ?round  in the console for variations of  round  and ways to change \n    the number of digits to round to.    # The errors vector\nerrors <- c(1.9, -2.6, 4.0, -9.5, -3.4, 7.3)\n\n# Sum of absolute rounded values of errors\nsum(abs(round(errors)))  ## [1] 29",
            "title": "Mathematical utilities"
        },
        {
            "location": "/Intermediate_R/#find-the-error",
            "text": "# Vectors\nvec1 <- c(1.5, 2.5, 8.4, 3.7, 6.3)\nvec2 <- rev(vec1)\n\n# Fix the error\nmean(abs(append(vec1, vec2)))  ## [1] 4.48",
            "title": "Find the error"
        },
        {
            "location": "/Intermediate_R/#data-utilities",
            "text": "seq ; generate sequences, by specifying the from, to and \n    by arguments.  rep ; replicate elements of vectors and lists.  sort ; sort a vector in ascending order. Works on numerics, but \n    also on character strings and logicals.  rev ; reverse the elements in a data structures for which reversal \n    is defined.  str ; display the structure of any R object. append; Merge vectors \n    or lists.  is.* ; check for the class of an R object.  as.* ; convert an R object from one class to another.  unlist ; flatten (possibly embedded) lists to produce a vector.    # The linkedin and facebook vectors\nlinkedin <- list(16, 9, 13, 5, 2, 17, 14)\nfacebook <- list(17, 7, 5, 16, 8, 13, 14)\n\n# Convert linkedin and facebook to a vector: li_vec and fb_vec\nli_vec <- unlist(as.vector(linkedin))\nfb_vec <- unlist(as.vector(facebook))\n\n# Append fb_vec to li_vec: social_vec\nsocial_vec <- append(li_vec, fb_vec)\n\n# Sort social_vec\nsort(social_vec, decreasing = TRUE)  ##  [1] 17 17 16 16 14 14 13 13  9  8  7  5  5  2  Find the error (2)  # Fix me\nround(sum(unlist(list(1.1, 3, 5))))  ## [1] 9  # Fix me\nrep(seq(1, 7, by = 2), times = 7)  ##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7  print(rep(seq(1, 7, by = 2), times = 7))  ##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7",
            "title": "Data utilities"
        },
        {
            "location": "/Intermediate_R/#beat-gauss-using-r",
            "text": "# Create first sequence: seq1\nseq1 <- seq(1,500, by = 3)\nprint(seq1)  ##   [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49\n##  [18]  52  55  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\n##  [35] 103 106 109 112 115 118 121 124 127 130 133 136 139 142 145 148 151\n##  [52] 154 157 160 163 166 169 172 175 178 181 184 187 190 193 196 199 202\n##  [69] 205 208 211 214 217 220 223 226 229 232 235 238 241 244 247 250 253\n##  [86] 256 259 262 265 268 271 274 277 280 283 286 289 292 295 298 301 304\n## [103] 307 310 313 316 319 322 325 328 331 334 337 340 343 346 349 352 355\n## [120] 358 361 364 367 370 373 376 379 382 385 388 391 394 397 400 403 406\n## [137] 409 412 415 418 421 424 427 430 433 436 439 442 445 448 451 454 457\n## [154] 460 463 466 469 472 475 478 481 484 487 490 493 496 499  # Create second sequence: seq2\nseq2 <- seq(1200, 900, by = -7)\nprint(seq2)  ##  [1] 1200 1193 1186 1179 1172 1165 1158 1151 1144 1137 1130 1123 1116 1109\n## [15] 1102 1095 1088 1081 1074 1067 1060 1053 1046 1039 1032 1025 1018 1011\n## [29] 1004  997  990  983  976  969  962  955  948  941  934  927  920  913\n## [43]  906  # Calculate total sum of the sequences\nprint(sum(append(seq1, seq2)))  ## [1] 87029",
            "title": "Beat Gauss using R"
        },
        {
            "location": "/Intermediate_R/#grepl-grep-and-the-likes",
            "text": "grepl ; return TRUE when a pattern is found in the corresponding \n    character string.  grep ; return a vector of indices of the character strings that \n    contains the pattern.    # The emails vector has\nemails <- c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use grepl() to match for 'edu'\nprint(grepl(pattern = 'edu', x = emails))  ## [1]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  # Use grep() to match for 'edu', save result to hits\nhits <- grep(pattern = 'edu', x = emails)\nhits  ## [1] 1 2 4 5  # Subset emails using hits\nprint(emails[hits])  ## [1] \"john.doe@ivyleague.edu\"   \"education@world.gov\"     \n## [3] \"invalid.edu\"              \"quant@bigdatacollege.edu\"  grepl  &  grep  (2)  Consult a regex character chart for more.  # The emails vector\nemails <- c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use grep() to match for .edu addresses more robustly\nprint(grep(pattern = '@.*\\\\.edu$',x = emails))  ## [1] 1 5  # Use grepl() to match for .edu addresses more robustly, save result to hits\nhits <- grepl(pattern = '@.*\\\\.edu$',x = emails)\nhits  ## [1]  TRUE FALSE FALSE FALSE  TRUE FALSE  # Subset emails using hits\nprint(emails[hits])  ## [1] \"john.doe@ivyleague.edu\"   \"quant@bigdatacollege.edu\"  sub  &  gsub  # The emails vector\nemails <- c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use sub() to convert the email domains to datacamp.edu (attempt 1)\nprint(sub(pattern = '@.*\\\\.edu$', replacement = 'datacamp.edu', x = emails))  ## [1] \"john.doedatacamp.edu\"     \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quantdatacamp.edu\"        \"cookie.monster@sesame.tv\"  # Use sub() to convert the email domains to datacamp.edu (attempt 2)\nprint(sub(pattern = '@.*\\\\.edu$', replacement = '@datacamp.edu', x = emails))  ## [1] \"john.doe@datacamp.edu\"    \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quant@datacamp.edu\"       \"cookie.monster@sesame.tv\"",
            "title": "grepl &amp; grep (and the likes)"
        },
        {
            "location": "/Intermediate_R/#time-is-of-the-essence",
            "text": "Right here, right now  # Get the current date: today\ntoday <- Sys.Date()\ntoday  ## [1] \"2017-04-14\"  # See what today looks like under the hood\nprint(unclass(today))  ## [1] 17270  # Get the current time: now\nnow <- Sys.time()\nnow  ## [1] \"2017-04-14 08:29:36 EDT\"  # See what now looks like under the hood\nprint(unclass(now))  ## [1] 1492172976  Create and format dates     Symbol  Meaning  Example      %d  day as a number (0-31)  31-janv    %a  abbreviated weekday  Mon    %A  unabbreviated weekday  Monday    %m  month (00-12)  00-12    %b  abbreviated month  Jan    %B  unabbreviated month  January    %y  2-digit year  07    %Y  4-digit year  2007    %H  hours as a decimal number  23    %M  minutes as a decimal number  10    %S  seconds as a decimal number  53    %T  shorthand notation for the typical format %H:%M:%S  23:10:53     Find out more with  ?strptime .  R offer default functions for dealing with time and dates. There are \nbetter packages:  date  and  lubridate .  lubridate  enhances time-series packages such as  zoo  and  xts , and \nworks well with  dplyr  for data wrangling.  library(date)\n\n# Definition of character strings representing dates\nstr1 <- \"May 23, 96\"\nstr2 <- \"2012-3-15\"\nstr3 <- \"30/January/2006\"\n\n# Convert the strings to dates: date1, date2, date3\ndate1 <- as.date(str1, order = \"mdy\")\ndate1  ## [1] 23May96  date1 <- as.POSIXct(date1, format = \"%d %m %y\")\ndate1  ## [1] \"1996-05-22 20:00:00 EDT\"  date2 <- as.date(str2, order = \"ymd\")\ndate2  ## [1] 15Mar2012  date2 <- as.POSIXct(date2, format = \"%d %m %y\")\ndate2  ## [1] \"2012-03-14 20:00:00 EDT\"  date3 <- as.date(str3, order = \"dmy\")\ndate3  ## [1] 30Jan2006  date3 <- as.POSIXct(date3, format = \"%d %m %y\")\ndate3  ## [1] \"2006-01-29 19:00:00 EST\"  # Convert dates to formatted strings\nformat(date1, \"%A\")  ## [1] \"mercredi\"  format(date2, \"%d\")  ## [1] \"14\"  format(date3, \"%b %Y\")  ## [1] \"janv. 2006\"  # convert dates to character data\nstrDate2 <- as.character(date2)\nstrDate2  ## [1] \"2012-03-14 20:00:00\"  Create and format times  # Definition of character strings representing times\nstr1 <- \"2012-3-12 14:23:08\"\n\n# Convert the strings to POSIXct objects: time1, time2\ntime1 <- as.POSIXct(str2, format = \"%Y-%m-%d %H:%M:%S\")\n\n# Convert times to formatted strings\n\n# Definition of character strings representing dates\nformat(time1, \"%M\")  ## [1] NA  format(time1, \"%I:%M %p\")  ## [1] NA  Calculations with dates  # day1, day2, day3, day4 and day5\nday1 <- as.Date(\"2016-11-21\")\nday2 <- as.Date(\"2016-11-16\")\nday3 <- as.Date(\"2016-11-27\")\nday4 <- as.Date(\"2016-11-14\")\nday5 <- as.Date(\"2016-12-02\")\n\n# Difference between last and first pizza day\nprint(day5 - day1)  ## Time difference of 11 days  # Create vector pizza\npizza <- c(day1, day2, day3, day4, day5)\n\n# Create differences between consecutive pizza days: day_diff\nday_diff <- diff(pizza, lag = 1, differences = 1)\nday_diff  ## Time differences in days\n## [1]  -5  11 -13  18  # Average period between two consecutive pizza days\nprint(mean(day_diff))  ## Time difference of 2.75 days  Calculus with times  # login and logout\nlogin <- as.POSIXct(c(\"2016-11-18 10:18:04 UTC\", \"2016-11-23 09:14:18 UTC\", \"2016-11-23 12:21:51 UTC\", \"2016-11-23 12:37:24 UTC\", \"2016-11-25 21:37:55 UTC\"))\n\nlogout <- as.POSIXct(c(\"2016-11-18 10:56:29 UTC\", \"2016-11-23 09:14:52 UTC\", \"2016-11-23 12:35:48 UTC\", \"2016-11-23 13:17:22 UTC\", \"2016-11-25 22:08:47 UTC\"))\n\n# Calculate the difference between login and logout: time_online\ntime_online <- logout - login\n\n# Inspect the variable time_online\n#class(time_online)\ntime_online  ## Time differences in secs\n## [1] 2305   34  837 2398 1852  # Calculate the total time online\nprint(sum(time_online))  ## Time difference of 7426 secs  # Calculate the average time online\nprint(mean(time_online))  ## Time difference of 1485.2 secs",
            "title": "Time is of the essence"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/",
            "text": "apply\n to Matrices and Arrays\n\n\nsweep\n\n\naggregate\n\n\nby\n\n\nsplit\n\n\nstrsplit\n\n\nVectorize\n\n\n\n\n\n\nlapply\n: list-apply\n\n\nsapply\n: simplify-list-apply\n\n\nvapply\n\n\nrapply\n: recursive-list-apply\n\n\n\n\n\n\nmapply\n: multivariate-apply\n\n\nVectorize\n\n\n\n\n\n\nAnd more\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n\n\nManipulate all data classes.\n\n\nTransform one data class into another class\n\n\nAvoid explicit use of loop constructs and speed up the code.\n\n\nAggregate or subset the data.\n\n\n\n\napply\n to Matrices and Arrays\n\u00b6\n\n\nFlatten a matrix into a vector.\n\n\n# Dataset\nX <- matrix(rnorm(30), nrow = 4, ncol = 4)\nX\n\n\n\n\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -0.1902972 -0.3250492  0.1597852 -0.4878550\n## [2,] -0.4195968 -1.8485172 -0.1185904  0.2307813\n## [3,] -0.6387653 -0.7848382 -0.4198662 -0.6314304\n## [4,]  0.7374748 -2.0634558  0.2374686 -1.6572091\n\n\n\n# Sum the values of each column\napply(X, 2, sum)\n\n\n\n\n## [1] -0.5111845 -5.0218604 -0.1412028 -2.5457132\n\n\n\n\n\nX\n is the array or matrix (2D, 3D, etc.).\n\n\nMARGIN=1\n for row, \n2\n for column.\n\n\nFUN=sum\n, \nmean\n, \nmedian\n, \nmin\n, \nmax\n, \nvar\n, \nsd\n, \nrange\n,\n\n\nlength\n, \nskew\n, \nkurtosis\n, \nabs\n, \nround\n, \ntolower\n,\n\n\ntoupper\n, etc.\n\n\nuser-defined function such as\n\n\nFUN = function(x) { c(m = mean(x), s = sd(x)) }\n.\n\n\n\n\n\n\n\n# More examples\napply(X, 2, max)\n\n\n\n\n## [1]  0.7374748 -0.3250492  0.2374686  0.2307813\n\n\n\napply(X, 2, range)\n\n\n\n\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -0.6387653 -2.0634558 -0.4198662 -1.6572091\n## [2,]  0.7374748 -0.3250492  0.2374686  0.2307813\n\n\n\napply(X, 2, abs)\n\n\n\n\n##           [,1]      [,2]      [,3]      [,4]\n## [1,] 0.1902972 0.3250492 0.1597852 0.4878550\n## [2,] 0.4195968 1.8485172 0.1185904 0.2307813\n## [3,] 0.6387653 0.7848382 0.4198662 0.6314304\n## [4,] 0.7374748 2.0634558 0.2374686 1.6572091\n\n\n\napply(X, 2, round, 2)\n\n\n\n\n##       [,1]  [,2]  [,3]  [,4]\n## [1,] -0.19 -0.33  0.16 -0.49\n## [2,] -0.42 -1.85 -0.12  0.23\n## [3,] -0.64 -0.78 -0.42 -0.63\n## [4,]  0.74 -2.06  0.24 -1.66\n\n\n\nLambda & custom function\n\n\n# Dataset\nX\n\n\n\n\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -0.1902972 -0.3250492  0.1597852 -0.4878550\n## [2,] -0.4195968 -1.8485172 -0.1185904  0.2307813\n## [3,] -0.6387653 -0.7848382 -0.4198662 -0.6314304\n## [4,]  0.7374748 -2.0634558  0.2374686 -1.6572091\n\n\n\n# Built-in function\napply(X, 2, max)\n\n\n\n\n## [1]  0.7374748 -0.3250492  0.2374686  0.2307813\n\n\n\n# Lambda function\napply(X, 2, function(x) x + 10)\n\n\n\n\n##           [,1]     [,2]      [,3]      [,4]\n## [1,]  9.809703 9.674951 10.159785  9.512145\n## [2,]  9.580403 8.151483  9.881410 10.230781\n## [3,]  9.361235 9.215162  9.580134  9.368570\n## [4,] 10.737475 7.936544 10.237469  8.342791\n\n\n\n# Custom function\nselect_first <- function(x) {\n    return(x[1])\n}\napply(X, 2, select_first)\n\n\n\n\n## [1] -0.1902972 -0.3250492  0.1597852 -0.4878550\n\n\n\n# Custom function with two arguments\nselect_el <- function(x, i) { \n  x[i] \n}\napply(X, 2, select_el, i = 2)\n\n\n\n\n## [1] -0.4195968 -1.8485172 -0.1185904  0.2307813\n\n\n\nStrings\n\n\n# Dataset\nY <- matrix(c('a', 'b', 'c', 'd'), nrow = 2, ncol = 2)\nY\n\n\n\n\n##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"\n\n\n\n# Change the format\napply(Y, 2, toupper)\n\n\n\n\n##      [,1] [,2]\n## [1,] \"A\"  \"C\" \n## [2,] \"B\"  \"D\"\n\n\n\nsweep\n\u00b6\n\n\nSeveral steps\n\n\n# Dataset\ndataPoints <- matrix(4:15, nrow = 4, ncol = 3)\ndataPoints\n\n\n\n\n##      [,1] [,2] [,3]\n## [1,]    4    8   12\n## [2,]    5    9   13\n## [3,]    6   10   14\n## [4,]    7   11   15\n\n\n\n# Find means (center) per column with `apply()`\ndataPoints_means <- apply(dataPoints, 2, mean)\ndataPoints_means\n\n\n\n\n## [1]  5.5  9.5 13.5\n\n\n\n# Find standard deviation (dispersion) with `apply()`\ndataPoints_sdev <- apply(dataPoints, 2, sd)\ndataPoints_sdev\n\n\n\n\n## [1] 1.290994 1.290994 1.290994\n\n\n\n# Center the points; shift all the points with respect to their center\ndataPoints_Trans1 <- sweep(dataPoints, 2, dataPoints_means,\"-\")\ndataPoints_Trans1\n\n\n\n\n##      [,1] [,2] [,3]\n## [1,] -1.5 -1.5 -1.5\n## [2,] -0.5 -0.5 -0.5\n## [3,]  0.5  0.5  0.5\n## [4,]  1.5  1.5  1.5\n\n\n\n# Normalize\ndataPoints_Trans2 <- sweep(dataPoints_Trans1, 2, dataPoints_sdev, \"/\")\ndataPoints_Trans2\n\n\n\n\n##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950\n\n\n\nsweep(dataPoints, 2, dataPoints_means, sum)\n\n\n\n\n## [1] 228\n\n\n\n\n\nX\n.\n\n\nMARGIN=1\n for row, \n2\n for column.\n\n\nSTATS\n for the summary statistics to be swept out: \nsum\n, \nmean\n,\n\n\nmedian\n, \nmin\n, \nmax\n, \nvar\n, \nsd\n, \nrange\n, \nlength\n, \nskew\n,\n\n\nkurtosis\n, \nse\n, etc.\n\n\nFUN=\"-\"\n, \n\"+\"\n, \n\"/\"\n, \n\"*\"\n, \n\"**\"\n, etc.\n\n\nuser-defined function such as\n\n\nFUN = function(x) { c(m = mean(x), s = sd(x)) }\n.\n\n\n\n\nOne step\n\n\n# Normalize the data with a nested call\ndataPoints_Trans <- sweep(sweep(dataPoints, 2, dataPoints_means,\"-\"), 2, dataPoints_sdev,\"/\")\n\ndataPoints_Trans\n\n\n\n\n##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950\n\n\n\nEven simpler\n\n\n# Automatic data scaling\nscale(dataPoints)\n\n\n\n\n##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950\n## attr(,\"scaled:center\")\n## [1]  5.5  9.5 13.5\n## attr(,\"scaled:scale\")\n## [1] 1.290994 1.290994 1.290994\n\n\n\naggregate\n\u00b6\n\n\n# Dataset\nhead(Mydf, 15)\n\n\n\n\n##     X DepPC DProgr Qty Delivered\n## 1   1    90      1   7     FALSE\n## 2   2    91      2   8      TRUE\n## 3   3    92      3   9     FALSE\n## 4   4    93      4  10      TRUE\n## 5   5    94      5  11      TRUE\n## 6   6    75      6  12     FALSE\n## 7   7    90      7  13      TRUE\n## 8   8    91      8  14      TRUE\n## 9   9    92      9  15     FALSE\n## 10 10    93     10  16     FALSE\n## 11 11    94     11  17      TRUE\n## 12 12    75     12  18     FALSE\n## 13 13    90     13  19     FALSE\n## 14 14    91     14  20      TRUE\n## 15 15    92     15  21      TRUE\n\n\n\n# Show data types for each column\nsapply(Mydf, class)\n\n\n\n\n##         X     DepPC    DProgr       Qty Delivered \n## \"integer\" \"integer\" \"integer\" \"integer\" \"logical\"\n\n\n\n# Return number of rows and columns\ndim(Mydf)\n\n\n\n\n## [1] 120   5\n\n\n\nnrow(Mydf)\n\n\n\n\n## [1] 120\n\n\n\nncol(Mydf)\n\n\n\n\n## [1] 5\n\n\n\n# How many departments? \nunique(Mydf$DepPC)\n\n\n\n\n## [1] 90 91 92 93 94 75\n\n\n\n# Dataset\nhead(Mydf, 5)\n\n\n\n\n##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE\n\n\n\n# Aggregate by a variable (categories) and sum up another variable\naggregate(Mydf$Qty, by = Mydf[\"DepPC\"], FUN = sum)\n\n\n\n\n##   DepPC   x\n## 1    75 878\n## 2    90 689\n## 3    91 684\n## 4    92 701\n## 5    93 707\n## 6    94 802\n\n\n\n# Aggregate by a variable (categories) and extract descriptive stats from another variable\naggregate(Mydf$Qty, by = Mydf[\"DepPC\"], FUN = summary)\n\n\n\n\n##   DepPC x.Min. x.1st Qu. x.Median x.Mean x.3rd Qu. x.Max.\n## 1    75   4.00     15.25    23.50  43.90     74.50 124.00\n## 2    90   2.00     11.75    19.50  34.45     27.25 119.00\n## 3    91   3.00      9.00    19.00  34.20     26.25 120.00\n## 4    92   1.00     10.00    20.00  35.05     27.25 121.00\n## 5    93   2.00     11.00    19.00  35.35     27.25 122.00\n## 6    94   3.00     12.00    20.00  40.10     46.50 123.00\n\n\n\nby\n\u00b6\n\n\nAn alternative to \naggregate\n with pros and cons.\n\n\n# Dataset\nhead(Mydf, 5)\n\n\n\n\n##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE\n\n\n\nby(Mydf$Qty, Mydf[\"DepPC\"], FUN = sum)\n\n\n\n\n## DepPC: 75\n## [1] 878\n## -------------------------------------------------------- \n## DepPC: 90\n## [1] 689\n## -------------------------------------------------------- \n## DepPC: 91\n## [1] 684\n## -------------------------------------------------------- \n## DepPC: 92\n## [1] 701\n## -------------------------------------------------------- \n## DepPC: 93\n## [1] 707\n## -------------------------------------------------------- \n## DepPC: 94\n## [1] 802\n\n\n\nby(Mydf$Qty, Mydf[\"DepPC\"], FUN = summary)\n\n\n\n\n## DepPC: 75\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    4.00   15.25   23.50   43.90   74.50  124.00 \n## -------------------------------------------------------- \n## DepPC: 90\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00   11.75   19.50   34.45   27.25  119.00 \n## -------------------------------------------------------- \n## DepPC: 91\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    3.00    9.00   19.00   34.20   26.25  120.00 \n## -------------------------------------------------------- \n## DepPC: 92\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    1.00   10.00   20.00   35.05   27.25  121.00 \n## -------------------------------------------------------- \n## DepPC: 93\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00   11.00   19.00   35.35   27.25  122.00 \n## -------------------------------------------------------- \n## DepPC: 94\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##     3.0    12.0    20.0    40.1    46.5   123.0\n\n\n\nsplit\n\u00b6\n\n\n# Dataset\nhead(Mydf, 5)\n\n\n\n\n##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE\n\n\n\nnrow(Mydf)\n\n\n\n\n## [1] 120\n\n\n\n# Split with a variable (categories)\nsplit(Mydf$Qty, Mydf[\"DepPC\"])\n\n\n\n\n## $`75`\n##  [1]  12  18  24  30  13  19 100 106 112 118 124   7  13  19  25  16  23\n## [18]  29   4  66\n## \n## $`90`\n##  [1]   7  13  19  25  31  14  20 101 107 113 119   2   8  14  20  26  17\n## [18]  24   4   5\n## \n## $`91`\n##  [1]   8  14  20  26   9  15  21 102 108 114 120   3   9  15  21  27  18\n## [18]  25   3   6\n## \n## $`92`\n##  [1]   9  15  21  27  10  16  22 103 109 115 121   4  10  16  22  28  19\n## [18]  26   1   7\n## \n## $`93`\n##  [1]  10  16  22  28  11  17  23 104 110 116 122   5  11  17  23  14  21\n## [18]  27   2   8\n## \n## $`94`\n##  [1]  11  17  23  29  12  18  99 105 111 117 123   6  12  18  24  15  22\n## [18]  28   3   9\n\n\n\n# Split by row\nunlist(Mydf$Qty)\n\n\n\n\n##   [1]   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23\n##  [18]  24  25  26  27  28  29  30  31   9  10  11  12  13  14  15  16  17\n##  [35]  18  19  20  21  22  23  99 100 101 102 103 104 105 106 107 108 109\n##  [52] 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124   2   3\n##  [69]   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20\n##  [86]  21  22  23  24  25  26  27  28  14  15  16  17  18  19  21  22  23\n## [103]  24  25  26  27  28  29   4   3   1   2   3   4   5   6   7   8   9\n## [120]  66\n\n\n\nsplit(Mydf$Qty, c(60, 120))\n\n\n\n\n## $`60`\n##  [1]   7   9  11  13  15  17  19  21  23  25  27  29  31  10  12  14  16\n## [18]  18  20  22  99 101 103 105 107 109 111 113 115 117 119 121 123   2\n## [35]   4   6   8  10  12  14  16  18  20  22  24  26  28  15  17  19  22\n## [52]  24  26  28   4   1   3   5   7   9\n## \n## $`120`\n##  [1]   8  10  12  14  16  18  20  22  24  26  28  30   9  11  13  15  17\n## [18]  19  21  23 100 102 104 106 108 110 112 114 116 118 120 122 124   3\n## [35]   5   7   9  11  13  15  17  19  21  23  25  27  14  16  18  21  23\n## [52]  25  27  29   3   2   4   6   8  66\n\n\n\nstrsplit\n\u00b6\n\n\n# The vector pioneers\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\n# Split names from birth year: split_math\nsplit_math <- strsplit(pioneers, ':')\n\n\n\n\nVectorize\n\u00b6\n\n\nVectorize a scalar function.\n\n\n# Scalar function\nrep(c(1, 2), 2)\n\n\n\n\n## [1] 1 2 1 2\n\n\n\n# Vector function\nvrep <- Vectorize(rep.int)\nvrep(c(1, 2), 2)\n\n\n\n\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    2\n\n\n\n# Scalar function\nf <- function(x = 1:3, y) c(x, y)\nf(1:3, 1:3)\n\n\n\n\n## [1] 1 2 3 1 2 3\n\n\n\n# Vector function\nvf <- Vectorize(f, SIMPLIFY = FALSE)\nvf(1:3, 1:3)\n\n\n\n\n## [[1]]\n## [1] 1 1\n## \n## [[2]]\n## [1] 2 2\n## \n## [[3]]\n## [1] 3 3\n\n\n\nlapply\n: list-apply\n\u00b6\n\n\nFor lists, vectors, and data frames.\n\n\n# Dataset\nA <- matrix(1:9, nrow = 3, ncol = 3)\nB <- matrix(10:18, nrow = 3, ncol = 3)\nC <- matrix(19:28, nrow = 3, ncol = 3)\n\n# Create a list of matrices; an array (a 3D matrix)\nMyList <- list(A,B,C)\nMyList\n\n\n\n\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n# Extract values\nMyList[[1]]\n\n\n\n\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n\n\n\nMyList[[1]][1,]\n\n\n\n\n## [1] 1 4 7\n\n\n\nMyList[[1]][,1]\n\n\n\n\n## [1] 1 2 3\n\n\n\nMyList[[1]][1,1]\n\n\n\n\n## [1] 1\n\n\n\n# Extract the 2nd column from `MyList` with the selection operator `[` with `lapply()`\nlapply(MyList,\"[\", , 2)\n\n\n\n\n## [[1]]\n## [1] 4 5 6\n## \n## [[2]]\n## [1] 13 14 15\n## \n## [[3]]\n## [1] 22 23 24\n\n\n\n# Extract the 1st row from `MyList`\nlapply(MyList,\"[\", 1, )\n\n\n\n\n## [[1]]\n## [1] 1 4 7\n## \n## [[2]]\n## [1] 10 13 16\n## \n## [[3]]\n## [1] 19 22 25\n\n\n\nLambda & custom function\n\n\n# Dataset\nMyList\n\n\n\n\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n# Built-in function\nlapply(MyList, max)\n\n\n\n\n## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 18\n## \n## [[3]]\n## [1] 27\n\n\n\n# Lambda function\nlapply(MyList, function(x) max(x) + 10)\n\n\n\n\n## [[1]]\n## [1] 19\n## \n## [[2]]\n## [1] 28\n## \n## [[3]]\n## [1] 37\n\n\n\n# Custom function\nselect_first <- function(x) {\n    return(x[1])\n}\nlapply(MyList, select_first)\n\n\n\n\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 10\n## \n## [[3]]\n## [1] 19\n\n\n\n# Custom function with two arguments\nselect_el <- function(x, i) { \n  x[i] \n}\nlapply(MyList, select_el, i = 2)\n\n\n\n\n## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20\n\n\n\nStrings\n\n\n# Dataset\nY\n\n\n\n\n##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"\n\n\n\n# Change the format\nlapply(Y, toupper)\n\n\n\n\n## [[1]]\n## [1] \"A\"\n## \n## [[2]]\n## [1] \"B\"\n## \n## [[3]]\n## [1] \"C\"\n## \n## [[4]]\n## [1] \"D\"\n\n\n\nsapply\n: simplify-list-apply\n\u00b6\n\n\nA wrapper that simplifies \nlapply\n.\n\n\n# Dataset\nMyList\n\n\n\n\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n# Return a list with `lapply()`\nlapply(MyList,\"[\", 2, 1)\n\n\n\n\n## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20\n\n\n\n# Return a vector with `sapply()`\nsapply(MyList,\"[\", 2, 1) # same result, but simpler\n\n\n\n\n## [1]  2 11 20\n\n\n\n# Return a list with `sapply()`\nsapply(MyList,\"[\", 2, 1, simplify = T) # by default\n\n\n\n\n## [1]  2 11 20\n\n\n\nsapply(MyList,\"[\", 2, 1, simplify = F)\n\n\n\n\n## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20\n\n\n\n# Return a vector with `unlist()`\nunlist(lapply(MyList,\"[\", 2, 1)) # similar\n\n\n\n\n## [1]  2 11 20\n\n\n\nLambda & custom function\n\n\n# Dataset\nMyList\n\n\n\n\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n# Built-in function\nsapply(MyList, max)\n\n\n\n\n## [1]  9 18 27\n\n\n\n# Lambda function\nlapply(MyList, function(x) max(x) + 10)\n\n\n\n\n## [[1]]\n## [1] 19\n## \n## [[2]]\n## [1] 28\n## \n## [[3]]\n## [1] 37\n\n\n\n# Custom function\nselect_first <- function(x) {\n    return(x[1])\n}\nsapply(MyList, select_first)\n\n\n\n\n## [1]  1 10 19\n\n\n\n# Custom function with two arguments\nselect_el <- function(x, i) { \n  x[i] \n}\nsapply(MyList, select_el, i = 2)\n\n\n\n\n## [1]  2 11 20\n\n\n\nStrings\n\n\n# Dataset\nY\n\n\n\n\n##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"\n\n\n\n# Change the format\nsapply(Y, toupper)\n\n\n\n\n##   a   b   c   d \n## \"A\" \"B\" \"C\" \"D\"\n\n\n\nvapply\n\u00b6\n\n\nA variant.\n\n\nrapply\n: recursive-list-apply\n\u00b6\n\n\n# Dataset\nMyList\n\n\n\n\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n# Build-in function\nrapply(MyList, sqrt, classes = \"ANY\", how = \"replace\")\n\n\n\n\n## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152\n\n\n\n# Lambda or custom the function\nrapply(MyList, function(x) x^2, classes = \"ANY\", how = \"replace\")\n\n\n\n\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1   16   49\n## [2,]    4   25   64\n## [3,]    9   36   81\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]  100  169  256\n## [2,]  121  196  289\n## [3,]  144  225  324\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]  361  484  625\n## [2,]  400  529  676\n## [3,]  441  576  729\n\n\n\nselect_first <- function(x) {\n    return(x[1])\n}\nrapply(MyList, select_first, classes = \"ANY\", how = \"replace\")\n\n\n\n\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 10\n## \n## [[3]]\n## [1] 19\n\n\n\n# Change classes =\nclass(MyList) # the object\n\n\n\n\n## [1] \"list\"\n\n\n\nclass(MyList[[1]][1]) # each entry\n\n\n\n\n## [1] \"integer\"\n\n\n\nrapply(MyList, sqrt, classes = \"list\", how = \"replace\") # not work\n\n\n\n\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\nrapply(MyList, sqrt, classes = \"ANY\", how = \"replace\")\n\n\n\n\n## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152\n\n\n\n# Change how = \nrapply(MyList, sqrt, classes = \"ANY\", how = \"list\")\n\n\n\n\n## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152\n\n\n\nrapply(MyList, sqrt, classes = \"ANY\", how = \"unlist\")\n\n\n\n\n##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751\n##  [8] 2.828427 3.000000 3.162278 3.316625 3.464102 3.605551 3.741657\n## [15] 3.872983 4.000000 4.123106 4.242641 4.358899 4.472136 4.582576\n## [22] 4.690416 4.795832 4.898979 5.000000 5.099020 5.196152\n\n\n\n\n\nBuilt-in or custom \nfunction(x) x\n.\n\n\nclasses = \"ANY\"\n for any object classes or a given object class.\n\n    Useful when the data is mixed and we want to focus on one\n\n    class only.\n\n\nhow = \"replace\"\n or \n\"list\"\n, \n\"unlist\"\n\n\n\n\nmapply\n: multivariate-apply\n\u00b6\n\n\n# Create a 4x4 matrix\nQ1 <- matrix(c(rep(1, 4), rep(2, 4), rep(3, 4), rep(4, 4)),4,4)\nQ1\n\n\n\n\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    1    2    3    4\n## [3,]    1    2    3    4\n## [4,]    1    2    3    4\n\n\n\n# Or use `mapply()`\nQ2 <- mapply(rep, 1:4, 4)\nQ2\n\n\n\n\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    1    2    3    4\n## [3,]    1    2    3    4\n## [4,]    1    2    3    4\n\n\n\nVectorize arguments to a function, \nrep\n, that is not usually accepting\n\nvectors as arguments. Applies a function to multiple lists, \n1:4\n, or\n\nmultiple vector, \nc()\n, arguments.\n\n\nVectorize\n\u00b6\n\n\nVectorize a scaler function.\n\n\n# Scalar function\nrep(c(1, 2), 2)\n\n\n\n\n## [1] 1 2 1 2\n\n\n\n# Vector function\nvrep <- Vectorize(rep.int)\nvrep(c(1, 2), 2)\n\n\n\n\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    2\n\n\n\n# Scalar function\nf <- function(x = 1:3, y) c(x, y)\nf(1:3, 1:3)\n\n\n\n\n## [1] 1 2 3 1 2 3\n\n\n\n# Vector function\nvf <- Vectorize(f, SIMPLIFY = FALSE)\nvf(1:3, 1:3)\n\n\n\n\n## [[1]]\n## [1] 1 1\n## \n## [[2]]\n## [1] 2 2\n## \n## [[3]]\n## [1] 3 3\n\n\n\nAnd more\n\u00b6\n\n\n\n\ntapply\n.\n\n\neapply\n.",
            "title": "Intermediate R - The apply Family"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#sweep",
            "text": "Several steps  # Dataset\ndataPoints <- matrix(4:15, nrow = 4, ncol = 3)\ndataPoints  ##      [,1] [,2] [,3]\n## [1,]    4    8   12\n## [2,]    5    9   13\n## [3,]    6   10   14\n## [4,]    7   11   15  # Find means (center) per column with `apply()`\ndataPoints_means <- apply(dataPoints, 2, mean)\ndataPoints_means  ## [1]  5.5  9.5 13.5  # Find standard deviation (dispersion) with `apply()`\ndataPoints_sdev <- apply(dataPoints, 2, sd)\ndataPoints_sdev  ## [1] 1.290994 1.290994 1.290994  # Center the points; shift all the points with respect to their center\ndataPoints_Trans1 <- sweep(dataPoints, 2, dataPoints_means,\"-\")\ndataPoints_Trans1  ##      [,1] [,2] [,3]\n## [1,] -1.5 -1.5 -1.5\n## [2,] -0.5 -0.5 -0.5\n## [3,]  0.5  0.5  0.5\n## [4,]  1.5  1.5  1.5  # Normalize\ndataPoints_Trans2 <- sweep(dataPoints_Trans1, 2, dataPoints_sdev, \"/\")\ndataPoints_Trans2  ##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950  sweep(dataPoints, 2, dataPoints_means, sum)  ## [1] 228   X .  MARGIN=1  for row,  2  for column.  STATS  for the summary statistics to be swept out:  sum ,  mean ,  median ,  min ,  max ,  var ,  sd ,  range ,  length ,  skew ,  kurtosis ,  se , etc.  FUN=\"-\" ,  \"+\" ,  \"/\" ,  \"*\" ,  \"**\" , etc.  user-defined function such as  FUN = function(x) { c(m = mean(x), s = sd(x)) } .   One step  # Normalize the data with a nested call\ndataPoints_Trans <- sweep(sweep(dataPoints, 2, dataPoints_means,\"-\"), 2, dataPoints_sdev,\"/\")\n\ndataPoints_Trans  ##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950  Even simpler  # Automatic data scaling\nscale(dataPoints)  ##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950\n## attr(,\"scaled:center\")\n## [1]  5.5  9.5 13.5\n## attr(,\"scaled:scale\")\n## [1] 1.290994 1.290994 1.290994",
            "title": "sweep"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#aggregate",
            "text": "# Dataset\nhead(Mydf, 15)  ##     X DepPC DProgr Qty Delivered\n## 1   1    90      1   7     FALSE\n## 2   2    91      2   8      TRUE\n## 3   3    92      3   9     FALSE\n## 4   4    93      4  10      TRUE\n## 5   5    94      5  11      TRUE\n## 6   6    75      6  12     FALSE\n## 7   7    90      7  13      TRUE\n## 8   8    91      8  14      TRUE\n## 9   9    92      9  15     FALSE\n## 10 10    93     10  16     FALSE\n## 11 11    94     11  17      TRUE\n## 12 12    75     12  18     FALSE\n## 13 13    90     13  19     FALSE\n## 14 14    91     14  20      TRUE\n## 15 15    92     15  21      TRUE  # Show data types for each column\nsapply(Mydf, class)  ##         X     DepPC    DProgr       Qty Delivered \n## \"integer\" \"integer\" \"integer\" \"integer\" \"logical\"  # Return number of rows and columns\ndim(Mydf)  ## [1] 120   5  nrow(Mydf)  ## [1] 120  ncol(Mydf)  ## [1] 5  # How many departments? \nunique(Mydf$DepPC)  ## [1] 90 91 92 93 94 75  # Dataset\nhead(Mydf, 5)  ##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE  # Aggregate by a variable (categories) and sum up another variable\naggregate(Mydf$Qty, by = Mydf[\"DepPC\"], FUN = sum)  ##   DepPC   x\n## 1    75 878\n## 2    90 689\n## 3    91 684\n## 4    92 701\n## 5    93 707\n## 6    94 802  # Aggregate by a variable (categories) and extract descriptive stats from another variable\naggregate(Mydf$Qty, by = Mydf[\"DepPC\"], FUN = summary)  ##   DepPC x.Min. x.1st Qu. x.Median x.Mean x.3rd Qu. x.Max.\n## 1    75   4.00     15.25    23.50  43.90     74.50 124.00\n## 2    90   2.00     11.75    19.50  34.45     27.25 119.00\n## 3    91   3.00      9.00    19.00  34.20     26.25 120.00\n## 4    92   1.00     10.00    20.00  35.05     27.25 121.00\n## 5    93   2.00     11.00    19.00  35.35     27.25 122.00\n## 6    94   3.00     12.00    20.00  40.10     46.50 123.00",
            "title": "aggregate"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#by",
            "text": "An alternative to  aggregate  with pros and cons.  # Dataset\nhead(Mydf, 5)  ##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE  by(Mydf$Qty, Mydf[\"DepPC\"], FUN = sum)  ## DepPC: 75\n## [1] 878\n## -------------------------------------------------------- \n## DepPC: 90\n## [1] 689\n## -------------------------------------------------------- \n## DepPC: 91\n## [1] 684\n## -------------------------------------------------------- \n## DepPC: 92\n## [1] 701\n## -------------------------------------------------------- \n## DepPC: 93\n## [1] 707\n## -------------------------------------------------------- \n## DepPC: 94\n## [1] 802  by(Mydf$Qty, Mydf[\"DepPC\"], FUN = summary)  ## DepPC: 75\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    4.00   15.25   23.50   43.90   74.50  124.00 \n## -------------------------------------------------------- \n## DepPC: 90\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00   11.75   19.50   34.45   27.25  119.00 \n## -------------------------------------------------------- \n## DepPC: 91\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    3.00    9.00   19.00   34.20   26.25  120.00 \n## -------------------------------------------------------- \n## DepPC: 92\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    1.00   10.00   20.00   35.05   27.25  121.00 \n## -------------------------------------------------------- \n## DepPC: 93\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00   11.00   19.00   35.35   27.25  122.00 \n## -------------------------------------------------------- \n## DepPC: 94\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##     3.0    12.0    20.0    40.1    46.5   123.0",
            "title": "by"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#split",
            "text": "# Dataset\nhead(Mydf, 5)  ##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE  nrow(Mydf)  ## [1] 120  # Split with a variable (categories)\nsplit(Mydf$Qty, Mydf[\"DepPC\"])  ## $`75`\n##  [1]  12  18  24  30  13  19 100 106 112 118 124   7  13  19  25  16  23\n## [18]  29   4  66\n## \n## $`90`\n##  [1]   7  13  19  25  31  14  20 101 107 113 119   2   8  14  20  26  17\n## [18]  24   4   5\n## \n## $`91`\n##  [1]   8  14  20  26   9  15  21 102 108 114 120   3   9  15  21  27  18\n## [18]  25   3   6\n## \n## $`92`\n##  [1]   9  15  21  27  10  16  22 103 109 115 121   4  10  16  22  28  19\n## [18]  26   1   7\n## \n## $`93`\n##  [1]  10  16  22  28  11  17  23 104 110 116 122   5  11  17  23  14  21\n## [18]  27   2   8\n## \n## $`94`\n##  [1]  11  17  23  29  12  18  99 105 111 117 123   6  12  18  24  15  22\n## [18]  28   3   9  # Split by row\nunlist(Mydf$Qty)  ##   [1]   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23\n##  [18]  24  25  26  27  28  29  30  31   9  10  11  12  13  14  15  16  17\n##  [35]  18  19  20  21  22  23  99 100 101 102 103 104 105 106 107 108 109\n##  [52] 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124   2   3\n##  [69]   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20\n##  [86]  21  22  23  24  25  26  27  28  14  15  16  17  18  19  21  22  23\n## [103]  24  25  26  27  28  29   4   3   1   2   3   4   5   6   7   8   9\n## [120]  66  split(Mydf$Qty, c(60, 120))  ## $`60`\n##  [1]   7   9  11  13  15  17  19  21  23  25  27  29  31  10  12  14  16\n## [18]  18  20  22  99 101 103 105 107 109 111 113 115 117 119 121 123   2\n## [35]   4   6   8  10  12  14  16  18  20  22  24  26  28  15  17  19  22\n## [52]  24  26  28   4   1   3   5   7   9\n## \n## $`120`\n##  [1]   8  10  12  14  16  18  20  22  24  26  28  30   9  11  13  15  17\n## [18]  19  21  23 100 102 104 106 108 110 112 114 116 118 120 122 124   3\n## [35]   5   7   9  11  13  15  17  19  21  23  25  27  14  16  18  21  23\n## [52]  25  27  29   3   2   4   6   8  66",
            "title": "split"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#strsplit",
            "text": "# The vector pioneers\npioneers <- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\n# Split names from birth year: split_math\nsplit_math <- strsplit(pioneers, ':')",
            "title": "strsplit"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#vectorize",
            "text": "Vectorize a scalar function.  # Scalar function\nrep(c(1, 2), 2)  ## [1] 1 2 1 2  # Vector function\nvrep <- Vectorize(rep.int)\nvrep(c(1, 2), 2)  ##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    2  # Scalar function\nf <- function(x = 1:3, y) c(x, y)\nf(1:3, 1:3)  ## [1] 1 2 3 1 2 3  # Vector function\nvf <- Vectorize(f, SIMPLIFY = FALSE)\nvf(1:3, 1:3)  ## [[1]]\n## [1] 1 1\n## \n## [[2]]\n## [1] 2 2\n## \n## [[3]]\n## [1] 3 3",
            "title": "Vectorize"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#lapply-list-apply",
            "text": "For lists, vectors, and data frames.  # Dataset\nA <- matrix(1:9, nrow = 3, ncol = 3)\nB <- matrix(10:18, nrow = 3, ncol = 3)\nC <- matrix(19:28, nrow = 3, ncol = 3)\n\n# Create a list of matrices; an array (a 3D matrix)\nMyList <- list(A,B,C)\nMyList  ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27  # Extract values\nMyList[[1]]  ##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9  MyList[[1]][1,]  ## [1] 1 4 7  MyList[[1]][,1]  ## [1] 1 2 3  MyList[[1]][1,1]  ## [1] 1  # Extract the 2nd column from `MyList` with the selection operator `[` with `lapply()`\nlapply(MyList,\"[\", , 2)  ## [[1]]\n## [1] 4 5 6\n## \n## [[2]]\n## [1] 13 14 15\n## \n## [[3]]\n## [1] 22 23 24  # Extract the 1st row from `MyList`\nlapply(MyList,\"[\", 1, )  ## [[1]]\n## [1] 1 4 7\n## \n## [[2]]\n## [1] 10 13 16\n## \n## [[3]]\n## [1] 19 22 25  Lambda & custom function  # Dataset\nMyList  ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27  # Built-in function\nlapply(MyList, max)  ## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 18\n## \n## [[3]]\n## [1] 27  # Lambda function\nlapply(MyList, function(x) max(x) + 10)  ## [[1]]\n## [1] 19\n## \n## [[2]]\n## [1] 28\n## \n## [[3]]\n## [1] 37  # Custom function\nselect_first <- function(x) {\n    return(x[1])\n}\nlapply(MyList, select_first)  ## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 10\n## \n## [[3]]\n## [1] 19  # Custom function with two arguments\nselect_el <- function(x, i) { \n  x[i] \n}\nlapply(MyList, select_el, i = 2)  ## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20  Strings  # Dataset\nY  ##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"  # Change the format\nlapply(Y, toupper)  ## [[1]]\n## [1] \"A\"\n## \n## [[2]]\n## [1] \"B\"\n## \n## [[3]]\n## [1] \"C\"\n## \n## [[4]]\n## [1] \"D\"",
            "title": "lapply: list-apply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#sapply-simplify-list-apply",
            "text": "A wrapper that simplifies  lapply .  # Dataset\nMyList  ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27  # Return a list with `lapply()`\nlapply(MyList,\"[\", 2, 1)  ## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20  # Return a vector with `sapply()`\nsapply(MyList,\"[\", 2, 1) # same result, but simpler  ## [1]  2 11 20  # Return a list with `sapply()`\nsapply(MyList,\"[\", 2, 1, simplify = T) # by default  ## [1]  2 11 20  sapply(MyList,\"[\", 2, 1, simplify = F)  ## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20  # Return a vector with `unlist()`\nunlist(lapply(MyList,\"[\", 2, 1)) # similar  ## [1]  2 11 20  Lambda & custom function  # Dataset\nMyList  ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27  # Built-in function\nsapply(MyList, max)  ## [1]  9 18 27  # Lambda function\nlapply(MyList, function(x) max(x) + 10)  ## [[1]]\n## [1] 19\n## \n## [[2]]\n## [1] 28\n## \n## [[3]]\n## [1] 37  # Custom function\nselect_first <- function(x) {\n    return(x[1])\n}\nsapply(MyList, select_first)  ## [1]  1 10 19  # Custom function with two arguments\nselect_el <- function(x, i) { \n  x[i] \n}\nsapply(MyList, select_el, i = 2)  ## [1]  2 11 20  Strings  # Dataset\nY  ##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"  # Change the format\nsapply(Y, toupper)  ##   a   b   c   d \n## \"A\" \"B\" \"C\" \"D\"",
            "title": "sapply: simplify-list-apply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#vapply",
            "text": "A variant.",
            "title": "vapply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#rapply-recursive-list-apply",
            "text": "# Dataset\nMyList  ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27  # Build-in function\nrapply(MyList, sqrt, classes = \"ANY\", how = \"replace\")  ## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152  # Lambda or custom the function\nrapply(MyList, function(x) x^2, classes = \"ANY\", how = \"replace\")  ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1   16   49\n## [2,]    4   25   64\n## [3,]    9   36   81\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]  100  169  256\n## [2,]  121  196  289\n## [3,]  144  225  324\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]  361  484  625\n## [2,]  400  529  676\n## [3,]  441  576  729  select_first <- function(x) {\n    return(x[1])\n}\nrapply(MyList, select_first, classes = \"ANY\", how = \"replace\")  ## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 10\n## \n## [[3]]\n## [1] 19  # Change classes =\nclass(MyList) # the object  ## [1] \"list\"  class(MyList[[1]][1]) # each entry  ## [1] \"integer\"  rapply(MyList, sqrt, classes = \"list\", how = \"replace\") # not work  ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27  rapply(MyList, sqrt, classes = \"ANY\", how = \"replace\")  ## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152  # Change how = \nrapply(MyList, sqrt, classes = \"ANY\", how = \"list\")  ## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152  rapply(MyList, sqrt, classes = \"ANY\", how = \"unlist\")  ##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751\n##  [8] 2.828427 3.000000 3.162278 3.316625 3.464102 3.605551 3.741657\n## [15] 3.872983 4.000000 4.123106 4.242641 4.358899 4.472136 4.582576\n## [22] 4.690416 4.795832 4.898979 5.000000 5.099020 5.196152   Built-in or custom  function(x) x .  classes = \"ANY\"  for any object classes or a given object class. \n    Useful when the data is mixed and we want to focus on one \n    class only.  how = \"replace\"  or  \"list\" ,  \"unlist\"",
            "title": "rapply: recursive-list-apply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#mapply-multivariate-apply",
            "text": "# Create a 4x4 matrix\nQ1 <- matrix(c(rep(1, 4), rep(2, 4), rep(3, 4), rep(4, 4)),4,4)\nQ1  ##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    1    2    3    4\n## [3,]    1    2    3    4\n## [4,]    1    2    3    4  # Or use `mapply()`\nQ2 <- mapply(rep, 1:4, 4)\nQ2  ##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    1    2    3    4\n## [3,]    1    2    3    4\n## [4,]    1    2    3    4  Vectorize arguments to a function,  rep , that is not usually accepting \nvectors as arguments. Applies a function to multiple lists,  1:4 , or \nmultiple vector,  c() , arguments.",
            "title": "mapply: multivariate-apply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#vectorize_1",
            "text": "Vectorize a scaler function.  # Scalar function\nrep(c(1, 2), 2)  ## [1] 1 2 1 2  # Vector function\nvrep <- Vectorize(rep.int)\nvrep(c(1, 2), 2)  ##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    2  # Scalar function\nf <- function(x = 1:3, y) c(x, y)\nf(1:3, 1:3)  ## [1] 1 2 3 1 2 3  # Vector function\nvf <- Vectorize(f, SIMPLIFY = FALSE)\nvf(1:3, 1:3)  ## [[1]]\n## [1] 1 1\n## \n## [[2]]\n## [1] 2 2\n## \n## [[3]]\n## [1] 3 3",
            "title": "Vectorize"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#and-more",
            "text": "tapply .  eapply .",
            "title": "And more"
        },
        {
            "location": "/Code___Plot_Chunk_Options/",
            "text": "1, Preliminary Options\n\n\n2, Chunks\n\n\n3, Inputting Data\n\n\n4, Formatting Code Chunks\n\n\neval=TRUE\n; show the results (default).\n\n\neval=FALSE\n; or no results.\n\n\nresults='markup'\n; show split code/results/code/results (default).\n\n\neval='asis'\n; show \u2018unboxed\u2019 results.\n\n\neval='hide'\n; show code only.\n\n\neval='hold'\n; show code block/results block.\n\n\necho=TRUE\n; show the code (default).\n\n\necho=FALSE\n; or no code.\n\n\nwarning\n, \nerror\n, \nmessage\n are set to TRUE by default. They can be set of FALSE when running a library() code to avoid polluting the report.\n\n\ntidy=TRUE/FALSE\n; with the \nformatR\n and \nshiny\n packages (you manage spaces and indents) (FALSE by default).\n\n\ncache=TRUE/FALSE; cache the results (FALSE by default).\n\n\ncomment='##'\n; the comments in results (by default).\n\n\ncomment='#'\n; the new comments.\n\n\ncode chunk \n{r}\n.\n\n\ncode chunk \n{code=NULL}\n.\n\n\ncode chunk \n{text}\n.\n\n\ncode chunk \n{python}\n.\n\n\nhightlight=TRUE\n; hightlight the code (default).\n\n\nhightlight=FALSE\n; or not.\n\n\nprompt=TRUE\n; add \n>\n before the code.\n\n\nprompt=FALSE\n; or not (default).\n\n\nstrip.white=TRUE\n; remove white space from the code (default).\n\n\nstrip.white=FALSE\n; or not.\n\n\n\n\n\n\n5, Formatting Plot Chunk\n\n\nfig.path='figure/'\n; new file path for this chunk.\n\n\nThe device prints .png files by default.\n\n\nfig.width= , fig.height=\n; change the box size (\n=7\n by default).\n\n\nfig.width=5, fig.height=5\n.\n\n\nfig.height=3\n.\n\n\nfig.width=3\n.\n\n\nout.height=100, out.width=100\n; in pixels.\n\n\nresize.height=200, resize.width=200\n; resize tike graphics for latex, in pixels.\n\n\nsanitize=TRUE\n; sanitize \u2018tike\u2019 graphics for latex.\n\n\nSet the device arguments:\n\n\ndpi multiplier for .html output on retina screens:\n\n\nfig.align='left'\n or \nfig.align='default'\n.\n\n\nfig.align='right'\n.\n\n\nfig.align='center'\n.\n\n\nFigure captions at the bottom of the plot; figure caption in latex:\n\n\nVersions\n\n\nfig.pos='test'\n; string to be used as the figure position arrangement in latex.\n\n\nShow\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, Preliminary Options\n\u00b6\n\n\nUsually, the following code is set to FALSE (not showing in a report). These are the general options. Code and plot chunks overrule the general options.\n\n\nknitr::opts_chunk$set(echo=TRUE, eval=TRUE, fig.height=3, fig.width=3)\n\n\n\n\n2, Chunks\n\u00b6\n\n\nNaming a chunk is including it in the document outline. The outline is a navigation tool to jump though the document.\n\n\n3, Inputting Data\n\u00b6\n\n\nThe dataset comes from the \nUS Census Bureau\n. On their website, open Excel file \u2018NST-EST2011-02\u2019 about the annual estimates of the resident population.\n\n\nThe data have become an object: a data frame. Check it out, and add column names:\n\n\nhead(USstatePops,3)\n\n\n\n\n##        V1      V2\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013\n\n\n\ncolnames(USstatePops) <- c('State', 'Pop')\n\nhead(USstatePops, 3)\n\n\n\n\n##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013\n\n\n\nCheck out the data frame:\n\n\nstr(USstatePops)\n\n\n\n\n## 'data.frame':    51 obs. of  2 variables:\n##  $ State: Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...\n\n\n\nThe states should be strings, not factors.\n\n\nExtract numbers or strings without any loss from a factor structure:\n\n\n# make a copy for safety\nUSstatePops2 <- USstatePops\n\nUSstatePops2$State <- as.character(levels(USstatePops2$State))\n\n\n\n\nCheck out the new data frame:\n\n\nstr(USstatePops2)\n\n\n\n\n## 'data.frame':    51 obs. of  2 variables:\n##  $ State: chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...\n\n\n\nhead(USstatePops2, 3)\n\n\n\n\n##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013\n\n\n\n4, Formatting Code Chunks\n\u00b6\n\n\neval=TRUE\n; show the results (default).\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\neval=FALSE\n; or no results.\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\nresults='markup'\n; show split code/results/code/results (default).\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nmedian(USstatePops2$Pop)\n\n\n\n\n## [1] 4339362\n\n\n\neval='asis'\n; show \u2018unboxed\u2019 results.\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n\n\n1\n 6053834\n\n\nmedian(USstatePops2$Pop)\n\n\n\n\n\n\n1\n 4339362\n\n\neval='hide'\n; show code only.\n\u00b6\n\n\nmean(USstatePops2$Pop)\nmedian(USstatePops2$Pop)\n\n\n\n\neval='hold'\n; show code block/results block.\n\u00b6\n\n\nmean(USstatePops2$Pop)\nmedian(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n## [1] 4339362\n\n\n\n\n\necho=TRUE\n; show the code (default).\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nmedian(USstatePops2$Pop)\n\n\n\n\n## [1] 4339362\n\n\n\necho=FALSE\n; or no code.\n\u00b6\n\n\n## [1] 6053834\n\n## [1] 4339362\n\n\n\n\n\nwarning\n, \nerror\n, \nmessage\n are set to TRUE by default. They can be set of FALSE when running a library() code to avoid polluting the report.\n\u00b6\n\n\n{r, warning=TRUE, error=TRUE, message=TRUE}\n\n\n\n\n\n\ntidy=TRUE/FALSE\n; with the \nformatR\n and \nshiny\n packages (you manage spaces and indents) (FALSE by default).\n\u00b6\n\n\n{r, tidy=TRUE}\n\n\n\n\n\n\ncache=TRUE/FALSE; cache the results (FALSE by default).\n\u00b6\n\n\nCan be resused in future knits since it \ncreates a subdir (the \u2018cache\u2019)\n with a R workspace, .rdb and .rdx files.\n\n\n{r, cache=TRUE}\n\n\n\n\nThe \ncache.path='cache/'\n can be changed. See \ncache-comments\n, \ncache.lazy\n, \ncache.vars\n, \nautodep\n, \ndependson\n.\n\n\n\n\ncomment='##'\n; the comments in results (by default).\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\ncomment='#'\n; the new comments.\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n# [1] 6053834\n\n\n\n\n\ncode chunk \n{r}\n.\n\u00b6\n\n\nlist <- c(1, 2, 3)\nlist\n\n\n\n\n## [1] 1 2 3\n\n\n\ncode chunk \n{code=NULL}\n.\n\u00b6\n\n\nlist <- c(1, 2, 3)\nlist\n\n\n\n\ncode chunk \n{text}\n.\n\u00b6\n\n\nlist <- c(1, 2, 3)\nlist\n\n\n\n\ncode chunk \n{python}\n.\n\u00b6\n\n\nlist = [1, 2, 3]\nprint(list)\n\n\n\n\nSet up the new language first.\n\n\n\n\nhightlight=TRUE\n; hightlight the code (default).\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nhightlight=FALSE\n; or not.\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nprompt=TRUE\n; add \n>\n before the code.\n\u00b6\n\n\n> mean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nprompt=FALSE\n; or not (default).\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nstrip.white=TRUE\n; remove white space from the code (default).\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nstrip.white=FALSE\n; or not.\n\u00b6\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\n5, Formatting Plot Chunk\n\u00b6\n\n\nPrints the plots in the .html report and and creates a subdir with the plot files (the references).\n\n\nfig.path='figure/'\n; new file path for this chunk.\n\u00b6\n\n\nOtherwise, the path is set in the general options.\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\nThe device prints .png files by default.\n\u00b6\n\n\nIt can be changed to other formats.\n\n\ndev='png'\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\ndev='jpeg'\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\ndev='pdf'\n; \n'pdf'\n cannot be printed in the .html report, but only included in the subdir.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nfig.width= , fig.height=\n; change the box size (\n=7\n by default).\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.width=5, fig.height=5\n.\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.height=3\n.\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.width=3\n.\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nout.height=100, out.width=100\n; in pixels.\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nresize.height=200, resize.width=200\n; resize tike graphics for latex, in pixels.\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nsanitize=TRUE\n; sanitize \u2018tike\u2019 graphics for latex.\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nSet the device arguments:\n\u00b6\n\n\ndev.args=list(bg='yellow', pointsize=10)\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\ndev.args=list(pointsize=8), fig.height=3\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\ndots per inch.\n\u00b6\n\n\ndpi=72\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\ndpi=90\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\ndpi multiplier for .html output on retina screens:\n\u00b6\n\n\nfig.retina=1\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.retina=2\n; double dpi.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nfig.align='left'\n or \nfig.align='default'\n.\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.align='right'\n.\n\u00b6\n\n\nfig.align='center'\n.\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nFigure captions at the bottom of the plot; figure caption in latex:\n\u00b6\n\n\nfig.cap='CAPTION 14'\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nSee:\n\n\n\n\nfig.lp=''\n; figure caption prefix\n\n\nfig.scap=''\n; short figure caption prefix.\n\n\nfig.subcap=''\n; subcaption.\n\n\nfig.env=''\n; the latex environment for figures.\n\n\n\n\n\n\nVersions\n\u00b6\n\n\nfig.keep='high'\n; merge low-level changes into high-level plots.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nSee:\n\n\n\n\nfig.keep='all'\n; keep all plots (low-level changes may produce new plots).\n\n\nfig.keep='first'/'last'\n; keep the first/last plot only.\n\n\nfig.keep='none'\n; discard all plots.\n\n\n\n\n\n\nfig.pos='test'\n; string to be used as the figure position arrangement in latex.\n\u00b6\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\n\n\nShow\n\u00b6\n\n\nfig.show='asis'\n.\n\n\nhist(USstatePops2$Pop, breaks = 10, main = '', xlab = 'Pop')\n\n\n\n\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\n\n\nfig.show='hold'\n; display the plots at the very end of the chunk.\n\n\n\n\n\n\n\nhist(USstatePops2$Pop, breaks = 10, main = '', xlab = 'Pop')\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\n\n\nSee:\n\n\n\n\nfig.show='hide'\n; generate the plots, but not in the final document.\n\n\nfig.show='animate'\n; combine all of the plots created into an animation. Additional packages and settings are required.",
            "title": "Code & Plot Chunk Options"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#2-chunks",
            "text": "Naming a chunk is including it in the document outline. The outline is a navigation tool to jump though the document.",
            "title": "2, Chunks"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#3-inputting-data",
            "text": "The dataset comes from the  US Census Bureau . On their website, open Excel file \u2018NST-EST2011-02\u2019 about the annual estimates of the resident population.  The data have become an object: a data frame. Check it out, and add column names:  head(USstatePops,3)  ##        V1      V2\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013  colnames(USstatePops) <- c('State', 'Pop')\n\nhead(USstatePops, 3)  ##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013  Check out the data frame:  str(USstatePops)  ## 'data.frame':    51 obs. of  2 variables:\n##  $ State: Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...  The states should be strings, not factors.  Extract numbers or strings without any loss from a factor structure:  # make a copy for safety\nUSstatePops2 <- USstatePops\n\nUSstatePops2$State <- as.character(levels(USstatePops2$State))  Check out the new data frame:  str(USstatePops2)  ## 'data.frame':    51 obs. of  2 variables:\n##  $ State: chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...  head(USstatePops2, 3)  ##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013",
            "title": "3, Inputting Data"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#4-formatting-code-chunks",
            "text": "",
            "title": "4, Formatting Code Chunks"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evaltrue-show-the-results-default",
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834",
            "title": "eval=TRUE; show the results (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evalfalse-or-no-results",
            "text": "mean(USstatePops2$Pop)",
            "title": "eval=FALSE; or no results."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#resultsmarkup-show-split-coderesultscoderesults-default",
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834  median(USstatePops2$Pop)  ## [1] 4339362",
            "title": "results='markup'; show split code/results/code/results (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evalasis-show-unboxed-results",
            "text": "mean(USstatePops2$Pop)   1  6053834  median(USstatePops2$Pop)   1  4339362",
            "title": "eval='asis'; show 'unboxed' results."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evalhide-show-code-only",
            "text": "mean(USstatePops2$Pop)\nmedian(USstatePops2$Pop)",
            "title": "eval='hide'; show code only."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evalhold-show-code-blockresults-block",
            "text": "mean(USstatePops2$Pop)\nmedian(USstatePops2$Pop)  ## [1] 6053834\n## [1] 4339362",
            "title": "eval='hold'; show code block/results block."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#echotrue-show-the-code-default",
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834  median(USstatePops2$Pop)  ## [1] 4339362",
            "title": "echo=TRUE; show the code (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#echofalse-or-no-code",
            "text": "## [1] 6053834\n\n## [1] 4339362",
            "title": "echo=FALSE; or no code."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#warning-error-message-are-set-to-true-by-default-they-can-be-set-of-false-when-running-a-library-code-to-avoid-polluting-the-report",
            "text": "{r, warning=TRUE, error=TRUE, message=TRUE}",
            "title": "warning, error, message are set to TRUE by default. They can be set of FALSE when running a library() code to avoid polluting the report."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#tidytruefalse-with-the-formatr-and-shiny-packages-you-manage-spaces-and-indents-false-by-default",
            "text": "{r, tidy=TRUE}",
            "title": "tidy=TRUE/FALSE; with the formatR and shiny packages (you manage spaces and indents) (FALSE by default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#cachetruefalse-cache-the-results-false-by-default",
            "text": "Can be resused in future knits since it  creates a subdir (the \u2018cache\u2019)  with a R workspace, .rdb and .rdx files.  {r, cache=TRUE}  The  cache.path='cache/'  can be changed. See  cache-comments ,  cache.lazy ,  cache.vars ,  autodep ,  dependson .",
            "title": "cache=TRUE/FALSE; cache the results (FALSE by default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#comment-the-comments-in-results-by-default",
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834",
            "title": "comment='##'; the comments in results (by default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#comment-the-new-comments",
            "text": "mean(USstatePops2$Pop)  # [1] 6053834",
            "title": "comment='#'; the new comments."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-r",
            "text": "list <- c(1, 2, 3)\nlist  ## [1] 1 2 3",
            "title": "code chunk {r}."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-codenull",
            "text": "list <- c(1, 2, 3)\nlist",
            "title": "code chunk {code=NULL}."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-text",
            "text": "list <- c(1, 2, 3)\nlist",
            "title": "code chunk {text}."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-python",
            "text": "list = [1, 2, 3]\nprint(list)  Set up the new language first.",
            "title": "code chunk {python}."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#hightlighttrue-hightlight-the-code-default",
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834",
            "title": "hightlight=TRUE; hightlight the code (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#hightlightfalse-or-not",
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834",
            "title": "hightlight=FALSE; or not."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#prompttrue-add-before-the-code",
            "text": "> mean(USstatePops2$Pop)  ## [1] 6053834",
            "title": "prompt=TRUE; add &gt; before the code."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#promptfalse-or-not-default",
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834",
            "title": "prompt=FALSE; or not (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#stripwhitetrue-remove-white-space-from-the-code-default",
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834",
            "title": "strip.white=TRUE; remove white space from the code (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#stripwhitefalse-or-not",
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834",
            "title": "strip.white=FALSE; or not."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#5-formatting-plot-chunk",
            "text": "Prints the plots in the .html report and and creates a subdir with the plot files (the references).",
            "title": "5, Formatting Plot Chunk"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figpathfigure-new-file-path-for-this-chunk",
            "text": "Otherwise, the path is set in the general options.  hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')",
            "title": "fig.path='figure/'; new file path for this chunk."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#the-device-prints-png-files-by-default",
            "text": "It can be changed to other formats.  dev='png' .  hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')   dev='jpeg' .  hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')   dev='pdf' ;  'pdf'  cannot be printed in the .html report, but only included in the subdir.  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "The device prints .png files by default."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figwidth-figheight-change-the-box-size-7-by-default",
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "fig.width= , fig.height=; change the box size (=7 by default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figwidth5-figheight5",
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "fig.width=5, fig.height=5."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figheight3",
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "fig.height=3."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figwidth3",
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "fig.width=3."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#outheight100-outwidth100-in-pixels",
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "out.height=100, out.width=100; in pixels."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#resizeheight200-resizewidth200-resize-tike-graphics-for-latex-in-pixels",
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "resize.height=200, resize.width=200; resize tike graphics for latex, in pixels."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#sanitizetrue-sanitize-tike-graphics-for-latex",
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "sanitize=TRUE; sanitize 'tike' graphics for latex."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#set-the-device-arguments",
            "text": "dev.args=list(bg='yellow', pointsize=10) .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   dev.args=list(pointsize=8), fig.height=3 .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "Set the device arguments:"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#dots-per-inch",
            "text": "dpi=72 .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   dpi=90 .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "dots per inch."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#dpi-multiplier-for-html-output-on-retina-screens",
            "text": "fig.retina=1  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   fig.retina=2 ; double dpi.  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "dpi multiplier for .html output on retina screens:"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figalignleft-or-figaligndefault",
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "fig.align='left' or fig.align='default'."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figalignright",
            "text": "",
            "title": "fig.align='right'."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figaligncenter",
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')",
            "title": "fig.align='center'."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figure-captions-at-the-bottom-of-the-plot-figure-caption-in-latex",
            "text": "fig.cap='CAPTION 14' .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   See:   fig.lp='' ; figure caption prefix  fig.scap='' ; short figure caption prefix.  fig.subcap='' ; subcaption.  fig.env='' ; the latex environment for figures.",
            "title": "Figure captions at the bottom of the plot; figure caption in latex:"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#versions",
            "text": "fig.keep='high' ; merge low-level changes into high-level plots.  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   See:   fig.keep='all' ; keep all plots (low-level changes may produce new plots).  fig.keep='first'/'last' ; keep the first/last plot only.  fig.keep='none' ; discard all plots.",
            "title": "Versions"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figpostest-string-to-be-used-as-the-figure-position-arrangement-in-latex",
            "text": "hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')",
            "title": "fig.pos='test'; string to be used as the figure position arrangement in latex."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#show",
            "text": "fig.show='asis' .  hist(USstatePops2$Pop, breaks = 10, main = '', xlab = 'Pop')   hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')    fig.show='hold' ; display the plots at the very end of the chunk.    hist(USstatePops2$Pop, breaks = 10, main = '', xlab = 'Pop')\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')    See:   fig.show='hide' ; generate the plots, but not in the final document.  fig.show='animate' ; combine all of the plots created into an animation. Additional packages and settings are required.",
            "title": "Show"
        },
        {
            "location": "/Working_with_RStudio_IDE/",
            "text": "Working with RStudio IDE (Part 1)\n\n\n1, Orientation\n\n\n2, Programming\n\n\n3, Project\n\n\n\n\n\n\nWorking with RStudio IDE (Part 2)\n\n\n1, Packages\n\n\n2, Version Control\n\n\n3, Reporting\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nWorking with RStudio IDE (Part 1)\n\u00b6\n\n\nHELP\n\n\n\n\nAlt+Shift+k or Tools/Keyboard Shortcuts Help.\n\n\n\n\n1, Orientation\n\u00b6\n\n\nCommands\n\n\n\n\nCtrl+Up; command history.\n\n\nTab; completion for all.\n\n\nCtrl+L; clear the console.\n\n\nTools/Global Options\u2026 set Up RStudio.\n\n\nview(dataframe)\n; open a spreadsheet, show in new window, sort the\n\n    rows, search, filter the rows.\n\n\n\n\n\n\n\ndf <- data.frame(colA = c(1, 2), colB = c(3, 6))\ndf\n\n\n\n\n##   colA colB\n## 1    1    3\n## 2    2    6\n\n\n\nView(df) # in a new window\n\n\n\n\n\n\nIDE panes\n\n\n\n\nEnvironment pane; load, save, remove objects, read a dataset,\n\n    import dataset.\n\n\nHistory pane; idem, clear all or one item at the time, copy the\n\n    command in the console pane, reload a command with Shift+Enter and\n\n    run it.\n\n\nFile pane; working directories, files, add a new folder, rename\n\n\ngetwd()\n; get working directory.\n\n\nsetwd()\n; set working directory.\n\n\nPlot pane; save (extension, size, resolution).\n\n\nPackage pane; update.\n\n\nHelp pane; pages.\n\n\nViewer pane; more than plots!\n\n\n\n\n2, Programming\n\u00b6\n\n\nScripting\n\n\n\n\nCtrl+Shift+m; pipe or \n%>%\n.\n\n\nAlt+-; \n<-\n.\n\n\nCtrl+Shift+C; add/delete a \n#\n for commenting.\n\n\n\n\n\n\n\n%>% \n\n<- \n\n# comment\n\n\n\n\nCode\n\n\n\n\n\n\nCtrl+Alt+i; new chunk.\n\n\n\n\n\n\nConvert into a function.\n\n\n\n\n\n\nType code,\n\n\n\n\nHighlight it,\n\n\nCode/Extract Function to create a function:\n\n\n\n\nThis:\n\n\nrnorm(10, 0, 1)\n\n\n\n\nBecomes:\n\n\nrnorm <- function() {\n  rnorm(10, 0, 1)\n}\n\n\n\n\n\n\nCtrl+Alt+click; multiple cursors for typing!\n\n\nSwitch between Default, Vim and Emacs modes with\n\n    Tools/Global Options/Code/Keybindings.\n\n\nShift+Alt+g; go to line.\n\n\nCtrl+f: find/replace in the current document.\n\n\nAlt+o, Alt+Shift+o; fold/unfold the code.\n\n\nCtrl+p; jump between symbols like (), {}, [].\n\n\nCtrl+Shift+Enter; run and source the code.\n\n\nCtrl+Enter; run and source the code.\n\n\nCtrl+Shift+F10; restart, refresh the R session.\n\n\n\n\nError handling\n\n\nRStudio traces back the error origin.\n\n\nToggle the show/hide traceback in the console when there is an error or\n\nrerun with the bug, watch the right pane for traceback.\n\n\nInvestigate, highlight the next line of code, click in the traceback\n\nwindow, click the dropdown menu in the Global Environment (upper-right).\n\nStop, continue in the debugger mode, press c, press q.\n\n\nAdd/remove breakpoints, where the line numbers are.\n\n\n\n\ndebugonce\n; automatically call the debugger when the function is\n\n    called, but only once.\n\n\ndebug\n, \nundebug\n; automatically call the debugger when the\n\n    function is called.\n\n\n\n\nAdd \noptions(error=browser)\n or \noptions(error=NULL)\n at the beginning\n\nof the script; R automatically open the debugger mode.\n\n\n\n\nn; next line.\n\n\nstep-into icon.\n\n\nShift+F4; step into.\n\n\nShift+F6; execute the remainder of the bug.\n\n\n\n\n3, Project\n\u00b6\n\n\n\n\nCreate a project with a folder and all the files (Global\n\n    Environment, History, etc.): New Directory, Existing Directory,\n\n    Version Control\u2026 Empty Project, R Package (project), Shiny\n\n    Web Application.\n\n\nCreate a Git repository with the new project\n\n\n\n\nCommands\n\n\n\n\nCtrl+Shift+f; find in files.\n\n\nCtrl+F9, F10; go backwards/forwards.\n\n\n\n\nPackrat\n\n\nPackrat is a dependency management system for R. Use one version of a\n\npackage for one project, another version of a package for another\n\nproject. Associate a project with its own set of packages.\n\n\n\n\nrstudio.github.io/packrat/\n\n\nActivate Packrat when creating a project or\n\n    Tools/Project Options/Packrat.\n\n\nThe library is virtually separate from R library.\n\n\nPerfect for collaborating with GitHub\n\n\n\n\n\n\nWorking with RStudio IDE (Part 2)\n\u00b6\n\n\n1, Packages\n\u00b6\n\n\nIntroduction to R packages\n\n\nAnything that can be automated, should be automated. Do as little as\n\npossible by hand. Do as much as possible with functions.\n\n\nProcess:\n\n\n\n\nWriting R functions.\n\n\nDocumenting functions.\n\n\nWriting tests.\n\n\nChecking compatibility.\n\n\nBuilding the package for sharing and using.\n\n\n\n\nSee the book: \nR Packages\n.\n\n\nCreate a new R package\n\n\nCreate a new directory with new files, folders, and meta-information.\n\n\nTo update or not to update\n\n\nRStudio generates the NAMESPACE content for you automatically.\n\n\nRead r-pkgs.had.co.nz/description.html\n\n\nImport & load source files\n\n\nMove existing functions from an existing project into the new package.\n\nThe function are moves to a new folder in the project.\n\n\nTest created functions all in once with the Load All command in the\n\nBuild Tab.\n\n\nSimulations:\n\n\n\n\nCtrl+Shift+L\n\n\nCtrl+Shift+F10; restart a R session.\n\n\n\n\nPackages documentation\n\n\nCreate a help page (.Rd file). Written in HTML.\n\n\nUse the \nroxigen\n \npackage\n to document.\n\n\nTools/Project Optons/Build Tools/Generate documentation. Generate the\n\ndoc, add comments.\n\n\nFirst, create a doc skeleton above the function:\n\n\n\n\nCtrl+Alt+Shift+R\n\n\n\n\n\n\n\n#' Title\n#'\n#' @param x \n#'\n#' @return\n#' @export\n#'\n#' @examples\ncEnter <- function(x) {\n  x - mean(x)\n}\n\n\n\n\nFill in the blanks.\n\n\nPackage documentation (2)\n\n\nTitle of the help page. Text on how to use.\n\n\nThe 4 tags (\n@\n) help organizing the doc: \n@export\n (tells R that this\n\nfunction should be made available to people who load your package. ),\n\n\n@params\n, \n@return\n, and \n@examples\n. There are many more advanced\n\ntags.\n\n\nHighlight a section and test it, run it.\n\n\nPackage documentation (3)\n\n\nBuild Tab/build the package.\n\n\n\n\nCtrl+Shift+d\n\n\n\n\nCompile all. Load all. Open the help page with \n?function\n.\n\n\nLearn the \nroxigen\n workflow to ease the work.\n\n\nTest your package\n\n\nWith the \ntestthat\n (and \ndevtools\n) package.\n\n\nInstall both packages. Run \ndevtools::use_testthat()\n and a now\n\ndirectory with subdir appears in the package. This is where we save\n\ntests.\n\n\nTest your package (2)\n\n\nReady. Write tests. Open a new R script and save it to the tests\n\ndirectory. Create a \ncontext\n function. Add \ntest_that\n functions with\n\narguments.\n\n\ncontext(\"cEnter\")\n\ntest_that(\"cEnter handles integers\", {\n  expect_equal(cEnter(1:3), -1:1)\n  expect_equal(cEnter(-(1:3)), 1:-1)\n})\n\ncontext(\"scale\")\n\ntest_that(\"scale handles integers\", {\n  expect_equal(scale(1:3), 1:3)\n  expect_equal(scale(-(1:3)), -(1:3))\n})\n\ncontext(\"standardize\")\n\ntest_that(\"standardize handles integers\", {\n  expect_equal(standardize(1:3), -1:1)\n  expect_equal(standardize(-(1:3)), 1:-1)\n})\n\n\n\n\nTest your package (3)\n\n\nRun the tests.\n\n\n\n\nCtl+Shift+T\n\n\n\n\nGet a summary (pass or not pass). Test and retest the package.\n\n\nRead r-pkgs.had.co.nz/tests.html\n\n\nTime to test your package\n\n\nRun the test with Build Tab/Test package.\n\n\nCheck your package\n\n\nUpload the package to GitHub. Recreate the package structure in the\n\nrepo. Download and install the package with \ninstall_github()\n from the\n\n\ndevtools\n package. Test the package on GitHub to complete the tests.\n\nType \nR CMD check\n in the terminal. Or Build Tab/Check icon.\n\n\n\n\nCtrl+Shift+E\n\n\n\n\nBuild your package\n\n\nA package is a tarball or package bundle.\n\n\nBuild Tab/Build & Reload. Install and load the package.\n\n\n\n\nCtrl+Shift+B\n\n\n\n\nBuild and reload will overwrite the existing package. Run\n\n\ndevtools::dev_mode()\n creates a separate library for development.\n\nRunning the command again cancels it.\n\n\nTwo formats: source package or binary package (more compresses and\n\noptimized).\n\n\nRead r-pkgs.had.co.nz/package.html#package\n\n\nWrap-up\n\n\nGet www.rstudio.com/resources/cheatsheets/\n\n\n2, Version Control\n\u00b6\n\n\nIntroduction to Git\n\n\nUse Git to work in team. Even on R scripts, reports and packages.\n\n\nInstall Git.\n\n\nIn RStudio, Tools/Global Options/Git/SVN + Tools/Project Options/Select\n\nVCS, restart RStudio. RStudio has now a Git pane and a Git icon.\n\n\nStage & commit\n\n\nThe Git Tab is a directory. The real life (local) version of the\n\nproject. The official version of the project as recorded with Git.\n\n\nThe two versions are different. View the differences. When you commit,\n\nyou add thing from the real to the official version.\n\n\nGreen highlighting indicates something you\u2019ve added to the official\n\nversion, while red highlighting indicates lines you have removed.\n\n\n.gitignore\n\n\nInside the project, ther is a .gignore file. Add file that are excluded\n\nfrom the offcial version. The file can be accessed from the Git pane.\n\n\nGit icons\n\n\nAdd, (cancel), commit, (cancel). The icons will changed.\n\n\nCommit history\n\n\nHistory viewer: each commitment. HEAD commit and parent commit. Master\n\nbranch or another.\n\n\nUndo commited changes: checkout\n\n\nCheckout command. Go back to a previous commit. The commit stays in the\n\nhistory (not deleted).\n\n\nRun it in the shell: Tools/shell.\n\n\ngit checkout sha# nameofthefile\n and Git will reverse the commit. The\n\nfile is in the stage area as before the commit.\n\n\nUndo uncommited changes\n\n\nThe file is in the stage area, ready to commit. Cancel the addition.\n\n\nIn the Change window, click on the Revert button.\n\n\nOr, click on Discard chunk.\n\n\nOr, Ctlr+Z to undo.\n\n\nThen, Save the file.\n\n\nIntroduction to GitHub\n\n\nCentralize, host, track issues, track metrics.\n\n\nInstall a R package From GitHub with \ninstall_github\n from the the\n\n\ndevtools\n packages.\n\n\nPull & Push\n\n\nLocal and GitHub\n\n\nWrap-up\n\n\nGo to help.github.com\n\n\nGo to stackoverflow.com\n\n\n3, Reporting\n\u00b6\n\n\nTools for reporting\n\n\nR Markdown and Shiny (over the web).\n\n\nIntroduction to R Markdown\n\n\nText and code.\n\n\nWeb link: \n<http://www>\n\n\nR Markdown in RStudio\n\n\nReport (HTML, PDF, Word) or presentation(slide).\n\n\nCreate a template. Load in new templates. The \nrticles\n package has\n\ntemplates for academic journals.\n\n\nThe outline (name the chunks).\n\n\nHelp/Markdown Quick Reference\n\n\nAdd a code chunk wih Ctl+Alt+I\n\n\nRendering R Markdown\n\n\nKnit or preview. For pdf, need LaTeX (install).\n\n\nAdding \nruntime: shiny\n generates a Shiny report. Launch the interactive\n\napp.\n\n\nPublish reports online.\n\n\nCompile notebook\n\n\nFile/Compile Notebook for R script. The script becomes a report (Word,\n\nPDF, HTML).\n\n\nRStudio\u2019s LaTex editor\n\n\nInstall LaTeX from www.latex-project.org\n\n\nOpen a .tex file in RStudio. RStudio has limited options to edit LaTeX;\n\nenough to write and compile.\n\n\nThe preview window is linked to the source window. If we click on a\n\ncharacter in the source window, press Ctrl+click: the corresponding\n\ncharacter is highlighted in the preview window. Vice-versa.\n\n\nTools/Global options/Sweave to change the LaTeX options.\n\n\nShiny\n\n\nThe server is online.\n\n\nshiny.rstudio.com\n\n\nWhen we create a Shiny app, we create two files: ui.R and server.R\n\n\nRun App (the app) or Ctrl+Shift+Enter\n\n\nPublish Shiny apps\n\n\nNeed an account and the \nshinyapps\n package.\n\n\ndevtools::install_github(\"rstudio/shinyapps\")\n\n\nDeploy the local app online. Unique URL. Monitor usage, view logs,\n\narchive or delete the app. Max 5 apps at a time for free. Paid account.",
            "title": "Working with the RStudio IDE"
        },
        {
            "location": "/Working_with_RStudio_IDE/#1-orientation",
            "text": "Commands   Ctrl+Up; command history.  Tab; completion for all.  Ctrl+L; clear the console.  Tools/Global Options\u2026 set Up RStudio.  view(dataframe) ; open a spreadsheet, show in new window, sort the \n    rows, search, filter the rows.    df <- data.frame(colA = c(1, 2), colB = c(3, 6))\ndf  ##   colA colB\n## 1    1    3\n## 2    2    6  View(df) # in a new window   IDE panes   Environment pane; load, save, remove objects, read a dataset, \n    import dataset.  History pane; idem, clear all or one item at the time, copy the \n    command in the console pane, reload a command with Shift+Enter and \n    run it.  File pane; working directories, files, add a new folder, rename  getwd() ; get working directory.  setwd() ; set working directory.  Plot pane; save (extension, size, resolution).  Package pane; update.  Help pane; pages.  Viewer pane; more than plots!",
            "title": "1, Orientation"
        },
        {
            "location": "/Working_with_RStudio_IDE/#2-programming",
            "text": "Scripting   Ctrl+Shift+m; pipe or  %>% .  Alt+-;  <- .  Ctrl+Shift+C; add/delete a  #  for commenting.    %>% \n\n<- \n\n# comment  Code    Ctrl+Alt+i; new chunk.    Convert into a function.    Type code,   Highlight it,  Code/Extract Function to create a function:   This:  rnorm(10, 0, 1)  Becomes:  rnorm <- function() {\n  rnorm(10, 0, 1)\n}   Ctrl+Alt+click; multiple cursors for typing!  Switch between Default, Vim and Emacs modes with \n    Tools/Global Options/Code/Keybindings.  Shift+Alt+g; go to line.  Ctrl+f: find/replace in the current document.  Alt+o, Alt+Shift+o; fold/unfold the code.  Ctrl+p; jump between symbols like (), {}, [].  Ctrl+Shift+Enter; run and source the code.  Ctrl+Enter; run and source the code.  Ctrl+Shift+F10; restart, refresh the R session.   Error handling  RStudio traces back the error origin.  Toggle the show/hide traceback in the console when there is an error or \nrerun with the bug, watch the right pane for traceback.  Investigate, highlight the next line of code, click in the traceback \nwindow, click the dropdown menu in the Global Environment (upper-right). \nStop, continue in the debugger mode, press c, press q.  Add/remove breakpoints, where the line numbers are.   debugonce ; automatically call the debugger when the function is \n    called, but only once.  debug ,  undebug ; automatically call the debugger when the \n    function is called.   Add  options(error=browser)  or  options(error=NULL)  at the beginning \nof the script; R automatically open the debugger mode.   n; next line.  step-into icon.  Shift+F4; step into.  Shift+F6; execute the remainder of the bug.",
            "title": "2, Programming"
        },
        {
            "location": "/Working_with_RStudio_IDE/#3-project",
            "text": "Create a project with a folder and all the files (Global \n    Environment, History, etc.): New Directory, Existing Directory, \n    Version Control\u2026 Empty Project, R Package (project), Shiny \n    Web Application.  Create a Git repository with the new project   Commands   Ctrl+Shift+f; find in files.  Ctrl+F9, F10; go backwards/forwards.   Packrat  Packrat is a dependency management system for R. Use one version of a \npackage for one project, another version of a package for another \nproject. Associate a project with its own set of packages.   rstudio.github.io/packrat/  Activate Packrat when creating a project or \n    Tools/Project Options/Packrat.  The library is virtually separate from R library.  Perfect for collaborating with GitHub",
            "title": "3, Project"
        },
        {
            "location": "/Working_with_RStudio_IDE/#working-with-rstudio-ide-part-2",
            "text": "",
            "title": "Working with RStudio IDE (Part 2)"
        },
        {
            "location": "/Working_with_RStudio_IDE/#1-packages",
            "text": "Introduction to R packages  Anything that can be automated, should be automated. Do as little as \npossible by hand. Do as much as possible with functions.  Process:   Writing R functions.  Documenting functions.  Writing tests.  Checking compatibility.  Building the package for sharing and using.   See the book:  R Packages .  Create a new R package  Create a new directory with new files, folders, and meta-information.  To update or not to update  RStudio generates the NAMESPACE content for you automatically.  Read r-pkgs.had.co.nz/description.html  Import & load source files  Move existing functions from an existing project into the new package. \nThe function are moves to a new folder in the project.  Test created functions all in once with the Load All command in the \nBuild Tab.  Simulations:   Ctrl+Shift+L  Ctrl+Shift+F10; restart a R session.   Packages documentation  Create a help page (.Rd file). Written in HTML.  Use the  roxigen   package  to document.  Tools/Project Optons/Build Tools/Generate documentation. Generate the \ndoc, add comments.  First, create a doc skeleton above the function:   Ctrl+Alt+Shift+R    #' Title\n#'\n#' @param x \n#'\n#' @return\n#' @export\n#'\n#' @examples\ncEnter <- function(x) {\n  x - mean(x)\n}  Fill in the blanks.  Package documentation (2)  Title of the help page. Text on how to use.  The 4 tags ( @ ) help organizing the doc:  @export  (tells R that this \nfunction should be made available to people who load your package. ),  @params ,  @return , and  @examples . There are many more advanced \ntags.  Highlight a section and test it, run it.  Package documentation (3)  Build Tab/build the package.   Ctrl+Shift+d   Compile all. Load all. Open the help page with  ?function .  Learn the  roxigen  workflow to ease the work.  Test your package  With the  testthat  (and  devtools ) package.  Install both packages. Run  devtools::use_testthat()  and a now \ndirectory with subdir appears in the package. This is where we save \ntests.  Test your package (2)  Ready. Write tests. Open a new R script and save it to the tests \ndirectory. Create a  context  function. Add  test_that  functions with \narguments.  context(\"cEnter\")\n\ntest_that(\"cEnter handles integers\", {\n  expect_equal(cEnter(1:3), -1:1)\n  expect_equal(cEnter(-(1:3)), 1:-1)\n})\n\ncontext(\"scale\")\n\ntest_that(\"scale handles integers\", {\n  expect_equal(scale(1:3), 1:3)\n  expect_equal(scale(-(1:3)), -(1:3))\n})\n\ncontext(\"standardize\")\n\ntest_that(\"standardize handles integers\", {\n  expect_equal(standardize(1:3), -1:1)\n  expect_equal(standardize(-(1:3)), 1:-1)\n})  Test your package (3)  Run the tests.   Ctl+Shift+T   Get a summary (pass or not pass). Test and retest the package.  Read r-pkgs.had.co.nz/tests.html  Time to test your package  Run the test with Build Tab/Test package.  Check your package  Upload the package to GitHub. Recreate the package structure in the \nrepo. Download and install the package with  install_github()  from the  devtools  package. Test the package on GitHub to complete the tests. \nType  R CMD check  in the terminal. Or Build Tab/Check icon.   Ctrl+Shift+E   Build your package  A package is a tarball or package bundle.  Build Tab/Build & Reload. Install and load the package.   Ctrl+Shift+B   Build and reload will overwrite the existing package. Run  devtools::dev_mode()  creates a separate library for development. \nRunning the command again cancels it.  Two formats: source package or binary package (more compresses and \noptimized).  Read r-pkgs.had.co.nz/package.html#package  Wrap-up  Get www.rstudio.com/resources/cheatsheets/",
            "title": "1, Packages"
        },
        {
            "location": "/Working_with_RStudio_IDE/#2-version-control",
            "text": "Introduction to Git  Use Git to work in team. Even on R scripts, reports and packages.  Install Git.  In RStudio, Tools/Global Options/Git/SVN + Tools/Project Options/Select \nVCS, restart RStudio. RStudio has now a Git pane and a Git icon.  Stage & commit  The Git Tab is a directory. The real life (local) version of the \nproject. The official version of the project as recorded with Git.  The two versions are different. View the differences. When you commit, \nyou add thing from the real to the official version.  Green highlighting indicates something you\u2019ve added to the official \nversion, while red highlighting indicates lines you have removed.  .gitignore  Inside the project, ther is a .gignore file. Add file that are excluded \nfrom the offcial version. The file can be accessed from the Git pane.  Git icons  Add, (cancel), commit, (cancel). The icons will changed.  Commit history  History viewer: each commitment. HEAD commit and parent commit. Master \nbranch or another.  Undo commited changes: checkout  Checkout command. Go back to a previous commit. The commit stays in the \nhistory (not deleted).  Run it in the shell: Tools/shell.  git checkout sha# nameofthefile  and Git will reverse the commit. The \nfile is in the stage area as before the commit.  Undo uncommited changes  The file is in the stage area, ready to commit. Cancel the addition.  In the Change window, click on the Revert button.  Or, click on Discard chunk.  Or, Ctlr+Z to undo.  Then, Save the file.  Introduction to GitHub  Centralize, host, track issues, track metrics.  Install a R package From GitHub with  install_github  from the the  devtools  packages.  Pull & Push  Local and GitHub  Wrap-up  Go to help.github.com  Go to stackoverflow.com",
            "title": "2, Version Control"
        },
        {
            "location": "/Working_with_RStudio_IDE/#3-reporting",
            "text": "Tools for reporting  R Markdown and Shiny (over the web).  Introduction to R Markdown  Text and code.  Web link:  <http://www>  R Markdown in RStudio  Report (HTML, PDF, Word) or presentation(slide).  Create a template. Load in new templates. The  rticles  package has \ntemplates for academic journals.  The outline (name the chunks).  Help/Markdown Quick Reference  Add a code chunk wih Ctl+Alt+I  Rendering R Markdown  Knit or preview. For pdf, need LaTeX (install).  Adding  runtime: shiny  generates a Shiny report. Launch the interactive \napp.  Publish reports online.  Compile notebook  File/Compile Notebook for R script. The script becomes a report (Word, \nPDF, HTML).  RStudio\u2019s LaTex editor  Install LaTeX from www.latex-project.org  Open a .tex file in RStudio. RStudio has limited options to edit LaTeX; \nenough to write and compile.  The preview window is linked to the source window. If we click on a \ncharacter in the source window, press Ctrl+click: the corresponding \ncharacter is highlighted in the preview window. Vice-versa.  Tools/Global options/Sweave to change the LaTeX options.  Shiny  The server is online.  shiny.rstudio.com  When we create a Shiny app, we create two files: ui.R and server.R  Run App (the app) or Ctrl+Shift+Enter  Publish Shiny apps  Need an account and the  shinyapps  package.  devtools::install_github(\"rstudio/shinyapps\")  Deploy the local app online. Unique URL. Monitor usage, view logs, \narchive or delete the app. Max 5 apps at a time for free. Paid account.",
            "title": "3, Reporting"
        },
        {
            "location": "/IO_snippets___Cleaning/",
            "text": "Datasets\n\n\nImporting Data Into R\n\n\n1, Importing Data from Flat\n\n    Files\n\n\n2, Importing Data from Excel\n\n\n3, Importing Data from Other Statistical\n\n    Software\n\n\n4, Importing Data from Relational\n\n    Data\n\n\n4b, Importing Data from Relational Data \u2013\n\n    More\n\n\n5, Importing Data from the Web\n\n\n6, Keyboard Inputting\n\n\n7, Exporting Data\n\n\n8, Inspecting Data - Missing\n\n    Data\n\n\n9, Labels & Levels\n\n\n\n\n\n\nHow to work with Quandl in R\n\n\n1, Importing Quandl Datasets\n\n\n2, Manipulating Quandl Datasets\n\n\n\n\n\n\nCleaning Data in R\n\n\n1, Introduction and Exploring Raw\n\n    Data\n\n\n2, Tidying Data\n\n\n3, Preparing Data for Analysis\n\n\n4, Putting it All Together\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDatasets\n\u00b6\n\n\n\n\nR Dataset\n\n    Packages\n;\n\n    by default in R.\n\n\nOther dataset can be imported with\n\n\ndata(Cars93, package = 'MASS')\n for example.\n\n\ncsv/doc\n\n    Datasets\n.\n\n\nFree Datasets\n from the\n\n    World Bank, Gapminder, Kaggle, Quandl, Reddit, and many\n\n    more websites.\n\n\nDatasets\n\n    to Practice Your Data Mining.\n\n\nHoughton Mifflin\n\n    Data\n\n    for linear regressions.\n\n\nRegression Datasets\n\n    for Generalized Linear Models (linear, logistic, poisson,\n\n    multinomial, survival).\n\n\nPublic Datasets on\n\n    GitHub\n\n\nAwesome Public\n\n    Datasets\n\n\n\n\n\n\nImporting Data Into R\n\u00b6\n\n\nThe packages:\n\n\n\n\nutils\n.\n\n\nreadr\n.\n\n\ndata.table\n.\n\n\nreadxl\n.\n\n\ngdata\n.\n\n\nXLConnect\n.\n\n\nhaven\n.\n\n\nforeign\n.\n\n\nDBI\n.\n\n\nhttr\n.\n\n\njsonlite\n.\n\n\n\n\n1, Importing Data from Flat Files\n\u00b6\n\n\nR functions, by default.\n\u00b6\n\n\n\n\nread.csv\n; \nsep = ','\n, \ndec = '.'\n.\n\n\nread.delim\n; .txt, \ndec = '.'\n.\n\n\nread.csv2\n; \nsep = ';'\n, \ndec = ','\n.\n\n\nread.delim2\n; .txt, \ndec = ','\n.\n\n\nNeeds arguments.\n\n\n\n\nread.csv\n for .csv files\n\n\n# List the files in your working directory\ndir()\n\n# Import swimming_pools.csv: pools\n# stringAsFactors = FALSE does not import strings as categorical variables\npools <- read.csv('swimming_pools.csv', stringsAsFactors = FALSE)\n\n\n\n\nstringsAsFactors\n\n\n# Import swimming_pools.csv correctly: pools\npools <- read.csv('swimming_pools.csv', stringsAsFactor = FALSE, header = TRUE, sep = ',')\n\n# Import swimming_pools.csv with factors: pools_factor\npools_factor <- read.csv('swimming_pools.csv', header = TRUE, sep = ',')\n\n\n\n\nread.delim\n for .txt files\n\n\n# Import hotdogs.txt: hotdogs\nhotdogs <- read.delim('hotdogs.txt', header = FALSE)\n\n# Name the columns of hotdogs appropriately\nnames(hotdogs) <- c('type', 'calories', 'sodium')\n\n\n\n\nArguments.\n\n\n# Load in the hotdogs data set: hotdogs\nhotdogs <- read.delim('hotdogs.txt', header = FALSE, sep = '\\t', col.names = c('type', 'calories', 'sodium'))\n\n# Select the hot dog with the least calories: lily\nlily <- hotdogs[which.min(hotdogs$calories), ]\n# Select the observation with the most sodium: tom\n\ntom <- hotdogs[which.max(hotdogs$sodium), ]\n\n\n\n\n# Previous call to import hotdogs.txt\nhotdogs <- read.delim('hotdogs.txt', header = FALSE, col.names = c('type', 'calories', 'sodium'))\n\n# Print a vector representing the classes of the columns\nsapply(hotdogs, class)\n\n# Edit the colClasses argument to import the data correctly: hotdogs2\nhotdogs2 <- read.delim('hotdogs.txt', header = FALSE, col.names = c('type', 'calories', 'sodium'), colClasses = c('factor', 'NULL', 'numeric'))\n\n\n\n\nThe \nutils\n package\n\u00b6\n\n\n\n\nread.table\n; \nsep = '\\t'\n, \n= ','\n, \n= ';'\n.\n\n\nRead any tabular as a d.f.\n\n\nNeeds arguments; lots of argument for precision.\n\n\nSlow.\n\n\n\n\n\n\n\nlibrary(utils)\n\n\n\n\nread.table\n .txt files\n\n\n# Create a path to the hotdogs.txt file\npath <- file.path('hotdogs', 'hotdogs.txt')\n\n# Import the hotdogs.txt file: hotdogs\nhotdogs <- read.table(path, header = FALSE, sep = '\\t', col.names = c('type', 'calories', 'sodium'))\n\n\n\n\n\n\n(from 5, Importing Data from the Web)\n\u00b6\n\n\n# https URL to the swimming_pools csv file.\nurl_csv <- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\n\n# Import the file using read.csv(): pools1\npools1 <- read.csv(url_csv)\n\n\n\n\n\n\nThe \nreadr\n package\n\u00b6\n\n\n\n\nread_delim\n; \ndelim = '\\t'\n, \n= ','\n.\n\n\nread_csv\n; read \n100.000, 200.000\n\n\nread_tsv\n; idem.\n\n\nread_csv2\n; read \n100,000; 200,000\n or European files..\n\n\nread_tsv2\n; idem.\n\n\nread_lines\n.\n\n\nread_file\n.\n\n\nwrite_csv\n.\n\n\nwrite_rds\n.\n\n\ntype_convert\n.\n\n\nparse_factor\n.\n\n\nparse_date\n.\n\n\nparse_number\n.\n\n\nspec_csv\n.\n\n\nspec_delim\n.\n\n\nFast, few arguments.\n\n\nDetect data type.\n\n\n\n\n\n\n\nlibrary(readr)\n\n\n\n\nread_delim\n .txt files\n\n\n# Import potatoes.txt using read_delim(): potatoes\npotatoes <- read_delim('potatoes.txt', delim = '\\t')\n\n\n\n\nread_csv\n .csv files\n\n\n# Column names\nproperties <- c('area', 'temp', 'size', 'storage', 'method', 'texture', 'flavor', 'moistness')\n\n# Import potatoes.csv with read_csv(): potatoes\npotatoes <- read_csv('potatoes.csv', col_names = properties)\n\n# Create a copy of potatoes: potatoes2\npotatoes2 <- potatoes\n\n# Convert the method column of potatoes2 to a factor\npotatoes2$method = factor(potatoes2$method)\n\n# or\n\npotatoes2$method = as.factor(potatoes2$method)\n\n\n\n\ncol_types\n, \nskip\n and \nn_max\n in .tsv files\n\n\n# Column names\nproperties <- c('area', 'temp', 'size', 'storage', 'method', 'texture', 'flavor', 'moistness')\n\n# Import 5 observations from potatoes.txt: potatoes_fragment\n# read_tsv or tab-separated values\npotatoes_fragment <- read_tsv('potatoes.txt', col_names = properties, skip = 7, n_max = 5)\n\n# Import all data, but force all columns to be character: potatoes_char\npotatoes_char <- read_tsv('potatoes.txt', col_types = 'cccccccc')\n\n\n\n\nSetting column types\n\n\ncols(\n  weight = col_integer(),\n  feed = col_character()\n)\n\n\n\n\nRemoving NA\n\n\nna = c('NA', 'null')\n\n\n\n\ncol_types\n with collectors .tsv files\n\n\n# Import without col_types\nhotdogs <- read_tsv('hotdogs.txt', col_names = c('type', 'calories', 'sodium'))\n\n# The collectors you will need to import the data\nfac <- col_factor(levels = c('Beef', 'Meat', 'Poultry'))\nint <- col_integer()\n\n# Edit the col_types argument to import the data correctly: hotdogs_factor\n# Change col_types to the correct vector of collectors; coerce the vector into a list\nhotdogs_factor <- read_tsv('hotdogs.txt', col_names = c('type', 'calories', 'sodium'), col_types = list(fac, int, int))\n\n\n\n\nSkiping columns\n\n\nsalaries <- read_tsv('Salaries.txt', col_names = FALSE, col_types = cols(\n  X2 = col_skip(),\n  X3 = col_skip(), \n  X4 = col_skip()\n))\n\n\n\n\nReading an ordinary text file\n\n\n# vector of character strings. \n# Import as a character vector, one item per line: tweets\ntweets <- read_lines('tweets.txt')\ntweets\n\n# returns a length 1 vector of the entire file, with line breaks represented as \\n\n# Import as a length 1 vector: tweets_all\ntweets_all <- read_file('tweets.txt')\ntweets_all\n\n\n\n\nWriting .csv and .tsv files\n\n\n# Save cwts as chickwts.csv\nwrite_csv(cwts, \"chickwts.csv\")\n\n# Append cwts2 to chickwts.csv\nwrite_csv(cwts2, \"chickwts.csv\", append = TRUE)\n\n\n\n\nWriting .rds files\n\n\n# Save trees as trees.rds\nwrite_rds(trees, 'trees.rds')\n\n# Import trees.rds: trees2\ntrees2 <- read_rds('trees.rds')\n\n# Check whether trees and trees2 are the same\nidentical(trees, trees2)\n\n\n\n\nCoercing columns to different data types\n\n\n# Convert all columns to double\ntrees2 <- type_convert(trees, col_types = cols(Girth = 'd', Height = 'd', Volume = 'd'))\n\n\n\n\nCoercing character columns into factors\n\n\n# Parse the title column\nsalaries$title <- parse_factor(salaries$title, levels = c('Prof', 'AsstProf', 'AssocProf'))\n\n# Parse the gender column\nsalaries$gender <- parse_factor(salaries$gender, levels = c('Male', 'Female'))\n\n\n\n\nCreating Date objects\n\n\n# Change type of date column\nweather$date <- parse_date(weather$date, format = '%m/%d/%Y')\n\n\n\n\nParsing number formats\n\n\n# Parse amount column as a number\ndebt$amount <- parse_number(debt$amount)\n\n\n\n\nViewing metadata before importing\n\n\n\n\nspec_csv\n for .csv and .tsv files.\n\n\nspec_delim\n for .txt files (among others).\n\n\n\n\n\n\n\n# Specifications of chickwts\nspec_csv('chickwts.csv')\n\n\n\n\n\n\n(from 5, Importing Data from the Web)\n\u00b6\n\n\nImport Flat files from the web\n\n\n# Import the csv file: pools\nurl_csv <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\npools <- read_csv(url_csv)\n\npools\n\n# Import the txt file: potatoes\nurl_delim <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/potatoes.txt'\npotatoes <- read_tsv(url_delim)\n\npotatoes\n\n\n\n\nSecure importing\n\n\n# Import the file using read_csv(): pools2\npools2 <- read_csv(url_csv)\n\n\n\n\n\n\nThe \ndata.table\n package\n\u00b6\n\n\n\n\nfread\n == \nread.table\n.\n\n\n.txt files only.\n\n\nFast.\n\n\n\n\n\n\n\nlibrary(data.table)\n\n\n\n\nfread\n for .txt files\n\n\n# Import potatoes.txt with fread(): potatoes\npotatoes <- fread('potatoes.txt')\n\n# Print out arranged version of potatoes\npotatoes[order(moistness),] \n\n# Import 20 rows of potatoes.txt with fread(): potatoes_part\npotatoes_part <- fread('potatoes.txt', nrows = 20)\n\n\n\n\nfread\n: more advanced use\n\n\n# Import columns 6, 7 and 8 of potatoes.txt: potatoes\npotatoes <- fread('potatoes.txt', select = c(6:8))\n\n# Keep only tasty potatoes (flavor > 3): tasty_potatoes\ntasty_potatoes <- subset(potatoes, potatoes$flavor > 3)\n\n\n\n\n2, Importing Data from Excel\n\u00b6\n\n\nThe \nreadxl\n package\n\u00b6\n\n\n\n\nexcel_sheets\n; list.\n\n\nread_excel\n; import.\n\n\n.xlsx files only.\n\n\n\n\n\n\n\nlibrary(readxl)\n\n\n\n\nList the sheets of an Excel file\n\n\n# Find the names of both spreadsheets: sheets\n# Before, find out what is in the directory with 'dir()'\nsheets <- excel_sheets('latitude.xlsx')\n\nsheets\n\n\n\n\nImporting an Excel sheet\n\n\n# Read the first sheet of latitude.xlsx: latitude_1\nlatitude_1 <- read_excel('latitude.xlsx', sheet = 1)\n\n# Read the second sheet of latitude.xlsx: latitude_2\nlatitude_2 <- read_excel('latitude.xlsx', sheet = 2)\n\n# Put latitude_1 and latitude_2 in a list: lat_list\nlat_list <- list(latitude_1, latitude_2)\n\n\n\n\nReading a workbook\n\n\n# Read all Excel sheets with lapply(): lat_list\nlat_list <- lapply(excel_sheets('latitude.xlsx'), read_excel, path = 'latitude.xlsx')\n\n\n\n\nThe \ncol_names\n argument\n\n\n# Import the the first Excel sheet of latitude_nonames.xlsx (R gives names): latitude_3\nlatitude_3 <- read_excel('latitude_nonames.xlsx', sheet = 1, col_names = FALSE)\n\n# Import the the second Excel sheet of latitude_nonames.xlsx (specify col_names): latitude_4 \nlatitude_4 <- read_excel('latitude_nonames.xlsx', sheet = 1, col_names = c('country', 'latitude'))\n\n\n\n\nThe \nskip\n argument\n\n\n# Import the second sheet of latitude.xlsx, skipping the first 21 rows: latitude_sel\nlatitude_sel <- read_excel('latitude.xlsx', skip = 21, col_names = FALSE)\n\n\n\n\n\n\n(from 5, Importing Data from the Web)\n\u00b6\n\n\nImport Excel files from the web\n\n\n# Download file behind URL, name it local_latitude.xls\ndownload.file(url_xls, 'local_latitude.xls')\n\n# Import the local .xls file with readxl: excel_readxl\nexcel_readxl <- read_excel('local_latitude.xls')\n\n\n\n\nDownloading any file, secure or not\n\n\n# https URL to the wine RData file.\nurl_rdata <- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/wine.RData'\n\n# Download the wine file to your working directory\ndownload.file(url_rdata, 'wine_local.RData')\n\n\n\n\n\n\nThe \nXLConnect\n package\n\u00b6\n\n\n\n\nloadWorkbook\n.\n\n\ngetSheets\n.\n\n\nreadWorksheet\n.\n\n\nreadWorksheetFromFile\n\n\nreadNamedRegion\n\n\n\n\nreadNamedRegionFromFile\n\n\n\n\n\n\n.xls & .xlsx files.\n\n\n\n\nLike reading a database.\n\n\n\n\n\n\n\nlibrary(XLConnectJars)\nlibrary(XLConnect)\n\n\n\n\nImport a workbook\n\n\n# Build connection to latitude.xlsx: my_book\nmy_book <- loadWorkbook('latitude.xlsx')\n\n\n\n\nList and read Excel sheets\n\n\n# Build connection to latitude.xlsx\nmy_book <- loadWorkbook('latitude.xlsx')\n\n# List the sheets in latitude.xlsx\ngetSheets(my_book)\n\n# Import the second sheet in latitude.xlsx\nreadWorksheet(my_book, sheet = 2)\n\n# Import the second column of the first sheet in latitude.xlsx\nreadWorksheet(my_book, sheet = 2, startCol = 2)\n\n\n\n\nAdd and populate worksheets\n\n\n# Build connection to latitude.xlsx\nmy_book <- loadWorkbook('latitude.xlsx')\n\n# Create data frame: summ\ndims1 <- dim(readWorksheet(my_book, 1))\ndims2 <- dim(readWorksheet(my_book, 2))\nsumm <- data.frame(sheets = getSheets(my_book), \n                   nrows = c(dims1[1], dims2[1]), \n                   ncols = c(dims1[2], dims2[2]))\n\n# Add a worksheet to my_book, named 'data_summary'\ncreateSheet(my_book, name = 'data_summary')\n\n# Populate 'data_summary' with summ data frame\nwriteWorksheet(my_book, summ, sheet = 'data_summary')\n# Save workbook as latitude_with_summ.xlsx\n\nsaveWorkbook(my_book, 'latitude_with_summ.xlsx')\n\n\n\n\nOne unique function\n\n\n# Read in the data set and assign to the object\nimpact <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'impact', header = TRUE, startCol = 1, startRow = 1)\n\n# more arguments\n# endCol = 1\n# endRow = 1\n# autofitRow = \n# autofitCol = \n# region =\n# rownames =\n# colTypes =\n# forceConversion =\n# dateTimeFormat =\n# check.names =\n# useCachedValues =\n# keep =\n# drop =\n# simplify =\n# readStrategy =\n\n\n\n\n3, Importing Data from Other Statistical Software\n\u00b6\n\n\nThe \nhaven\n package\n\u00b6\n\n\n\n\nread_sas\n; sas7bdat & sas7bcat files.\n\n\nread_stata\n; version; dta files.\n\n\nread_dta\n; idem.\n\n\nread_spss\n; sav & por files (and see below).\n\n\nread_por\n.\n\n\nread_sav\n.\n\n\nSimple, few arguments.\n\n\nCreate a d.f.\n\n\n\n\n\n\n\nlibrary(haven)\n\n\n\n\nImport SAS data with haven\n\n\n# Import sales.sas7bdat: sales\nsales <- read_sas('sales.sas7bdat')\n\n\n\n\nImport STATA data with haven\n\n\n# Import the data from the URL: sugar\nsugar <- read_dta('http://assets.datacamp.com/course/importing_data_into_r/trade.dta')\n\n\n\n\nImport SPSS data with haven\n\n\n# Specify the file path using file.path(): path\npath <- file.path('datasets', 'person.sav')\n\n# Import person.sav, which is in the datasets folder: traits\ntraits <- read_sav(path)\n\n\n\n\nFactorize, round two\n\n\n# Import SPSS data from the URL: work\nwork <- read_sav('http://assets.datacamp.com/course/importing_data_into_r/employee.sav')\n\n\n\n\nThe\nforeign\n package\n\u00b6\n\n\n\n\nCannot import SAS, see the \nsas7bdat\n package.\n\n\nread.dta\n; dta files.\n\n\nread.spss\n; sav & por files.\n\n\nComprehensive.\n\n\n\n\n\n\n\nlibrary(foreign)\n\n\n\n\nImport STATA data with foreign (1)\n\n\n# Import florida.dta and name the resulting data frame florida\nflorida <- read.dta('florida.dta')\n\n\n\n\nImport STATA data with foreign (2)\n\n\n# Specify the file path using file.path(): path\npath <- file.path('worldbank', 'edequality.dta')\n\n# Create and print structure of edu_equal_1\nedu_equal_1 <- read.dta(path)\n\n# Create and print structure of edu_equal_2\nedu_equal_2 <- read.dta(path, convert.factors = FALSE)\n\n# Create and print structure of edu_equal_3\nedu_equal_3 <- read.dta(path, convert.underscore = TRUE) \n\n\n\n\nImport SPSS data with foreign (1)\n\n\n# Import international.sav as a data frame: demo\ndemo <- read.spss('international.sav', to.data.frame = TRUE)\n\n\n\n\nImport SPSS data with foreign (2)\n\n\n# Import international.sav as demo_1\ndemo_1 <- read.spss('international.sav', to.data.frame = TRUE)\n\n# Import international.sav as demo_2\ndemo_2 <- read.spss('international.sav', to.data.frame = TRUE, use.value.labels = FALSE)\n\n\n\n\n4, Importing Data from Relational Data\n\u00b6\n\n\nThe \nDBI\n package\n\u00b6\n\n\n\n\ndbConnect\n.\n\n\ndbReadTable\n.\n\n\ndbGetQuery\n.\n\n\ndbFetch\n.\n\n\ndbDisconnect\n.\n\n\n\n\n\n\n\nlibrary(DBI)\n\n\n\n\nStep 1: Establish a connection\n\n\n# Connect to the MySQL database: con\ncon <- dbConnect(RMySQL::MySQL(), dbname = 'tweater', host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', port = 3306, user = 'student', password = 'datacamp') \n\ncon\n\n\n\n\nStep 2: List the database tables\n\n\n# Connect to the MySQL database: con\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Build a vector of table names: tables\ntables <- dbListTables(con)\n\n# Display structure of tables\nstr(tables)\n\n\n\n\nStep 3: Import data from a table\n\n\n# Connect to the MySQL database: con\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Import the users table from tweater: users\nusers <- dbReadTable(con, 'users')\n\nusers\n\n# Import and print the tweats table from tweater: tweats\ntweats <- dbReadTable(con, 'tweats')\n\ntweats\n\n# Import and print the comments table from tweater: comments\ncomments <- dbReadTable(con, 'comments')\n\ncomments\n\n\n\n\nYour very first SQL query\n\n\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Import post column of tweats where date is higher than '2015-09-21': latest\nlatest <- dbGetQuery(con, 'SELECT post FROM tweats WHERE date > \\'2015-09-21\\'')\n\nlatest\n\n# Import tweat_id column of comments where user_id is 1: elisabeth\nelisabeth <- dbGetQuery(con, 'SELECT tweat_id FROM comments WHERE user_id = 1')\n\nelisabeth\n\n\n\n\nMore advanced SQL queries\n\n\n# Connect to the database\ncon <- dbConnect(RMySQL::MySQL(),\n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Create data frame specific\nspecific <- dbGetQuery(con, 'SELECT message FROM comments WHERE tweat_id = 77 AND user_id > 4')\n\nspecific\n\n# Create data frame short\nshort <- dbGetQuery(con, 'SELECT id, name FROM users WHERE CHAR_LENGTH(name) < 5')\n\nshort\n\n\n\n\nSend - Fetch - Clear\n\n\n# Connect to the database\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Send query to the database with dbSendQuery(): res\nres <- dbSendQuery(con, 'SELECT * FROM comments WHERE user_id > 4')\n\n# Display information contained in res\ndbGetInfo(res)\n\n# Use dbFetch() twice\nwhile (!dbHasCompleted(res)) {\n    chunk <- dbFetch(res, n = 2)\n    chunk2 <- dbFetch(res)\n    print(chunk)\n}\n\n# Clear res\ndbClearResult(res)\n\n\n\n\nBe polite and \u2026\n\n\n# Database specifics\ndbname <- 'tweater'\nhost <- 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\nport <- 3306\nuser <- 'student'\npassword <- 'datacamp'\n\n# Connect to the database\ncon <- dbConnect(RMySQL::MySQL(), dbname = 'tweater', host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', port = 3306 , user = 'student', password = 'datacamp')\n\n# Create the data frame  long_tweats\nlong_tweats <- dbGetQuery(con, 'SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40')\n\n# Print long_tweats\nprint(long_tweats)\n\n# Disconnect from the database\ndbDisconnect(con)\n\n\n\n\nOther general packages\n\n\nThe \nRODBC\n package provides access to databases (including Microsoft\n\nAccess and Microsoft SQL Server) through an ODBC interface.\n\n\nThe \nRJDBC\n package provides access to databases through a JDBC\n\ninterface.\n\n\nSpecialized packages\n\n\n\n\nROracle\n provides an interface for Oracle.\n\n\nRMySQL\n provides access to MySQL.\n\n\nRpostgreSQL\n to PostgreSQL.\n\n\nRSQLite\n to SQLite.\n\n\nAnd there are manu more packages for NoSQL databases such\n\n    as MongoDB.\n\n\n\n\n4b, Importing Data from Relational Data \u2013 More\n\u00b6\n\n\nDBI\n\u00b6\n\n\nFirst, change the working directory with \nsetwd\n. Install the \nDBI\n\nlibrary.\n\n\nConnect and read preliminary results\n\n\nlibrary(DBI)\nlibrary(sqliter)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nsqlite = dbDriver('SQLite')\n\n# Assign the connection string to a connection object\nsqlitedb <- dbConnect(RSQLite::SQLite(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(sqlitedb)\n\n\n\n\nExtract some data\n\n\n# Assign the results of a SQL query to an object\nresults = dbSendQuery(sqlitedb, \"SELECT * FROM albums\")\n\n# Return results from a custom object to a data.frame\ndata = fetch(results)\n\n# Print data frame to console\nhead(data)\n\n\n\n\n# Clear the results and close the connection\ndbClearResult(results)\n\n# Disconnect from the database\ndbDisconnect(sqlitedb)\n\n\n\n\nRSQLite\n\u00b6\n\n\nFirst, change the working directory with \nsetwd\n. Install the \nRSQLite\n\nlibrary.\n\n\nConnect and read preliminary results\n\n\nlibrary(RSQLite)\nlibrary(sqliter)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nsqlite = dbDriver('SQLite')\n\n# Assign the connection string to a connection object\nmysqldb = dbConnect(sqlite, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(sqlitedb)\n\n\n\n\nExtract some data\n\n\n# Assign the results of a SQL query to an object\nresults = dbSendQuery(sqlitedb, \"SELECT * FROM albums\")\n\n# Check the object\nresults\ndbGetInfo(results)\n\n# Return results from a custom object to a data.frame\ndata = fetch(results)\n\n# Print data frame to console\nhead(data)\n\n\n\n\n# Clear the results and close the connection\ndbClearResult(results)\n\n# Disconnect from the database\ndbDisconnect(sqlitedb)\n\n\n\n\nMySQL with \nDBI\n or \nRMySQL\n\u00b6\n\n\nlibrary(DBI)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nmysql = dbDriver('MySQL')\n\n# Assign the connection string to a connection object\nmysqldb <- dbConnect(RMySQL::MySQL(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Disconnect from the database\ndbDisconnect(mysqldb)\n\n\n\n\nlibrary(RMySQL)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nmysql = dbDriver('MySQL')\n\n# Assign the connection string to a connection object\nmysqldb = dbConnect(mysql, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Disconnect from the database\ndbDisconnect(mysqldb)\n\n\n\n\nPosgreSQL with \nDBI\n or \nRPostgreSQL\n\u00b6\n\n\nlibrary(DBI)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\npostgresql = dbDriver('PostgreSQL')\n\n# Assign the connection string to a connection object\npostgresqldb <- dbConnect(RPostgreSQL::PostgreSQL(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Disconnect from the database\ndbDisconnect(postgresqldb)\n\n\n\n\nlibrary(RPostgreSQL)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\npostgresql = dbDriver('PostgreSQL')\n\n# Assign the connection string to a connection object\npostgresqldb = dbConnect(postgresql, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Disconnect from the database\ndbDisconnect(postgresqldb)\n\n\n\n\n5, Importing Data from the Web\n\u00b6\n\n\nThe other package above can download files from the web. The next\n\npackages are web-oriented.\n\n\nThe \nhttr\n package\n\u00b6\n\n\n\n\nGET\n pages and files from the web.\n\n\nConcise.\n\n\nParse JSON files.\n\n\nCommunicate with APIs.\n\n\n\n\n\n\n\nlibrary(httr)\n\n\n\n\nHTTP? \nhttr\n!\n\n\n# Get the url, save response to resp\nurl <- 'http://docs.datacamp.com/teach/'\nresp <- GET(url)\n\nresp\n\n# Get the raw content of resp\nraw_content <- content(resp, as = 'raw')\n\n# Print the head of content\nhead(raw_content)\n\n\n\n\n# Get the url\nurl <- 'https://www.omdbapi.com/?t=Annie+Hall&y=&plot=short&r=json'\n\nresp <- GET(url)\n\n# Print resp\nresp\n\n# Print content of resp as text\ncontent(resp, as = 'text')\n\n# Print content of resp\ncontent(resp)\n\n\n\n\nThe \njsonlite\n package\n\u00b6\n\n\n\n\nRobust.\n\n\nImprove the imported data.\n\n\nfromJSON\n.\n\n\nfrom an R object to \ntoJSON\n\n\nprettify\n.\n\n\nminify\n.\n\n\n\n\n\n\n\nlibrary(jsonlite)\n\n\n\n\nFrom \nJSON\n to R\n\n\n# Convert wine_json to a list: wine\nwine_json <- '{'name':'Chateau Migraine', 'year':1997, 'alcohol_pct':12.4, 'color':'red', 'awarded':false}'\nwine <- fromJSON(wine_json)\n\nstr(wine)\n\n# Import Quandl data: quandl_data\nquandl_url <- 'http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz'\nquandl_data <- fromJSON(quandl_url)\n\nstr(quandl_data)\n\n\n\n\n# Experiment 1\njson1 <- '[1, 2, 3, 4, 5, 6]'\nfromJSON(json1)\n\n# Experiment 2\njson2 <- '{'a': [1, 2, 3], 'b': [4, 5, 6]}'\nfromJSON(json2)\n\n# Experiment 3\njson3 <- '[[1, 2], [3, 4]]'\nfromJSON(json3)\n\n# Experiment 4\njson4 <- '[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]'\nfromJSON(json4)\n\n\n\n\nAsk OMDb\n\n\n# Definition of the URLs\nurl_sw4 <- 'http://www.omdbapi.com/?i=tt0076759&r=json'\nurl_sw3 <- 'http://www.omdbapi.com/?i=tt0121766&r=json'\n\n# Import two URLs with fromJSON(): sw4 and sw3\nsw4 <- fromJSON(url_sw4)\nsw3 <- fromJSON(url_sw3)\n\n# Print out the Title element of both lists\nsw4$Title\nsw3$Title\n\n# Is the release year of sw4 later than sw3\nsw4$Year > sw3$Year\n\n\n\n\nFrom R to \nJSON\n\n\n# URL pointing to the .csv file\nurl_csv <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/water.csv'\n\n# Import the .csv file located at url_csv\nwater <- read.csv(url_csv, stringsAsFactors = FALSE)\n\n# Generate a summary of water\nsummary(water)\n\n# Convert the data file according to the requirements\nwater_json <- toJSON(water)\n\nwater_json\n\n\n\n\nMinify\n and \nprettify\n\n\n# Convert mtcars to a pretty JSON: pretty_json\npretty_json <- toJSON(mtcars, pretty = TRUE)\n\n# Print pretty_json\npretty_json\n\n# Minify pretty_json: mini_json\nmini_json <- minify(pretty_json)\n\n# Print mini_json\nmini_json\n\n\n\n\n6, Keyboard Inputting\n\u00b6\n\n\nCoding\n\n\n# create a data frame from scratch\nage <- c(25, 30, 56)\ngender <- c(\"male\", \"female\", \"male\")\nweight <- c(160, 110, 220)\nmydata <- data.frame(age,gender,weight)\n\n\n\n\nSpreadsheet-like\n\n\n# enter data using editor\nmydata <- data.frame(age = numeric(0), gender = character(0), weight = numeric(0))\n\nmydata <- edit(mydata)\n# note that without the assignment in the line above, the edits are not saved! \n\n\n\n\n7, Exporting Data\n\u00b6\n\n\nTo a Tab-Delimited Text File\n\u00b6\n\n\nwrite.table(mydata, 'c:/mydata.txt', sep = \"\\t\")\n\n\n\n\nTo an Excel Spreadsheet\n\u00b6\n\n\nlibrary(xlsx)\n\nwrite.xlsx(mydata, \"c:/mydata.xlsx\")\n\n\n\n\nWorksheet\n\n\nlibrary(XLConnect)\n# xls or xlsx\n\n# write a worksheet in steps\nwb <- loadWorkbook('XLConnectExample1.xls', create = TRUE)\ncreateSheet(wb, name = 'chickSheet')\nwriteWorksheet(wb, ChickWeight, sheet = 'chickSheet', startRow = 3, startCol = 4)\nsaveWorkbook(wb)\n\n# write a worksheet all in one step\nChickWeight <- 1\n\nwriteWorksheetToFile('XLConnectExample2.xlsx', data = ChickWeight, sheet = 'chickSheet', startRow = 3, startCol = 4)\n\n\n\n\nField\n\n\n# write a field in steps\nwb = loadWorkbook('XLConnectExample3.xlsx', create = TRUE)\ncreateSheet(wb, name = 'womenData')\ncreateName(wb, name = 'womenName', formula = 'womenData!$C$5', overwrite = TRUE)\nwriteNamedRegion(wb, women, name = \"womenName\")\nsaveWorkbook(wb)\n\n# write a field all in one step\nwriteNamedRegionToFile(\"XLConnectExample4.xlsx\", women, name = \"womenName\", formula = \"womenData!$C$5\")\n\n\n\n\nI/O\n\n\n# Build connection to latitude.xlsx\nmy_book <- loadWorkbook('latitude.xlsx')\n\n# Create data frame: summ\ndims1 <- dim(readWorksheet(my_book, 1))\ndims2 <- dim(readWorksheet(my_book, 2))\nsumm <- data.frame(sheets = getSheets(my_book), \n                   nrows = c(dims1[1], dims2[1]), \n                   ncols = c(dims1[2], dims2[2]))\n\n# Add a worksheet to my_book, named 'data_summary'\ncreateSheet(my_book, name = 'data_summary')\n\n# Populate 'data_summary' with summ data frame\nwriteWorksheet(my_book, summ, sheet = 'data_summary')\n# Save workbook as latitude_with_summ.xlsx\n\nsaveWorkbook(my_book, 'latitude_with_summ.xlsx')\n\n\n\n\nTo SPSS\n\u00b6\n\n\nlibrary(foreign)\n\nwrite.foreign(mydata, \"c:/mydata.txt\", \"c:/mydata.sps\", package = \"SPSS\")\n\n\n\n\nTo SAS\n\u00b6\n\n\nlibrary(foreign)\n\nwrite.foreign(mydata, \"c:/mydata.txt\", \"c:/mydata.sas\", package = \"SAS\") \n\n\n\n\nTo Stata\n\u00b6\n\n\nlibrary(foreign)\n\nwrite.dta(mydata, \"c:/mydata.dta\") \n\n\n\n\n8, Inspecting Data - Missing Data\n\u00b6\n\n\nInspecting\n\u00b6\n\n\n\n\nls(object)\n\n\nnames(object)\n\n\nstr(object)\n\n\nlevels(object$v1)\n\n\ndim(object)\n\n\nclass(object)\n\n\nprint(object)\n\n\nhead(object, 10)\n\n\ntail(object, 20)\n\n\n\n\nTesting for Missing Values\n\n\ny <- c(1, 2, 3, NA) # returns TRUE of x is missing\nis.na(y) # returns a vector (F F F T) \n\n\n\n\nRecoding Values to Missing\n\n\n# recode 99 to missing for variable v1\n# select rows where v1 is 99 and recode column v1\nmydata$v1[mydata$v1 == 99] <- NA \n\n\n\n\nExcluding Missing Values from Analyses\n\n\nx <- c(1, 2, NA, 3)\n\nmean(x) # returns NA\nmean(x, na.rm = TRUE) # returns 2 \n\n\n\n\n# list rows of data that have missing values\nmydata[!complete.cases(mydata),]\n\n\n\n\n# create new dataset without missing data\nnewdata <- na.omit(mydata) \n\n\n\n\nThe \ndplyr\n package\n\u00b6\n\n\nlibrary(dplyr)\n\ntbl_df(iris) # almost like head/tail\nglimpse(iris) # almost like str\nView(iris) # open a spreadsheet\n\n\n\n\nFor thorough cleaning\n\u00b6\n\n\n\n\nThe \nAmelia II\n software.\n\n\nThe \nmitools\n package.\n\n\n\n\n9, Labels & Levels\n\u00b6\n\n\nBasic\n\n\n# variable v1 is coded 1, 2 or 3\n# we want to attach value labels 1=red, 2=blue, 3=green\nmydata$v1 <- factor(mydata$v1, \n                    levels = c(1,2,3),\n                    labels = c(\"red\", \"blue\", \"green\"))\n\n\n\n\n# variable y is coded 1, 3 or 5\n# we want to attach value labels 1=Low, 3=Medium, 5=High\nmydata$v1 <- ordered(mydata$y,\n                     levels = c(1,3, 5),\n                     labels = c(\"Low\", \"Medium\", \"High\")) \n\n\n\n\nOrder\n\n\n# Create a vector of temperature observations\ntemperature_vector <- c('High', 'Low', 'High', 'Low', 'Medium')\n\n# Specify that they are ordinal variables with the given levels\nfactor_temperature_vector <- factor(temperature_vector, order = TRUE, levels = c('Low', 'Medium', 'High'))\n\n\n\n\nAdd comments to an object\n\n\nnames(iris)[5] <- \"This is the label for variable 5\"\n\nnames(iris)[5] # the comment\niris[5] # the data\n\n\n\n\n# labeling the variables\nlibrary(Hmisc)\n\nlabel(iris$Species) <- \"Variable label for variable myvar\"\n\ndescribe(iris$Species) # commented\n#vs\ndescribe(iris$Sepal.Length) # not commented\n\n\n\n\n\n\nHow to work with Quandl in R\n\u00b6\n\n\n1, Importing Quandl Datasets\n\u00b6\n\n\nQuandl\n delivers financial, economic and\n\nalternative data to the world\u2019s top hedge funds, asset managers and\n\ninvestment banks in several formats:\n\n\n\n\nExcel.\n\n\nR.\n\n\nPython.\n\n\nAPI.\n\n\nDB.\n\n\n\n\nThe packages used:\n\n\n\n\nQuandl\n.\n\n\nquantmod\n for plotting.\n\n\n\n\nQuandl - A first date\n\n\n# Load in the Quandl package\nlibrary(Quandl)\n\n# Assign your first dataset to the variable:\nmydata <- Quandl('NSE/OIL')\n\n\n\n\nIdentifying a dataset with its ID\n\n\n# Assign the Prague Stock Exchange to:\nPragueStockExchange <- Quandl('PRAGUESE/PX')\n\n\n\n\nPlotting a stock chart\n\n\n# The quantmod package\nlibrary(quantmod)\n\n# Load the Facebook data with the help of Quandl\nFacebook <- Quandl('GOOG/NASDAQ_FB', type = 'xts')\n\n# Plot the chart with the help of candleChart()\ncandleChart(Facebook)\n\n\n\n\nSearching a Quandl dataset in R\n\n\n# Look up the first 3 results for 'Bitcoin' within the Quandl database:\nresults <- Quandl.search(query = 'Bitcoin', silent = FALSE)\n\n# Print out the results\nstr(results)\n\n# Assign the data set with code BCHAIN/TOTBC\nBitCoin <- Quandl('BCHAIN/TOTBC')\n\n\n\n\n2, Manipulating Quandl Datasets\n\u00b6\n\n\nManipulating data\n\n\n# Assign to the variable Exchange\nExchange <- Quandl('BNP/USDEUR', start_date = '2013-01-01', end_date = '2013-12-01')\n\n\n\n\nTransforming your Quandl dataset\n\n\n# API transformation\n# The result:\nGDP_Change <- Quandl('FRED/CANRGDPR', transformation = 'rdiff')\nhead(GDP_Change)\nGDP_Chang <- Quandl('FRED/CANRGDPR')\nhead(GDP_Chang)\n\n\n\n\nThe magic of frequency collapsing\n\n\n# The result:\neiaQuarterly <- Quandl('DOE/RWTC', collapse = 'quarterly')\n\n\n\n\nTruncation and sort\n\n\n# Assign to TruSo the first 5 observations of the crude oil prices\nTruSo <- Quandl('DOE/RWTC', sort = 'asc', rows = 5)\n\n# Print the result\nTruSo\n\n\n\n\nA complex example\n\n\n# Here you should place the return:\nFinal <- Quandl('DOE/RWTC', collapse = 'daily', transformation = 'rdiff', start_date = '2005-01-01', end_date = '2010-03-01', sort = 'asc')\n\n\n\n\n\n\nCleaning Data in R\n\u00b6\n\n\nThe packages used:\n\n\n\n\ndplyr\n & \ntidyr\n for data wrangling.\n\n\nstringr\n for regex.\n\n\nlubridate\n for time and date.\n\n\n\n\n1, Introduction and Exploring Raw Data\n\u00b6\n\n\nHere\u2019s what messy data look like\n\n\n# View the first 6 rows of data\nhead(weather)\n\n# View the last 6 rows of data\ntail(weather)\n\n# View a condensed summary of the data\nstr(weather)\n\n\n\n\nGetting a feel for your data\n\n\n# Check the class of bmi\nclass(bmi)\n\n# Check the dimensions of bmi\ndim(bmi)\n\n# View the column names of bmi\nnames(bmi)\n\n\n\n\nViewing the structure of your data\n\n\n# Check the structure of bmi\nstr(bmi)\n\n# Load dplyr\nlibrary(dplyr)\n\n# Check the structure of bmi, the dplyr way\nglimpse(bmi)\n\n# View a summary of bmi\nsummary(bmi)\n\n\n\n\nLooking at your data\n\n\n# Print bmi to the console\nprint(bmi)\n\n# View the first 6 rows\nhead(bmi, 6)\n\n# View the first 15 rows\nhead(bmi, 15)\n\n# View the last 6 rows\ntail(bmi, 6)\n\n# View the last 10 rows\ntail(bmi, 10)\n\n\n\n\nVisualizing your data\n\n\n# Histogram of BMIs from 2008\nhist(bmi$Y2008)\n\n# Scatter plot comparing BMIs from 1980 to those from 2008\nplot(bmi$Y1980, bmi$Y2008)\n\n\n\n\n2, Tidying Data\n\u00b6\n\n\nGathering columns into key-value pairs\n\n\n# Load tidyr\nlibrary(tidyr)\n\n# Apply gather() to bmi and save the result as bmi_long\nbmi_long <- gather(bmi, year, bmi_val, -Country)\n\n# View the first 20 rows of the result\nhead(bmi_long, 20)\n\n\n\n\nSpreading key-value pairs into columns\n\n\n# Apply spread() to bmi_long\nbmi_wide <- spread(bmi_long, year, bmi_val)\n\n# View the head of bmi_wide\nhead(bmi_wide)\n\n\n\n\nSeparating columns\n\n\n# Apply separate() to bmi_cc\nbmi_cc_clean <- separate(bmi_cc, col = Country_ISO, into = c('Country', 'ISO'), sep = '/')\n\n# Print the head of the result\nhead(bmi_cc_clean)\n\n\n\n\nUniting columns\n\n\n# Apply unite() to bmi_cc_clean\nbmi_cc <- unite(bmi_cc_clean, Country_ISO, Country, ISO, sep = '-')\n\n# View the head of the result\nhead(bmi_cc)\n\n\n\n\nColumn headers are values, not variable names\n\n\n# View the head of census\nhead(census)\n\n# Gather the month columns\ncensus2 <- gather(census, month, amount, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC)\n\n# Arrange rows by YEAR using dplyr's arrange\ncensus2 <- arrange(census2, YEAR)\n\n# View first 20 rows of census2\nhead(census2, 20)\n\n\n\n\nVariables are stored in both rows and columns\n\n\n# View first 50 rows of census_long\nhead(census_long, 50)\n\n# Spread the type column\ncensus_long2 <- spread(census_long, type, amount)\n\n# View first 20 rows of census_long2\nhead(census_long2, 20)\n\n\n\n\nMultiple values are stored in one column\n\n\n# View the head of census_long3\nhead(census_long3)\n\n# Separate the yr_month column into two\ncensus_long4 <- separate(census_long3, yr_month, c('year', 'month'), '_')\n\n# View the first 6 rows of the result\nhead(census_long4, 6)\n\n\n\n\n3, Preparing Data for Analysis\n\u00b6\n\n\nTypes of variables in R\n\n\n# Make this evaluate to character\nclass('true')\n\n# Make this evaluate to numeric\nclass(8484.00)\n\n# Make this evaluate to integer\nclass(99L)\n\n# Make this evaluate to factor\nclass(factor('factor'))\n\n# Make this evaluate to logical\nclass(FALSE)\n\n\n\n\nCommon type conversions\n\n\n# Preview students with str()\nstr(students)\n\n# Coerce Grades to character\nstudents$Grades <- as.character(students$Grades)\n\n# Coerce Medu to factor\nstudents$Medu <- as.factor(students$Medu)\n\n# Coerce Fedu to factor\nstudents$Fedu <- as.factor(students$Fedu)\n\n # Look at students once more with str()\nstr(students)\n\n\n\n\nWorking with dates\n\n\n# Preview students2 with str()\nstr(students2)\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Parse as date\nymd('2015-Sep-17')\n\n# Parse as date and time (with no seconds!)\nymd_hm('2012-July-15, 12.56')\n\n# Coerce dob to a date (with no time)\nstudents2$dob <- ymd(students2$dob)\n\n# Coerce nurse_visit to a date and time\nstudents2$nurse_visit <- ymd_hms(students2$nurse_visit)\n\n# Look at students2 once more with str()\nstr(students2)\n\n\n\n\nTrimming and padding strings\n\n\n# Load the stringr package\nlibrary(stringr)\n\n# Trim all leading and trailing whitespace\nstr_trim(c('   Filip ', 'Nick  ', ' Jonathan'))\n\n# Pad these strings with leading zeros\nstr_pad(c('23485W', '8823453Q', '994Z'), width = 9, side = 'left', pad = '0')\n\n\n\n\nUpper and lower case\n\n\n# Print state abbreviations\nstates\n\n# Make states all uppercase and save result to states_upper\nstates_upper <- toupper(states)\nstates_upper\n\n# Make states_upper all lowercase again\ntolower(states_upper)\n\n\n\n\nFinding and replacing strings\n\n\n# stringr has been loaded for you\n# Look at the head of students2\nhead(students2)\n\n# Detect all dates of birth (dob) in 1997\nstr_detect(students2$dob, '1997')\n\n# In the sex column, replace 'F' with 'Female'...\nstudents2$sex <- str_replace(students2$sex, 'F', 'Female')\n\n# ...And 'M' with 'Male'\nstudents2$sex <- str_replace(students2$sex, 'M', 'Male')\n\n# View the head of students2\nhead(students2)\n\n\n\n\nFinding missing values\n\n\n# Call is.na() on the full social_df to spot all NAs\nis.na(social_df)\n\n# Use the any() function to ask whether there are any NAs in the data\nany(is.na(social_df))\nsum(is.na(social_df))\n\n# View a summary() of the dataset\nsummary(social_df)\n\n# Call table() on the status column\ntable(social_df$status)\n\n\n\n\nDealing with missing values\n\n\n# Use str_replace() to replace all missing strings in status with NA\nsocial_df$status <- str_replace(social_df$status, '^$', NA)\n\n# Print social_df to the console\nsocial_df\n\n# Use complete.cases() to see which rows have no missing values\ncomplete.cases(social_df)\n\n# Use na.omit() to remove all rows with any missing values\nna.omit(social_df)\n\n\n\n\nDealing with outliers and obvious errors\n\n\n# Look at a summary() of students3\nsummary(students3)\n\n# View a histogram of the age variable\nhist(students3$age, breaks = 20)\n\n# View a histogram of the absences variable\nhist(students3$absences, breaks = 20)\n\n# View a histogram of absences, but force zeros to be bucketed to the right of zero\nhist(students3$absences, breaks = 20, right = FALSE)\n\n\n\n\nAnother look at strange values\n\n\n# View a boxplot of age\nboxplot(students3$age)\n\n# View a boxplot of absences\nboxplot(students3$absences)\n\n\n\n\n4, Putting it All Together\n\u00b6\n\n\nGet a feel for the data\n\n\n# Verify that weather is a data.frame\nclass(weather)\n\n# Check the dimensions\ndim(weather)\n\n# View the column names\nnames(weather)\n\n\n\n\nSummarize the data\n\n\n# View the structure of the data\nstr(weather)\n\n# Load dplyr package\nlibrary(dplyr)\n\n# Look at the structure using dplyr's glimpse()\nglimpse(weather)\n\n# View a summary of the data\nsummary(weather)\n\n\n\n\nTake a closer look\n\n\n# View first 6 rows\nhead(weather, 6)\n\n# View first 15 rows\nhead(weather, 15)\n\n# View the last 6 rows\ntail(weather, 6)\n\n# View the last 10 rows\ntail(weather, 10)\n\n\n\n\nColumn names are values\n\n\n# Load the tidyr package\nlibrary(tidyr)\n\n# Gather the columns\nweather2 <- gather(weather, day, value, X1:X31, na.rm = TRUE)\n\n# View the head\nhead(weather2)\n\n\n\n\nValues are variable names\n\n\n# First remove column of row names\nweather2 <- weather2[, -1]\n\n# Spread the data\nweather3 <- spread(weather2, measure, value)\n\n# View the head\nhead(weather3)\n\n\n\n\nClean up dates\n\n\n# Load the stringr and lubridate packages\nlibrary(stringr)\nlibrary(lubridate)\n\n# Remove X's from day column\n# weather3$day <- str_pad(str_replace(weather3$day, 'X', ''), width = 2, side = 'left', pad = '0')\nweather3$day <- str_replace(weather3$day, 'X', '')\n\n# Unite the year, month, and day columns\nweather4 <- unite(weather3, date, year, month, day, sep = '-')\n\n# Convert date column to proper date format using stringr's ymd()\nweather4$date <- ymd(weather4$date)\n\n# Rearrange columns using dplyr's select()\nweather5 <- select(weather4, date, Events, CloudCover:WindDirDegrees)\n\n# View the head\nhead(weather5)\n\n\n\n\nA closer look at column types\n\n\n# View the structure of weather5\nstr(weather5)\n\n# Examine the first 20 rows of weather5. Are most of the characters numeric?\nhead(weather5, 20)\n\n# See what happens if we try to convert PrecipitationIn to numeric\nas.numeric(weather5$PrecipitationIn)\n\n\n\n\nColumn type conversions\n\n\n# The dplyr package is already loaded\n# Replace T with 0 (T = trace)\nweather5$PrecipitationIn <- str_replace(weather5$PrecipitationIn, 'T', '0')\n\n# Convert characters to numerics\nweather6 <- mutate_each(weather5, funs(as.numeric), CloudCover:WindDirDegrees)\n\n# Look at result\nstr(weather6)\n\n\n\n\nFind missing values\n\n\n# Count missing values\nsum(is.na(weather6))\n\n# Find missing values\nsummary(weather6)\n\n# Find indices of NAs in Max.Gust.SpeedMPH\nind <- which(is.na(weather6$Max.Gust.SpeedMPH))\nind\n\n# Look at the full rows for records missing Max.Gust.SpeedMPH\nweather6[ind, ]\n\n\n\n\nAn obvious error\n\n\n# Review distibutions for all variables\nsummary(weather6)\n\n# Find row with Max.Humidity of 1000\nind <- which(weather6$Max.Humidity == 1000)\n\n# Look at the data for that day\nweather6[ind, ]\n\n# Change 1000 to 100\nweather6$Max.Humidity[ind] <- 100\n\n\n\n\nAnother obvious error\n\n\n# Look at summary of Mean.VisibilityMiles\nsummary(weather6$Mean.VisibilityMiles)\n\n# Get index of row with -1 value\nind <- which(weather6$Mean.VisibilityMiles == -1)\n\n# Look at full row\nweather6[ind, ]\n\n# Set Mean.VisibilityMiles to the appropriate value\nweather6$Mean.VisibilityMiles[ind] <- 10\n\n\n\n\nCheck other extreme values\n\n\n# Review summary of full data once more\nsummary(weather6)\n\n# Look at histogram for MeanDew.PointF\nhist(weather6$MeanDew.PointF)\n\n# Look at histogram for Min.TemperatureF\nhist(weather6$Min.TemperatureF)\n\n# Compare to histogram for Mean.TemperatureF\nhist(weather6$Mean.TemperatureF)\n\n\n\n\nFinishing touches\n\n\n# Clean up column names\nnames(weather6) <- new_colnames\n\n# Replace empty cells in Events column\nweather6$events[weather6$events == ''] <- 'None'\n\n# Print the first 6 rows of weather6\nhead(weather6, 6)",
            "title": "I/O snippets & Cleaning"
        },
        {
            "location": "/IO_snippets___Cleaning/#importing-data-into-r",
            "text": "The packages:   utils .  readr .  data.table .  readxl .  gdata .  XLConnect .  haven .  foreign .  DBI .  httr .  jsonlite .",
            "title": "Importing Data Into R"
        },
        {
            "location": "/IO_snippets___Cleaning/#1-importing-data-from-flat-files",
            "text": "",
            "title": "1, Importing Data from Flat Files"
        },
        {
            "location": "/IO_snippets___Cleaning/#r-functions-by-default",
            "text": "read.csv ;  sep = ',' ,  dec = '.' .  read.delim ; .txt,  dec = '.' .  read.csv2 ;  sep = ';' ,  dec = ',' .  read.delim2 ; .txt,  dec = ',' .  Needs arguments.   read.csv  for .csv files  # List the files in your working directory\ndir()\n\n# Import swimming_pools.csv: pools\n# stringAsFactors = FALSE does not import strings as categorical variables\npools <- read.csv('swimming_pools.csv', stringsAsFactors = FALSE)  stringsAsFactors  # Import swimming_pools.csv correctly: pools\npools <- read.csv('swimming_pools.csv', stringsAsFactor = FALSE, header = TRUE, sep = ',')\n\n# Import swimming_pools.csv with factors: pools_factor\npools_factor <- read.csv('swimming_pools.csv', header = TRUE, sep = ',')  read.delim  for .txt files  # Import hotdogs.txt: hotdogs\nhotdogs <- read.delim('hotdogs.txt', header = FALSE)\n\n# Name the columns of hotdogs appropriately\nnames(hotdogs) <- c('type', 'calories', 'sodium')  Arguments.  # Load in the hotdogs data set: hotdogs\nhotdogs <- read.delim('hotdogs.txt', header = FALSE, sep = '\\t', col.names = c('type', 'calories', 'sodium'))\n\n# Select the hot dog with the least calories: lily\nlily <- hotdogs[which.min(hotdogs$calories), ]\n# Select the observation with the most sodium: tom\n\ntom <- hotdogs[which.max(hotdogs$sodium), ]  # Previous call to import hotdogs.txt\nhotdogs <- read.delim('hotdogs.txt', header = FALSE, col.names = c('type', 'calories', 'sodium'))\n\n# Print a vector representing the classes of the columns\nsapply(hotdogs, class)\n\n# Edit the colClasses argument to import the data correctly: hotdogs2\nhotdogs2 <- read.delim('hotdogs.txt', header = FALSE, col.names = c('type', 'calories', 'sodium'), colClasses = c('factor', 'NULL', 'numeric'))",
            "title": "R functions, by default."
        },
        {
            "location": "/IO_snippets___Cleaning/#the-utils-package",
            "text": "read.table ;  sep = '\\t' ,  = ',' ,  = ';' .  Read any tabular as a d.f.  Needs arguments; lots of argument for precision.  Slow.    library(utils)  read.table  .txt files  # Create a path to the hotdogs.txt file\npath <- file.path('hotdogs', 'hotdogs.txt')\n\n# Import the hotdogs.txt file: hotdogs\nhotdogs <- read.table(path, header = FALSE, sep = '\\t', col.names = c('type', 'calories', 'sodium'))",
            "title": "The utils package"
        },
        {
            "location": "/IO_snippets___Cleaning/#from-5-importing-data-from-the-web",
            "text": "# https URL to the swimming_pools csv file.\nurl_csv <- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\n\n# Import the file using read.csv(): pools1\npools1 <- read.csv(url_csv)",
            "title": "(from 5, Importing Data from the Web)"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-readr-package",
            "text": "read_delim ;  delim = '\\t' ,  = ',' .  read_csv ; read  100.000, 200.000  read_tsv ; idem.  read_csv2 ; read  100,000; 200,000  or European files..  read_tsv2 ; idem.  read_lines .  read_file .  write_csv .  write_rds .  type_convert .  parse_factor .  parse_date .  parse_number .  spec_csv .  spec_delim .  Fast, few arguments.  Detect data type.    library(readr)  read_delim  .txt files  # Import potatoes.txt using read_delim(): potatoes\npotatoes <- read_delim('potatoes.txt', delim = '\\t')  read_csv  .csv files  # Column names\nproperties <- c('area', 'temp', 'size', 'storage', 'method', 'texture', 'flavor', 'moistness')\n\n# Import potatoes.csv with read_csv(): potatoes\npotatoes <- read_csv('potatoes.csv', col_names = properties)\n\n# Create a copy of potatoes: potatoes2\npotatoes2 <- potatoes\n\n# Convert the method column of potatoes2 to a factor\npotatoes2$method = factor(potatoes2$method)\n\n# or\n\npotatoes2$method = as.factor(potatoes2$method)  col_types ,  skip  and  n_max  in .tsv files  # Column names\nproperties <- c('area', 'temp', 'size', 'storage', 'method', 'texture', 'flavor', 'moistness')\n\n# Import 5 observations from potatoes.txt: potatoes_fragment\n# read_tsv or tab-separated values\npotatoes_fragment <- read_tsv('potatoes.txt', col_names = properties, skip = 7, n_max = 5)\n\n# Import all data, but force all columns to be character: potatoes_char\npotatoes_char <- read_tsv('potatoes.txt', col_types = 'cccccccc')  Setting column types  cols(\n  weight = col_integer(),\n  feed = col_character()\n)  Removing NA  na = c('NA', 'null')  col_types  with collectors .tsv files  # Import without col_types\nhotdogs <- read_tsv('hotdogs.txt', col_names = c('type', 'calories', 'sodium'))\n\n# The collectors you will need to import the data\nfac <- col_factor(levels = c('Beef', 'Meat', 'Poultry'))\nint <- col_integer()\n\n# Edit the col_types argument to import the data correctly: hotdogs_factor\n# Change col_types to the correct vector of collectors; coerce the vector into a list\nhotdogs_factor <- read_tsv('hotdogs.txt', col_names = c('type', 'calories', 'sodium'), col_types = list(fac, int, int))  Skiping columns  salaries <- read_tsv('Salaries.txt', col_names = FALSE, col_types = cols(\n  X2 = col_skip(),\n  X3 = col_skip(), \n  X4 = col_skip()\n))  Reading an ordinary text file  # vector of character strings. \n# Import as a character vector, one item per line: tweets\ntweets <- read_lines('tweets.txt')\ntweets\n\n# returns a length 1 vector of the entire file, with line breaks represented as \\n\n# Import as a length 1 vector: tweets_all\ntweets_all <- read_file('tweets.txt')\ntweets_all  Writing .csv and .tsv files  # Save cwts as chickwts.csv\nwrite_csv(cwts, \"chickwts.csv\")\n\n# Append cwts2 to chickwts.csv\nwrite_csv(cwts2, \"chickwts.csv\", append = TRUE)  Writing .rds files  # Save trees as trees.rds\nwrite_rds(trees, 'trees.rds')\n\n# Import trees.rds: trees2\ntrees2 <- read_rds('trees.rds')\n\n# Check whether trees and trees2 are the same\nidentical(trees, trees2)  Coercing columns to different data types  # Convert all columns to double\ntrees2 <- type_convert(trees, col_types = cols(Girth = 'd', Height = 'd', Volume = 'd'))  Coercing character columns into factors  # Parse the title column\nsalaries$title <- parse_factor(salaries$title, levels = c('Prof', 'AsstProf', 'AssocProf'))\n\n# Parse the gender column\nsalaries$gender <- parse_factor(salaries$gender, levels = c('Male', 'Female'))  Creating Date objects  # Change type of date column\nweather$date <- parse_date(weather$date, format = '%m/%d/%Y')  Parsing number formats  # Parse amount column as a number\ndebt$amount <- parse_number(debt$amount)  Viewing metadata before importing   spec_csv  for .csv and .tsv files.  spec_delim  for .txt files (among others).    # Specifications of chickwts\nspec_csv('chickwts.csv')",
            "title": "The readr package"
        },
        {
            "location": "/IO_snippets___Cleaning/#from-5-importing-data-from-the-web_1",
            "text": "Import Flat files from the web  # Import the csv file: pools\nurl_csv <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\npools <- read_csv(url_csv)\n\npools\n\n# Import the txt file: potatoes\nurl_delim <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/potatoes.txt'\npotatoes <- read_tsv(url_delim)\n\npotatoes  Secure importing  # Import the file using read_csv(): pools2\npools2 <- read_csv(url_csv)",
            "title": "(from 5, Importing Data from the Web)"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-datatable-package",
            "text": "fread  ==  read.table .  .txt files only.  Fast.    library(data.table)  fread  for .txt files  # Import potatoes.txt with fread(): potatoes\npotatoes <- fread('potatoes.txt')\n\n# Print out arranged version of potatoes\npotatoes[order(moistness),] \n\n# Import 20 rows of potatoes.txt with fread(): potatoes_part\npotatoes_part <- fread('potatoes.txt', nrows = 20)  fread : more advanced use  # Import columns 6, 7 and 8 of potatoes.txt: potatoes\npotatoes <- fread('potatoes.txt', select = c(6:8))\n\n# Keep only tasty potatoes (flavor > 3): tasty_potatoes\ntasty_potatoes <- subset(potatoes, potatoes$flavor > 3)",
            "title": "The data.table package"
        },
        {
            "location": "/IO_snippets___Cleaning/#2-importing-data-from-excel",
            "text": "",
            "title": "2, Importing Data from Excel"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-readxl-package",
            "text": "excel_sheets ; list.  read_excel ; import.  .xlsx files only.    library(readxl)  List the sheets of an Excel file  # Find the names of both spreadsheets: sheets\n# Before, find out what is in the directory with 'dir()'\nsheets <- excel_sheets('latitude.xlsx')\n\nsheets  Importing an Excel sheet  # Read the first sheet of latitude.xlsx: latitude_1\nlatitude_1 <- read_excel('latitude.xlsx', sheet = 1)\n\n# Read the second sheet of latitude.xlsx: latitude_2\nlatitude_2 <- read_excel('latitude.xlsx', sheet = 2)\n\n# Put latitude_1 and latitude_2 in a list: lat_list\nlat_list <- list(latitude_1, latitude_2)  Reading a workbook  # Read all Excel sheets with lapply(): lat_list\nlat_list <- lapply(excel_sheets('latitude.xlsx'), read_excel, path = 'latitude.xlsx')  The  col_names  argument  # Import the the first Excel sheet of latitude_nonames.xlsx (R gives names): latitude_3\nlatitude_3 <- read_excel('latitude_nonames.xlsx', sheet = 1, col_names = FALSE)\n\n# Import the the second Excel sheet of latitude_nonames.xlsx (specify col_names): latitude_4 \nlatitude_4 <- read_excel('latitude_nonames.xlsx', sheet = 1, col_names = c('country', 'latitude'))  The  skip  argument  # Import the second sheet of latitude.xlsx, skipping the first 21 rows: latitude_sel\nlatitude_sel <- read_excel('latitude.xlsx', skip = 21, col_names = FALSE)",
            "title": "The readxl package"
        },
        {
            "location": "/IO_snippets___Cleaning/#from-5-importing-data-from-the-web_2",
            "text": "Import Excel files from the web  # Download file behind URL, name it local_latitude.xls\ndownload.file(url_xls, 'local_latitude.xls')\n\n# Import the local .xls file with readxl: excel_readxl\nexcel_readxl <- read_excel('local_latitude.xls')  Downloading any file, secure or not  # https URL to the wine RData file.\nurl_rdata <- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/wine.RData'\n\n# Download the wine file to your working directory\ndownload.file(url_rdata, 'wine_local.RData')",
            "title": "(from 5, Importing Data from the Web)"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-xlconnect-package",
            "text": "loadWorkbook .  getSheets .  readWorksheet .  readWorksheetFromFile  readNamedRegion   readNamedRegionFromFile    .xls & .xlsx files.   Like reading a database.    library(XLConnectJars)\nlibrary(XLConnect)  Import a workbook  # Build connection to latitude.xlsx: my_book\nmy_book <- loadWorkbook('latitude.xlsx')  List and read Excel sheets  # Build connection to latitude.xlsx\nmy_book <- loadWorkbook('latitude.xlsx')\n\n# List the sheets in latitude.xlsx\ngetSheets(my_book)\n\n# Import the second sheet in latitude.xlsx\nreadWorksheet(my_book, sheet = 2)\n\n# Import the second column of the first sheet in latitude.xlsx\nreadWorksheet(my_book, sheet = 2, startCol = 2)  Add and populate worksheets  # Build connection to latitude.xlsx\nmy_book <- loadWorkbook('latitude.xlsx')\n\n# Create data frame: summ\ndims1 <- dim(readWorksheet(my_book, 1))\ndims2 <- dim(readWorksheet(my_book, 2))\nsumm <- data.frame(sheets = getSheets(my_book), \n                   nrows = c(dims1[1], dims2[1]), \n                   ncols = c(dims1[2], dims2[2]))\n\n# Add a worksheet to my_book, named 'data_summary'\ncreateSheet(my_book, name = 'data_summary')\n\n# Populate 'data_summary' with summ data frame\nwriteWorksheet(my_book, summ, sheet = 'data_summary')\n# Save workbook as latitude_with_summ.xlsx\n\nsaveWorkbook(my_book, 'latitude_with_summ.xlsx')  One unique function  # Read in the data set and assign to the object\nimpact <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'impact', header = TRUE, startCol = 1, startRow = 1)\n\n# more arguments\n# endCol = 1\n# endRow = 1\n# autofitRow = \n# autofitCol = \n# region =\n# rownames =\n# colTypes =\n# forceConversion =\n# dateTimeFormat =\n# check.names =\n# useCachedValues =\n# keep =\n# drop =\n# simplify =\n# readStrategy =",
            "title": "The XLConnect package"
        },
        {
            "location": "/IO_snippets___Cleaning/#3-importing-data-from-other-statistical-software",
            "text": "",
            "title": "3, Importing Data from Other Statistical Software"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-haven-package",
            "text": "read_sas ; sas7bdat & sas7bcat files.  read_stata ; version; dta files.  read_dta ; idem.  read_spss ; sav & por files (and see below).  read_por .  read_sav .  Simple, few arguments.  Create a d.f.    library(haven)  Import SAS data with haven  # Import sales.sas7bdat: sales\nsales <- read_sas('sales.sas7bdat')  Import STATA data with haven  # Import the data from the URL: sugar\nsugar <- read_dta('http://assets.datacamp.com/course/importing_data_into_r/trade.dta')  Import SPSS data with haven  # Specify the file path using file.path(): path\npath <- file.path('datasets', 'person.sav')\n\n# Import person.sav, which is in the datasets folder: traits\ntraits <- read_sav(path)  Factorize, round two  # Import SPSS data from the URL: work\nwork <- read_sav('http://assets.datacamp.com/course/importing_data_into_r/employee.sav')",
            "title": "The haven package"
        },
        {
            "location": "/IO_snippets___Cleaning/#theforeign-package",
            "text": "Cannot import SAS, see the  sas7bdat  package.  read.dta ; dta files.  read.spss ; sav & por files.  Comprehensive.    library(foreign)  Import STATA data with foreign (1)  # Import florida.dta and name the resulting data frame florida\nflorida <- read.dta('florida.dta')  Import STATA data with foreign (2)  # Specify the file path using file.path(): path\npath <- file.path('worldbank', 'edequality.dta')\n\n# Create and print structure of edu_equal_1\nedu_equal_1 <- read.dta(path)\n\n# Create and print structure of edu_equal_2\nedu_equal_2 <- read.dta(path, convert.factors = FALSE)\n\n# Create and print structure of edu_equal_3\nedu_equal_3 <- read.dta(path, convert.underscore = TRUE)   Import SPSS data with foreign (1)  # Import international.sav as a data frame: demo\ndemo <- read.spss('international.sav', to.data.frame = TRUE)  Import SPSS data with foreign (2)  # Import international.sav as demo_1\ndemo_1 <- read.spss('international.sav', to.data.frame = TRUE)\n\n# Import international.sav as demo_2\ndemo_2 <- read.spss('international.sav', to.data.frame = TRUE, use.value.labels = FALSE)",
            "title": "Theforeign package"
        },
        {
            "location": "/IO_snippets___Cleaning/#4-importing-data-from-relational-data",
            "text": "",
            "title": "4, Importing Data from Relational Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-dbi-package",
            "text": "dbConnect .  dbReadTable .  dbGetQuery .  dbFetch .  dbDisconnect .    library(DBI)  Step 1: Establish a connection  # Connect to the MySQL database: con\ncon <- dbConnect(RMySQL::MySQL(), dbname = 'tweater', host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', port = 3306, user = 'student', password = 'datacamp') \n\ncon  Step 2: List the database tables  # Connect to the MySQL database: con\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Build a vector of table names: tables\ntables <- dbListTables(con)\n\n# Display structure of tables\nstr(tables)  Step 3: Import data from a table  # Connect to the MySQL database: con\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Import the users table from tweater: users\nusers <- dbReadTable(con, 'users')\n\nusers\n\n# Import and print the tweats table from tweater: tweats\ntweats <- dbReadTable(con, 'tweats')\n\ntweats\n\n# Import and print the comments table from tweater: comments\ncomments <- dbReadTable(con, 'comments')\n\ncomments  Your very first SQL query  con <- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Import post column of tweats where date is higher than '2015-09-21': latest\nlatest <- dbGetQuery(con, 'SELECT post FROM tweats WHERE date > \\'2015-09-21\\'')\n\nlatest\n\n# Import tweat_id column of comments where user_id is 1: elisabeth\nelisabeth <- dbGetQuery(con, 'SELECT tweat_id FROM comments WHERE user_id = 1')\n\nelisabeth  More advanced SQL queries  # Connect to the database\ncon <- dbConnect(RMySQL::MySQL(),\n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Create data frame specific\nspecific <- dbGetQuery(con, 'SELECT message FROM comments WHERE tweat_id = 77 AND user_id > 4')\n\nspecific\n\n# Create data frame short\nshort <- dbGetQuery(con, 'SELECT id, name FROM users WHERE CHAR_LENGTH(name) < 5')\n\nshort  Send - Fetch - Clear  # Connect to the database\ncon <- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Send query to the database with dbSendQuery(): res\nres <- dbSendQuery(con, 'SELECT * FROM comments WHERE user_id > 4')\n\n# Display information contained in res\ndbGetInfo(res)\n\n# Use dbFetch() twice\nwhile (!dbHasCompleted(res)) {\n    chunk <- dbFetch(res, n = 2)\n    chunk2 <- dbFetch(res)\n    print(chunk)\n}\n\n# Clear res\ndbClearResult(res)  Be polite and \u2026  # Database specifics\ndbname <- 'tweater'\nhost <- 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\nport <- 3306\nuser <- 'student'\npassword <- 'datacamp'\n\n# Connect to the database\ncon <- dbConnect(RMySQL::MySQL(), dbname = 'tweater', host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', port = 3306 , user = 'student', password = 'datacamp')\n\n# Create the data frame  long_tweats\nlong_tweats <- dbGetQuery(con, 'SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40')\n\n# Print long_tweats\nprint(long_tweats)\n\n# Disconnect from the database\ndbDisconnect(con)  Other general packages  The  RODBC  package provides access to databases (including Microsoft \nAccess and Microsoft SQL Server) through an ODBC interface.  The  RJDBC  package provides access to databases through a JDBC \ninterface.  Specialized packages   ROracle  provides an interface for Oracle.  RMySQL  provides access to MySQL.  RpostgreSQL  to PostgreSQL.  RSQLite  to SQLite.  And there are manu more packages for NoSQL databases such \n    as MongoDB.",
            "title": "The DBI package"
        },
        {
            "location": "/IO_snippets___Cleaning/#4b-importing-data-from-relational-data-more",
            "text": "",
            "title": "4b, Importing Data from Relational Data -- More"
        },
        {
            "location": "/IO_snippets___Cleaning/#dbi",
            "text": "First, change the working directory with  setwd . Install the  DBI \nlibrary.  Connect and read preliminary results  library(DBI)\nlibrary(sqliter)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nsqlite = dbDriver('SQLite')\n\n# Assign the connection string to a connection object\nsqlitedb <- dbConnect(RSQLite::SQLite(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(sqlitedb)  Extract some data  # Assign the results of a SQL query to an object\nresults = dbSendQuery(sqlitedb, \"SELECT * FROM albums\")\n\n# Return results from a custom object to a data.frame\ndata = fetch(results)\n\n# Print data frame to console\nhead(data)  # Clear the results and close the connection\ndbClearResult(results)\n\n# Disconnect from the database\ndbDisconnect(sqlitedb)",
            "title": "DBI"
        },
        {
            "location": "/IO_snippets___Cleaning/#rsqlite",
            "text": "First, change the working directory with  setwd . Install the  RSQLite \nlibrary.  Connect and read preliminary results  library(RSQLite)\nlibrary(sqliter)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nsqlite = dbDriver('SQLite')\n\n# Assign the connection string to a connection object\nmysqldb = dbConnect(sqlite, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(sqlitedb)  Extract some data  # Assign the results of a SQL query to an object\nresults = dbSendQuery(sqlitedb, \"SELECT * FROM albums\")\n\n# Check the object\nresults\ndbGetInfo(results)\n\n# Return results from a custom object to a data.frame\ndata = fetch(results)\n\n# Print data frame to console\nhead(data)  # Clear the results and close the connection\ndbClearResult(results)\n\n# Disconnect from the database\ndbDisconnect(sqlitedb)",
            "title": "RSQLite"
        },
        {
            "location": "/IO_snippets___Cleaning/#mysql-with-dbi-or-rmysql",
            "text": "library(DBI)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nmysql = dbDriver('MySQL')\n\n# Assign the connection string to a connection object\nmysqldb <- dbConnect(RMySQL::MySQL(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Disconnect from the database\ndbDisconnect(mysqldb)  library(RMySQL)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nmysql = dbDriver('MySQL')\n\n# Assign the connection string to a connection object\nmysqldb = dbConnect(mysql, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Disconnect from the database\ndbDisconnect(mysqldb)",
            "title": "MySQL with DBI or RMySQL"
        },
        {
            "location": "/IO_snippets___Cleaning/#posgresql-with-dbi-or-rpostgresql",
            "text": "library(DBI)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\npostgresql = dbDriver('PostgreSQL')\n\n# Assign the connection string to a connection object\npostgresqldb <- dbConnect(RPostgreSQL::PostgreSQL(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Disconnect from the database\ndbDisconnect(postgresqldb)  library(RPostgreSQL)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\npostgresql = dbDriver('PostgreSQL')\n\n# Assign the connection string to a connection object\npostgresqldb = dbConnect(postgresql, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Disconnect from the database\ndbDisconnect(postgresqldb)",
            "title": "PosgreSQL with DBI or RPostgreSQL"
        },
        {
            "location": "/IO_snippets___Cleaning/#5-importing-data-from-the-web",
            "text": "The other package above can download files from the web. The next \npackages are web-oriented.",
            "title": "5, Importing Data from the Web"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-httr-package",
            "text": "GET  pages and files from the web.  Concise.  Parse JSON files.  Communicate with APIs.    library(httr)  HTTP?  httr !  # Get the url, save response to resp\nurl <- 'http://docs.datacamp.com/teach/'\nresp <- GET(url)\n\nresp\n\n# Get the raw content of resp\nraw_content <- content(resp, as = 'raw')\n\n# Print the head of content\nhead(raw_content)  # Get the url\nurl <- 'https://www.omdbapi.com/?t=Annie+Hall&y=&plot=short&r=json'\n\nresp <- GET(url)\n\n# Print resp\nresp\n\n# Print content of resp as text\ncontent(resp, as = 'text')\n\n# Print content of resp\ncontent(resp)",
            "title": "The httr package"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-jsonlite-package",
            "text": "Robust.  Improve the imported data.  fromJSON .  from an R object to  toJSON  prettify .  minify .    library(jsonlite)  From  JSON  to R  # Convert wine_json to a list: wine\nwine_json <- '{'name':'Chateau Migraine', 'year':1997, 'alcohol_pct':12.4, 'color':'red', 'awarded':false}'\nwine <- fromJSON(wine_json)\n\nstr(wine)\n\n# Import Quandl data: quandl_data\nquandl_url <- 'http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz'\nquandl_data <- fromJSON(quandl_url)\n\nstr(quandl_data)  # Experiment 1\njson1 <- '[1, 2, 3, 4, 5, 6]'\nfromJSON(json1)\n\n# Experiment 2\njson2 <- '{'a': [1, 2, 3], 'b': [4, 5, 6]}'\nfromJSON(json2)\n\n# Experiment 3\njson3 <- '[[1, 2], [3, 4]]'\nfromJSON(json3)\n\n# Experiment 4\njson4 <- '[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]'\nfromJSON(json4)  Ask OMDb  # Definition of the URLs\nurl_sw4 <- 'http://www.omdbapi.com/?i=tt0076759&r=json'\nurl_sw3 <- 'http://www.omdbapi.com/?i=tt0121766&r=json'\n\n# Import two URLs with fromJSON(): sw4 and sw3\nsw4 <- fromJSON(url_sw4)\nsw3 <- fromJSON(url_sw3)\n\n# Print out the Title element of both lists\nsw4$Title\nsw3$Title\n\n# Is the release year of sw4 later than sw3\nsw4$Year > sw3$Year  From R to  JSON  # URL pointing to the .csv file\nurl_csv <- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/water.csv'\n\n# Import the .csv file located at url_csv\nwater <- read.csv(url_csv, stringsAsFactors = FALSE)\n\n# Generate a summary of water\nsummary(water)\n\n# Convert the data file according to the requirements\nwater_json <- toJSON(water)\n\nwater_json  Minify  and  prettify  # Convert mtcars to a pretty JSON: pretty_json\npretty_json <- toJSON(mtcars, pretty = TRUE)\n\n# Print pretty_json\npretty_json\n\n# Minify pretty_json: mini_json\nmini_json <- minify(pretty_json)\n\n# Print mini_json\nmini_json",
            "title": "The jsonlite package"
        },
        {
            "location": "/IO_snippets___Cleaning/#6-keyboard-inputting",
            "text": "Coding  # create a data frame from scratch\nage <- c(25, 30, 56)\ngender <- c(\"male\", \"female\", \"male\")\nweight <- c(160, 110, 220)\nmydata <- data.frame(age,gender,weight)  Spreadsheet-like  # enter data using editor\nmydata <- data.frame(age = numeric(0), gender = character(0), weight = numeric(0))\n\nmydata <- edit(mydata)\n# note that without the assignment in the line above, the edits are not saved!",
            "title": "6, Keyboard Inputting"
        },
        {
            "location": "/IO_snippets___Cleaning/#7-exporting-data",
            "text": "",
            "title": "7, Exporting Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-a-tab-delimited-text-file",
            "text": "write.table(mydata, 'c:/mydata.txt', sep = \"\\t\")",
            "title": "To a Tab-Delimited Text File"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-an-excel-spreadsheet",
            "text": "library(xlsx)\n\nwrite.xlsx(mydata, \"c:/mydata.xlsx\")  Worksheet  library(XLConnect)\n# xls or xlsx\n\n# write a worksheet in steps\nwb <- loadWorkbook('XLConnectExample1.xls', create = TRUE)\ncreateSheet(wb, name = 'chickSheet')\nwriteWorksheet(wb, ChickWeight, sheet = 'chickSheet', startRow = 3, startCol = 4)\nsaveWorkbook(wb)\n\n# write a worksheet all in one step\nChickWeight <- 1\n\nwriteWorksheetToFile('XLConnectExample2.xlsx', data = ChickWeight, sheet = 'chickSheet', startRow = 3, startCol = 4)  Field  # write a field in steps\nwb = loadWorkbook('XLConnectExample3.xlsx', create = TRUE)\ncreateSheet(wb, name = 'womenData')\ncreateName(wb, name = 'womenName', formula = 'womenData!$C$5', overwrite = TRUE)\nwriteNamedRegion(wb, women, name = \"womenName\")\nsaveWorkbook(wb)\n\n# write a field all in one step\nwriteNamedRegionToFile(\"XLConnectExample4.xlsx\", women, name = \"womenName\", formula = \"womenData!$C$5\")  I/O  # Build connection to latitude.xlsx\nmy_book <- loadWorkbook('latitude.xlsx')\n\n# Create data frame: summ\ndims1 <- dim(readWorksheet(my_book, 1))\ndims2 <- dim(readWorksheet(my_book, 2))\nsumm <- data.frame(sheets = getSheets(my_book), \n                   nrows = c(dims1[1], dims2[1]), \n                   ncols = c(dims1[2], dims2[2]))\n\n# Add a worksheet to my_book, named 'data_summary'\ncreateSheet(my_book, name = 'data_summary')\n\n# Populate 'data_summary' with summ data frame\nwriteWorksheet(my_book, summ, sheet = 'data_summary')\n# Save workbook as latitude_with_summ.xlsx\n\nsaveWorkbook(my_book, 'latitude_with_summ.xlsx')",
            "title": "To an Excel Spreadsheet"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-spss",
            "text": "library(foreign)\n\nwrite.foreign(mydata, \"c:/mydata.txt\", \"c:/mydata.sps\", package = \"SPSS\")",
            "title": "To SPSS"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-sas",
            "text": "library(foreign)\n\nwrite.foreign(mydata, \"c:/mydata.txt\", \"c:/mydata.sas\", package = \"SAS\")",
            "title": "To SAS"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-stata",
            "text": "library(foreign)\n\nwrite.dta(mydata, \"c:/mydata.dta\")",
            "title": "To Stata"
        },
        {
            "location": "/IO_snippets___Cleaning/#8-inspecting-data-missing-data",
            "text": "",
            "title": "8, Inspecting Data - Missing Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#inspecting",
            "text": "ls(object)  names(object)  str(object)  levels(object$v1)  dim(object)  class(object)  print(object)  head(object, 10)  tail(object, 20)   Testing for Missing Values  y <- c(1, 2, 3, NA) # returns TRUE of x is missing\nis.na(y) # returns a vector (F F F T)   Recoding Values to Missing  # recode 99 to missing for variable v1\n# select rows where v1 is 99 and recode column v1\nmydata$v1[mydata$v1 == 99] <- NA   Excluding Missing Values from Analyses  x <- c(1, 2, NA, 3)\n\nmean(x) # returns NA\nmean(x, na.rm = TRUE) # returns 2   # list rows of data that have missing values\nmydata[!complete.cases(mydata),]  # create new dataset without missing data\nnewdata <- na.omit(mydata)",
            "title": "Inspecting"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-dplyr-package",
            "text": "library(dplyr)\n\ntbl_df(iris) # almost like head/tail\nglimpse(iris) # almost like str\nView(iris) # open a spreadsheet",
            "title": "The dplyr package"
        },
        {
            "location": "/IO_snippets___Cleaning/#for-thorough-cleaning",
            "text": "The  Amelia II  software.  The  mitools  package.",
            "title": "For thorough cleaning"
        },
        {
            "location": "/IO_snippets___Cleaning/#9-labels-levels",
            "text": "Basic  # variable v1 is coded 1, 2 or 3\n# we want to attach value labels 1=red, 2=blue, 3=green\nmydata$v1 <- factor(mydata$v1, \n                    levels = c(1,2,3),\n                    labels = c(\"red\", \"blue\", \"green\"))  # variable y is coded 1, 3 or 5\n# we want to attach value labels 1=Low, 3=Medium, 5=High\nmydata$v1 <- ordered(mydata$y,\n                     levels = c(1,3, 5),\n                     labels = c(\"Low\", \"Medium\", \"High\"))   Order  # Create a vector of temperature observations\ntemperature_vector <- c('High', 'Low', 'High', 'Low', 'Medium')\n\n# Specify that they are ordinal variables with the given levels\nfactor_temperature_vector <- factor(temperature_vector, order = TRUE, levels = c('Low', 'Medium', 'High'))  Add comments to an object  names(iris)[5] <- \"This is the label for variable 5\"\n\nnames(iris)[5] # the comment\niris[5] # the data  # labeling the variables\nlibrary(Hmisc)\n\nlabel(iris$Species) <- \"Variable label for variable myvar\"\n\ndescribe(iris$Species) # commented\n#vs\ndescribe(iris$Sepal.Length) # not commented",
            "title": "9, Labels &amp; Levels"
        },
        {
            "location": "/IO_snippets___Cleaning/#how-to-work-with-quandl-in-r",
            "text": "",
            "title": "How to work with Quandl in R"
        },
        {
            "location": "/IO_snippets___Cleaning/#1-importing-quandl-datasets",
            "text": "Quandl  delivers financial, economic and \nalternative data to the world\u2019s top hedge funds, asset managers and \ninvestment banks in several formats:   Excel.  R.  Python.  API.  DB.   The packages used:   Quandl .  quantmod  for plotting.   Quandl - A first date  # Load in the Quandl package\nlibrary(Quandl)\n\n# Assign your first dataset to the variable:\nmydata <- Quandl('NSE/OIL')  Identifying a dataset with its ID  # Assign the Prague Stock Exchange to:\nPragueStockExchange <- Quandl('PRAGUESE/PX')  Plotting a stock chart  # The quantmod package\nlibrary(quantmod)\n\n# Load the Facebook data with the help of Quandl\nFacebook <- Quandl('GOOG/NASDAQ_FB', type = 'xts')\n\n# Plot the chart with the help of candleChart()\ncandleChart(Facebook)  Searching a Quandl dataset in R  # Look up the first 3 results for 'Bitcoin' within the Quandl database:\nresults <- Quandl.search(query = 'Bitcoin', silent = FALSE)\n\n# Print out the results\nstr(results)\n\n# Assign the data set with code BCHAIN/TOTBC\nBitCoin <- Quandl('BCHAIN/TOTBC')",
            "title": "1, Importing Quandl Datasets"
        },
        {
            "location": "/IO_snippets___Cleaning/#2-manipulating-quandl-datasets",
            "text": "Manipulating data  # Assign to the variable Exchange\nExchange <- Quandl('BNP/USDEUR', start_date = '2013-01-01', end_date = '2013-12-01')  Transforming your Quandl dataset  # API transformation\n# The result:\nGDP_Change <- Quandl('FRED/CANRGDPR', transformation = 'rdiff')\nhead(GDP_Change)\nGDP_Chang <- Quandl('FRED/CANRGDPR')\nhead(GDP_Chang)  The magic of frequency collapsing  # The result:\neiaQuarterly <- Quandl('DOE/RWTC', collapse = 'quarterly')  Truncation and sort  # Assign to TruSo the first 5 observations of the crude oil prices\nTruSo <- Quandl('DOE/RWTC', sort = 'asc', rows = 5)\n\n# Print the result\nTruSo  A complex example  # Here you should place the return:\nFinal <- Quandl('DOE/RWTC', collapse = 'daily', transformation = 'rdiff', start_date = '2005-01-01', end_date = '2010-03-01', sort = 'asc')",
            "title": "2, Manipulating Quandl Datasets"
        },
        {
            "location": "/IO_snippets___Cleaning/#cleaning-data-in-r",
            "text": "The packages used:   dplyr  &  tidyr  for data wrangling.  stringr  for regex.  lubridate  for time and date.",
            "title": "Cleaning Data in R"
        },
        {
            "location": "/IO_snippets___Cleaning/#1-introduction-and-exploring-raw-data",
            "text": "Here\u2019s what messy data look like  # View the first 6 rows of data\nhead(weather)\n\n# View the last 6 rows of data\ntail(weather)\n\n# View a condensed summary of the data\nstr(weather)  Getting a feel for your data  # Check the class of bmi\nclass(bmi)\n\n# Check the dimensions of bmi\ndim(bmi)\n\n# View the column names of bmi\nnames(bmi)  Viewing the structure of your data  # Check the structure of bmi\nstr(bmi)\n\n# Load dplyr\nlibrary(dplyr)\n\n# Check the structure of bmi, the dplyr way\nglimpse(bmi)\n\n# View a summary of bmi\nsummary(bmi)  Looking at your data  # Print bmi to the console\nprint(bmi)\n\n# View the first 6 rows\nhead(bmi, 6)\n\n# View the first 15 rows\nhead(bmi, 15)\n\n# View the last 6 rows\ntail(bmi, 6)\n\n# View the last 10 rows\ntail(bmi, 10)  Visualizing your data  # Histogram of BMIs from 2008\nhist(bmi$Y2008)\n\n# Scatter plot comparing BMIs from 1980 to those from 2008\nplot(bmi$Y1980, bmi$Y2008)",
            "title": "1, Introduction and Exploring Raw Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#2-tidying-data",
            "text": "Gathering columns into key-value pairs  # Load tidyr\nlibrary(tidyr)\n\n# Apply gather() to bmi and save the result as bmi_long\nbmi_long <- gather(bmi, year, bmi_val, -Country)\n\n# View the first 20 rows of the result\nhead(bmi_long, 20)  Spreading key-value pairs into columns  # Apply spread() to bmi_long\nbmi_wide <- spread(bmi_long, year, bmi_val)\n\n# View the head of bmi_wide\nhead(bmi_wide)  Separating columns  # Apply separate() to bmi_cc\nbmi_cc_clean <- separate(bmi_cc, col = Country_ISO, into = c('Country', 'ISO'), sep = '/')\n\n# Print the head of the result\nhead(bmi_cc_clean)  Uniting columns  # Apply unite() to bmi_cc_clean\nbmi_cc <- unite(bmi_cc_clean, Country_ISO, Country, ISO, sep = '-')\n\n# View the head of the result\nhead(bmi_cc)  Column headers are values, not variable names  # View the head of census\nhead(census)\n\n# Gather the month columns\ncensus2 <- gather(census, month, amount, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC)\n\n# Arrange rows by YEAR using dplyr's arrange\ncensus2 <- arrange(census2, YEAR)\n\n# View first 20 rows of census2\nhead(census2, 20)  Variables are stored in both rows and columns  # View first 50 rows of census_long\nhead(census_long, 50)\n\n# Spread the type column\ncensus_long2 <- spread(census_long, type, amount)\n\n# View first 20 rows of census_long2\nhead(census_long2, 20)  Multiple values are stored in one column  # View the head of census_long3\nhead(census_long3)\n\n# Separate the yr_month column into two\ncensus_long4 <- separate(census_long3, yr_month, c('year', 'month'), '_')\n\n# View the first 6 rows of the result\nhead(census_long4, 6)",
            "title": "2, Tidying Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#3-preparing-data-for-analysis",
            "text": "Types of variables in R  # Make this evaluate to character\nclass('true')\n\n# Make this evaluate to numeric\nclass(8484.00)\n\n# Make this evaluate to integer\nclass(99L)\n\n# Make this evaluate to factor\nclass(factor('factor'))\n\n# Make this evaluate to logical\nclass(FALSE)  Common type conversions  # Preview students with str()\nstr(students)\n\n# Coerce Grades to character\nstudents$Grades <- as.character(students$Grades)\n\n# Coerce Medu to factor\nstudents$Medu <- as.factor(students$Medu)\n\n# Coerce Fedu to factor\nstudents$Fedu <- as.factor(students$Fedu)\n\n # Look at students once more with str()\nstr(students)  Working with dates  # Preview students2 with str()\nstr(students2)\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Parse as date\nymd('2015-Sep-17')\n\n# Parse as date and time (with no seconds!)\nymd_hm('2012-July-15, 12.56')\n\n# Coerce dob to a date (with no time)\nstudents2$dob <- ymd(students2$dob)\n\n# Coerce nurse_visit to a date and time\nstudents2$nurse_visit <- ymd_hms(students2$nurse_visit)\n\n# Look at students2 once more with str()\nstr(students2)  Trimming and padding strings  # Load the stringr package\nlibrary(stringr)\n\n# Trim all leading and trailing whitespace\nstr_trim(c('   Filip ', 'Nick  ', ' Jonathan'))\n\n# Pad these strings with leading zeros\nstr_pad(c('23485W', '8823453Q', '994Z'), width = 9, side = 'left', pad = '0')  Upper and lower case  # Print state abbreviations\nstates\n\n# Make states all uppercase and save result to states_upper\nstates_upper <- toupper(states)\nstates_upper\n\n# Make states_upper all lowercase again\ntolower(states_upper)  Finding and replacing strings  # stringr has been loaded for you\n# Look at the head of students2\nhead(students2)\n\n# Detect all dates of birth (dob) in 1997\nstr_detect(students2$dob, '1997')\n\n# In the sex column, replace 'F' with 'Female'...\nstudents2$sex <- str_replace(students2$sex, 'F', 'Female')\n\n# ...And 'M' with 'Male'\nstudents2$sex <- str_replace(students2$sex, 'M', 'Male')\n\n# View the head of students2\nhead(students2)  Finding missing values  # Call is.na() on the full social_df to spot all NAs\nis.na(social_df)\n\n# Use the any() function to ask whether there are any NAs in the data\nany(is.na(social_df))\nsum(is.na(social_df))\n\n# View a summary() of the dataset\nsummary(social_df)\n\n# Call table() on the status column\ntable(social_df$status)  Dealing with missing values  # Use str_replace() to replace all missing strings in status with NA\nsocial_df$status <- str_replace(social_df$status, '^$', NA)\n\n# Print social_df to the console\nsocial_df\n\n# Use complete.cases() to see which rows have no missing values\ncomplete.cases(social_df)\n\n# Use na.omit() to remove all rows with any missing values\nna.omit(social_df)  Dealing with outliers and obvious errors  # Look at a summary() of students3\nsummary(students3)\n\n# View a histogram of the age variable\nhist(students3$age, breaks = 20)\n\n# View a histogram of the absences variable\nhist(students3$absences, breaks = 20)\n\n# View a histogram of absences, but force zeros to be bucketed to the right of zero\nhist(students3$absences, breaks = 20, right = FALSE)  Another look at strange values  # View a boxplot of age\nboxplot(students3$age)\n\n# View a boxplot of absences\nboxplot(students3$absences)",
            "title": "3, Preparing Data for Analysis"
        },
        {
            "location": "/IO_snippets___Cleaning/#4-putting-it-all-together",
            "text": "Get a feel for the data  # Verify that weather is a data.frame\nclass(weather)\n\n# Check the dimensions\ndim(weather)\n\n# View the column names\nnames(weather)  Summarize the data  # View the structure of the data\nstr(weather)\n\n# Load dplyr package\nlibrary(dplyr)\n\n# Look at the structure using dplyr's glimpse()\nglimpse(weather)\n\n# View a summary of the data\nsummary(weather)  Take a closer look  # View first 6 rows\nhead(weather, 6)\n\n# View first 15 rows\nhead(weather, 15)\n\n# View the last 6 rows\ntail(weather, 6)\n\n# View the last 10 rows\ntail(weather, 10)  Column names are values  # Load the tidyr package\nlibrary(tidyr)\n\n# Gather the columns\nweather2 <- gather(weather, day, value, X1:X31, na.rm = TRUE)\n\n# View the head\nhead(weather2)  Values are variable names  # First remove column of row names\nweather2 <- weather2[, -1]\n\n# Spread the data\nweather3 <- spread(weather2, measure, value)\n\n# View the head\nhead(weather3)  Clean up dates  # Load the stringr and lubridate packages\nlibrary(stringr)\nlibrary(lubridate)\n\n# Remove X's from day column\n# weather3$day <- str_pad(str_replace(weather3$day, 'X', ''), width = 2, side = 'left', pad = '0')\nweather3$day <- str_replace(weather3$day, 'X', '')\n\n# Unite the year, month, and day columns\nweather4 <- unite(weather3, date, year, month, day, sep = '-')\n\n# Convert date column to proper date format using stringr's ymd()\nweather4$date <- ymd(weather4$date)\n\n# Rearrange columns using dplyr's select()\nweather5 <- select(weather4, date, Events, CloudCover:WindDirDegrees)\n\n# View the head\nhead(weather5)  A closer look at column types  # View the structure of weather5\nstr(weather5)\n\n# Examine the first 20 rows of weather5. Are most of the characters numeric?\nhead(weather5, 20)\n\n# See what happens if we try to convert PrecipitationIn to numeric\nas.numeric(weather5$PrecipitationIn)  Column type conversions  # The dplyr package is already loaded\n# Replace T with 0 (T = trace)\nweather5$PrecipitationIn <- str_replace(weather5$PrecipitationIn, 'T', '0')\n\n# Convert characters to numerics\nweather6 <- mutate_each(weather5, funs(as.numeric), CloudCover:WindDirDegrees)\n\n# Look at result\nstr(weather6)  Find missing values  # Count missing values\nsum(is.na(weather6))\n\n# Find missing values\nsummary(weather6)\n\n# Find indices of NAs in Max.Gust.SpeedMPH\nind <- which(is.na(weather6$Max.Gust.SpeedMPH))\nind\n\n# Look at the full rows for records missing Max.Gust.SpeedMPH\nweather6[ind, ]  An obvious error  # Review distibutions for all variables\nsummary(weather6)\n\n# Find row with Max.Humidity of 1000\nind <- which(weather6$Max.Humidity == 1000)\n\n# Look at the data for that day\nweather6[ind, ]\n\n# Change 1000 to 100\nweather6$Max.Humidity[ind] <- 100  Another obvious error  # Look at summary of Mean.VisibilityMiles\nsummary(weather6$Mean.VisibilityMiles)\n\n# Get index of row with -1 value\nind <- which(weather6$Mean.VisibilityMiles == -1)\n\n# Look at full row\nweather6[ind, ]\n\n# Set Mean.VisibilityMiles to the appropriate value\nweather6$Mean.VisibilityMiles[ind] <- 10  Check other extreme values  # Review summary of full data once more\nsummary(weather6)\n\n# Look at histogram for MeanDew.PointF\nhist(weather6$MeanDew.PointF)\n\n# Look at histogram for Min.TemperatureF\nhist(weather6$Min.TemperatureF)\n\n# Compare to histogram for Mean.TemperatureF\nhist(weather6$Mean.TemperatureF)  Finishing touches  # Clean up column names\nnames(weather6) <- new_colnames\n\n# Replace empty cells in Events column\nweather6$events[weather6$events == ''] <- 'None'\n\n# Print the first 6 rows of weather6\nhead(weather6, 6)",
            "title": "4, Putting it All Together"
        },
        {
            "location": "/Reading_Data_into_R_with_readr/",
            "text": "1, Importing data with \nreadr\n\n\n2, Parsing Data with \nreadr\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, Importing data with \nreadr\n\u00b6\n\n\nReading a .csv file\n\n\nYour first task will be to master the use of the \nread_csv()\n function.\n\nThere are many arguments available, but the only required argument is\n\n\nfile\n, a path to a CSV file on your computer (or the web).\n\n\nOne big advantage that \nread_csv()\n has over \nread.csv()\n is that it\n\ndoesn\u2019t convert strings into factors by default.\n\n\nread_csv()\n recognizes 8 different data types (integer, logical, etc.)\n\nand leaves anything else as characters. That means you don\u2019t have to set\n\n\nstringsAsFactors = FALSE\n every time you import a CSV file with\n\ncharacter strings!\n\n\n#install.packages('readr')\nlibrary(readr)\n\ngetwd()\n\n\n\n\n## [1] \"D:/.../Rprojects/Data Wrangling\"\n\n\n\nsetwd(\"D:/.../Rprojects/Data Wrangling\")\n\n\n\n\nImport .csv (only \u2018,\u2019).\n\n\n# Import chickwts.csv: cwts\ncwts <- read_csv('chickwts.csv')\n\n# View the head of cwts\nhead(cwts)\n\n\n\n\n## # A tibble: 6 \u00d7 2\n##   weight      feed\n##    <int>     <chr>\n## 1    179 horsebean\n## 2    160 horsebean\n## 3    136 horsebean\n## 4    227 horsebean\n## 5    217 horsebean\n## 6    168 horsebean\n\n\n\nReading a (.txt) .tsv file\n\n\nSkipping columns with \ncol_skip()\n.\n\n\nCode only:\n\n\n\n\nSetting the column type.\n\n\n\n\n\n\n\ncols(\n  weight = col_integer(),\n  feed = col_character()\n)\n\n\n\n\n\n\nSetting the column names.\n\n\n\n\n\n\n\ncol_names = c('name', 'state', 'phone')\n\n\n\n\n\n\nRemoving NA.\n\n\n\n\n\n\n\nna = c('NA', 'null')\n\n\n\n\nIn practice.\n\n\n# Import data\nsalaries <- read_tsv('Salaries.txt', col_names = FALSE, col_types = cols(\n  X2 = col_skip(),\n  X3 = col_skip(), \n  X4 = col_skip()\n))\n\n# View first six rows of salaries\nhead(salaries)\n\n\n\n\n## # A tibble: 6 \u00d7 3\n##          X1    X5     X6\n##       <chr> <chr>  <int>\n## 1      Prof  Male 139750\n## 2      Prof  Male 173200\n## 3  AsstProf  Male  79750\n## 4      Prof  Male 115000\n## 5      Prof  Male 141500\n## 6 AssocProf  Male  97000\n\n\n\nReading a European .csv\n\n\nIn most of Europe, commas (rather than periods) are used as decimal\n\npoints.\n\n\n# Import data with read_csv2(): trees\ntrees <- read_csv2('trees.csv')\n\n# View dimensions and head of trees\ndim(trees)\n\n\n\n\n## [1] 9 3\n\n\n\nhead(trees)\n\n\n\n\n## # A tibble: 6 \u00d7 3\n##   Girth Height Volume\n##   <dbl>  <int>  <dbl>\n## 1    83     70    103\n## 2    86     65    103\n## 3    88     63    102\n## 4   105     72    164\n## 5   107     81    188\n## 6   108     83    197\n\n\n\nRead a fixed-width file\n\n\nFiles containing columns of data that are separated by whitespace and\n\nall line up on one side.\n\n\nCode only:\n\n\n# Import names.txt: names\nnames <- read_table('names.txt', col_names = c('name', 'state', 'phone'), na = c('NA', 'null'))\n\n\n\n\nReading a text file\n\n\nImport ordinary text files.\n\n\n# vector of character strings. \n# Import as a character vector, one item per line: tweets\ntweets <- read_lines('tweets.txt')\ntweets\n\n\n\n\n## [1] \"carrots  can be eat by most people\"                                                                                          \n## [2] \"On predisents day we honor the big US man himself: Aberham Liclon.   Tall, skinny, dry, and cruncy - he was america's carrot\"\n## [3] \"knock knoc who is there? yup: carosot   ( joke )\"                                                                            \n## [4] \"it is 2016 time for a carot emoji   please!\"                                                                                 \n## [5] \"when life give you lemnos ,  have a carrot\"                                                                                  \n## [6] \"If you squent your eyes real hard a football  look like  a dry brown carrot   Honestly\"\n\n\n\n# returns a length 1 vector of the entire file, with line breaks represented as \\n\n# Import as a length 1 vector: tweets_all\ntweets_all <- read_file('tweets.txt')\ntweets_all\n\n\n\n\n## [1] \"carrots  can be eat by most people\\r\\nOn predisents day we honor the big US man himself: Aberham Liclon.   Tall, skinny, dry, and cruncy - he was america's carrot\\r\\nknock knoc who is there? yup: carosot   ( joke )\\r\\nit is 2016 time for a carot emoji   please!\\r\\nwhen life give you lemnos ,  have a carrot\\r\\nIf you squent your eyes real hard a football  look like  a dry brown carrot   Honestly\"\n\n\n\nWriting .csv and .tsv files\n\n\nCode only:\n\n\n# Save cwts as chickwts.csv\nwrite_csv(cwts, \"chickwts.csv\")\n\n# Append cwts2 to chickwts.csv\nwrite_csv(cwts2, \"chickwts.csv\", append = TRUE)\n\n\n\n\nWriting .rds files\n\n\nIf the R object you\u2019re working with has metadata associated with it,\n\nsaving to a CSV will cause that information to be lost.\n\n\nExports an entire R object (metadata and all).\n\n\nCode only:\n\n\n# Save trees as trees.rds\nwrite_rds(trees, 'trees.rds')\n\n# Import trees.rds: trees2\ntrees2 <- read_rds('trees.rds')\n\n# Check whether trees and trees2 are the same\nidentical(trees, trees2)\n\n\n\n\n2, Parsing Data with \nreadr\n\u00b6\n\n\nCoercing columns to different data types\n\n\nreadr\n functions are quite good at guessing the correct data type for\n\neach column in a dataset. Of course, they aren\u2019t perfect, so sometimes\n\nyou will need to change the type of a column after importing.\n\n\nCode only:\n\n\n# Convert all columns to double\ntrees2 <- type_convert(trees, col_types = cols(Girth = 'd', Height = 'd', Volume = 'd'))\n\n\n\n\nCoercing character columns into factors\n\n\nreadr\n import functions is that they don\u2019t automatically convert\n\nstrings into factors like \nread.csv\n does.\n\n\nCode only:\n\n\n# Parse the title column\nsalaries$title <- parse_factor(salaries$title, levels = c('Prof', 'AsstProf', 'AssocProf'))\n\n# Parse the gender column\nsalaries$gender <- parse_factor(salaries$gender, levels = c('Male', 'Female'))\n\n\n\n\nCreating Date objects\n\n\nThe \nreadr\n import functions can automatically recognize dates in\n\nstandard ISO 8601 format (YYYY-MM-DD) and parse columns accordingly. If\n\nyou want to import a dataset with dates in other formats, you can use\n\n\nparse_date\n.\n\n\nCode only:\n\n\n# Change type of date column\nweather$date <- parse_date(weather$date, format = '%m/%d/%Y')\n\n\n\n\nParsing number formats\n\n\nThe \nreadr\n importing functions can sometimes run into trouble parsing a\n\ncolumn as numbers when it contains non-numeric symbols in addition to\n\nnumerals.\n\n\nCode only:\n\n\n# Parse amount column as a number\ndebt$amount <- parse_number(debt$amount)\n\n\n\n\nViewing metadata before importing\n\n\nIn some cases, it may be easier to get an idea of how \nreadr\n plans to\n\nparse a dataset before you actually import it. When you see the planned\n\ncolumn specification, you might decide to change the type of one or more\n\ncolumns, for example.\n\n\n\n\nspec_csv\n for .csv and .tsv files.\n\n\nspec_delim\n for .txt files (among others).\n\n\n\n\n\n\n\n# Specifications of chickwts\nspec_csv('chickwts.csv')\n\n\n\n\n## cols(\n##   weight = col_integer(),\n##   feed = col_character()\n## )",
            "title": "Reading Data into R with readr"
        },
        {
            "location": "/Reading_Data_into_R_with_readr/#2-parsing-data-with-readr",
            "text": "Coercing columns to different data types  readr  functions are quite good at guessing the correct data type for \neach column in a dataset. Of course, they aren\u2019t perfect, so sometimes \nyou will need to change the type of a column after importing.  Code only:  # Convert all columns to double\ntrees2 <- type_convert(trees, col_types = cols(Girth = 'd', Height = 'd', Volume = 'd'))  Coercing character columns into factors  readr  import functions is that they don\u2019t automatically convert \nstrings into factors like  read.csv  does.  Code only:  # Parse the title column\nsalaries$title <- parse_factor(salaries$title, levels = c('Prof', 'AsstProf', 'AssocProf'))\n\n# Parse the gender column\nsalaries$gender <- parse_factor(salaries$gender, levels = c('Male', 'Female'))  Creating Date objects  The  readr  import functions can automatically recognize dates in \nstandard ISO 8601 format (YYYY-MM-DD) and parse columns accordingly. If \nyou want to import a dataset with dates in other formats, you can use  parse_date .  Code only:  # Change type of date column\nweather$date <- parse_date(weather$date, format = '%m/%d/%Y')  Parsing number formats  The  readr  importing functions can sometimes run into trouble parsing a \ncolumn as numbers when it contains non-numeric symbols in addition to \nnumerals.  Code only:  # Parse amount column as a number\ndebt$amount <- parse_number(debt$amount)  Viewing metadata before importing  In some cases, it may be easier to get an idea of how  readr  plans to \nparse a dataset before you actually import it. When you see the planned \ncolumn specification, you might decide to change the type of one or more \ncolumns, for example.   spec_csv  for .csv and .tsv files.  spec_delim  for .txt files (among others).    # Specifications of chickwts\nspec_csv('chickwts.csv')  ## cols(\n##   weight = col_integer(),\n##   feed = col_character()\n## )",
            "title": "2, Parsing Data with readr"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/",
            "text": "Documentation\n\n\n1, \ndata.table\n novice\n\n\nCreate and subset a\n\n\ndata.table\n\n\nGetting to know a \ndata.table\n\n\nSubsetting data tables\n\n\nThe \nby\n basics\n\n\nUsing \n.N\n and \nby\n\n\nReturn multiple numbers in \nj\n\n\n\n\n\n\n2, \ndata.table\n yeoman\n\n\nChaining, the basics\n\n\nProgramming time vs\n\n    readability\n\n\nIntroducing \n.SDcols\n\n\nMixing it together: \nlapply\n, \n.SD\n, \nSDcols\n and\n\n\n.N\n\n\nAdding, updating, and removing\n\n    columns\n\n\nThe functional form\n\n\nReady, \nset\n, go!\n\n\n\n\n\n\n3, \ndata.table\n expert\n\n\nSelecting rows the \ndata.table\n\n    way\n\n\nRemoving columns and adapting your column\n\n    names\n\n\nUnderstanding automatic\n\n    indexing\n\n\nSelecting groups or parts of\n\n    groups\n\n\nRolling joins\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\ndata.table\n\n\n\n\nextension of \ndata.frame\n.\n\n\nFast aggregation of large data (e.g. 100GB in RAM), fast ordered\n\n    joins, fast add/modify/delete of columns by group using no copies at\n\n    all, list columns, a fast friendly file reader and parallel\n\n    file writer. Offers a natural and flexible syntax, for\n\n    faster development.\n\n\n\n\ndplyr\n\n\n\n\nA fast, consistent tool for working with data frame like objects,\n\n    both in memory and out of memory.\n\n\nPipelines.\n\n\n\n\ntidyr\n\n\n\n\nAn evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data\n\n    tidying (not general reshaping or aggregating) and works well with\n\n    dplyr data pipelines.\n\n\n\n\n\n\n\n\n\n\npackage\n\n\n'narrower'\n\n\n'wider'\n\n\n\n\n\n\n\n\n\n\ntidyr\n\n\ngather\n\n\nspread\n\n\n\n\n\n\nreshape2\n\n\nmelt\n\n\ncast\n\n\n\n\n\n\nspreadsheets\n\n\nunpivot\n\n\npivot\n\n\n\n\n\n\ndatabases\n\n\nfold\n\n\nunfold\n\n\n\n\n\n\n\n\n\n1, \ndata.table\n novice\n\u00b6\n\n\nFind out more with \n?data.table\n.\n\n\nCreate and subset a \ndata.table\n\u00b6\n\n\n# The data.table package\nlibrary(data.table)\n\n# Create my_first_data_table\nmy_first_data_table <- data.table(x = c('a', 'b', 'c', 'd', 'e'), y = c(1, 2, 3, 4, 5))\n\n# Create a data.table using recycling\nDT <- data.table(a = 1:2, b = c('A', 'B', 'C', 'D'))\n\n# Print the third row to the console\nDT[3,]\n\n\n\n\n##    a b\n## 1: 1 C\n\n\n\n# Print the second and third row to the console, but do not commas\nDT[2:3]\n\n\n\n\n##    a b\n## 1: 2 B\n## 2: 1 C\n\n\n\nGetting to know a \ndata.table\n\u00b6\n\n\nLike \nhead\n, \ntail\n.\n\n\n# Print the penultimate row of DT using .N\nDT[.N - 1]\n\n\n\n\n##    a b\n## 1: 1 C\n\n\n\n# Print the column names of DT, and number of rows and number of columns\ncolnames(DT)\n\n\n\n\n## [1] \"a\" \"b\"\n\n\n\ndim(DT)\n\n\n\n\n## [1] 4 2\n\n\n\n# Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1.\nDT[c(2, 2, 3)]\n\n\n\n\n##    a b\n## 1: 2 B\n## 2: 2 B\n## 3: 1 C\n\n\n\nDT\n is a data.table/data.frame, but \nDT[ , B]\n is a vector;\n\n\nDT[ , .(B)]\n is a subsetted data.table.\n\n\nSubsetting data tables\n\u00b6\n\n\nDT[i, j, by]\n means take \nDT\n, subset rows using \ni\n, then calculate\n\n\nj\n grouped by \nby\n. You can wrap \nj\n with \n.()\n.\n\n\nA <- c(1, 2, 3, 4, 5)\nB <- c('a', 'b', 'c', 'd', 'e')\nC <- c(6, 7, 8, 9, 10)\nDT <- data.table(A, B, C)\n\n# Subset rows 1 and 3, and columns B and C\nDT[c(1,3) ,.(B, C)]\n\n\n\n\n##    B C\n## 1: a 6\n## 2: c 8\n\n\n\n# Assign to ans the correct value\nans <- data.table(DT[, .(B, val = A * C)])\n\n# Fill in the blanks such that ans2 equals target\n#target <- data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'),  val = as.integer(c(6:10, 1:5)))\nans2 <- data.table(DT[, .(B, val = as.integer(c(6:10, 1:5)))])\n\n\n\n\nThe \nby\n basics\n\u00b6\n\n\n# iris and iris3 are already available in the workspace\n\n# Convert iris to a data.table: DT\nDT <- as.data.table(iris)\n\n# For each Species, print the mean Sepal.Length\nDT[, .(mean(Sepal.Length)), by = .(Species)]\n\n\n\n\n##       Species    V1\n## 1:     setosa 5.006\n## 2: versicolor 5.936\n## 3:  virginica 6.588\n\n\n\n# Print mean Sepal.Length, grouping by first letter of Species\nDT[, .(mean(Sepal.Length)), by = .(substr(Species, 1,1))]\n\n\n\n\n##    substr    V1\n## 1:      s 5.006\n## 2:      v 6.262\n\n\n\nUsing \n.N\n and \nby\n\u00b6\n\n\n.N\n, number, in row or column.\n\n\n# data.table version of iris: DT\nDT <- as.data.table(iris)\n\n# Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group.\nDT[, .N, by = 10 * round(Sepal.Length * Sepal.Width / 10)]\n\n\n\n\n##    round   N\n## 1:    20 117\n## 2:    10  29\n## 3:    30   4\n\n\n\n# Now name the output columns `Area` and `Count`\nDT[, .(Count = .N), by = .(Area = 10 * round(Sepal.Length * Sepal.Width / 10))]\n\n\n\n\n##    Area Count\n## 1:   20   117\n## 2:   10    29\n## 3:   30     4\n\n\n\nReturn multiple numbers in \nj\n\u00b6\n\n\n# Create the data.table DT\nset.seed(1L)\nDT <- data.table(A = rep(letters[2:1], each = 4L), B = rep(1:4, each = 2L), C = sample(8))\n\n# Create the new data.table, DT2\nDT2 <- DT[, .(C = cumsum(C)), by=.(A,B)]\n\n# Select from DT2 the last two values from C while you group by A\nDT2[, .(C = tail(C,2)), by=A]\n\n\n\n\n##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8\n\n\n\n2, \ndata.table\n yeoman\n\u00b6\n\n\nChaining, the basics\n\u00b6\n\n\n# Build DT\nset.seed(1L)\nDT <- data.table(A = rep(letters[2:1], each = 4L), B = rep(1:4, each = 2L), C = sample(8)) \nDT\n\n\n\n\n##    A B C\n## 1: b 1 3\n## 2: b 1 8\n## 3: b 2 4\n## 4: b 2 5\n## 5: a 3 1\n## 6: a 3 7\n## 7: a 4 2\n## 8: a 4 6\n\n\n\n# Use chaining\n# Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A\nDT[, .(C = cumsum(C)), by = .(A,B)][, .(C = tail(C,2)), by = .(A)]\n\n\n\n\n##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8\n\n\n\nChaining your \niris\n dataset\n\n\nDT <- data.table(iris)\n\n# Perform chained operations on DT\nDT[, .(Sepal.Length = median(Sepal.Length), Sepal.Width = median(Sepal.Width), Petal.Length = median(Petal.Length), Petal.Width = median(Petal.Width)), by = .(Species)][order(Species, decreasing = TRUE)]\n\n\n\n\n##       Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1:  virginica          6.5         3.0         5.55         2.0\n## 2: versicolor          5.9         2.8         4.35         1.3\n## 3:     setosa          5.0         3.4         1.50         0.2\n\n\n\nDT\n\n\n\n\n##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica\n\n\n\nProgramming time vs readability\n\u00b6\n\n\nx <- c(2, 1, 2, 1, 2, 2, 1)\ny <- c(1, 3, 5, 7, 9, 11, 13)\nz <- c(2, 4, 6, 8, 10, 12, 14)\nDT <- data.table(x, y, z)\n\n# Mean of columns\nDT[, lapply(.SD, mean), by = .(x)] \n\n\n\n\n##    x        y        z\n## 1: 2 6.500000 7.500000\n## 2: 1 7.666667 8.666667\n\n\n\n# Median of columns\nDT[, lapply(.SD, median), by = .(x)]\n\n\n\n\n##    x y z\n## 1: 2 7 8\n## 2: 1 7 8\n\n\n\nIntroducing \n.SDcols\n\u00b6\n\n\n.SDcols\n specifies the columns of \nDT\n that are included in \n.SD\n.\n\n\ngrp <- c(6, 6, 8, 8, 8)\nQ1 <- c(4, 3, 3, 5, 3)\nQ2 <- c(1, 4, 1, 4, 4)\nQ3 <- c(3, 1, 5, 5, 2)\nH1 <- c(1, 2, 3, 2, 4)\nH2 <- c(1, 4, 3, 4, 3)\nDT <- data.table(grp, Q1, Q2, Q3, H1, H2)\n\n# Calculate the sum of the Q columns\nDT[, lapply(.SD, sum), .SDcols = 2:4]\n\n\n\n\n##    Q1 Q2 Q3\n## 1: 18 14 16\n\n\n\n# Calculate the sum of columns H1 and H2 \nDT[, lapply(.SD, sum), .SDcols = 5:6]\n\n\n\n\n##    H1 H2\n## 1: 12 15\n\n\n\n# Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns. \nDT[, .SD[-1], .SDcols = 2:4, by = .(grp)]\n\n\n\n\n##    grp Q1 Q2 Q3\n## 1:   6  3  4  1\n## 2:   8  5  4  5\n## 3:   8  3  4  2\n\n\n\nMixing it together: \nlapply\n, \n.SD\n, \nSDcols\n and \n.N\n\u00b6\n\n\nx <- c(2, 1, 2, 1, 2, 2, 1)\ny <- c(1, 3, 5, 7, 9, 11, 13)\nz <- c(2, 4, 6, 8, 10, 12, 14)\nDT <- data.table(x, y, z)\n\n# Sum of all columns and the number of rows\n# For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together.\nDT\n\n\n\n\n##    x  y  z\n## 1: 2  1  2\n## 2: 1  3  4\n## 3: 2  5  6\n## 4: 1  7  8\n## 5: 2  9 10\n## 6: 2 11 12\n## 7: 1 13 14\n\n\n\nDT[,c(lapply(.SD, sum), .N), .SDcols = 1:3, by = x]\n\n\n\n\n##    x x  y  z N\n## 1: 2 8 26 30 4\n## 2: 1 3 23 26 3\n\n\n\n# Cumulative sum of column x and y while grouping by x and z > 8\nDT[,lapply(.SD, cumsum), .SDcols = 1:2, by = .(by1 = x, by2 = z > 8)]\n\n\n\n\n##    by1   by2 x  y\n## 1:   2 FALSE 2  1\n## 2:   2 FALSE 4  6\n## 3:   1 FALSE 1  3\n## 4:   1 FALSE 2 10\n## 5:   2  TRUE 2  9\n## 6:   2  TRUE 4 20\n## 7:   1  TRUE 1 13\n\n\n\n# Chaining\nDT[,lapply(.SD, cumsum), .SDcols = 1:2, by = .(by1 = x, by2 = z > 8)][,lapply(.SD, max), .SDcols = 3:4, by = by1]\n\n\n\n\n##    by1 x  y\n## 1:   2 4 20\n## 2:   1 2 13\n\n\n\nAdding, updating, and removing columns\n\u00b6\n\n\n:=\n is defined for use in \nj\n only.\n\n\n# The data.table DT\nDT <- data.table(A = letters[c(1, 1, 1, 2, 2)], B = 1:5)\nDT\n\n\n\n\n##    A B\n## 1: a 1\n## 2: a 2\n## 3: a 3\n## 4: b 4\n## 5: b 5\n\n\n\n# Add column by reference: Total\nDT[, ('Total') := sum(B), by = .(A)]\n\n\n\n\n##    A B Total\n## 1: a 1     6\n## 2: a 2     6\n## 3: a 3     6\n## 4: b 4     9\n## 5: b 5     9\n\n\n\n# Add 1 to column B\nDT[c(2,4), ('B') := as.integer(1 + B)]\n\n\n\n\n##    A B Total\n## 1: a 1     6\n## 2: a 3     6\n## 3: a 3     6\n## 4: b 5     9\n## 5: b 5     9\n\n\n\n# Add a new column Total2\nDT[2:4, ':='(Total2 = sum(B)), by = .(A)]\n\n\n\n\n##    A B Total Total2\n## 1: a 1     6     NA\n## 2: a 3     6      6\n## 3: a 3     6      6\n## 4: b 5     9      5\n## 5: b 5     9     NA\n\n\n\n# Remove the Total column\nDT[, Total := NULL]\n\n\n\n\n##    A B Total2\n## 1: a 1     NA\n## 2: a 3      6\n## 3: a 3      6\n## 4: b 5      5\n## 5: b 5     NA\n\n\n\n# Select the third column using `[[`\nDT[[3]]\n\n\n\n\n## [1] NA  6  6  5 NA\n\n\n\nThe functional form\n\u00b6\n\n\n# A data.table DT\nDT <- data.table(A = c(1, 1, 1, 2, 2), B = 1:5)\n\n# Update B, add C and D\nDT[, `:=`(B = B + 1,  C = A + B, D = 2)]\n\n\n\n\n##    A B C D\n## 1: 1 2 2 2\n## 2: 1 3 3 2\n## 3: 1 4 4 2\n## 4: 2 5 6 2\n## 5: 2 6 7 2\n\n\n\n# Delete my_cols\nmy_cols <- c('B', 'C')\nDT[, (my_cols) := NULL]\n\n\n\n\n##    A D\n## 1: 1 2\n## 2: 1 2\n## 3: 1 2\n## 4: 2 2\n## 5: 2 2\n\n\n\n# Delete column 2 by number\nDT[, 2 := NULL]\n\n\n\n\n##    A\n## 1: 1\n## 2: 1\n## 3: 1\n## 4: 2\n## 5: 2\n\n\n\nReady, \nset\n, go!\n\u00b6\n\n\nThe \nset\n function is used to repeatedly update a data.table by\n\nreference. You can think of the \nset\n function as a loopable.\n\n\nA <- c(2, 2, 3, 5, 2, 5, 5, 4, 4, 1)\nB <- c(2, 1, 4, 2, 4, 3, 4, 5, 2, 4)\nC <- c(5, 2, 4, 1, 2, 2, 1, 2, 5, 2)\nD <- c(3, 3, 3, 1, 5, 4, 4, 1, 4, 3)\nDT <- data.table(A, B, C, D)\n\n# Set the seed\nset.seed(1)\n\n# Check the DT\nDT\n\n\n\n\n##     A B C D\n##  1: 2 2 5 3\n##  2: 2 1 2 3\n##  3: 3 4 4 3\n##  4: 5 2 1 1\n##  5: 2 4 2 5\n##  6: 5 3 2 4\n##  7: 5 4 1 4\n##  8: 4 5 2 1\n##  9: 4 2 5 4\n## 10: 1 4 2 3\n\n\n\n# For loop with set\nfor (l in 2:4) set(DT, sample(10,3), l, NA)\n\n# Change the column names to lowercase\nsetnames(DT,c('A','B','C','D'), c('a','b','c','d'))\n\n# Print the resulting DT to the console\nDT\n\n\n\n\n##     a  b  c  d\n##  1: 2  2  5  3\n##  2: 2  1 NA  3\n##  3: 3 NA  4  3\n##  4: 5 NA  1  1\n##  5: 2 NA  2  5\n##  6: 5  3  2 NA\n##  7: 5  4  1  4\n##  8: 4  5 NA  1\n##  9: 4  2  5 NA\n## 10: 1  4 NA NA\n\n\n\nThe \nset\n family\n\n\n# Define DT\nDT <- data.table(a = letters[c(1, 1, 1, 2, 2)], b = 1)\nDT\n\n\n\n\n##    a b\n## 1: a 1\n## 2: a 1\n## 3: a 1\n## 4: b 1\n## 5: b 1\n\n\n\n# Add a postfix '_2' to all column names\nsetnames(DT, c(1:2), paste0(c('a','b'), '_2'))\nDT\n\n\n\n\n##    a_2 b_2\n## 1:   a   1\n## 2:   a   1\n## 3:   a   1\n## 4:   b   1\n## 5:   b   1\n\n\n\n# Change column name 'a_2' to 'A2'\nsetnames(DT, 'a_2', 'A2')\nDT\n\n\n\n\n##    A2 b_2\n## 1:  a   1\n## 2:  a   1\n## 3:  a   1\n## 4:  b   1\n## 5:  b   1\n\n\n\n# Reverse the order of the columns\nsetcolorder(DT, c('b_2','A2'))\n\n\n\n\n3, \ndata.table\n expert\n\u00b6\n\n\nSelecting rows the \ndata.table\n way\n\u00b6\n\n\n# Convert iris to a data.table\niris <- data.table('Sepal.Length' = iris$Sepal.Length, 'Sepal.Width' = iris$Sepal.Width, 'Petal.Length' = iris$Petal.Length, 'Petal.Width' = iris$Petal.Width, 'Species' = iris$Species)\niris\n\n\n\n\n##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica\n\n\n\n# Species is 'virginica'\nhead(iris[Species == 'virginica'], 20)\n\n\n\n\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##  1:          6.3         3.3          6.0         2.5 virginica\n##  2:          5.8         2.7          5.1         1.9 virginica\n##  3:          7.1         3.0          5.9         2.1 virginica\n##  4:          6.3         2.9          5.6         1.8 virginica\n##  5:          6.5         3.0          5.8         2.2 virginica\n##  6:          7.6         3.0          6.6         2.1 virginica\n##  7:          4.9         2.5          4.5         1.7 virginica\n##  8:          7.3         2.9          6.3         1.8 virginica\n##  9:          6.7         2.5          5.8         1.8 virginica\n## 10:          7.2         3.6          6.1         2.5 virginica\n## 11:          6.5         3.2          5.1         2.0 virginica\n## 12:          6.4         2.7          5.3         1.9 virginica\n## 13:          6.8         3.0          5.5         2.1 virginica\n## 14:          5.7         2.5          5.0         2.0 virginica\n## 15:          5.8         2.8          5.1         2.4 virginica\n## 16:          6.4         3.2          5.3         2.3 virginica\n## 17:          6.5         3.0          5.5         1.8 virginica\n## 18:          7.7         3.8          6.7         2.2 virginica\n## 19:          7.7         2.6          6.9         2.3 virginica\n## 20:          6.0         2.2          5.0         1.5 virginica\n\n\n\n# Species is either 'virginica' or 'versicolor'\nhead(iris[Species %in% c('virginica', 'versicolor')], 20)\n\n\n\n\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n##  1:          7.0         3.2          4.7         1.4 versicolor\n##  2:          6.4         3.2          4.5         1.5 versicolor\n##  3:          6.9         3.1          4.9         1.5 versicolor\n##  4:          5.5         2.3          4.0         1.3 versicolor\n##  5:          6.5         2.8          4.6         1.5 versicolor\n##  6:          5.7         2.8          4.5         1.3 versicolor\n##  7:          6.3         3.3          4.7         1.6 versicolor\n##  8:          4.9         2.4          3.3         1.0 versicolor\n##  9:          6.6         2.9          4.6         1.3 versicolor\n## 10:          5.2         2.7          3.9         1.4 versicolor\n## 11:          5.0         2.0          3.5         1.0 versicolor\n## 12:          5.9         3.0          4.2         1.5 versicolor\n## 13:          6.0         2.2          4.0         1.0 versicolor\n## 14:          6.1         2.9          4.7         1.4 versicolor\n## 15:          5.6         2.9          3.6         1.3 versicolor\n## 16:          6.7         3.1          4.4         1.4 versicolor\n## 17:          5.6         3.0          4.5         1.5 versicolor\n## 18:          5.8         2.7          4.1         1.0 versicolor\n## 19:          6.2         2.2          4.5         1.5 versicolor\n## 20:          5.6         2.5          3.9         1.1 versicolor\n\n\n\nRemoving columns and adapting your column names\n\u00b6\n\n\nRefer to a regex cheat sheet for metacharacter.\n\n\n# iris as a data.table\niris <- as.data.table(iris)\niris\n\n\n\n\n##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica\n\n\n\n# Remove the 'Sepal.' prefix\n#gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x\nsetnames(iris, c('Sepal.Length', 'Sepal.Width'), c('Length','Width')) \n#gsub('^Sepal\\\\.','', iris)\n\n# Remove the two columns starting with 'Petal'\niris[, c('Petal.Length', 'Petal.Width') := NULL]\n\n\n\n\n##      Length Width   Species\n##   1:    5.1   3.5    setosa\n##   2:    4.9   3.0    setosa\n##   3:    4.7   3.2    setosa\n##   4:    4.6   3.1    setosa\n##   5:    5.0   3.6    setosa\n##  ---                       \n## 146:    6.7   3.0 virginica\n## 147:    6.3   2.5 virginica\n## 148:    6.5   3.0 virginica\n## 149:    6.2   3.4 virginica\n## 150:    5.9   3.0 virginica\n\n\n\nUnderstanding automatic indexing\n\u00b6\n\n\n# Cleaned up iris data.table\niris2 <- data.frame(Length = iris$Sepal.Length, Width = iris$Sepal.Width, Species = iris$Species)\niris2 <- as.data.table(iris2)\n\n\n\n\n# Area is greater than 20 square centimeters\niris2[ Width * Length > 20 ], 20\n\n\n\n\n   Length Width    Species is_large\n 1:    5.4   3.9     setosa    FALSE\n 2:    5.8   4.0     setosa    FALSE\n 3:    5.7   4.4     setosa     TRUE\n 4:    5.4   3.9     setosa    FALSE\n 5:    5.7   3.8     setosa    FALSE\n 6:    5.2   4.1     setosa    FALSE\n 7:    5.5   4.2     setosa    FALSE\n 8:    7.0   3.2 versicolor    FALSE\n 9:    6.4   3.2 versicolor    FALSE\n10:    6.9   3.1 versicolor    FALSE\n11:    6.3   3.3 versicolor    FALSE\n12:    6.7   3.1 versicolor    FALSE\n13:    6.7   3.0 versicolor    FALSE\n14:    6.0   3.4 versicolor    FALSE\n15:    6.7   3.1 versicolor    FALSE\n16:    6.3   3.3  virginica    FALSE\n17:    7.1   3.0  virginica    FALSE\n18:    7.6   3.0  virginica    FALSE\n19:    7.3   2.9  virginica    FALSE\n20:    7.2   3.6  virginica     TRUE\n...\n\n\n\n\n# Add new boolean column\niris2[, is_large := Width * Length > 25]\n\n\n\n\n# Now large observations with is_large\niris2[is_large == TRUE]\n\n\n\n\n   Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE\n\n\n\n\niris2[(is_large)] # Also OK\n\n\n\n\n   Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE\n\n\n\n\nSelecting groups or parts of groups\n\u00b6\n\n\n# The 'keyed' data.table DT\nDT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],\n                 B = c(5, 4, 1, 9,8 ,8, 6), \n                 C = 6:12)\nsetkey(DT, A, B)\n\n# Select the 'b' group\nDT['b']\n\n\n\n\n##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11\n\n\n\n# 'b' and 'c' groups\nDT[c('b', 'c')]\n\n\n\n\n##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11\n## 4: c 6 12\n## 5: c 9  9\n\n\n\n# The first row of the 'b' and 'c' groups\nDT[c('b', 'c'), mult = 'first']\n\n\n\n\n##    A B  C\n## 1: b 1  8\n## 2: c 6 12\n\n\n\n# First and last row of the 'b' and 'c' groups\nDT[c('b', 'c'), .SD[c(1, .N)], by = .EACHI]\n\n\n\n\n##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9\n\n\n\n# Copy and extend code for instruction 4: add printout\nDT[c('b', 'c'), { print(.SD); .SD[c(1, .N)] }, by = .EACHI]\n\n\n\n\n##    B  C\n## 1: 1  8\n## 2: 5  6\n## 3: 8 11\n##    B  C\n## 1: 6 12\n## 2: 9  9\n\n##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9\n\n\n\nRolling joins\n\u00b6\n\n\nRolling joins \u2013 part one\n\n\n# Keyed data.table DT\nDT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],                  B = c(5, 4, 1, 9, 8, 8, 6), \n                 C = 6:12, \n                 key = 'A,B')\n\n# Get the key of DT\nkey(DT)\n\n\n\n\n## [1] \"A\" \"B\"\n\n\n\n# Row where A == 'b' & B == 6\nsetkey(DT, A, B)\nDT[.('b', 6)]\n\n\n\n\n##    A B  C\n## 1: b 6 NA\n\n\n\n# Return the prevailing row\nDT[.('b',6), roll = TRUE]\n\n\n\n\n##    A B C\n## 1: b 6 6\n\n\n\n# Return the nearest row\nDT[.('b',6), roll =+ Inf]\n\n\n\n\n##    A B C\n## 1: b 6 6\n\n\n\nRolling joins \u2013 part two\n\n\n# Keyed data.table DT\nDT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],                  B = c(5, 4, 1, 9, 8, 8, 6), \n                 C = 6:12, \n                 key = 'A,B')\n\n# Look at the sequence (-2):10 for the 'b' group\nDT[.('b', (-2):10)]\n\n\n\n\n##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2 NA\n##  6: b  3 NA\n##  7: b  4 NA\n##  8: b  5  6\n##  9: b  6 NA\n## 10: b  7 NA\n## 11: b  8 11\n## 12: b  9 NA\n## 13: b 10 NA\n\n\n\n# Add code: carry the prevailing values forwards\nDT[.('b', (-2):10), roll = TRUE]\n\n\n\n\n##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11\n\n\n\n# Add code: carry the first observation backwards\nDT[.('b', (-2):10), roll = TRUE, rollends = TRUE]\n\n\n\n\n##     A  B  C\n##  1: b -2  8\n##  2: b -1  8\n##  3: b  0  8\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11",
            "title": "Data Analysis in R, the data.table Way"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#1-datatable-novice",
            "text": "Find out more with  ?data.table .",
            "title": "1, data.table novice"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#create-and-subset-a-datatable",
            "text": "# The data.table package\nlibrary(data.table)\n\n# Create my_first_data_table\nmy_first_data_table <- data.table(x = c('a', 'b', 'c', 'd', 'e'), y = c(1, 2, 3, 4, 5))\n\n# Create a data.table using recycling\nDT <- data.table(a = 1:2, b = c('A', 'B', 'C', 'D'))\n\n# Print the third row to the console\nDT[3,]  ##    a b\n## 1: 1 C  # Print the second and third row to the console, but do not commas\nDT[2:3]  ##    a b\n## 1: 2 B\n## 2: 1 C",
            "title": "Create and subset a data.table"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#getting-to-know-a-datatable",
            "text": "Like  head ,  tail .  # Print the penultimate row of DT using .N\nDT[.N - 1]  ##    a b\n## 1: 1 C  # Print the column names of DT, and number of rows and number of columns\ncolnames(DT)  ## [1] \"a\" \"b\"  dim(DT)  ## [1] 4 2  # Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1.\nDT[c(2, 2, 3)]  ##    a b\n## 1: 2 B\n## 2: 2 B\n## 3: 1 C  DT  is a data.table/data.frame, but  DT[ , B]  is a vector;  DT[ , .(B)]  is a subsetted data.table.",
            "title": "Getting to know a data.table"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#subsetting-data-tables",
            "text": "DT[i, j, by]  means take  DT , subset rows using  i , then calculate  j  grouped by  by . You can wrap  j  with  .() .  A <- c(1, 2, 3, 4, 5)\nB <- c('a', 'b', 'c', 'd', 'e')\nC <- c(6, 7, 8, 9, 10)\nDT <- data.table(A, B, C)\n\n# Subset rows 1 and 3, and columns B and C\nDT[c(1,3) ,.(B, C)]  ##    B C\n## 1: a 6\n## 2: c 8  # Assign to ans the correct value\nans <- data.table(DT[, .(B, val = A * C)])\n\n# Fill in the blanks such that ans2 equals target\n#target <- data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'),  val = as.integer(c(6:10, 1:5)))\nans2 <- data.table(DT[, .(B, val = as.integer(c(6:10, 1:5)))])",
            "title": "Subsetting data tables"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#the-by-basics",
            "text": "# iris and iris3 are already available in the workspace\n\n# Convert iris to a data.table: DT\nDT <- as.data.table(iris)\n\n# For each Species, print the mean Sepal.Length\nDT[, .(mean(Sepal.Length)), by = .(Species)]  ##       Species    V1\n## 1:     setosa 5.006\n## 2: versicolor 5.936\n## 3:  virginica 6.588  # Print mean Sepal.Length, grouping by first letter of Species\nDT[, .(mean(Sepal.Length)), by = .(substr(Species, 1,1))]  ##    substr    V1\n## 1:      s 5.006\n## 2:      v 6.262",
            "title": "The by basics"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#using-n-and-by",
            "text": ".N , number, in row or column.  # data.table version of iris: DT\nDT <- as.data.table(iris)\n\n# Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group.\nDT[, .N, by = 10 * round(Sepal.Length * Sepal.Width / 10)]  ##    round   N\n## 1:    20 117\n## 2:    10  29\n## 3:    30   4  # Now name the output columns `Area` and `Count`\nDT[, .(Count = .N), by = .(Area = 10 * round(Sepal.Length * Sepal.Width / 10))]  ##    Area Count\n## 1:   20   117\n## 2:   10    29\n## 3:   30     4",
            "title": "Using .N and by"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#return-multiple-numbers-in-j",
            "text": "# Create the data.table DT\nset.seed(1L)\nDT <- data.table(A = rep(letters[2:1], each = 4L), B = rep(1:4, each = 2L), C = sample(8))\n\n# Create the new data.table, DT2\nDT2 <- DT[, .(C = cumsum(C)), by=.(A,B)]\n\n# Select from DT2 the last two values from C while you group by A\nDT2[, .(C = tail(C,2)), by=A]  ##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8",
            "title": "Return multiple numbers in j"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#2-datatable-yeoman",
            "text": "",
            "title": "2, data.table yeoman"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#chaining-the-basics",
            "text": "# Build DT\nset.seed(1L)\nDT <- data.table(A = rep(letters[2:1], each = 4L), B = rep(1:4, each = 2L), C = sample(8)) \nDT  ##    A B C\n## 1: b 1 3\n## 2: b 1 8\n## 3: b 2 4\n## 4: b 2 5\n## 5: a 3 1\n## 6: a 3 7\n## 7: a 4 2\n## 8: a 4 6  # Use chaining\n# Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A\nDT[, .(C = cumsum(C)), by = .(A,B)][, .(C = tail(C,2)), by = .(A)]  ##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8  Chaining your  iris  dataset  DT <- data.table(iris)\n\n# Perform chained operations on DT\nDT[, .(Sepal.Length = median(Sepal.Length), Sepal.Width = median(Sepal.Width), Petal.Length = median(Petal.Length), Petal.Width = median(Petal.Width)), by = .(Species)][order(Species, decreasing = TRUE)]  ##       Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1:  virginica          6.5         3.0         5.55         2.0\n## 2: versicolor          5.9         2.8         4.35         1.3\n## 3:     setosa          5.0         3.4         1.50         0.2  DT  ##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica",
            "title": "Chaining, the basics"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#programming-time-vs-readability",
            "text": "x <- c(2, 1, 2, 1, 2, 2, 1)\ny <- c(1, 3, 5, 7, 9, 11, 13)\nz <- c(2, 4, 6, 8, 10, 12, 14)\nDT <- data.table(x, y, z)\n\n# Mean of columns\nDT[, lapply(.SD, mean), by = .(x)]   ##    x        y        z\n## 1: 2 6.500000 7.500000\n## 2: 1 7.666667 8.666667  # Median of columns\nDT[, lapply(.SD, median), by = .(x)]  ##    x y z\n## 1: 2 7 8\n## 2: 1 7 8",
            "title": "Programming time vs readability"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#introducing-sdcols",
            "text": ".SDcols  specifies the columns of  DT  that are included in  .SD .  grp <- c(6, 6, 8, 8, 8)\nQ1 <- c(4, 3, 3, 5, 3)\nQ2 <- c(1, 4, 1, 4, 4)\nQ3 <- c(3, 1, 5, 5, 2)\nH1 <- c(1, 2, 3, 2, 4)\nH2 <- c(1, 4, 3, 4, 3)\nDT <- data.table(grp, Q1, Q2, Q3, H1, H2)\n\n# Calculate the sum of the Q columns\nDT[, lapply(.SD, sum), .SDcols = 2:4]  ##    Q1 Q2 Q3\n## 1: 18 14 16  # Calculate the sum of columns H1 and H2 \nDT[, lapply(.SD, sum), .SDcols = 5:6]  ##    H1 H2\n## 1: 12 15  # Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns. \nDT[, .SD[-1], .SDcols = 2:4, by = .(grp)]  ##    grp Q1 Q2 Q3\n## 1:   6  3  4  1\n## 2:   8  5  4  5\n## 3:   8  3  4  2",
            "title": "Introducing .SDcols"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#mixing-it-together-lapply-sd-sdcols-and-n",
            "text": "x <- c(2, 1, 2, 1, 2, 2, 1)\ny <- c(1, 3, 5, 7, 9, 11, 13)\nz <- c(2, 4, 6, 8, 10, 12, 14)\nDT <- data.table(x, y, z)\n\n# Sum of all columns and the number of rows\n# For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together.\nDT  ##    x  y  z\n## 1: 2  1  2\n## 2: 1  3  4\n## 3: 2  5  6\n## 4: 1  7  8\n## 5: 2  9 10\n## 6: 2 11 12\n## 7: 1 13 14  DT[,c(lapply(.SD, sum), .N), .SDcols = 1:3, by = x]  ##    x x  y  z N\n## 1: 2 8 26 30 4\n## 2: 1 3 23 26 3  # Cumulative sum of column x and y while grouping by x and z > 8\nDT[,lapply(.SD, cumsum), .SDcols = 1:2, by = .(by1 = x, by2 = z > 8)]  ##    by1   by2 x  y\n## 1:   2 FALSE 2  1\n## 2:   2 FALSE 4  6\n## 3:   1 FALSE 1  3\n## 4:   1 FALSE 2 10\n## 5:   2  TRUE 2  9\n## 6:   2  TRUE 4 20\n## 7:   1  TRUE 1 13  # Chaining\nDT[,lapply(.SD, cumsum), .SDcols = 1:2, by = .(by1 = x, by2 = z > 8)][,lapply(.SD, max), .SDcols = 3:4, by = by1]  ##    by1 x  y\n## 1:   2 4 20\n## 2:   1 2 13",
            "title": "Mixing it together: lapply, .SD, SDcols and .N"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#adding-updating-and-removing-columns",
            "text": ":=  is defined for use in  j  only.  # The data.table DT\nDT <- data.table(A = letters[c(1, 1, 1, 2, 2)], B = 1:5)\nDT  ##    A B\n## 1: a 1\n## 2: a 2\n## 3: a 3\n## 4: b 4\n## 5: b 5  # Add column by reference: Total\nDT[, ('Total') := sum(B), by = .(A)]  ##    A B Total\n## 1: a 1     6\n## 2: a 2     6\n## 3: a 3     6\n## 4: b 4     9\n## 5: b 5     9  # Add 1 to column B\nDT[c(2,4), ('B') := as.integer(1 + B)]  ##    A B Total\n## 1: a 1     6\n## 2: a 3     6\n## 3: a 3     6\n## 4: b 5     9\n## 5: b 5     9  # Add a new column Total2\nDT[2:4, ':='(Total2 = sum(B)), by = .(A)]  ##    A B Total Total2\n## 1: a 1     6     NA\n## 2: a 3     6      6\n## 3: a 3     6      6\n## 4: b 5     9      5\n## 5: b 5     9     NA  # Remove the Total column\nDT[, Total := NULL]  ##    A B Total2\n## 1: a 1     NA\n## 2: a 3      6\n## 3: a 3      6\n## 4: b 5      5\n## 5: b 5     NA  # Select the third column using `[[`\nDT[[3]]  ## [1] NA  6  6  5 NA",
            "title": "Adding, updating, and removing columns"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#the-functional-form",
            "text": "# A data.table DT\nDT <- data.table(A = c(1, 1, 1, 2, 2), B = 1:5)\n\n# Update B, add C and D\nDT[, `:=`(B = B + 1,  C = A + B, D = 2)]  ##    A B C D\n## 1: 1 2 2 2\n## 2: 1 3 3 2\n## 3: 1 4 4 2\n## 4: 2 5 6 2\n## 5: 2 6 7 2  # Delete my_cols\nmy_cols <- c('B', 'C')\nDT[, (my_cols) := NULL]  ##    A D\n## 1: 1 2\n## 2: 1 2\n## 3: 1 2\n## 4: 2 2\n## 5: 2 2  # Delete column 2 by number\nDT[, 2 := NULL]  ##    A\n## 1: 1\n## 2: 1\n## 3: 1\n## 4: 2\n## 5: 2",
            "title": "The functional form"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#ready-set-go",
            "text": "The  set  function is used to repeatedly update a data.table by \nreference. You can think of the  set  function as a loopable.  A <- c(2, 2, 3, 5, 2, 5, 5, 4, 4, 1)\nB <- c(2, 1, 4, 2, 4, 3, 4, 5, 2, 4)\nC <- c(5, 2, 4, 1, 2, 2, 1, 2, 5, 2)\nD <- c(3, 3, 3, 1, 5, 4, 4, 1, 4, 3)\nDT <- data.table(A, B, C, D)\n\n# Set the seed\nset.seed(1)\n\n# Check the DT\nDT  ##     A B C D\n##  1: 2 2 5 3\n##  2: 2 1 2 3\n##  3: 3 4 4 3\n##  4: 5 2 1 1\n##  5: 2 4 2 5\n##  6: 5 3 2 4\n##  7: 5 4 1 4\n##  8: 4 5 2 1\n##  9: 4 2 5 4\n## 10: 1 4 2 3  # For loop with set\nfor (l in 2:4) set(DT, sample(10,3), l, NA)\n\n# Change the column names to lowercase\nsetnames(DT,c('A','B','C','D'), c('a','b','c','d'))\n\n# Print the resulting DT to the console\nDT  ##     a  b  c  d\n##  1: 2  2  5  3\n##  2: 2  1 NA  3\n##  3: 3 NA  4  3\n##  4: 5 NA  1  1\n##  5: 2 NA  2  5\n##  6: 5  3  2 NA\n##  7: 5  4  1  4\n##  8: 4  5 NA  1\n##  9: 4  2  5 NA\n## 10: 1  4 NA NA  The  set  family  # Define DT\nDT <- data.table(a = letters[c(1, 1, 1, 2, 2)], b = 1)\nDT  ##    a b\n## 1: a 1\n## 2: a 1\n## 3: a 1\n## 4: b 1\n## 5: b 1  # Add a postfix '_2' to all column names\nsetnames(DT, c(1:2), paste0(c('a','b'), '_2'))\nDT  ##    a_2 b_2\n## 1:   a   1\n## 2:   a   1\n## 3:   a   1\n## 4:   b   1\n## 5:   b   1  # Change column name 'a_2' to 'A2'\nsetnames(DT, 'a_2', 'A2')\nDT  ##    A2 b_2\n## 1:  a   1\n## 2:  a   1\n## 3:  a   1\n## 4:  b   1\n## 5:  b   1  # Reverse the order of the columns\nsetcolorder(DT, c('b_2','A2'))",
            "title": "Ready, set, go!"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#3-datatable-expert",
            "text": "",
            "title": "3, data.table expert"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#selecting-rows-the-datatable-way",
            "text": "# Convert iris to a data.table\niris <- data.table('Sepal.Length' = iris$Sepal.Length, 'Sepal.Width' = iris$Sepal.Width, 'Petal.Length' = iris$Petal.Length, 'Petal.Width' = iris$Petal.Width, 'Species' = iris$Species)\niris  ##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica  # Species is 'virginica'\nhead(iris[Species == 'virginica'], 20)  ##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##  1:          6.3         3.3          6.0         2.5 virginica\n##  2:          5.8         2.7          5.1         1.9 virginica\n##  3:          7.1         3.0          5.9         2.1 virginica\n##  4:          6.3         2.9          5.6         1.8 virginica\n##  5:          6.5         3.0          5.8         2.2 virginica\n##  6:          7.6         3.0          6.6         2.1 virginica\n##  7:          4.9         2.5          4.5         1.7 virginica\n##  8:          7.3         2.9          6.3         1.8 virginica\n##  9:          6.7         2.5          5.8         1.8 virginica\n## 10:          7.2         3.6          6.1         2.5 virginica\n## 11:          6.5         3.2          5.1         2.0 virginica\n## 12:          6.4         2.7          5.3         1.9 virginica\n## 13:          6.8         3.0          5.5         2.1 virginica\n## 14:          5.7         2.5          5.0         2.0 virginica\n## 15:          5.8         2.8          5.1         2.4 virginica\n## 16:          6.4         3.2          5.3         2.3 virginica\n## 17:          6.5         3.0          5.5         1.8 virginica\n## 18:          7.7         3.8          6.7         2.2 virginica\n## 19:          7.7         2.6          6.9         2.3 virginica\n## 20:          6.0         2.2          5.0         1.5 virginica  # Species is either 'virginica' or 'versicolor'\nhead(iris[Species %in% c('virginica', 'versicolor')], 20)  ##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n##  1:          7.0         3.2          4.7         1.4 versicolor\n##  2:          6.4         3.2          4.5         1.5 versicolor\n##  3:          6.9         3.1          4.9         1.5 versicolor\n##  4:          5.5         2.3          4.0         1.3 versicolor\n##  5:          6.5         2.8          4.6         1.5 versicolor\n##  6:          5.7         2.8          4.5         1.3 versicolor\n##  7:          6.3         3.3          4.7         1.6 versicolor\n##  8:          4.9         2.4          3.3         1.0 versicolor\n##  9:          6.6         2.9          4.6         1.3 versicolor\n## 10:          5.2         2.7          3.9         1.4 versicolor\n## 11:          5.0         2.0          3.5         1.0 versicolor\n## 12:          5.9         3.0          4.2         1.5 versicolor\n## 13:          6.0         2.2          4.0         1.0 versicolor\n## 14:          6.1         2.9          4.7         1.4 versicolor\n## 15:          5.6         2.9          3.6         1.3 versicolor\n## 16:          6.7         3.1          4.4         1.4 versicolor\n## 17:          5.6         3.0          4.5         1.5 versicolor\n## 18:          5.8         2.7          4.1         1.0 versicolor\n## 19:          6.2         2.2          4.5         1.5 versicolor\n## 20:          5.6         2.5          3.9         1.1 versicolor",
            "title": "Selecting rows the data.table way"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#removing-columns-and-adapting-your-column-names",
            "text": "Refer to a regex cheat sheet for metacharacter.  # iris as a data.table\niris <- as.data.table(iris)\niris  ##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica  # Remove the 'Sepal.' prefix\n#gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x\nsetnames(iris, c('Sepal.Length', 'Sepal.Width'), c('Length','Width')) \n#gsub('^Sepal\\\\.','', iris)\n\n# Remove the two columns starting with 'Petal'\niris[, c('Petal.Length', 'Petal.Width') := NULL]  ##      Length Width   Species\n##   1:    5.1   3.5    setosa\n##   2:    4.9   3.0    setosa\n##   3:    4.7   3.2    setosa\n##   4:    4.6   3.1    setosa\n##   5:    5.0   3.6    setosa\n##  ---                       \n## 146:    6.7   3.0 virginica\n## 147:    6.3   2.5 virginica\n## 148:    6.5   3.0 virginica\n## 149:    6.2   3.4 virginica\n## 150:    5.9   3.0 virginica",
            "title": "Removing columns and adapting your column names"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#understanding-automatic-indexing",
            "text": "# Cleaned up iris data.table\niris2 <- data.frame(Length = iris$Sepal.Length, Width = iris$Sepal.Width, Species = iris$Species)\niris2 <- as.data.table(iris2)  # Area is greater than 20 square centimeters\niris2[ Width * Length > 20 ], 20     Length Width    Species is_large\n 1:    5.4   3.9     setosa    FALSE\n 2:    5.8   4.0     setosa    FALSE\n 3:    5.7   4.4     setosa     TRUE\n 4:    5.4   3.9     setosa    FALSE\n 5:    5.7   3.8     setosa    FALSE\n 6:    5.2   4.1     setosa    FALSE\n 7:    5.5   4.2     setosa    FALSE\n 8:    7.0   3.2 versicolor    FALSE\n 9:    6.4   3.2 versicolor    FALSE\n10:    6.9   3.1 versicolor    FALSE\n11:    6.3   3.3 versicolor    FALSE\n12:    6.7   3.1 versicolor    FALSE\n13:    6.7   3.0 versicolor    FALSE\n14:    6.0   3.4 versicolor    FALSE\n15:    6.7   3.1 versicolor    FALSE\n16:    6.3   3.3  virginica    FALSE\n17:    7.1   3.0  virginica    FALSE\n18:    7.6   3.0  virginica    FALSE\n19:    7.3   2.9  virginica    FALSE\n20:    7.2   3.6  virginica     TRUE\n...  # Add new boolean column\niris2[, is_large := Width * Length > 25]  # Now large observations with is_large\niris2[is_large == TRUE]     Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE  iris2[(is_large)] # Also OK     Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE",
            "title": "Understanding automatic indexing"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#selecting-groups-or-parts-of-groups",
            "text": "# The 'keyed' data.table DT\nDT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],\n                 B = c(5, 4, 1, 9,8 ,8, 6), \n                 C = 6:12)\nsetkey(DT, A, B)\n\n# Select the 'b' group\nDT['b']  ##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11  # 'b' and 'c' groups\nDT[c('b', 'c')]  ##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11\n## 4: c 6 12\n## 5: c 9  9  # The first row of the 'b' and 'c' groups\nDT[c('b', 'c'), mult = 'first']  ##    A B  C\n## 1: b 1  8\n## 2: c 6 12  # First and last row of the 'b' and 'c' groups\nDT[c('b', 'c'), .SD[c(1, .N)], by = .EACHI]  ##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9  # Copy and extend code for instruction 4: add printout\nDT[c('b', 'c'), { print(.SD); .SD[c(1, .N)] }, by = .EACHI]  ##    B  C\n## 1: 1  8\n## 2: 5  6\n## 3: 8 11\n##    B  C\n## 1: 6 12\n## 2: 9  9\n\n##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9",
            "title": "Selecting groups or parts of groups"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#rolling-joins",
            "text": "Rolling joins \u2013 part one  # Keyed data.table DT\nDT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],                  B = c(5, 4, 1, 9, 8, 8, 6), \n                 C = 6:12, \n                 key = 'A,B')\n\n# Get the key of DT\nkey(DT)  ## [1] \"A\" \"B\"  # Row where A == 'b' & B == 6\nsetkey(DT, A, B)\nDT[.('b', 6)]  ##    A B  C\n## 1: b 6 NA  # Return the prevailing row\nDT[.('b',6), roll = TRUE]  ##    A B C\n## 1: b 6 6  # Return the nearest row\nDT[.('b',6), roll =+ Inf]  ##    A B C\n## 1: b 6 6  Rolling joins \u2013 part two  # Keyed data.table DT\nDT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],                  B = c(5, 4, 1, 9, 8, 8, 6), \n                 C = 6:12, \n                 key = 'A,B')\n\n# Look at the sequence (-2):10 for the 'b' group\nDT[.('b', (-2):10)]  ##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2 NA\n##  6: b  3 NA\n##  7: b  4 NA\n##  8: b  5  6\n##  9: b  6 NA\n## 10: b  7 NA\n## 11: b  8 11\n## 12: b  9 NA\n## 13: b 10 NA  # Add code: carry the prevailing values forwards\nDT[.('b', (-2):10), roll = TRUE]  ##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11  # Add code: carry the first observation backwards\nDT[.('b', (-2):10), roll = TRUE, rollends = TRUE]  ##     A  B  C\n##  1: b -2  8\n##  2: b -1  8\n##  3: b  0  8\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11",
            "title": "Rolling joins"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/",
            "text": "Documentation\n\n\n1, Introduction to \ndplyr\n\n\nLoad the \ndplyr\n and \nhflights\n\n    package\n\n\nConvert \ndata.frame\n to table\n\n\nChanging labels of \nhflights\n\n\n\n\n\n\n2, \nselect\n and \nmutate\n\n\nThe five verbs and their\n\n    meaning\n\n\nThe \nselect\n verb\n\n\nHelper functions for variable\n\n    selection\n\n\nComparison to basic R\n\n\nmutate\n is creating\n\n\nAdd multiple variables using\n\n\nmutate\n\n\n\n\n\n\n3, \nfilter\n and \narrange\n\n\nLogical operators\n\n\nCombining tests using boolean\n\n    operators\n\n\nBlend together\n\n\nArranging your data\n\n\nReverse the order of\n\n    arranging\n\n\n\n\n\n\n4, \nsummarise\n and the Pipe\n\n    Operator\n\n\nThe syntax of \nsummarise\n\n\nAggregate functions\n\n\ndplyr\n aggregate functions\n\n\n\n\n\n\n5, \ngroup_by\n and working with\n\n    data\n\n\nUnite and conquer using\n\n\ngroup_by\n\n\nCombine \ngroup_by\n with\n\n\nmutate\n\n\nAdvanced \ngroup_by\n\n\ndplyr\n deals with different\n\n    types\n\n\ndplyr\n and mySQL databases\n\n\n\n\n\n\nAdding \ntidyr\n Functions\n\n\nJoining Data in R with \ndplyr\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\ndata.table\n\n\n\n\nextension of \ndata.frame\n.\n\n\nFast aggregation of large data (e.g. 100GB in RAM), fast ordered\n\n    joins, fast add/modify/delete of columns by group using no copies at\n\n    all, list columns, a fast friendly file reader and parallel\n\n    file writer. Offers a natural and flexible syntax, for\n\n    faster development.\n\n\n\n\ndplyr\n\n\n\n\nA fast, consistent tool for working with data frame like objects,\n\n    both in memory and out of memory.\n\n\nPipelines.\n\n\n\n\ntidyr\n\n\n\n\nAn evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data\n\n    tidying (not general reshaping or aggregating) and works well with\n\n    dplyr data pipelines.\n\n\n\n\n\n\n\n\n\n\npackage\n\n\n'narrower'\n\n\n'wider'\n\n\n\n\n\n\n\n\n\n\ntidyr\n\n\ngather\n\n\nspread\n\n\n\n\n\n\nreshape2\n\n\nmelt\n\n\ncast\n\n\n\n\n\n\nspreadsheets\n\n\nunpivot\n\n\npivot\n\n\n\n\n\n\ndatabases\n\n\nfold\n\n\nunfold\n\n\n\n\n\n\n\n\n\n1, Introduction to \ndplyr\n\u00b6\n\n\nLoad the \ndplyr\n and \nhflights\n package\n\u00b6\n\n\n# Load the dplyr package\nlibrary(dplyr)\nlibrary(dtplyr)\n\n# Load the hflights package\n# A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011\nlibrary(hflights)\n\n# Call both head() and summary() on hflights\nhead(hflights)\n\n\n\n\n##      Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## 5424 2011     1          1         6    1400    1500            AA\n## 5425 2011     1          2         7    1401    1501            AA\n## 5426 2011     1          3         1    1352    1502            AA\n## 5427 2011     1          4         2    1403    1513            AA\n## 5428 2011     1          5         3    1405    1507            AA\n## 5429 2011     1          6         4    1359    1503            AA\n##      FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin\n## 5424       428  N576AA                60      40      -10        0    IAH\n## 5425       428  N557AA                60      45       -9        1    IAH\n## 5426       428  N541AA                70      48       -8       -8    IAH\n## 5427       428  N403AA                70      39        3        3    IAH\n## 5428       428  N492AA                62      44       -3        5    IAH\n## 5429       428  N262AA                64      45       -7       -1    IAH\n##      Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted\n## 5424  DFW      224      7      13         0                         0\n## 5425  DFW      224      6       9         0                         0\n## 5426  DFW      224      5      17         0                         0\n## 5427  DFW      224      9      22         0                         0\n## 5428  DFW      224      9       9         0                         0\n## 5429  DFW      224      6      13         0                         0\n\n\n\nsummary(hflights)\n\n\n\n\n##       Year          Month          DayofMonth      DayOfWeek    \n##  Min.   :2011   Min.   : 1.000   Min.   : 1.00   Min.   :1.000  \n##  1st Qu.:2011   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:2.000  \n##  Median :2011   Median : 7.000   Median :16.00   Median :4.000  \n##  Mean   :2011   Mean   : 6.514   Mean   :15.74   Mean   :3.948  \n##  3rd Qu.:2011   3rd Qu.: 9.000   3rd Qu.:23.00   3rd Qu.:6.000  \n##  Max.   :2011   Max.   :12.000   Max.   :31.00   Max.   :7.000  \n##                                                                 \n##     DepTime        ArrTime     UniqueCarrier        FlightNum   \n##  Min.   :   1   Min.   :   1   Length:227496      Min.   :   1  \n##  1st Qu.:1021   1st Qu.:1215   Class :character   1st Qu.: 855  \n##  Median :1416   Median :1617   Mode  :character   Median :1696  \n##  Mean   :1396   Mean   :1578                      Mean   :1962  \n##  3rd Qu.:1801   3rd Qu.:1953                      3rd Qu.:2755  \n##  Max.   :2400   Max.   :2400                      Max.   :7290  \n##  NA's   :2905   NA's   :3066                                    \n##    TailNum          ActualElapsedTime    AirTime         ArrDelay      \n##  Length:227496      Min.   : 34.0     Min.   : 11.0   Min.   :-70.000  \n##  Class :character   1st Qu.: 77.0     1st Qu.: 58.0   1st Qu.: -8.000  \n##  Mode  :character   Median :128.0     Median :107.0   Median :  0.000  \n##                     Mean   :129.3     Mean   :108.1   Mean   :  7.094  \n##                     3rd Qu.:165.0     3rd Qu.:141.0   3rd Qu.: 11.000  \n##                     Max.   :575.0     Max.   :549.0   Max.   :978.000  \n##                     NA's   :3622      NA's   :3622    NA's   :3622     \n##     DepDelay          Origin              Dest              Distance     \n##  Min.   :-33.000   Length:227496      Length:227496      Min.   :  79.0  \n##  1st Qu.: -3.000   Class :character   Class :character   1st Qu.: 376.0  \n##  Median :  0.000   Mode  :character   Mode  :character   Median : 809.0  \n##  Mean   :  9.445                                         Mean   : 787.8  \n##  3rd Qu.:  9.000                                         3rd Qu.:1042.0  \n##  Max.   :981.000                                         Max.   :3904.0  \n##  NA's   :2905                                                            \n##      TaxiIn           TaxiOut         Cancelled       CancellationCode  \n##  Min.   :  1.000   Min.   :  1.00   Min.   :0.00000   Length:227496     \n##  1st Qu.:  4.000   1st Qu.: 10.00   1st Qu.:0.00000   Class :character  \n##  Median :  5.000   Median : 14.00   Median :0.00000   Mode  :character  \n##  Mean   :  6.099   Mean   : 15.09   Mean   :0.01307                     \n##  3rd Qu.:  7.000   3rd Qu.: 18.00   3rd Qu.:0.00000                     \n##  Max.   :165.000   Max.   :163.00   Max.   :1.00000                     \n##  NA's   :3066      NA's   :2947                                         \n##     Diverted       \n##  Min.   :0.000000  \n##  1st Qu.:0.000000  \n##  Median :0.000000  \n##  Mean   :0.002853  \n##  3rd Qu.:0.000000  \n##  Max.   :1.000000  \n##\n\n\n\nConvert \ndata.frame\n to table\n\u00b6\n\n\n# Convert the hflights data.frame into a hflights tbl\nhflights <- tbl_df(hflights)\n\n# Display the hflights tbl\nhflights\n\n\n\n\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500            AA\n## 2   2011     1          2         7    1401    1501            AA\n## 3   2011     1          3         1    1352    1502            AA\n## 4   2011     1          4         2    1403    1513            AA\n## 5   2011     1          5         3    1405    1507            AA\n## 6   2011     1          6         4    1359    1503            AA\n## 7   2011     1          7         5    1359    1509            AA\n## 8   2011     1          8         6    1355    1454            AA\n## 9   2011     1          9         7    1443    1554            AA\n## 10  2011     1         10         1    1443    1553            AA\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# Create the object carriers, containing only the UniqueCarrier variable of hflights\ncarriers <- hflights$UniqueCarrier\n\n\n\n\nChanging labels of \nhflights\n\u00b6\n\n\npart 1 of 2\n\n\n# add\nlut <- c('AA' = 'American', 'AS' = 'Alaska', 'B6' = 'JetBlue', 'CO' = 'Continental', \n         'DL' = 'Delta', 'OO' = 'SkyWest', 'UA' = 'United', 'US' = 'US_Airways', \n         'WN' = 'Southwest', 'EV' = 'Atlantic_Southeast', 'F9' = 'Frontier', \n         'FL' = 'AirTran', 'MQ' = 'American_Eagle', 'XE' = 'ExpressJet', 'YV' = 'Mesa')\n\n# Use lut to translate the UniqueCarrier column of hflights\nhflights$UniqueCarrier <- lut[hflights$UniqueCarrier]\n\n# Inspect the resulting raw values of your variables\nglimpse(hflights)\n\n\n\n\n## Observations: 227,496\n## Variables: 21\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\npart 2 of 2\n\n\n# Build the lookup table: lut\nlut <- c(\"A\" = \"carrier\", \"B\" = \"weather\" ,\"C\" = \"FFA\" ,\"D\" = \"security\", \"E\" = \"not cancelled\")\n\n# Add the Code column\nhflights$Code <- lut[hflights$CancellationCode]\n\n# Glimpse at hflights\nglimpse(hflights)\n\n\n\n\nResult.\n\n\nObservations: 227,496\nVariables: 22\n$ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011,...\n$ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n$ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...\n$ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,...\n$ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359,...\n$ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503,...\n$ UniqueCarrier     <chr> \"American\", \"American\", \"American\",...\n$ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, ...\n$ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403...\n$ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71,...\n$ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41,...\n$ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44...\n$ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43,...\n$ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", ...\n$ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", ...\n$ Distance          <int> 224, 224, 224, 224, 224, 224, 224, ...\n$ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4...\n$ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 1...\n$ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",...\n$ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ Code              <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,...\n\n\n\n\n2, \nselect\n and \nmutate\n\u00b6\n\n\nThe five verbs and their meaning\n\u00b6\n\n\n\n\nselect\n; which returns a subset of the columns.\n\n\nfilter\n; that is able to return a subset of the rows.\n\n\narrange\n; that reorders the rows according to single or\n\n    multiple variables.\n\n\nmutate\n; used to add columns from existing data.\n\n\nsummarise\n; which reduces each group to a single row by calculating\n\n    aggregate measures.\n\n\n\n\nThe \nselect\n verb\n\u00b6\n\n\n# Print out a tbl with the four columns of hflights related to delay\nselect(hflights, ActualElapsedTime, AirTime, ArrDelay, DepDelay)\n\n\n\n\n## # A tibble: 227,496 \u00d7 4\n##    ActualElapsedTime AirTime ArrDelay DepDelay\n## *              <int>   <int>    <int>    <int>\n## 1                 60      40      -10        0\n## 2                 60      45       -9        1\n## 3                 70      48       -8       -8\n## 4                 70      39        3        3\n## 5                 62      44       -3        5\n## 6                 64      45       -7       -1\n## 7                 70      43       -1       -1\n## 8                 59      40      -16       -5\n## 9                 71      41       44       43\n## 10                70      45       43       43\n## # ... with 227,486 more rows\n\n\n\n# Print out hflights, nothing has changed!\nhflights\n\n\n\n\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# Print out the columns Origin up to Cancelled of hflights\nselect(hflights, 14:19)\n\n\n\n\n## # A tibble: 227,496 \u00d7 6\n##    Origin  Dest Distance TaxiIn TaxiOut Cancelled\n## *   <chr> <chr>    <int>  <int>   <int>     <int>\n## 1     IAH   DFW      224      7      13         0\n## 2     IAH   DFW      224      6       9         0\n## 3     IAH   DFW      224      5      17         0\n## 4     IAH   DFW      224      9      22         0\n## 5     IAH   DFW      224      9       9         0\n## 6     IAH   DFW      224      6      13         0\n## 7     IAH   DFW      224     12      15         0\n## 8     IAH   DFW      224      7      12         0\n## 9     IAH   DFW      224      8      22         0\n## 10    IAH   DFW      224      6      19         0\n## # ... with 227,486 more rows\n\n\n\n# Answer to last question: be concise!\nselect(hflights, 1:4, 12:21)\n\n\n\n\n## # A tibble: 227,496 \u00d7 14\n##     Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin  Dest\n## *  <int> <int>      <int>     <int>    <int>    <int>  <chr> <chr>\n## 1   2011     1          1         6      -10        0    IAH   DFW\n## 2   2011     1          2         7       -9        1    IAH   DFW\n## 3   2011     1          3         1       -8       -8    IAH   DFW\n## 4   2011     1          4         2        3        3    IAH   DFW\n## 5   2011     1          5         3       -3        5    IAH   DFW\n## 6   2011     1          6         4       -7       -1    IAH   DFW\n## 7   2011     1          7         5       -1       -1    IAH   DFW\n## 8   2011     1          8         6      -16       -5    IAH   DFW\n## 9   2011     1          9         7       44       43    IAH   DFW\n## 10  2011     1         10         1       43       43    IAH   DFW\n## # ... with 227,486 more rows, and 6 more variables: Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\nHelper functions for variable selection\n\u00b6\n\n\nselect\n:\n\n\n\n\nstarts_with(\"X\")\n; every name that starts with \n\"X\"\n,\n\n\nends_with(\"X\")\n; every name that ends with \n\"X\"\n,\n\n\ncontains(\"X\")\n; every name that contains \n\"X\"\n,\n\n\nmatches(\"X\")\n; every name that matches \n\"X\"\n, where \n\"X\"\n can be a\n\n    regular expression,\n\n\nnum_range(\"x\", 1:5)\n; the variables named \nx01\n, \nx02\n, \nx03\n, -\n\n\nx04\n and \nx05\n,\n\n\none_of(x)\n; every name that appears in \nx\n, which should be a\n\n    character vector.\n\n\n\n\n\n\n\n# Print out a tbl containing just ArrDelay and DepDelay\nselect(hflights, ArrDelay, DepDelay)\n\n\n\n\n## # A tibble: 227,496 \u00d7 2\n##    ArrDelay DepDelay\n## *     <int>    <int>\n## 1       -10        0\n## 2        -9        1\n## 3        -8       -8\n## 4         3        3\n## 5        -3        5\n## 6        -7       -1\n## 7        -1       -1\n## 8       -16       -5\n## 9        44       43\n## 10       43       43\n## # ... with 227,486 more rows\n\n\n\n# Print out a tbl as described in the second instruction, using both helper functions and variable names\nselect(hflights, UniqueCarrier, ends_with('Num'), starts_with('Cancel'))\n\n\n\n\n## # A tibble: 227,496 \u00d7 5\n##    UniqueCarrier FlightNum TailNum Cancelled CancellationCode\n## *          <chr>     <int>   <chr>     <int>            <chr>\n## 1       American       428  N576AA         0                 \n## 2       American       428  N557AA         0                 \n## 3       American       428  N541AA         0                 \n## 4       American       428  N403AA         0                 \n## 5       American       428  N492AA         0                 \n## 6       American       428  N262AA         0                 \n## 7       American       428  N493AA         0                 \n## 8       American       428  N477AA         0                 \n## 9       American       428  N476AA         0                 \n## 10      American       428  N504AA         0                 \n## # ... with 227,486 more rows\n\n\n\n# Print out a tbl as described in the third instruction, using only helper functions.\nselect(hflights, ends_with('Time'), ends_with('Delay'))\n\n\n\n\n## # A tibble: 227,496 \u00d7 6\n##    DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay\n## *    <int>   <int>             <int>   <int>    <int>    <int>\n## 1     1400    1500                60      40      -10        0\n## 2     1401    1501                60      45       -9        1\n## 3     1352    1502                70      48       -8       -8\n## 4     1403    1513                70      39        3        3\n## 5     1405    1507                62      44       -3        5\n## 6     1359    1503                64      45       -7       -1\n## 7     1359    1509                70      43       -1       -1\n## 8     1355    1454                59      40      -16       -5\n## 9     1443    1554                71      41       44       43\n## 10    1443    1553                70      45       43       43\n## # ... with 227,486 more rows\n\n\n\nComparison to basic R\n\u00b6\n\n\n# add\nex1r <- hflights[c('TaxiIn','TaxiOut','Distance')]\n\nex1d <- select(hflights, starts_with('Taxi'), Distance)\n\nex2r <- hflights[c('Year','Month','DayOfWeek','DepTime','ArrTime')]\n\nex2d <- select(hflights, Year, Month, DayOfWeek, DepTime, ArrTime)\n\nex3r <- hflights[c('TailNum','TaxiIn','TaxiOut')]\n\nex3d <- select(hflights, TailNum, starts_with('Taxi'))\n\n\n\n\nmutate\n is creating\n\u00b6\n\n\n# Add the new variable ActualGroundTime to a copy of hflights and save the result as g1\ng1 <- mutate(hflights, ActualGroundTime = ActualElapsedTime - AirTime)\nglimpse(hflights)\n\n\n\n\n## Observations: 227,496\n## Variables: 21\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\nglimpse(g1)\n\n\n\n\n## Observations: 227,496\n## Variables: 22\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ ActualGroundTime  <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n\n\n\n# Add the new variable GroundTime to a g1; save the result as g2\ng2 <- mutate(g1, GroundTime = TaxiIn + TaxiOut)\n\nhead(g1$ActualGroundTime == g2$GroundTime, 20)\n\n\n\n\n##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n## [15] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\n# Add the new variable AverageSpeed to g2; save the result as g3\ng3 <- mutate(g2, AverageSpeed = Distance / AirTime * 60)\ng3\n\n\n\n\n## # A tibble: 227,496 \u00d7 24\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 17 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>, ActualGroundTime <int>, GroundTime <int>,\n## #   AverageSpeed <dbl>\n\n\n\nAdd multiple variables using \nmutate\n\u00b6\n\n\n# Add a second variable loss_percent to the dataset: m1\nm1 <- mutate(hflights, loss = ArrDelay - DepDelay, loss_percent = (ArrDelay - DepDelay)/DepDelay*100)\n\n# Copy and adapt the previous command to reduce redendancy: m2\nm2 <- mutate(hflights, loss = ArrDelay - DepDelay, loss_percent = loss/DepDelay * 100)\n\n# Add the three variables as described in the third instruction: m3\nm3 <- mutate(hflights, TotalTaxi = TaxiIn + TaxiOut, ActualGroundTime = ActualElapsedTime - AirTime, Diff = TotalTaxi - ActualGroundTime)\nglimpse(m3)\n\n\n\n\n## Observations: 227,496\n## Variables: 24\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ TotalTaxi         <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ ActualGroundTime  <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ Diff              <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\n3, \nfilter\n and \narrange\n\u00b6\n\n\nLogical operators\n\u00b6\n\n\nfilter\n:\n\n\n\n\nx < y\n; \nTRUE\n if \nx\n is less than \ny\n.\n\n\nx <= y\n; \nTRUE\n if \nx\n is less than or equal to \ny\n.\n\n\nx == y\n; \nTRUE\n if \nx\n equals \ny\n.\n\n\nx != y\n; \nTRUE\n if \nx\n does not equal \ny\n.\n\n\nx >= y\n; \nTRUE\n if \nx\n is greater than or equal to \ny\n.\n\n\nx > y\n; \nTRUE\n if \nx\n is greater than \ny\n.\n\n\nx %in% c(a, b, c)\n; \nTRUE\n if \nx\n is in the vector \nc(a, b, c)\n.\n\n\n\n\n\n\n\n# All flights that traveled 3000 miles or more\nfilter(hflights, Distance >= 3000)\n\n\n\n\n## # A tibble: 527 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         31         1     924    1413   Continental\n## 2   2011     1         30         7     925    1410   Continental\n## 3   2011     1         29         6    1045    1445   Continental\n## 4   2011     1         28         5    1516    1916   Continental\n## 5   2011     1         27         4     950    1344   Continental\n## 6   2011     1         26         3     944    1350   Continental\n## 7   2011     1         25         2     924    1337   Continental\n## 8   2011     1         24         1    1144    1605   Continental\n## 9   2011     1         23         7     926    1335   Continental\n## 10  2011     1         22         6     942    1340   Continental\n## # ... with 517 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# All flights flown by one of JetBlue, Southwest, or Delta\nfilter(hflights, UniqueCarrier %in% c('JetBlue','Southwest','Delta'))\n\n\n\n\n## # A tibble: 48,679 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6     654    1124       JetBlue\n## 2   2011     1          1         6    1639    2110       JetBlue\n## 3   2011     1          2         7     703    1113       JetBlue\n## 4   2011     1          2         7    1604    2040       JetBlue\n## 5   2011     1          3         1     659    1100       JetBlue\n## 6   2011     1          3         1    1801    2200       JetBlue\n## 7   2011     1          4         2     654    1103       JetBlue\n## 8   2011     1          4         2    1608    2034       JetBlue\n## 9   2011     1          5         3     700    1103       JetBlue\n## 10  2011     1          5         3    1544    1954       JetBlue\n## # ... with 48,669 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# All flights where taxiing took longer than flying\nfilter(hflights, (TaxiIn + TaxiOut) > AirTime)\n\n\n\n\n## # A tibble: 1,389 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         24         1     731     904      American\n## 2   2011     1         30         7    1959    2132      American\n## 3   2011     1         24         1    1621    1749      American\n## 4   2011     1         10         1     941    1113      American\n## 5   2011     1         31         1    1301    1356   Continental\n## 6   2011     1         31         1    2113    2215   Continental\n## 7   2011     1         31         1    1434    1539   Continental\n## 8   2011     1         31         1     900    1006   Continental\n## 9   2011     1         30         7    1304    1408   Continental\n## 10  2011     1         30         7    2004    2128   Continental\n## # ... with 1,379 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\nCombining tests using boolean operators\n\u00b6\n\n\n# All flights that departed before 5am or arrived after 10pm\nfilter(hflights, DepTime < 500 | ArrTime > 2200)\n\n\n\n\n## # A tibble: 27,799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          4         2    2100    2207      American\n## 2   2011     1         14         5    2119    2229      American\n## 3   2011     1         10         1    1934    2235      American\n## 4   2011     1         26         3    1905    2211      American\n## 5   2011     1         30         7    1856    2209      American\n## 6   2011     1          9         7    1938    2228        Alaska\n## 7   2011     1         31         1    1919    2231   Continental\n## 8   2011     1         31         1    2116    2344   Continental\n## 9   2011     1         31         1    1850    2211   Continental\n## 10  2011     1         31         1    2102    2216   Continental\n## # ... with 27,789 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# All flights that departed late but arrived ahead of schedule\nfilter(hflights, DepDelay > 0 & ArrDelay < 0)\n\n\n\n\n## # A tibble: 27,712 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          2         7    1401    1501      American\n## 2   2011     1          5         3    1405    1507      American\n## 3   2011     1         18         2    1408    1508      American\n## 4   2011     1         18         2     721     827      American\n## 5   2011     1         12         3    2015    2113      American\n## 6   2011     1         13         4    2020    2116      American\n## 7   2011     1         26         3    2009    2103      American\n## 8   2011     1          1         6    1631    1736      American\n## 9   2011     1         10         1    1639    1740      American\n## 10  2011     1         12         3    1631    1739      American\n## # ... with 27,702 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# All cancelled weekend flights\nfilter(hflights, DayOfWeek %in% c(6,7) & Cancelled == 1)\n\n\n\n\n## # A tibble: 585 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>              <chr>\n## 1   2011     1          9         7      NA      NA           American\n## 2   2011     1         29         6      NA      NA        Continental\n## 3   2011     1          9         7      NA      NA        Continental\n## 4   2011     1          9         7      NA      NA              Delta\n## 5   2011     1          9         7      NA      NA            SkyWest\n## 6   2011     1          2         7      NA      NA          Southwest\n## 7   2011     1         29         6      NA      NA              Delta\n## 8   2011     1          9         7      NA      NA Atlantic_Southeast\n## 9   2011     1          1         6      NA      NA            AirTran\n## 10  2011     1          9         7      NA      NA            AirTran\n## # ... with 575 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# All flights that were cancelled after being delayed\nfilter(hflights, DepDelay > 0 & Cancelled == 1)\n\n\n\n\n## # A tibble: 40 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         26         3    1926      NA   Continental\n## 2   2011     1         11         2    1100      NA    US_Airways\n## 3   2011     1         19         3    1811      NA    ExpressJet\n## 4   2011     1          7         5    2028      NA    ExpressJet\n## 5   2011     2          4         5    1638      NA      American\n## 6   2011     2          8         2    1057      NA   Continental\n## 7   2011     2          2         3     802      NA    ExpressJet\n## 8   2011     2          9         3     904      NA    ExpressJet\n## 9   2011     2          1         2    1508      NA       SkyWest\n## 10  2011     3         31         4    1016      NA   Continental\n## # ... with 30 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\nBlend together\n\u00b6\n\n\n# Select the flights that had JFK as their destination: c1\nc1 <- filter(hflights, Dest == 'JFK')\n\n# Combine the Year, Month and DayofMonth variables to create a Date column: c2\nc2 <- mutate(c1, Date = paste(Year, Month, DayofMonth, sep = '-'))\n\n# Print out a selection of columns of c2\nselect(c2, Date, DepTime, ArrTime, TailNum)\n\n\n\n\n## # A tibble: 695 \u00d7 4\n##        Date DepTime ArrTime TailNum\n##       <chr>   <int>   <int>   <chr>\n## 1  2011-1-1     654    1124  N324JB\n## 2  2011-1-1    1639    2110  N324JB\n## 3  2011-1-2     703    1113  N324JB\n## 4  2011-1-2    1604    2040  N324JB\n## 5  2011-1-3     659    1100  N229JB\n## 6  2011-1-3    1801    2200  N206JB\n## 7  2011-1-4     654    1103  N267JB\n## 8  2011-1-4    1608    2034  N267JB\n## 9  2011-1-5     700    1103  N708JB\n## 10 2011-1-5    1544    1954  N644JB\n## # ... with 685 more rows\n\n\n\nArranging your data\n\u00b6\n\n\n# Definition of dtc\ndtc <- filter(hflights, Cancelled == 1, !is.na(DepDelay))\n\n# Arrange dtc by departure delays\narrange(dtc, DepDelay)\n\n\n\n\n## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011     7         23         6     605      NA       Frontier\n## 2   2011     1         17         1     916      NA     ExpressJet\n## 3   2011    12          1         4     541      NA     US_Airways\n## 4   2011    10         12         3    2022      NA American_Eagle\n## 5   2011     7         29         5    1424      NA    Continental\n## 6   2011     9         29         4    1639      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     5          9         1     715      NA        SkyWest\n## 9   2011     1         20         4    1413      NA         United\n## 10  2011     1         17         1     831      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# Arrange dtc so that cancellation reasons are grouped\narrange(dtc, CancellationCode)\n\n\n\n\n## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011     1         20         4    1413      NA         United\n## 2   2011     1          7         5    2028      NA     ExpressJet\n## 3   2011     2          4         5    1638      NA       American\n## 4   2011     2          8         2    1057      NA    Continental\n## 5   2011     2          1         2    1508      NA        SkyWest\n## 6   2011     2         21         1    2257      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     3         18         5     727      NA         United\n## 9   2011     4          4         1    1632      NA          Delta\n## 10  2011     4          8         5    1608      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# Arrange dtc according to carrier and departure delays\narrange(dtc, UniqueCarrier, DepDelay)\n\n\n\n\n## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>              <chr>\n## 1   2011     6         11         6    1649      NA            AirTran\n## 2   2011     8         18         4    1808      NA           American\n## 3   2011     2          4         5    1638      NA           American\n## 4   2011    10         12         3    2022      NA     American_Eagle\n## 5   2011     2          9         3     555      NA     American_Eagle\n## 6   2011     7         17         7    1917      NA     American_Eagle\n## 7   2011     4         30         6     612      NA Atlantic_Southeast\n## 8   2011     4         10         7    1147      NA Atlantic_Southeast\n## 9   2011     5         23         1     657      NA Atlantic_Southeast\n## 10  2011     9         29         4     723      NA Atlantic_Southeast\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\nReverse the order of arranging\n\u00b6\n\n\n# Arrange according to carrier and decreasing departure delays\narrange(hflights, UniqueCarrier, desc(DepDelay))\n\n\n\n\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     2         19         6    1902    2143       AirTran\n## 2   2011     3         14         1    2024    2309       AirTran\n## 3   2011     2         16         3    2349     227       AirTran\n## 4   2011    11         13         7    2312     213       AirTran\n## 5   2011     5         26         4    2353     305       AirTran\n## 6   2011     5         26         4    1922    2229       AirTran\n## 7   2011     4         28         4    1045    1328       AirTran\n## 8   2011     6          5         7    2207      52       AirTran\n## 9   2011     5          7         6    1009    1256       AirTran\n## 10  2011     7         25         1    2107      14       AirTran\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# Arrange flights by total delay (normal order).\narrange(hflights, (ArrDelay+DepDelay))\n\n\n\n\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     7          3         7    1914    2039    ExpressJet\n## 2   2011     8         31         3     934    1039       SkyWest\n## 3   2011     8         21         7     935    1039       SkyWest\n## 4   2011     8         28         7    2059    2206       SkyWest\n## 5   2011     8         29         1     935    1041       SkyWest\n## 6   2011    12         25         7     741     926       SkyWest\n## 7   2011     1         30         7     620     812       SkyWest\n## 8   2011     8          3         3    1741    1810    ExpressJet\n## 9   2011     8          4         4     930    1041       SkyWest\n## 10  2011     8         18         4     939    1043       SkyWest\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n# Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime \narrange(filter(hflights, Dest == 'DFW' & DepTime<800), desc(AirTime))\n\n\n\n\n## # A tibble: 799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011    11         22         2     635     825       American\n## 2   2011     8         25         4     602     758 American_Eagle\n## 3   2011    10         12         3     559     738 American_Eagle\n## 4   2011     5          2         1     716     854       American\n## 5   2011     4          4         1     741     949       American\n## 6   2011     4          4         1     627     742 American_Eagle\n## 7   2011     6         21         2     726     848     ExpressJet\n## 8   2011     9          1         4     715     844       American\n## 9   2011     3         14         1     729     917    Continental\n## 10  2011    12          5         1     724     847    Continental\n## # ... with 789 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n4, \nsummarise\n and the Pipe Operator\n\u00b6\n\n\nThe syntax of \nsummarise\n\u00b6\n\n\n# Print out a summary with variables min_dist and max_dist\nsummarise(hflights, min_dist = min(Distance), max_dist = max(Distance))\n\n\n\n\n## # A tibble: 1 \u00d7 2\n##   min_dist max_dist\n##      <int>    <int>\n## 1       79     3904\n\n\n\n# Print out a summary with variable max_div\nsummarise(filter(hflights, Diverted == 1), max_div = max(Distance))\n\n\n\n\n## # A tibble: 1 \u00d7 1\n##   max_div\n##     <int>\n## 1    3904\n\n\n\nAggregate functions\n\u00b6\n\n\nsummarise\n:\n\n\n\n\nmin(x)\n; minimum value of vector x.\n\n\nmax(x)\n; maximum value of vector x.\n\n\nmean(x)\n; mean value of vector x.\n\n\nmedian(x)\n; median value of vector x.\n\n\nquantile(x, p)\n; pth quantile of vector x.\n\n\nsd(x)\n; standard deviation of vector x.\n\n\nvar(x)\n; variance of vector x.\n\n\nIQR(x)\n; Inter Quartile Range (IQR) of vector x.\n\n\ndiff(range(x))\n; total range of vector x.\n\n\n\n\n\n\n\n# Remove rows that have NA ArrDelay: temp1\ntemp1 <- filter(hflights, !is.na(ArrDelay))\n\n# Generate summary about ArrDelay column of temp1\nsummarise(temp1, earliest = min(ArrDelay), average = mean(ArrDelay), latest = max(ArrDelay), sd = sd(ArrDelay))\n\n\n\n\n## # A tibble: 1 \u00d7 4\n##   earliest  average latest       sd\n##      <int>    <dbl>  <int>    <dbl>\n## 1      -70 7.094334    978 30.70852\n\n\n\n# Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2\ntemp2 <- filter(hflights, !is.na(TaxiIn), !is.na(TaxiOut))\n\n# Print the maximum taxiing difference of temp2 with summarise()\nsummarise(temp2, max_taxi_diff = max(abs(TaxiIn - TaxiOut)))\n\n\n\n\n## # A tibble: 1 \u00d7 1\n##   max_taxi_diff\n##           <int>\n## 1           160\n\n\n\ndplyr\n aggregate functions\n\u00b6\n\n\n\n\nfirst(x)\n; the first element of vector \nx\n.\n\n\nlast(x)\n; the last element of vector \nx\n.\n\n\nnth(x, n)\n; The \nn\nth element of vector \nx\n.\n\n\nn()\n; The number of rows in the data.frame or group of observations\n\n    that \nsummarise()\n describes.\n\n\nn_distinct(x)\n; The number of unique values in vector \nx\n.\n\n\n\n\n\n\n\n# Generate summarizing statistics for hflights\nsummarise(hflights, n_obs = n(), n_carrier = n_distinct(UniqueCarrier), n_dest = n_distinct(Dest), dest100 = nth(Dest, 100))\n\n\n\n\n## # A tibble: 1 \u00d7 4\n##    n_obs n_carrier n_dest dest100\n##    <int>     <int>  <int>   <chr>\n## 1 227496        15    116     DFW\n\n\n\n# Filter hflights to keep all American Airline flights: aa\naa <- filter(hflights, UniqueCarrier == 'American')\n\n# Generate summarizing statistics for aa \nsummarise(aa, n_flights = n(), n_canc = sum(Cancelled), p_canc = n_canc/n_flights*100, avg_delay = mean(ArrDelay, na.rm = TRUE))\n\n\n\n\n## # A tibble: 1 \u00d7 4\n##   n_flights n_canc   p_canc avg_delay\n##       <int>  <int>    <dbl>     <dbl>\n## 1      3244     60 1.849568 0.8917558\n\n\n\nOverview of syntax\n\n\n# Write the 'piped' version of the English sentences\nhflights %>%\n    mutate(diff = TaxiOut - TaxiIn) %>%\n    filter(!is.na(diff)) %>%\n    summarise( avg = mean(diff))\n\n\n\n\n## # A tibble: 1 \u00d7 1\n##        avg\n##      <dbl>\n## 1 8.992064\n\n\n\nDrive or fly? Part 1 of 2\n\n\n# Part 1, concerning the selection and creation of columns\nd <- hflights %>%\n  select(Dest, UniqueCarrier, Distance, ActualElapsedTime) %>%  \n  mutate(RealTime = ActualElapsedTime + 100, mph = Distance / RealTime * 60)\n\n# Part 2, concerning flights that had an actual average speed of < 70 mph.\nd %>%\n  filter(!is.na(mph), mph < 70) %>%\n  summarise( n_less = n(), \n             n_dest = n_distinct(Dest), \n             min_dist = min(Distance), \n             max_dist = max(Distance))\n\n\n\n\n## # A tibble: 1 \u00d7 4\n##   n_less n_dest min_dist max_dist\n##    <int>  <int>    <int>    <int>\n## 1   6726     13       79      305\n\n\n\nDrive or fly? Part 2 of 2\n\n\n# Solve the exercise using a combination of dplyr verbs and %>%\nhflights %>%\n    #summarise(all_flights = n()) %>%\n    filter(((Distance / (ActualElapsedTime + 100) * 60) < 105) | Cancelled == 1 | Diverted == 1) %>%\n    summarise(n_non = n(), p_non = n_non / 22751 *100, n_dest = n_distinct(Dest), min_dist = min(Distance), max_dist = max(Distance))\n\n\n\n\n## # A tibble: 1 \u00d7 5\n##   n_non    p_non n_dest min_dist max_dist\n##   <int>    <dbl>  <int>    <int>    <int>\n## 1 42400 186.3654    113       79     3904\n\n\n\nAdvanced piping exercise\n\n\n# Count the number of overnight flights\nhflights %>%\n    filter(ArrTime < DepTime & !is.na(DepTime) & !is.na(ArrTime)) %>%\n    summarise(n = n())\n\n\n\n\n## # A tibble: 1 \u00d7 1\n##       n\n##   <int>\n## 1  2718\n\n\n\n5, \ngroup_by\n and working with data\n\u00b6\n\n\nUnite and conquer using \ngroup_by\n\u00b6\n\n\n# Make an ordered per-carrier summary of hflights\nhflights %>%\n   group_by(UniqueCarrier) %>%\n   summarise(n_flights = n(), \n             n_canc = sum(Cancelled == 1), \n             p_canc = mean(Cancelled == 1) * 100, \n             avg_delay = mean(ArrDelay, na.rm = TRUE)) %>%\n   arrange(avg_delay, p_canc)\n\n\n\n\n## # A tibble: 15 \u00d7 5\n##         UniqueCarrier n_flights n_canc    p_canc  avg_delay\n##                 <chr>     <int>  <int>     <dbl>      <dbl>\n## 1          US_Airways      4082     46 1.1268986 -0.6307692\n## 2            American      3244     60 1.8495684  0.8917558\n## 3             AirTran      2139     21 0.9817672  1.8536239\n## 4              Alaska       365      0 0.0000000  3.1923077\n## 5                Mesa        79      1 1.2658228  4.0128205\n## 6               Delta      2641     42 1.5903067  6.0841374\n## 7         Continental     70032    475 0.6782614  6.0986983\n## 8      American_Eagle      4648    135 2.9044750  7.1529751\n## 9  Atlantic_Southeast      2204     76 3.4482759  7.2569543\n## 10          Southwest     45343    703 1.5504047  7.5871430\n## 11           Frontier       838      6 0.7159905  7.6682692\n## 12         ExpressJet     73053   1132 1.5495599  8.1865242\n## 13            SkyWest     16061    224 1.3946828  8.6934922\n## 14            JetBlue       695     18 2.5899281  9.8588410\n## 15             United      2072     34 1.6409266 10.4628628\n\n\n\n# Make an ordered per-day summary of hflights\nhflights %>% \n   group_by(DayOfWeek) %>%\n   summarise(avg_taxi = mean(TaxiIn + TaxiOut, na.rm=TRUE)) %>%\n   arrange(desc(avg_taxi))\n\n\n\n\n## # A tibble: 7 \u00d7 2\n##   DayOfWeek avg_taxi\n##       <int>    <dbl>\n## 1         1 21.77027\n## 2         2 21.43505\n## 3         4 21.26076\n## 4         3 21.19055\n## 5         5 21.15805\n## 6         7 20.93726\n## 7         6 20.43061\n\n\n\nCombine \ngroup_by\n with \nmutate\n\u00b6\n\n\n# Solution to first instruction\nhflights %>%\n    filter(!is.na(ArrDelay)) %>%\n    group_by(UniqueCarrier) %>%\n    summarise(p_delay = sum(ArrDelay > 0) / n()) %>%\n    mutate(rank = rank(p_delay)) %>%\n    arrange(rank)\n\n\n\n\n## # A tibble: 15 \u00d7 3\n##         UniqueCarrier   p_delay  rank\n##                 <chr>     <dbl> <dbl>\n## 1            American 0.3030208     1\n## 2             AirTran 0.3112269     2\n## 3          US_Airways 0.3267990     3\n## 4  Atlantic_Southeast 0.3677511     4\n## 5      American_Eagle 0.3696714     5\n## 6               Delta 0.3871092     6\n## 7             JetBlue 0.3952452     7\n## 8              Alaska 0.4368132     8\n## 9           Southwest 0.4644557     9\n## 10               Mesa 0.4743590    10\n## 11        Continental 0.4907385    11\n## 12         ExpressJet 0.4943420    12\n## 13             United 0.4963109    13\n## 14            SkyWest 0.5350105    14\n## 15           Frontier 0.5564904    15\n\n\n\n# Solution to second instruction\nhflights %>%\n    filter(!is.na(ArrDelay), ArrDelay > 0) %>%\n    group_by(UniqueCarrier) %>%\n    summarise(avg = mean(ArrDelay)) %>%\n    mutate(rank = rank(avg)) %>%\n    arrange(rank)\n\n\n\n\n## # A tibble: 15 \u00d7 3\n##         UniqueCarrier      avg  rank\n##                 <chr>    <dbl> <dbl>\n## 1                Mesa 18.67568     1\n## 2            Frontier 18.68683     2\n## 3          US_Airways 20.70235     3\n## 4         Continental 22.13374     4\n## 5              Alaska 22.91195     5\n## 6             SkyWest 24.14663     6\n## 7          ExpressJet 24.19337     7\n## 8           Southwest 25.27750     8\n## 9             AirTran 27.85693     9\n## 10           American 28.49740    10\n## 11              Delta 32.12463    11\n## 12             United 32.48067    12\n## 13     American_Eagle 38.75135    13\n## 14 Atlantic_Southeast 40.24231    14\n## 15            JetBlue 45.47744    15\n\n\n\nAdvanced \ngroup_by\n\u00b6\n\n\n# Which plane (by tail number) flew out of Houston the most times? How many times? adv1\nadv1 <- hflights %>%\n          group_by(TailNum) %>%\n          summarise(n = n()) %>%\n          filter(n == max(n))\n\n# How many airplanes only flew to one destination from Houston? adv2\nadv2 <- hflights %>%\n          group_by(TailNum) %>%\n          summarise(ndest = n_distinct(Dest)) %>%\n          filter(ndest == 1) %>%\n          summarise(nplanes = n())\n\n# Find the most visited destination for each carrier: adv3\nadv3 <- hflights %>% \n          group_by(UniqueCarrier, Dest) %>%\n          summarise(n = n()) %>%\n          mutate(rank = rank(desc(n))) %>%\n          filter(rank == 1)\n\n# Find the carrier that travels to each destination the most: adv4\nadv4 <- hflights %>% \n          group_by(Dest, UniqueCarrier) %>%\n          summarise(n = n()) %>%\n          mutate(rank = rank(desc(n))) %>%\n          filter(rank == 1)\n\n\n\n\ndplyr\n deals with different types\n\u00b6\n\n\n# Use summarise to calculate n_carrier\ns2 <- hflights %>%\n    summarise(n_carrier = n_distinct(UniqueCarrier))\n\n\n\n\ndplyr\n and mySQL databases\n\u00b6\n\n\nCode only.\n\n\n# set up a src that connects to the mysql database (src_mysql is provided by dplyr)\nmy_db <- src_mysql(dbname = 'dplyr', \n                  host = 'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                  port = 3306,\n                  user = 'dplyr',\n                  password = 'dplyr')\n\n# and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table\nnycflights <- tbl(my_db, 'dplyr')\n\n# glimpse at nycflights\nglimpse(nycflights)\n\n# Calculate the grouped summaries detailed in the instructions\nnycflights %>%\n   group_by(carrier) %>%\n   summarise(n_flights = n(), avg_delay = mean(arr_delay)) %>%\n   arrange(avg_delay)\n\n\n\n\nAdding \ntidyr\n Functions\n\u00b6\n\n\n\n\ncomplete\n.\n\n\ndrop_na\n.\n\n\nexpand\n.\n\n\nextract\n.\n\n\nextract_numeric\n.\n\n\ncomplete\n.\n\n\nfill\n.\n\n\nfull_seq\n.\n\n\ngather\n.\n\n\nnest\n.\n\n\nreplace_na\n.\n\n\nseparate\n.\n\n\nseparate_rows\n.\n\n\nseparate_rows_\n.\n\n\nsmiths\n.\n\n\nspread\n.\n\n\ntable1\n.\n\n\nunite\n.\n\n\nunnest\n.\n\n\nwho\n.\n\n\n\n\n\n\nJoining Data in R with \ndplyr\n\u00b6\n\n\nIn development\n\n\n## 1, Mutating Joins\n\n## 2, Filtering Joins and Set Operations\n\n## 3, Assembling Data\n\n## 4, Advanced Joining\n\n## 5, Case Study",
            "title": "Data Manipulation in R with dplyr"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#1-introduction-to-dplyr",
            "text": "",
            "title": "1, Introduction to dplyr"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#load-the-dplyr-and-hflights-package",
            "text": "# Load the dplyr package\nlibrary(dplyr)\nlibrary(dtplyr)\n\n# Load the hflights package\n# A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011\nlibrary(hflights)\n\n# Call both head() and summary() on hflights\nhead(hflights)  ##      Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## 5424 2011     1          1         6    1400    1500            AA\n## 5425 2011     1          2         7    1401    1501            AA\n## 5426 2011     1          3         1    1352    1502            AA\n## 5427 2011     1          4         2    1403    1513            AA\n## 5428 2011     1          5         3    1405    1507            AA\n## 5429 2011     1          6         4    1359    1503            AA\n##      FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin\n## 5424       428  N576AA                60      40      -10        0    IAH\n## 5425       428  N557AA                60      45       -9        1    IAH\n## 5426       428  N541AA                70      48       -8       -8    IAH\n## 5427       428  N403AA                70      39        3        3    IAH\n## 5428       428  N492AA                62      44       -3        5    IAH\n## 5429       428  N262AA                64      45       -7       -1    IAH\n##      Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted\n## 5424  DFW      224      7      13         0                         0\n## 5425  DFW      224      6       9         0                         0\n## 5426  DFW      224      5      17         0                         0\n## 5427  DFW      224      9      22         0                         0\n## 5428  DFW      224      9       9         0                         0\n## 5429  DFW      224      6      13         0                         0  summary(hflights)  ##       Year          Month          DayofMonth      DayOfWeek    \n##  Min.   :2011   Min.   : 1.000   Min.   : 1.00   Min.   :1.000  \n##  1st Qu.:2011   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:2.000  \n##  Median :2011   Median : 7.000   Median :16.00   Median :4.000  \n##  Mean   :2011   Mean   : 6.514   Mean   :15.74   Mean   :3.948  \n##  3rd Qu.:2011   3rd Qu.: 9.000   3rd Qu.:23.00   3rd Qu.:6.000  \n##  Max.   :2011   Max.   :12.000   Max.   :31.00   Max.   :7.000  \n##                                                                 \n##     DepTime        ArrTime     UniqueCarrier        FlightNum   \n##  Min.   :   1   Min.   :   1   Length:227496      Min.   :   1  \n##  1st Qu.:1021   1st Qu.:1215   Class :character   1st Qu.: 855  \n##  Median :1416   Median :1617   Mode  :character   Median :1696  \n##  Mean   :1396   Mean   :1578                      Mean   :1962  \n##  3rd Qu.:1801   3rd Qu.:1953                      3rd Qu.:2755  \n##  Max.   :2400   Max.   :2400                      Max.   :7290  \n##  NA's   :2905   NA's   :3066                                    \n##    TailNum          ActualElapsedTime    AirTime         ArrDelay      \n##  Length:227496      Min.   : 34.0     Min.   : 11.0   Min.   :-70.000  \n##  Class :character   1st Qu.: 77.0     1st Qu.: 58.0   1st Qu.: -8.000  \n##  Mode  :character   Median :128.0     Median :107.0   Median :  0.000  \n##                     Mean   :129.3     Mean   :108.1   Mean   :  7.094  \n##                     3rd Qu.:165.0     3rd Qu.:141.0   3rd Qu.: 11.000  \n##                     Max.   :575.0     Max.   :549.0   Max.   :978.000  \n##                     NA's   :3622      NA's   :3622    NA's   :3622     \n##     DepDelay          Origin              Dest              Distance     \n##  Min.   :-33.000   Length:227496      Length:227496      Min.   :  79.0  \n##  1st Qu.: -3.000   Class :character   Class :character   1st Qu.: 376.0  \n##  Median :  0.000   Mode  :character   Mode  :character   Median : 809.0  \n##  Mean   :  9.445                                         Mean   : 787.8  \n##  3rd Qu.:  9.000                                         3rd Qu.:1042.0  \n##  Max.   :981.000                                         Max.   :3904.0  \n##  NA's   :2905                                                            \n##      TaxiIn           TaxiOut         Cancelled       CancellationCode  \n##  Min.   :  1.000   Min.   :  1.00   Min.   :0.00000   Length:227496     \n##  1st Qu.:  4.000   1st Qu.: 10.00   1st Qu.:0.00000   Class :character  \n##  Median :  5.000   Median : 14.00   Median :0.00000   Mode  :character  \n##  Mean   :  6.099   Mean   : 15.09   Mean   :0.01307                     \n##  3rd Qu.:  7.000   3rd Qu.: 18.00   3rd Qu.:0.00000                     \n##  Max.   :165.000   Max.   :163.00   Max.   :1.00000                     \n##  NA's   :3066      NA's   :2947                                         \n##     Diverted       \n##  Min.   :0.000000  \n##  1st Qu.:0.000000  \n##  Median :0.000000  \n##  Mean   :0.002853  \n##  3rd Qu.:0.000000  \n##  Max.   :1.000000  \n##",
            "title": "Load the dplyr and hflights package"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#convert-dataframe-to-table",
            "text": "# Convert the hflights data.frame into a hflights tbl\nhflights <- tbl_df(hflights)\n\n# Display the hflights tbl\nhflights  ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500            AA\n## 2   2011     1          2         7    1401    1501            AA\n## 3   2011     1          3         1    1352    1502            AA\n## 4   2011     1          4         2    1403    1513            AA\n## 5   2011     1          5         3    1405    1507            AA\n## 6   2011     1          6         4    1359    1503            AA\n## 7   2011     1          7         5    1359    1509            AA\n## 8   2011     1          8         6    1355    1454            AA\n## 9   2011     1          9         7    1443    1554            AA\n## 10  2011     1         10         1    1443    1553            AA\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # Create the object carriers, containing only the UniqueCarrier variable of hflights\ncarriers <- hflights$UniqueCarrier",
            "title": "Convert data.frame to table"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#changing-labels-of-hflights",
            "text": "part 1 of 2  # add\nlut <- c('AA' = 'American', 'AS' = 'Alaska', 'B6' = 'JetBlue', 'CO' = 'Continental', \n         'DL' = 'Delta', 'OO' = 'SkyWest', 'UA' = 'United', 'US' = 'US_Airways', \n         'WN' = 'Southwest', 'EV' = 'Atlantic_Southeast', 'F9' = 'Frontier', \n         'FL' = 'AirTran', 'MQ' = 'American_Eagle', 'XE' = 'ExpressJet', 'YV' = 'Mesa')\n\n# Use lut to translate the UniqueCarrier column of hflights\nhflights$UniqueCarrier <- lut[hflights$UniqueCarrier]\n\n# Inspect the resulting raw values of your variables\nglimpse(hflights)  ## Observations: 227,496\n## Variables: 21\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  part 2 of 2  # Build the lookup table: lut\nlut <- c(\"A\" = \"carrier\", \"B\" = \"weather\" ,\"C\" = \"FFA\" ,\"D\" = \"security\", \"E\" = \"not cancelled\")\n\n# Add the Code column\nhflights$Code <- lut[hflights$CancellationCode]\n\n# Glimpse at hflights\nglimpse(hflights)  Result.  Observations: 227,496\nVariables: 22\n$ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011,...\n$ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n$ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...\n$ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,...\n$ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359,...\n$ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503,...\n$ UniqueCarrier     <chr> \"American\", \"American\", \"American\",...\n$ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, ...\n$ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403...\n$ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71,...\n$ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41,...\n$ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44...\n$ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43,...\n$ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", ...\n$ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", ...\n$ Distance          <int> 224, 224, 224, 224, 224, 224, 224, ...\n$ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4...\n$ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 1...\n$ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",...\n$ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ Code              <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,...",
            "title": "Changing labels of hflights"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#2-select-and-mutate",
            "text": "",
            "title": "2, select and mutate"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#the-five-verbs-and-their-meaning",
            "text": "select ; which returns a subset of the columns.  filter ; that is able to return a subset of the rows.  arrange ; that reorders the rows according to single or \n    multiple variables.  mutate ; used to add columns from existing data.  summarise ; which reduces each group to a single row by calculating \n    aggregate measures.",
            "title": "The five verbs and their meaning"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#the-select-verb",
            "text": "# Print out a tbl with the four columns of hflights related to delay\nselect(hflights, ActualElapsedTime, AirTime, ArrDelay, DepDelay)  ## # A tibble: 227,496 \u00d7 4\n##    ActualElapsedTime AirTime ArrDelay DepDelay\n## *              <int>   <int>    <int>    <int>\n## 1                 60      40      -10        0\n## 2                 60      45       -9        1\n## 3                 70      48       -8       -8\n## 4                 70      39        3        3\n## 5                 62      44       -3        5\n## 6                 64      45       -7       -1\n## 7                 70      43       -1       -1\n## 8                 59      40      -16       -5\n## 9                 71      41       44       43\n## 10                70      45       43       43\n## # ... with 227,486 more rows  # Print out hflights, nothing has changed!\nhflights  ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # Print out the columns Origin up to Cancelled of hflights\nselect(hflights, 14:19)  ## # A tibble: 227,496 \u00d7 6\n##    Origin  Dest Distance TaxiIn TaxiOut Cancelled\n## *   <chr> <chr>    <int>  <int>   <int>     <int>\n## 1     IAH   DFW      224      7      13         0\n## 2     IAH   DFW      224      6       9         0\n## 3     IAH   DFW      224      5      17         0\n## 4     IAH   DFW      224      9      22         0\n## 5     IAH   DFW      224      9       9         0\n## 6     IAH   DFW      224      6      13         0\n## 7     IAH   DFW      224     12      15         0\n## 8     IAH   DFW      224      7      12         0\n## 9     IAH   DFW      224      8      22         0\n## 10    IAH   DFW      224      6      19         0\n## # ... with 227,486 more rows  # Answer to last question: be concise!\nselect(hflights, 1:4, 12:21)  ## # A tibble: 227,496 \u00d7 14\n##     Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin  Dest\n## *  <int> <int>      <int>     <int>    <int>    <int>  <chr> <chr>\n## 1   2011     1          1         6      -10        0    IAH   DFW\n## 2   2011     1          2         7       -9        1    IAH   DFW\n## 3   2011     1          3         1       -8       -8    IAH   DFW\n## 4   2011     1          4         2        3        3    IAH   DFW\n## 5   2011     1          5         3       -3        5    IAH   DFW\n## 6   2011     1          6         4       -7       -1    IAH   DFW\n## 7   2011     1          7         5       -1       -1    IAH   DFW\n## 8   2011     1          8         6      -16       -5    IAH   DFW\n## 9   2011     1          9         7       44       43    IAH   DFW\n## 10  2011     1         10         1       43       43    IAH   DFW\n## # ... with 227,486 more rows, and 6 more variables: Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "The select verb"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#helper-functions-for-variable-selection",
            "text": "select :   starts_with(\"X\") ; every name that starts with  \"X\" ,  ends_with(\"X\") ; every name that ends with  \"X\" ,  contains(\"X\") ; every name that contains  \"X\" ,  matches(\"X\") ; every name that matches  \"X\" , where  \"X\"  can be a \n    regular expression,  num_range(\"x\", 1:5) ; the variables named  x01 ,  x02 ,  x03 , -  x04  and  x05 ,  one_of(x) ; every name that appears in  x , which should be a \n    character vector.    # Print out a tbl containing just ArrDelay and DepDelay\nselect(hflights, ArrDelay, DepDelay)  ## # A tibble: 227,496 \u00d7 2\n##    ArrDelay DepDelay\n## *     <int>    <int>\n## 1       -10        0\n## 2        -9        1\n## 3        -8       -8\n## 4         3        3\n## 5        -3        5\n## 6        -7       -1\n## 7        -1       -1\n## 8       -16       -5\n## 9        44       43\n## 10       43       43\n## # ... with 227,486 more rows  # Print out a tbl as described in the second instruction, using both helper functions and variable names\nselect(hflights, UniqueCarrier, ends_with('Num'), starts_with('Cancel'))  ## # A tibble: 227,496 \u00d7 5\n##    UniqueCarrier FlightNum TailNum Cancelled CancellationCode\n## *          <chr>     <int>   <chr>     <int>            <chr>\n## 1       American       428  N576AA         0                 \n## 2       American       428  N557AA         0                 \n## 3       American       428  N541AA         0                 \n## 4       American       428  N403AA         0                 \n## 5       American       428  N492AA         0                 \n## 6       American       428  N262AA         0                 \n## 7       American       428  N493AA         0                 \n## 8       American       428  N477AA         0                 \n## 9       American       428  N476AA         0                 \n## 10      American       428  N504AA         0                 \n## # ... with 227,486 more rows  # Print out a tbl as described in the third instruction, using only helper functions.\nselect(hflights, ends_with('Time'), ends_with('Delay'))  ## # A tibble: 227,496 \u00d7 6\n##    DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay\n## *    <int>   <int>             <int>   <int>    <int>    <int>\n## 1     1400    1500                60      40      -10        0\n## 2     1401    1501                60      45       -9        1\n## 3     1352    1502                70      48       -8       -8\n## 4     1403    1513                70      39        3        3\n## 5     1405    1507                62      44       -3        5\n## 6     1359    1503                64      45       -7       -1\n## 7     1359    1509                70      43       -1       -1\n## 8     1355    1454                59      40      -16       -5\n## 9     1443    1554                71      41       44       43\n## 10    1443    1553                70      45       43       43\n## # ... with 227,486 more rows",
            "title": "Helper functions for variable selection"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#comparison-to-basic-r",
            "text": "# add\nex1r <- hflights[c('TaxiIn','TaxiOut','Distance')]\n\nex1d <- select(hflights, starts_with('Taxi'), Distance)\n\nex2r <- hflights[c('Year','Month','DayOfWeek','DepTime','ArrTime')]\n\nex2d <- select(hflights, Year, Month, DayOfWeek, DepTime, ArrTime)\n\nex3r <- hflights[c('TailNum','TaxiIn','TaxiOut')]\n\nex3d <- select(hflights, TailNum, starts_with('Taxi'))",
            "title": "Comparison to basic R"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#mutate-is-creating",
            "text": "# Add the new variable ActualGroundTime to a copy of hflights and save the result as g1\ng1 <- mutate(hflights, ActualGroundTime = ActualElapsedTime - AirTime)\nglimpse(hflights)  ## Observations: 227,496\n## Variables: 21\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  glimpse(g1)  ## Observations: 227,496\n## Variables: 22\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ ActualGroundTime  <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...  # Add the new variable GroundTime to a g1; save the result as g2\ng2 <- mutate(g1, GroundTime = TaxiIn + TaxiOut)\n\nhead(g1$ActualGroundTime == g2$GroundTime, 20)  ##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n## [15] TRUE TRUE TRUE TRUE TRUE TRUE  # Add the new variable AverageSpeed to g2; save the result as g3\ng3 <- mutate(g2, AverageSpeed = Distance / AirTime * 60)\ng3  ## # A tibble: 227,496 \u00d7 24\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 17 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>, ActualGroundTime <int>, GroundTime <int>,\n## #   AverageSpeed <dbl>",
            "title": "mutate is creating"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#add-multiple-variables-using-mutate",
            "text": "# Add a second variable loss_percent to the dataset: m1\nm1 <- mutate(hflights, loss = ArrDelay - DepDelay, loss_percent = (ArrDelay - DepDelay)/DepDelay*100)\n\n# Copy and adapt the previous command to reduce redendancy: m2\nm2 <- mutate(hflights, loss = ArrDelay - DepDelay, loss_percent = loss/DepDelay * 100)\n\n# Add the three variables as described in the third instruction: m3\nm3 <- mutate(hflights, TotalTaxi = TaxiIn + TaxiOut, ActualGroundTime = ActualElapsedTime - AirTime, Diff = TotalTaxi - ActualGroundTime)\nglimpse(m3)  ## Observations: 227,496\n## Variables: 24\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ TotalTaxi         <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ ActualGroundTime  <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ Diff              <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...",
            "title": "Add multiple variables using mutate"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#3-filter-and-arrange",
            "text": "",
            "title": "3, filter and arrange"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#logical-operators",
            "text": "filter :   x < y ;  TRUE  if  x  is less than  y .  x <= y ;  TRUE  if  x  is less than or equal to  y .  x == y ;  TRUE  if  x  equals  y .  x != y ;  TRUE  if  x  does not equal  y .  x >= y ;  TRUE  if  x  is greater than or equal to  y .  x > y ;  TRUE  if  x  is greater than  y .  x %in% c(a, b, c) ;  TRUE  if  x  is in the vector  c(a, b, c) .    # All flights that traveled 3000 miles or more\nfilter(hflights, Distance >= 3000)  ## # A tibble: 527 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         31         1     924    1413   Continental\n## 2   2011     1         30         7     925    1410   Continental\n## 3   2011     1         29         6    1045    1445   Continental\n## 4   2011     1         28         5    1516    1916   Continental\n## 5   2011     1         27         4     950    1344   Continental\n## 6   2011     1         26         3     944    1350   Continental\n## 7   2011     1         25         2     924    1337   Continental\n## 8   2011     1         24         1    1144    1605   Continental\n## 9   2011     1         23         7     926    1335   Continental\n## 10  2011     1         22         6     942    1340   Continental\n## # ... with 517 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # All flights flown by one of JetBlue, Southwest, or Delta\nfilter(hflights, UniqueCarrier %in% c('JetBlue','Southwest','Delta'))  ## # A tibble: 48,679 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6     654    1124       JetBlue\n## 2   2011     1          1         6    1639    2110       JetBlue\n## 3   2011     1          2         7     703    1113       JetBlue\n## 4   2011     1          2         7    1604    2040       JetBlue\n## 5   2011     1          3         1     659    1100       JetBlue\n## 6   2011     1          3         1    1801    2200       JetBlue\n## 7   2011     1          4         2     654    1103       JetBlue\n## 8   2011     1          4         2    1608    2034       JetBlue\n## 9   2011     1          5         3     700    1103       JetBlue\n## 10  2011     1          5         3    1544    1954       JetBlue\n## # ... with 48,669 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # All flights where taxiing took longer than flying\nfilter(hflights, (TaxiIn + TaxiOut) > AirTime)  ## # A tibble: 1,389 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         24         1     731     904      American\n## 2   2011     1         30         7    1959    2132      American\n## 3   2011     1         24         1    1621    1749      American\n## 4   2011     1         10         1     941    1113      American\n## 5   2011     1         31         1    1301    1356   Continental\n## 6   2011     1         31         1    2113    2215   Continental\n## 7   2011     1         31         1    1434    1539   Continental\n## 8   2011     1         31         1     900    1006   Continental\n## 9   2011     1         30         7    1304    1408   Continental\n## 10  2011     1         30         7    2004    2128   Continental\n## # ... with 1,379 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "Logical operators"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#combining-tests-using-boolean-operators",
            "text": "# All flights that departed before 5am or arrived after 10pm\nfilter(hflights, DepTime < 500 | ArrTime > 2200)  ## # A tibble: 27,799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          4         2    2100    2207      American\n## 2   2011     1         14         5    2119    2229      American\n## 3   2011     1         10         1    1934    2235      American\n## 4   2011     1         26         3    1905    2211      American\n## 5   2011     1         30         7    1856    2209      American\n## 6   2011     1          9         7    1938    2228        Alaska\n## 7   2011     1         31         1    1919    2231   Continental\n## 8   2011     1         31         1    2116    2344   Continental\n## 9   2011     1         31         1    1850    2211   Continental\n## 10  2011     1         31         1    2102    2216   Continental\n## # ... with 27,789 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # All flights that departed late but arrived ahead of schedule\nfilter(hflights, DepDelay > 0 & ArrDelay < 0)  ## # A tibble: 27,712 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          2         7    1401    1501      American\n## 2   2011     1          5         3    1405    1507      American\n## 3   2011     1         18         2    1408    1508      American\n## 4   2011     1         18         2     721     827      American\n## 5   2011     1         12         3    2015    2113      American\n## 6   2011     1         13         4    2020    2116      American\n## 7   2011     1         26         3    2009    2103      American\n## 8   2011     1          1         6    1631    1736      American\n## 9   2011     1         10         1    1639    1740      American\n## 10  2011     1         12         3    1631    1739      American\n## # ... with 27,702 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # All cancelled weekend flights\nfilter(hflights, DayOfWeek %in% c(6,7) & Cancelled == 1)  ## # A tibble: 585 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>              <chr>\n## 1   2011     1          9         7      NA      NA           American\n## 2   2011     1         29         6      NA      NA        Continental\n## 3   2011     1          9         7      NA      NA        Continental\n## 4   2011     1          9         7      NA      NA              Delta\n## 5   2011     1          9         7      NA      NA            SkyWest\n## 6   2011     1          2         7      NA      NA          Southwest\n## 7   2011     1         29         6      NA      NA              Delta\n## 8   2011     1          9         7      NA      NA Atlantic_Southeast\n## 9   2011     1          1         6      NA      NA            AirTran\n## 10  2011     1          9         7      NA      NA            AirTran\n## # ... with 575 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # All flights that were cancelled after being delayed\nfilter(hflights, DepDelay > 0 & Cancelled == 1)  ## # A tibble: 40 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         26         3    1926      NA   Continental\n## 2   2011     1         11         2    1100      NA    US_Airways\n## 3   2011     1         19         3    1811      NA    ExpressJet\n## 4   2011     1          7         5    2028      NA    ExpressJet\n## 5   2011     2          4         5    1638      NA      American\n## 6   2011     2          8         2    1057      NA   Continental\n## 7   2011     2          2         3     802      NA    ExpressJet\n## 8   2011     2          9         3     904      NA    ExpressJet\n## 9   2011     2          1         2    1508      NA       SkyWest\n## 10  2011     3         31         4    1016      NA   Continental\n## # ... with 30 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "Combining tests using boolean operators"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#blend-together",
            "text": "# Select the flights that had JFK as their destination: c1\nc1 <- filter(hflights, Dest == 'JFK')\n\n# Combine the Year, Month and DayofMonth variables to create a Date column: c2\nc2 <- mutate(c1, Date = paste(Year, Month, DayofMonth, sep = '-'))\n\n# Print out a selection of columns of c2\nselect(c2, Date, DepTime, ArrTime, TailNum)  ## # A tibble: 695 \u00d7 4\n##        Date DepTime ArrTime TailNum\n##       <chr>   <int>   <int>   <chr>\n## 1  2011-1-1     654    1124  N324JB\n## 2  2011-1-1    1639    2110  N324JB\n## 3  2011-1-2     703    1113  N324JB\n## 4  2011-1-2    1604    2040  N324JB\n## 5  2011-1-3     659    1100  N229JB\n## 6  2011-1-3    1801    2200  N206JB\n## 7  2011-1-4     654    1103  N267JB\n## 8  2011-1-4    1608    2034  N267JB\n## 9  2011-1-5     700    1103  N708JB\n## 10 2011-1-5    1544    1954  N644JB\n## # ... with 685 more rows",
            "title": "Blend together"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#arranging-your-data",
            "text": "# Definition of dtc\ndtc <- filter(hflights, Cancelled == 1, !is.na(DepDelay))\n\n# Arrange dtc by departure delays\narrange(dtc, DepDelay)  ## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011     7         23         6     605      NA       Frontier\n## 2   2011     1         17         1     916      NA     ExpressJet\n## 3   2011    12          1         4     541      NA     US_Airways\n## 4   2011    10         12         3    2022      NA American_Eagle\n## 5   2011     7         29         5    1424      NA    Continental\n## 6   2011     9         29         4    1639      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     5          9         1     715      NA        SkyWest\n## 9   2011     1         20         4    1413      NA         United\n## 10  2011     1         17         1     831      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # Arrange dtc so that cancellation reasons are grouped\narrange(dtc, CancellationCode)  ## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011     1         20         4    1413      NA         United\n## 2   2011     1          7         5    2028      NA     ExpressJet\n## 3   2011     2          4         5    1638      NA       American\n## 4   2011     2          8         2    1057      NA    Continental\n## 5   2011     2          1         2    1508      NA        SkyWest\n## 6   2011     2         21         1    2257      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     3         18         5     727      NA         United\n## 9   2011     4          4         1    1632      NA          Delta\n## 10  2011     4          8         5    1608      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # Arrange dtc according to carrier and departure delays\narrange(dtc, UniqueCarrier, DepDelay)  ## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>              <chr>\n## 1   2011     6         11         6    1649      NA            AirTran\n## 2   2011     8         18         4    1808      NA           American\n## 3   2011     2          4         5    1638      NA           American\n## 4   2011    10         12         3    2022      NA     American_Eagle\n## 5   2011     2          9         3     555      NA     American_Eagle\n## 6   2011     7         17         7    1917      NA     American_Eagle\n## 7   2011     4         30         6     612      NA Atlantic_Southeast\n## 8   2011     4         10         7    1147      NA Atlantic_Southeast\n## 9   2011     5         23         1     657      NA Atlantic_Southeast\n## 10  2011     9         29         4     723      NA Atlantic_Southeast\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "Arranging your data"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#reverse-the-order-of-arranging",
            "text": "# Arrange according to carrier and decreasing departure delays\narrange(hflights, UniqueCarrier, desc(DepDelay))  ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     2         19         6    1902    2143       AirTran\n## 2   2011     3         14         1    2024    2309       AirTran\n## 3   2011     2         16         3    2349     227       AirTran\n## 4   2011    11         13         7    2312     213       AirTran\n## 5   2011     5         26         4    2353     305       AirTran\n## 6   2011     5         26         4    1922    2229       AirTran\n## 7   2011     4         28         4    1045    1328       AirTran\n## 8   2011     6          5         7    2207      52       AirTran\n## 9   2011     5          7         6    1009    1256       AirTran\n## 10  2011     7         25         1    2107      14       AirTran\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # Arrange flights by total delay (normal order).\narrange(hflights, (ArrDelay+DepDelay))  ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     7          3         7    1914    2039    ExpressJet\n## 2   2011     8         31         3     934    1039       SkyWest\n## 3   2011     8         21         7     935    1039       SkyWest\n## 4   2011     8         28         7    2059    2206       SkyWest\n## 5   2011     8         29         1     935    1041       SkyWest\n## 6   2011    12         25         7     741     926       SkyWest\n## 7   2011     1         30         7     620     812       SkyWest\n## 8   2011     8          3         3    1741    1810    ExpressJet\n## 9   2011     8          4         4     930    1041       SkyWest\n## 10  2011     8         18         4     939    1043       SkyWest\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>  # Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime \narrange(filter(hflights, Dest == 'DFW' & DepTime<800), desc(AirTime))  ## # A tibble: 799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011    11         22         2     635     825       American\n## 2   2011     8         25         4     602     758 American_Eagle\n## 3   2011    10         12         3     559     738 American_Eagle\n## 4   2011     5          2         1     716     854       American\n## 5   2011     4          4         1     741     949       American\n## 6   2011     4          4         1     627     742 American_Eagle\n## 7   2011     6         21         2     726     848     ExpressJet\n## 8   2011     9          1         4     715     844       American\n## 9   2011     3         14         1     729     917    Continental\n## 10  2011    12          5         1     724     847    Continental\n## # ... with 789 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "Reverse the order of arranging"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#4-summarise-and-the-pipe-operator",
            "text": "",
            "title": "4, summarise and the Pipe Operator"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#the-syntax-of-summarise",
            "text": "# Print out a summary with variables min_dist and max_dist\nsummarise(hflights, min_dist = min(Distance), max_dist = max(Distance))  ## # A tibble: 1 \u00d7 2\n##   min_dist max_dist\n##      <int>    <int>\n## 1       79     3904  # Print out a summary with variable max_div\nsummarise(filter(hflights, Diverted == 1), max_div = max(Distance))  ## # A tibble: 1 \u00d7 1\n##   max_div\n##     <int>\n## 1    3904",
            "title": "The syntax of summarise"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#aggregate-functions",
            "text": "summarise :   min(x) ; minimum value of vector x.  max(x) ; maximum value of vector x.  mean(x) ; mean value of vector x.  median(x) ; median value of vector x.  quantile(x, p) ; pth quantile of vector x.  sd(x) ; standard deviation of vector x.  var(x) ; variance of vector x.  IQR(x) ; Inter Quartile Range (IQR) of vector x.  diff(range(x)) ; total range of vector x.    # Remove rows that have NA ArrDelay: temp1\ntemp1 <- filter(hflights, !is.na(ArrDelay))\n\n# Generate summary about ArrDelay column of temp1\nsummarise(temp1, earliest = min(ArrDelay), average = mean(ArrDelay), latest = max(ArrDelay), sd = sd(ArrDelay))  ## # A tibble: 1 \u00d7 4\n##   earliest  average latest       sd\n##      <int>    <dbl>  <int>    <dbl>\n## 1      -70 7.094334    978 30.70852  # Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2\ntemp2 <- filter(hflights, !is.na(TaxiIn), !is.na(TaxiOut))\n\n# Print the maximum taxiing difference of temp2 with summarise()\nsummarise(temp2, max_taxi_diff = max(abs(TaxiIn - TaxiOut)))  ## # A tibble: 1 \u00d7 1\n##   max_taxi_diff\n##           <int>\n## 1           160",
            "title": "Aggregate functions"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#dplyr-aggregate-functions",
            "text": "first(x) ; the first element of vector  x .  last(x) ; the last element of vector  x .  nth(x, n) ; The  n th element of vector  x .  n() ; The number of rows in the data.frame or group of observations \n    that  summarise()  describes.  n_distinct(x) ; The number of unique values in vector  x .    # Generate summarizing statistics for hflights\nsummarise(hflights, n_obs = n(), n_carrier = n_distinct(UniqueCarrier), n_dest = n_distinct(Dest), dest100 = nth(Dest, 100))  ## # A tibble: 1 \u00d7 4\n##    n_obs n_carrier n_dest dest100\n##    <int>     <int>  <int>   <chr>\n## 1 227496        15    116     DFW  # Filter hflights to keep all American Airline flights: aa\naa <- filter(hflights, UniqueCarrier == 'American')\n\n# Generate summarizing statistics for aa \nsummarise(aa, n_flights = n(), n_canc = sum(Cancelled), p_canc = n_canc/n_flights*100, avg_delay = mean(ArrDelay, na.rm = TRUE))  ## # A tibble: 1 \u00d7 4\n##   n_flights n_canc   p_canc avg_delay\n##       <int>  <int>    <dbl>     <dbl>\n## 1      3244     60 1.849568 0.8917558  Overview of syntax  # Write the 'piped' version of the English sentences\nhflights %>%\n    mutate(diff = TaxiOut - TaxiIn) %>%\n    filter(!is.na(diff)) %>%\n    summarise( avg = mean(diff))  ## # A tibble: 1 \u00d7 1\n##        avg\n##      <dbl>\n## 1 8.992064  Drive or fly? Part 1 of 2  # Part 1, concerning the selection and creation of columns\nd <- hflights %>%\n  select(Dest, UniqueCarrier, Distance, ActualElapsedTime) %>%  \n  mutate(RealTime = ActualElapsedTime + 100, mph = Distance / RealTime * 60)\n\n# Part 2, concerning flights that had an actual average speed of < 70 mph.\nd %>%\n  filter(!is.na(mph), mph < 70) %>%\n  summarise( n_less = n(), \n             n_dest = n_distinct(Dest), \n             min_dist = min(Distance), \n             max_dist = max(Distance))  ## # A tibble: 1 \u00d7 4\n##   n_less n_dest min_dist max_dist\n##    <int>  <int>    <int>    <int>\n## 1   6726     13       79      305  Drive or fly? Part 2 of 2  # Solve the exercise using a combination of dplyr verbs and %>%\nhflights %>%\n    #summarise(all_flights = n()) %>%\n    filter(((Distance / (ActualElapsedTime + 100) * 60) < 105) | Cancelled == 1 | Diverted == 1) %>%\n    summarise(n_non = n(), p_non = n_non / 22751 *100, n_dest = n_distinct(Dest), min_dist = min(Distance), max_dist = max(Distance))  ## # A tibble: 1 \u00d7 5\n##   n_non    p_non n_dest min_dist max_dist\n##   <int>    <dbl>  <int>    <int>    <int>\n## 1 42400 186.3654    113       79     3904  Advanced piping exercise  # Count the number of overnight flights\nhflights %>%\n    filter(ArrTime < DepTime & !is.na(DepTime) & !is.na(ArrTime)) %>%\n    summarise(n = n())  ## # A tibble: 1 \u00d7 1\n##       n\n##   <int>\n## 1  2718",
            "title": "dplyr aggregate functions"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#5-group_by-and-working-with-data",
            "text": "",
            "title": "5, group_by and working with data"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#unite-and-conquer-using-group_by",
            "text": "# Make an ordered per-carrier summary of hflights\nhflights %>%\n   group_by(UniqueCarrier) %>%\n   summarise(n_flights = n(), \n             n_canc = sum(Cancelled == 1), \n             p_canc = mean(Cancelled == 1) * 100, \n             avg_delay = mean(ArrDelay, na.rm = TRUE)) %>%\n   arrange(avg_delay, p_canc)  ## # A tibble: 15 \u00d7 5\n##         UniqueCarrier n_flights n_canc    p_canc  avg_delay\n##                 <chr>     <int>  <int>     <dbl>      <dbl>\n## 1          US_Airways      4082     46 1.1268986 -0.6307692\n## 2            American      3244     60 1.8495684  0.8917558\n## 3             AirTran      2139     21 0.9817672  1.8536239\n## 4              Alaska       365      0 0.0000000  3.1923077\n## 5                Mesa        79      1 1.2658228  4.0128205\n## 6               Delta      2641     42 1.5903067  6.0841374\n## 7         Continental     70032    475 0.6782614  6.0986983\n## 8      American_Eagle      4648    135 2.9044750  7.1529751\n## 9  Atlantic_Southeast      2204     76 3.4482759  7.2569543\n## 10          Southwest     45343    703 1.5504047  7.5871430\n## 11           Frontier       838      6 0.7159905  7.6682692\n## 12         ExpressJet     73053   1132 1.5495599  8.1865242\n## 13            SkyWest     16061    224 1.3946828  8.6934922\n## 14            JetBlue       695     18 2.5899281  9.8588410\n## 15             United      2072     34 1.6409266 10.4628628  # Make an ordered per-day summary of hflights\nhflights %>% \n   group_by(DayOfWeek) %>%\n   summarise(avg_taxi = mean(TaxiIn + TaxiOut, na.rm=TRUE)) %>%\n   arrange(desc(avg_taxi))  ## # A tibble: 7 \u00d7 2\n##   DayOfWeek avg_taxi\n##       <int>    <dbl>\n## 1         1 21.77027\n## 2         2 21.43505\n## 3         4 21.26076\n## 4         3 21.19055\n## 5         5 21.15805\n## 6         7 20.93726\n## 7         6 20.43061",
            "title": "Unite and conquer using group_by"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#combine-group_by-with-mutate",
            "text": "# Solution to first instruction\nhflights %>%\n    filter(!is.na(ArrDelay)) %>%\n    group_by(UniqueCarrier) %>%\n    summarise(p_delay = sum(ArrDelay > 0) / n()) %>%\n    mutate(rank = rank(p_delay)) %>%\n    arrange(rank)  ## # A tibble: 15 \u00d7 3\n##         UniqueCarrier   p_delay  rank\n##                 <chr>     <dbl> <dbl>\n## 1            American 0.3030208     1\n## 2             AirTran 0.3112269     2\n## 3          US_Airways 0.3267990     3\n## 4  Atlantic_Southeast 0.3677511     4\n## 5      American_Eagle 0.3696714     5\n## 6               Delta 0.3871092     6\n## 7             JetBlue 0.3952452     7\n## 8              Alaska 0.4368132     8\n## 9           Southwest 0.4644557     9\n## 10               Mesa 0.4743590    10\n## 11        Continental 0.4907385    11\n## 12         ExpressJet 0.4943420    12\n## 13             United 0.4963109    13\n## 14            SkyWest 0.5350105    14\n## 15           Frontier 0.5564904    15  # Solution to second instruction\nhflights %>%\n    filter(!is.na(ArrDelay), ArrDelay > 0) %>%\n    group_by(UniqueCarrier) %>%\n    summarise(avg = mean(ArrDelay)) %>%\n    mutate(rank = rank(avg)) %>%\n    arrange(rank)  ## # A tibble: 15 \u00d7 3\n##         UniqueCarrier      avg  rank\n##                 <chr>    <dbl> <dbl>\n## 1                Mesa 18.67568     1\n## 2            Frontier 18.68683     2\n## 3          US_Airways 20.70235     3\n## 4         Continental 22.13374     4\n## 5              Alaska 22.91195     5\n## 6             SkyWest 24.14663     6\n## 7          ExpressJet 24.19337     7\n## 8           Southwest 25.27750     8\n## 9             AirTran 27.85693     9\n## 10           American 28.49740    10\n## 11              Delta 32.12463    11\n## 12             United 32.48067    12\n## 13     American_Eagle 38.75135    13\n## 14 Atlantic_Southeast 40.24231    14\n## 15            JetBlue 45.47744    15",
            "title": "Combine group_by with mutate"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#advanced-group_by",
            "text": "# Which plane (by tail number) flew out of Houston the most times? How many times? adv1\nadv1 <- hflights %>%\n          group_by(TailNum) %>%\n          summarise(n = n()) %>%\n          filter(n == max(n))\n\n# How many airplanes only flew to one destination from Houston? adv2\nadv2 <- hflights %>%\n          group_by(TailNum) %>%\n          summarise(ndest = n_distinct(Dest)) %>%\n          filter(ndest == 1) %>%\n          summarise(nplanes = n())\n\n# Find the most visited destination for each carrier: adv3\nadv3 <- hflights %>% \n          group_by(UniqueCarrier, Dest) %>%\n          summarise(n = n()) %>%\n          mutate(rank = rank(desc(n))) %>%\n          filter(rank == 1)\n\n# Find the carrier that travels to each destination the most: adv4\nadv4 <- hflights %>% \n          group_by(Dest, UniqueCarrier) %>%\n          summarise(n = n()) %>%\n          mutate(rank = rank(desc(n))) %>%\n          filter(rank == 1)",
            "title": "Advanced group_by"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#dplyr-deals-with-different-types",
            "text": "# Use summarise to calculate n_carrier\ns2 <- hflights %>%\n    summarise(n_carrier = n_distinct(UniqueCarrier))",
            "title": "dplyr deals with different types"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#dplyr-and-mysql-databases",
            "text": "Code only.  # set up a src that connects to the mysql database (src_mysql is provided by dplyr)\nmy_db <- src_mysql(dbname = 'dplyr', \n                  host = 'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                  port = 3306,\n                  user = 'dplyr',\n                  password = 'dplyr')\n\n# and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table\nnycflights <- tbl(my_db, 'dplyr')\n\n# glimpse at nycflights\nglimpse(nycflights)\n\n# Calculate the grouped summaries detailed in the instructions\nnycflights %>%\n   group_by(carrier) %>%\n   summarise(n_flights = n(), avg_delay = mean(arr_delay)) %>%\n   arrange(avg_delay)",
            "title": "dplyr and mySQL databases"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#adding-tidyr-functions",
            "text": "complete .  drop_na .  expand .  extract .  extract_numeric .  complete .  fill .  full_seq .  gather .  nest .  replace_na .  separate .  separate_rows .  separate_rows_ .  smiths .  spread .  table1 .  unite .  unnest .  who .",
            "title": "Adding tidyr Functions"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#joining-data-in-r-with-dplyr",
            "text": "In development  ## 1, Mutating Joins\n\n## 2, Filtering Joins and Set Operations\n\n## 3, Assembling Data\n\n## 4, Advanced Joining\n\n## 5, Case Study",
            "title": "Joining Data in R with dplyr"
        },
        {
            "location": "/Plot_snippets_-_Basics/",
            "text": "Plotting Packages\n\n\nData Type & Dataset\n\n\nData Types\n\n\nFunctions\n\n\nDataset\n\n\n\n\n\n\nThe Basic Package\n\n\nBasic Plots, Options & Parameters\n\n\nUnivariate Plots\n\n\nBivariate (Multivariate) Plots\n\n\nMultivariate Plots\n\n\nTimes Series\n\n\nRegressions and Residual Plots\n\n\n\n\n\n\nThe \nlattice\n and \nlatticeExtra\n Packages\n\n\nColoring\n\n\nDocumentation\n\n\nA note on reordering the levels (factors)\n\n\nUni-, Bi-, Multivariate Plots\n\n\n\n\n\n\nAdditional Packages\n\n\nThe \nsm\n Package (density)\n\n\nThe \ncar\n Package (scatter)\n\n\nThe \nvioplot\n Package (boxplot)\n\n\nThe \nvcd\n Package (count, correlation, mosaic)\n\n\nThe \nhexbin\n Package (scatter)\n\n\nThe \ncar\n Package (scatter)\n\n\nThe \nscatterplot3d\n Package\n\n\nThe \nrgl\n Package (interactive)\n\n\nThe \ncluster\n Package (dendrogram)\n\n\nThe \nextracat\n Package (splom)\n\n\nThe \nash\n Package (density)\n\n\nThe \nKernSmooth\n Package (density)\n\n\nThe \ncorrplot\n Package (correlation)\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nCode snippets and results.\n\n\nSome data might necessitate more specialized packages.\n\n\nFor explaining data, presenting results, reporting and publishing,\n\n    we can generate prettier graphics with \nggvis\n or \nggplot2\n, and\n\n    interactive packages such as \nshiny\n.\n\n\n\n\n\n\nPlotting Packages\n\u00b6\n\n\nGraphics:\n\n\n\n\nmaps\n for grids and mapping.\n\n\ndiagram\n for flow charts.\n\n\nplotrix\n for ternary, polar plots.\n\n\ngplots\n.\n\n\npixmap\n, \npng\n, \nrtiff\n, \nReadImages\n, \nEBImage\n, \nRImageJ\n.\n\n\nleaflet\n.\n\n\n\n\nGrid:\n\n\n\n\nvcd\n for mosaic, ternary plots.\n\n\ngrImport\n for vectors.\n\n\nggplot2\n and extensions.\n\n\nlattice\n and \nlatticeExtra\n.\n\n\ngridBase\n.\n\n\n\n\ngrDevices:\n\n\n\n\nJavaGD\n.\n\n\nCairo\n.\n\n\ntikzDevice\n.\n\n\n\n\nInteractive:\n\n\n\n\nrgl\n.\n\n\nggvis\n.\n\n\niplots\n.\n\n\nrggobi\n.\n\n\n\n\nOthers:\n\n\n\n\nash\n for density plots.\n\n\ncluster\n for dendrograms.\n\n\ncopula\n for multivariate analyses.\n\n\ncorrplot\n for correlations.\n\n\ncompositions\n for geometries, ternary plots.\n\n\nextracat\n for missing values.\n\n\nsoiltexture\n for ternary plots and more.\n\n\nKernSmooth\n for histograms-density plots.\n\n\nopenair\n for polar, circular plots.\n\n\nsm\n for density plots.\n\n\ncar\n for scatter plots.\n\n\nvioplot\n for boxplots.\n\n\nvcd\n for mosaic plots and multivariate analyses.\n\n\nhexbin\n for scatter plots.\n\n\nscatterplot3d\n for 3D scatter plots.\n\n\ncluster\n for dendrograms.\n\n\nshiny\n for interactive plots.\n\n\nggvis\n.\n\n\n\n\nData Type & Dataset\n\u00b6\n\n\nData Types\n\u00b6\n\n\n\n\ncontinuous vs categorical (or discrete).\n\n\ncontinuous: float, x-y-z, 3D, map coordinates, trianguar, lat-long,\n\n    polar, degree-distance, angle-vector.\n\n\ncategorical: integer, binary, dichotomic, dummy, factor,\n\n    ordinal (ordered).\n\n\n\n\nContinuous variable characteristics:\n\n\n\n\nasymmetry.\n\n\noutliers.\n\n\nmultimodality.\n\n\ngaps, missing values.\n\n\nheaping, redundance.\n\n\nrounding, integer.\n\n\nimpossibilities, anomalies.\n\n\nerrors.\n\n\n\u2026\n\n\n\n\nCategorical variable characteristics:\n\n\n\n\nunexpected pattern of results.\n\n\nuneven distribution.\n\n\nextra categories.\n\n\nunbalanced experiments.\n\n\nlarge numbers of categories.\n\n\nNA, errors, missings\u2026\n\n\nnominal: no fixed order.\n\n\nordinal: fixed order (scale of 1 to 5).\n\n\ndiscrete: counts, integers.\n\n\ndependencies, correlation, associations.\n\n\ncausal relationships, outliers, groups, clusters, gaps, barriers,\n\n    conditional relationship.\n\n\n\u2026\n\n\n\n\nUnivariate main plots:\n\n\n\n\nhistogram.\n\n\ndensity.\n\n\nqqmath chart.\n\n\nbox & whickers chart.\n\n\nbar chart.\n\n\ndot.\n\n\n\n\nBivariate main plots:\n\n\n\n\nxy chart.\n\n\nqq chart.\n\n\n\n\nTrivariate main plots:\n\n\n\n\ncloud.\n\n\nwireframe.\n\n\ncountour.\n\n\nlevel.\n\n\n\n\nMultivariate main plots:\n\n\n\n\nsploms.\n\n\nparallel charts (coordinate).\n\n\n\n\nSpecialized plots:\n\n\n\n\nfrequencies, crosstabs: bar charts, mosaic plots, association plots.\n\n\ncorrelations: sploms, pairs, correlograms.\n\n\nt-tests, non-parrametric tests of group differences: box plot,\n\n    density plot.\n\n\nregression: scatter plot.\n\n\nANOVA: box plots, line plots.\n\n\n\n\nFunctions\n\u00b6\n\n\nCreate a new variable\n\n\niris2 <- within(iris, area <- Petal.Width*Petal.Length)\nhead(iris2, 3)\n\n\n\n\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species area\n## 1          5.1         3.5          1.4         0.2  setosa 0.28\n## 2          4.9         3.0          1.4         0.2  setosa 0.28\n## 3          4.7         3.2          1.3         0.2  setosa 0.26\n\n\n\narea <- with(iris, area <- Petal.Width*Petal.Length)\nhead(area, 3)\n\n\n\n\n## [1] 0.28 0.28 0.26\n\n\n\nDataset\n\u00b6\n\n\nFor most examples, we use the \nmtcars\n dataset.\n\n\nPrepare the dataset.\n\n\nattach(mtcars)\n\n\n\n\nGet data attached to a package (an example).\n\n\ndata(gvhd10, package = 'latticeExtra')\n\n\n\n\nThe Basic Package\n\u00b6\n\n\nBasic Plots, Options & Parameters\n\u00b6\n\n\nStandardize the parameters (an example)\n\n\n# color and tick mark text orientation\npar(col = 'black', las = 1)\n\n\n\n\nGrid and layout\n\n\nOne plot.\n\n\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\n\n\n\n\n\nA grid of plots.\n\n\npar(mfrow = c(2, 1))\n\nplot(mpg, hp, ylab = 'horsepower', xlab = 'miles per gallon')\nboxplot(mpg ~ cyl, xlab = 'mile per gallon', ylab = 'number of cylinders', horizontal = TRUE)\n\npar(mfrow = c(1, 2))\n\nplot(mpg, hp, ylab = 'horsepower', xlab = 'miles per gallon')\nboxplot(mpg ~ cyl, xlab = 'mile per gallon', ylab = 'number of cylinders', horizontal = TRUE)\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nOther grids.\n\n\nlayout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))\n\nplot(mpg, xlab = 'observations', ylab = 'miles per gallon')\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\nboxplot(mpg ~ cyl, ylab = 'mile per gallon', xlab = 'number of cylinders')\n\n\n\n\n\n\n# view\nmatrix(c(1,2,1,3), 2, 2, byrow = TRUE)\n\n\n\n\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    3\n\n\n\nlayout(matrix(c(1,2,1,3), 2, 2, byrow = TRUE))\n\nhist(wt)\nhist(mpg)\nhist(disp)\n\n\n\n\n\n\nlayout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE),  widths = c(3,1), heights = c(1,2))\n\nhist(wt)\nhist(mpg)\nhist(disp)\n\n\n\n\n\n\nnf <- layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths = lcm(12), heights = lcm(6))\nlayout.show(nf)\n\n\n\n\n\n\nplot(mpg, xlab = 'observations', ylab = 'miles per gallon')\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\nboxplot(mpg ~ cyl, ylab = 'mile per gallon', xlab = 'number of cylinders')\n\n\n\n\n\n\nGridview with additional packages.\n\n\nlibrary(vcd)\n\n\n\n\nmplot(A, B, C)\n\n\n\n\n\n\nSee the \nlattice\n and \nlatticeExtra\n packages for built-in\n\nfacet/gridview. \nggplot2\n as well.\n\n\nPlot and add ablines\n\n\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\n# abline(h = yvalues, v = xvalues)\nabline(lm(mpg ~ hp))\n\n# main = 'Title' or...\ntitle('Title')\n\n\n\n\n\n\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\nabline(h = c(20, 25))\nabline(v = c(50, 150))\nabline(v = seq(200, 300, 50), lty = 2, col = 'blue')\n\n\n\n\n\n\nAdd a legend\n\n\nboxplot(mpg ~ cyl, main = 'Title',\n   yaxt = 'n', xlab = 'mile per gallon', horizontal = TRUE, col = terrain.colors(3))\n\nlegend('topright', inset = 0.05, title = 'number of cylinders', c('4','6','8'), fill = terrain.colors(3), horiz = TRUE)\n\n\n\n\n\n\nSave\n\n\nmygraph <- plot(hp, mpg, main = 'Title', xlab = 'horsepower', ylab = 'miles per gallon')\n\npdf('mygraph.pdf')\npng('mygraph.png')\njpeg('mygraph.jpg')\nbmp('mygraph.bmp')\npostscript('mygraph.ps')\n\n\n\n\nView in a new window\n\n\nTyping the function will open a new window to render the plot.\n\n\n\n\nwindows()\n for Windows.\n\n\nX11()\n for Linux.\n\n\nquartz()\n for OS X.\n\n\n\n\n\n\n\n# open the new windows\nwindows()\n\nplot(hp, mpg, main = 'Title', xlab = 'horsepower', ylab = 'miles per gallon')\n\n\n\n\nEnrich the plot, add text\n\n\nplot(hp, mpg,\n     main = 'Title', col.main = 'blue',\n     sub = 'figure 1', col.sub = 'blue',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     col.lab = 'red', cex.lab = 0.9,\n     xlim = c(50, 350),\n     ylim = c(0, 40))\n\ntext(100, 10, 'text 1') # x and y coordinate\nmtext('text 2', 4, line = 0.5) # pos = 1 (bottom), 2 (left), 3 (top), 4 (right); line (margin)\n\n\n\n\n\n\nWith \nlocator()\n, use the mouse; with 1 for 1 click, 2 for\u2026 Find the\n\ncoordinates to be entered in the code. For example (after two clicks):\n\n\n> locator(2)\n$x\n[1] 212.5308 293.7854\n\n$y\n[1] 33.34040 31.87281\n\n\n\n\nplot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\ntext(hp, mpg, row.names(mtcars), cex = 0.7, pos = 4, col = 'red')\n\n\n\n\n\n\nEnrich the plot, add symbols\n\n\nplot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\nsymbols(250, 20, squares = 1, add = TRUE, inches = 0.1, fg = 'red')\nsymbols(250, 25, circles = 1, add = TRUE, inches = 0.1, fg = 'red')\n\n\n\n\n\n\n#rectangles\n#stars\n#thermometers\n#boxplots\n\n\n\n\nCombine plots; change \npch =\n & \ncol =\n\n\npar(mfrow = c(2,2))\n\n# 1\nplot(hp, mpg,\n     main = 'P1',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 1,\n     col = 'black')\n\n# 2\nplot(hp, mpg,\n     main = 'P2',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 3,\n     col = 'blue',\n     cex = 0.5)\n\n# 3\nplot(hp, mpg,\n     main = 'P3',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 5,\n     col = 'red',\n     cex = 2)\n\n# 4\nplot(hp, mpg,\n     main = 'P4',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 7,\n     col = 'green')\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1))\n\n\n\n\nChange \ncol =\n\n\n\n\nChange \npch =\n\n\n\n\npar(fig = c(0,0.8,0,0.8))\n\nplot(mtcars$wt, mtcars$mpg, xlab = 'Car Weight',   ylab = 'miles Per Gallon')\n\npar(fig = c(0,0.8,0.55,1), new = TRUE)\n\nboxplot(mtcars$wt, horizontal = TRUE, axes = FALSE)\n\npar(fig = c(0.65,1,0,0.8), new = TRUE)\n\nboxplot(mtcars$mpg, axes = FALSE)\n\nmtext('Enhanced Scatterplot', side = 3, outer = TRUE, line = -3)\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1))\n\n\n\n\nChange \ntype =\n; without dots\n\n\nx <- c(1:5); y <- x\n\npar(pch = 22, col = 'red') # plotting symbol and color\n\npar(mfrow = c(2,4)) # all plots on one page\nopts = c('p','l','o','b','c','s','S','h')\n\nfor (i in 1:length(opts)) {\n  heading = paste('type =',opts[i])\n  plot(x, y, type = 'n', main = heading)\n  lines(x, y, type = opts[i])\n}\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1), col = 'black')\n\n\n\n\nChange \ntype =\n; with dots\n\n\nx <- c(1:5); y <- x\n\npar(pch = 22, col = 'blue') # plotting symbol and color\n\npar(mfrow = c(2,4)) # all plots on one page\nopts = c('p','l','o','b','c','s','S','h')\n\nfor (i in 1:length(opts)) {\n  heading = paste('type =',opts[i])\n  plot(x, y, main = heading)\n  lines(x, y, type = opts[i])\n}\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1), col = 'black')\n\n\n\n\nAdd or modify the axes\n\n\nplot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     xaxt = 'n',\n     yaxt = 'n')\n\naxis(1, at = c(100, 200, 300), labels = NULL, pos = 15, lty = 'dashed', col = 'green', las = 2, tck = -0.05)\n\naxis(4, at = c(20, 30), labels = c('bt', 'up'), pos = 125, lty = 'dashed', col = 'blue', las = 2, tck = -0.05)\n\n\n\n\n\n\n# reverse\npar(las = 1)\n\n\n\n\nAdd layers to the first plot\n\n\nplot(mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\n# add lines\nlines(mpg[1:10], type = 'l', col = 'green')\n\n\n\n\n\n\nUnivariate Plots\n\u00b6\n\n\nPlot; continuous\n\n\nplot(mpg, main = 'Title', xlab = 'observations', ylab = 'miles per gallon')\n\n\n\n\n\n\nPlot; categorical\n\n\nplot(cyl, main = 'Title', xlab = 'observations', ylab = 'cylinders')\n\n\n\n\n\n\nQQnorm; continuous\n\n\nqqnorm(mpg, main = 'Title', xlab = 'observations', ylab = 'cylinders')\n\n\n\n\n\n\nQQnorm; categorical\n\n\nqqnorm(cyl, main = 'Title', xlab = 'observations', ylab = 'cylinders')\n\n\n\n\n\n\nStripchart; continuous\n\n\nstripchart(mpg, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\nStripchart; categorical\n\n\nstripchart(cyl, main = 'Title', xlab = 'cylinders')\n\n\n\n\n\n\nBarplot (vertical); continuous\n\n\nbarplot(mpg[1:10], main = 'Title', xlab = 'observations', ylab = 'miles per gallon')\n\n\n\n\n\n\nBarplot (horizontal); categorical\n\n\nbarplot(cyl[1:10], main = 'Title', horiz = TRUE, xlab = 'cylinders', ylab = 'observations')\n\n\n\n\n\n\nBarplots options\n\n\nGroup with \ntable()\n.\n\n\ncounts <- table(cyl)\ncounts\n\n\n\n\n## cyl\n##  4  6  8 \n## 11  7 14\n\n\n\nbarplot(counts, main = 'Title', horiz = TRUE, xlab = 'count', names.arg = c('4 Cyl', '6 Cyl', '8 Cyl'))\n\n\n\n\n\n\ncounts <- table(vs, gear)\ncounts\n\n\n\n\n##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1\n\n\n\nbarplot(counts, main = 'Title', xlab = 'gearbox', col = c('darkblue', 'red'), legend = rownames(counts)) \n\n\n\n\n\n\ncounts <- table(vs, gear)\ncounts\n\n\n\n\n##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1\n\n\n\nbarplot(counts, main = 'Title', xlab='gearbox', col = c('darkblue', 'red'), legend =  rownames(counts), beside = TRUE)\n\n\n\n\n\n\nGroup with \naggregate()\n.\n\n\naggregate(mtcars, by = list(cyl, vs), FUN = mean, na.rm = TRUE)\n\n\n\n\n##   Group.1 Group.2      mpg cyl   disp       hp     drat       wt     qsec\n## 1       4       0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000\n## 2       6       0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667\n## 3       8       0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214\n## 4       4       1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100\n## 5       6       1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500\n##   vs        am     gear     carb\n## 1  0 1.0000000 5.000000 2.000000\n## 2  0 1.0000000 4.333333 4.666667\n## 3  0 0.1428571 3.285714 3.500000\n## 4  1 0.7000000 4.000000 1.500000\n## 5  1 0.0000000 3.500000 2.500000\n\n\n\npar(las = 2) # make label text perpendicular to axis\n\npar(mar = c(5, 8, 4, 2)) # increase y-axis margin.\n\ncounts <- table(mtcars$gear)\nbarplot(counts, main = 'Car Distribution', horiz = TRUE, names.arg = c('3 Gears', '4 Gears', '5   Gears'), cex.names = 0.8)\n\n\n\n\n\n\n# reverse\npar(las = 1)\n\n\n\n\nColors.\n\n\nlibrary(RColorBrewer)\n\npar(mfrow = c(2, 1))\n\nbarplot(iris$Petal.Length)\nbarplot(table(iris$Species, iris$Sepal.Length), col = brewer.pal(3, 'Set1'))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nPie Chart\n\n\nAvoid!\n\n\nDotchart; continuous\n\n\ndotchart(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'observations')\n\n\n\n\n\n\nDotchart; categorical\n\n\ndotchart(cyl, main = 'Title', xlab = 'cylinders', ylab = 'observations')\n\n\n\n\n\n\nDotchart options\n\n\ndotchart(mpg,labels = row.names(mtcars), cex = 0.7, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\n# sort by mpg\nx <- mtcars[order(mpg),]\n\n# must be factors\nx$cyl <- factor(x$cyl)\nx$color[x$cyl == 4] <- 'red'\nx$color[x$cyl == 6] <- 'blue'\nx$color[x$cyl == 8] <- 'darkgreen'\n\ndotchart(x$mpg, labels = row.names(x), cex = 0.7, groups = x$cyl, main = 'Title',  xlab = 'miles per gallon', gcolor = 'black', color = x$color)\n\n\n\n\n\n\nMore with the \nhmisc\n package and \npanel.dotplot()\n and in the \nlattice\n\npackage section.\n\n\nBoxplot; continuous\n\n\nboxplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'observations')\n\n\n\n\n\n\nStem; continuous\n\n\nstem(mpg)\n\n\n\n\n## \n##   The decimal point is at the |\n## \n##   10 | 44\n##   12 | 3\n##   14 | 3702258\n##   16 | 438\n##   18 | 17227\n##   20 | 00445\n##   22 | 88\n##   24 | 4\n##   26 | 03\n##   28 | \n##   30 | 44\n##   32 | 49\n\n\n\nHistogram; continuous\n\n\nhist(mpg, main = 'Title', xlab = 'miles per gallon - bins', ylab = 'count')\n\n\n\n\n\n\nHistogram; categorical\n\n\nhist(cyl, main = 'Title', xlab = 'cylinders - bins', ylab = 'count')\n\n\n\n\n\n\nHistogram options\n\n\nhist(mpg, breaks = 12, col = 'red')\n\n\n\n\n\n\nx <- mpg\n\nh <- hist(x, breaks = 10, main = 'Title', xlab = 'miles per gallon')\n\nxfit <- seq(min(x), max(x),length = 40)\nyfit <- dnorm(xfit, mean = mean(x), sd = sd(x))\nyfit <- yfit*diff(h$mids[1:2])*length(x)\n\nlines(xfit, yfit, col = 'blue', lwd = 2)\n\n\n\n\n\n\nColors.\n\n\nlibrary(RColorBrewer)\n\npar(mfrow = c(2, 3))\n\nhist(VADeaths, breaks = 10, col = brewer.pal(3, 'Set3'), main = '3, Set3')\nhist(VADeaths, breaks = 4, col = brewer.pal(3, 'Set2'), main = '3, Set2')\nhist(VADeaths, breaks = 8, col = brewer.pal(3, 'Set1'), main = '3, Set1')\nhist(VADeaths, breaks = 2, col = brewer.pal(8, 'Set3'), main = '8, Set3')\nhist(VADeaths, breaks = 10, col = brewer.pal(8, 'Greys'), main = '8, Greys')\nhist(VADeaths, breaks = 10, col = brewer.pal(8, 'Greens'), main = '8, Greens')\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nDensity Plot; continuous\n\n\nplot(density(mpg), main = 'Title')\n\n\n\n\n\n\nplot(density(mpg), main = 'Title')\n\npolygon(density(mpg), col = 'red', border = 'blue') \n\n\n\n\n\n\nd1 <- density(mtcars$mpg)\nplot(d1)\nrug(mtcars$mpg)\n\nlines(density(mtcars$mpg, d1$bw/2), col = 'green')\nlines(density(mtcars$mpg, d1$bw/5), col = 'blue')\n\n\n\n\n\n\nBivariate (Multivariate) Plots\n\u00b6\n\n\nPlot, continuous/continuous\n\n\nplot(mpg, hp, main = 'Title', xlab = 'miles per gallon', ylab = 'horsepowers')\n\n\n\n\n\n\nPlot, continuous/categorical\n\n\nplot(mpg, cyl, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\nPlot options\n\n\nplot(wt, mpg, main = 'Title', xlab = 'weight', ylab = 'miles per gallon ')\n\nabline(lm(mpg ~ wt), col = 'red') # regression\nlines(lowess(wt, mpg), col = 'blue') # lowess line\n\n\n\n\n\n\nSmoothScatter; continuous/continuous\n\n\nsmoothScatter(mpg, hp, main = 'Title', xlab = 'miles per gallon', ylab = 'horsepowers')\n\n\n\n\n\n\nSunflowerplot; categorical/categorical\n\n\nSpecial symbols at each location: one observation = one dot; more\n\nobservations = cross, star, etc.\n\n\nsunflowerplot(gear, cyl, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')\n\n\n\n\n\n\nBoxplot\n\n\nboxplot(mpg ~ cyl, main = 'Title',   xlab = 'cylinders', ylab = 'miles per gallon') \n\n\n\n\n\n\nColors.\n\n\nlibrary(RColorBrewer)\n\npar(mfrow = c(1, 2))\n\nboxplot(iris$Sepal.Length, col = 'red')\nboxplot(iris$Sepal.Length ~ iris$Species, col = topo.colors(3))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nlibrary(dplyr)\n\ndata(Pima.tr2, package = 'MASS')\n\nPimaV <- select(Pima.tr2, glu:age)\nboxplot(scale(PimaV), pch = 16, outcol = 'red')\n\n\n\n\n\n\nBoxplot options\n\n\nfour <- subset(mpg, cyl == 4)\nsix <- subset(mpg, cyl == 6)\neight <- subset(mpg, cyl == 8)\n\nboxplot(four, six, eight, main = 'Title', ylab = 'miles per gallon')\n\naxis(1, at = c(1, 2, 3), labels = c('4 Cyl', '6 Cyl', '8 Cyl'))\n\n\n\n\n\n\nDotchart\n\n\ncounts <- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\ndotchart(counts, main = 'Title', xlab = 'count', ylab = 'cylinders/gearbox')\n\n\n\n\n\n\ncounts <- table(cyl, gear)\ncounts\n\n\n\n\n##    gear\n## cyl  3  4  5\n##   4  1  8  2\n##   6  2  4  1\n##   8 12  0  2\n\n\n\ndotchart(counts, main = 'Title', xlab = 'count', ylab = 'gearbox/cylinders')\n\n\n\n\n\n\nBarplot with its options\n\n\nVertical or horizontal. The legend as well can be horizontal or\n\nvertical.\n\n\ncounts <- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\nbarplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 20), col = terrain.colors(3))\n\nlegend('topleft', inset = .04, title = 'gearbox',\n   c('3','4','5'), fill = terrain.colors(3), horiz = TRUE)\n\n\n\n\n\n\ncounts <- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\nbarplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 25), col = terrain.colors(3), legend = rownames(counts))\n\n\n\n\n\n\ncounts <- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\nbarplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 20), col = terrain.colors(3), legend = rownames(counts), beside = TRUE)\n\n\n\n\n\n\nSpineplot\n\n\n\u2018Count\u2019 = blocks; categorical (with factors).\n\n\ncyl2 <- as.factor(cyl) # mandatory for the y\ngear2 <- as.factor(gear)\n\nspineplot(gear2, cyl2, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')\n\n\n\n\n\n\nCount = blocks; continuous.\n\n\nspineplot(mpg, cyl2, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\nMosaicplot\n\n\nCount = blocks.\n\n\ncounts <- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\nmosaicplot(counts, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')\n\n\n\n\n\n\nMultivariate Plots\n\u00b6\n\n\nPairs\n\n\npairs( ~mpg + disp + hp)\n\n\n\n\n\n\nCoplot\n\n\ncoplot(mpg ~ hp | wt)\n\n\n\n\n\n\nCorrelograms\n\n\nlibrary(corrgram)\n\ncorrgram(mtcars, order = TRUE, lower.panel = panel.shade, upper.panel=panel.pie, text.panel = panel.txt, main = 'Car Milage Data in PC2/PC1 Order')\n\n\n\n\n\n\nPlot a dataset with colors\n\n\nlibrary(RColorBrewer)\n\nplot(iris, col = brewer.pal(3, 'Set1'))\n\n\n\n\n\n\nStars\n\n\nThe star branches are explanatory; be careful with the interpretation!\n\nWell-advised for visual and pattern exploration.\n\n\nmtcars[1:4, c(1, 4, 6)]\n\n\n\n\n##                 mpg  hp    wt\n## Mazda RX4      21.0 110 2.620\n## Mazda RX4 Wag  21.0 110 2.875\n## Datsun 710     22.8  93 2.320\n## Hornet 4 Drive 21.4 110 3.215\n\n\n\nstars(mtcars[1:4, c(1, 4, 6)])\n\n\n\n\n\n\nTrivariate plots\n\n\n\n\nimage()\n.\n\n\ncontour()\n.\n\n\nfilled.contour()\n.\n\n\npersp()\n.\n\n\nsymbols()\n.\n\n\n\n\nTimes Series\n\u00b6\n\n\nAdd packages: \nzoo\n and \nxts\n.\n\n\nBasics\n\n\nplot(AirPassengers, type = 'l')\n\n\n\n\n\n\nChange the \ntype =\n\n\ny1 <- rnorm(100)\n\npar(mfrow = c(2, 1))\n\nplot(y1, type = 'p', main = 'p vs l')\nplot(y1, type = 'l')\n\n\n\n\n\n\nplot(y1, type = 'l', main = 'l vs h')\nplot(y1, type = 'h')\n\n\n\n\n\n\nplot(y1, type = 'l', lty = 3, main = 'l 3 vs o')\nplot(y1, type = 'o')\n\n\n\n\n\n\nplot(y1, type = 'b', main = 'b vs c')\nplot(y1, type = 'c')\n\n\n\n\n\n\nplot(y1, type = 's', main = 's vs S')\nplot(y1, type = 'S')\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1, 1))\n\n\n\n\nAdd a box\n\n\ny1 <- rnorm(100)\ny2 <- rnorm(100)\n\npar(mfrow = (c(2, 1)))\n\nplot(y1, type = 'l', axes = FALSE, xlab = '', ylab = '', main = '')\n\nbox(col = 'gray')\n\nlines(x = c(20, 20, 40, 40), y = c(-7, max(y1), max(y1), -7), lwd = 3, col = 'gray')\n\nplot(y2, type = 'l', axes = FALSE, xlab = '', ylab = '', main = '')\n\nbox(col = 'gray')\n\nlines(x = c(20, 20, 40, 40), y = c(7, min(y2), min(y2), 7), lwd = 3, col = 'gray')\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1))\n\n\n\n\nAdd lines and text within the plot\n\n\ny1 <- rnorm(100)\n\n# x goes from 0 to 100\n# xaxt = 'n' remove the x ticks\nplot(y1, type = 'l', lwd = 2, lty = 'longdash', main = 'Title', ylab = 'y', xlab = 'time', xaxt = 'n')\n\nabline(h = 0, lty = 'longdash')\n\nabline(v = 20, lty = 'longdash')\nabline(v = 50, lty = 'longdash')\nabline(v = 95, lty = 'longdash')\n\ntext(17, 1.5, srt = 90, adj = 0, labels = 'Tag 1', cex = 0.8)\ntext(47, 1.5, srt = 90, adj = 0, labels = 'Tag a', cex = 0.8)\ntext(92, 1.5, srt = 90, adj = 0, labels = 'Tag alpha', cex = 0.8)\n\n\n\n\n\n\nA comprehensive example\n\n\n# new data\nhead(Orange)\n\n\n\n\n##   Tree  age circumference\n## 1    1  118            30\n## 2    1  484            58\n## 3    1  664            87\n## 4    1 1004           115\n## 5    1 1231           120\n## 6    1 1372           142\n\n\n\n# convert factor to numeric for convenience\nOrange$Tree <- as.numeric(Orange$Tree)\nntrees <- max(Orange$Tree)\n\n# get the range for the x and y axis\nxrange <- range(Orange$age)\nyrange <- range(Orange$circumference)\n\n# set up the plot\nplot(xrange, yrange, type = 'n', xlab = 'Age (days)',\n   ylab = 'Circumference (mm)' )\ncolors <- rainbow(ntrees)\nlinetype <- c(1:ntrees)\nplotchar <- seq(18, 18 + ntrees, 1)\n\n# add lines\nfor (i in 1:ntrees) {\n  tree <- subset(Orange, Tree == i)\n  lines(tree$age, tree$circumference, type = 'b', lwd = 1.5,\n    lty = linetype[i], col = colors[i], pch = plotchar[i])\n}\n\n# add a title and subtitle\ntitle('Tree Growth', 'example of line plot')\n\n# add a legend\nlegend(xrange[1], yrange[2], 1:ntrees, cex = 0.8, col = colors,\n   pch = plotchar, lty = linetype, title = 'Tree')\n\n\n\n\n\n\nChange \nlty =\n\n\n\n\nRegressions and Residual Plots\n\u00b6\n\n\n# first\nregr <- lm(mpg ~ hp)\n\nsummary(regr)\n\n\n\n\n## \n## Call:\n## lm(formula = mpg ~ hp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7121 -2.1122 -0.8854  1.5819  8.2360 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 30.09886    1.63392  18.421  < 2e-16 ***\n## hp          -0.06823    0.01012  -6.742 1.79e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.863 on 30 degrees of freedom\n## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 \n## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n\n\nplot(mpg ~ hp)\nabline(regr)\n\n\n\n\n\n\npar(mfrow = c(2, 2))\n\n# then\nplot(regr)\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1, 1))\n\n\n\n\nThe \nlattice\n and \nlatticeExtra\n Packages\n\u00b6\n\n\nlibrary(lattice)\n\n\n\n\nColoring\n\u00b6\n\n\n# Show the default settings\nshow.settings()\n\n\n\n\n\n\n# Save the default theme\nmytheme <- trellis.par.get()\n\n# Turn the B&W\ntrellis.par.set(canonical.theme(color = FALSE))\nshow.settings()\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\n\n\nNational Park Service, Advanced\n\n    Graphics (Lattice)\n\n\nTreillis\n\n    Plots\n\n\n\n\nA note on reordering the levels (factors)\n\u00b6\n\n\n# start\ncyl <- mtcars$cyl\ncyl <- as.factor(cyl)\ncyl\n\n\n\n\n##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n## Levels: 4 6 8\n\n\n\nlevels(cyl)\n\n\n\n\n## [1] \"4\" \"6\" \"8\"\n\n\n\n# option 1\ncyl <- factor(cyl, levels = c('8', '6', '4'))\n# or levels = 3:1\n# or levels = letters[3:1]\nlevels(cyl)\n\n\n\n\n## [1] \"8\" \"6\" \"4\"\n\n\n\ncyl <- mtcars$cyl\ncyl <- as.factor(cyl)\n# option 2\ncyl <- reorder(cyl, new.order = 3:1)\nlevels(cyl)\n\n\n\n\n## [1] \"8\" \"6\" \"4\"\n\n\n\nlibrary(lattice)\n\n# normalized x-axis for comparison\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2))\n\n\n\n\n\n\n# free x-axis\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2), scales = list(x = 'free'))\n\n\n\n\n\n\n# or\nbc.titanic <- barchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2), scales = list(x = 'free'))\n\nbc.titanic\n\n\n\n\n\n\n# add bg grid\nupdate(bc.titanic, panel = function(...) {\n  panel.grid(h = 0, v = -1)\n  panel.barchart(...)\n})\n\n\n\n\n\n\n# remove lines\nupdate(bc.titanic, panel = function(...) {\n  panel.barchart(..., border = 'transparent')\n})\n\n\n\n\n\n\n# or\nupdate(bc.titanic, border = 'transparent')\n\n\n\n\n\n\nTitanic1 <- as.data.frame(as.table(Titanic[, , 'Adult' ,]))\nTitanic1\n\n\n\n\n##    Class    Sex Survived Freq\n## 1    1st   Male       No  118\n## 2    2nd   Male       No  154\n## 3    3rd   Male       No  387\n## 4   Crew   Male       No  670\n## 5    1st Female       No    4\n## 6    2nd Female       No   13\n## 7    3rd Female       No   89\n## 8   Crew Female       No    3\n## 9    1st   Male      Yes   57\n## 10   2nd   Male      Yes   14\n## 11   3rd   Male      Yes   75\n## 12  Crew   Male      Yes  192\n## 13   1st Female      Yes  140\n## 14   2nd Female      Yes   80\n## 15   3rd Female      Yes   76\n## 16  Crew Female      Yes   20\n\n\n\nbarchart(Class ~ Freq | Sex, Titanic1, groups = Survived, stack = TRUE, auto.key = list(title = 'Survived', columns = 2))\n\n\n\n\n\n\nTitanic2 <- reshape(Titanic1, direction = 'wide', v.names = 'Freq', idvar = c('Class', 'Sex'), timevar = 'Survived')\n\nnames(Titanic2) <- c('Class', 'Sex', 'Dead', 'Alive')\n\nbarchart(Class ~ Dead + Alive | Sex, Titanic2, stack = TRUE, auto.key = list(columns = 2))\n\n\n\n\n\n\nUni-, Bi-, Multivariate Plots\n\u00b6\n\n\nBarchart\n\n\nLike \nbarplot()\n.\n\n\n# y ~ x\nbarchart(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\n# y ~ x\nbarchart(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', horizontal = FALSE)\n\n\n\n\n\n\nbarchart(VADeaths, groups = FALSE, layout = c(1, 4), aspect = 0.7, reference =FALSE, main = 'Title', xlab = 'rate per 100')\n\n\n\n\n\n\ndata(postdoc, package = 'latticeExtra')\n\nbarchart(prop.table(postdoc, margin = 1), xlab = 'Proportion', auto.key = list(adj = 1))\n\n\n\n\n\n\nChange \nlayout = c(x, y, page)\n\n\nbarchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'horsepowers', ylab = 'cylinders - miles per gallon', layout = c(1,3))\n\n\n\n\n\n\nbarchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'cylinders - horsepowers', ylab = 'miles per gallon', layout = c(3,1))\n\n\n\n\n\n\nChange \naspect = 1\n\n\n1\n for square.\n\n\nbarchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1)\n\n\n\n\n\n\nColors\n\n\nbarchart(mpg ~ hp, group = cyl, auto.key = list(space = 'right'), main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\n\n\nshingle()\n; control the ranges.\n\n\nequal.count()\n; grid.\n\n\n\n\nDotplot\n\n\nLike \ndotchart()\n.\n\n\ndotplot(mpg, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(3,1))\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3)\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3, origin = 0)\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3, origin = 0, type = c('p', 'h'))\n\n\n\n\n\n\nSet \nauto.key\n.\n\n\n# maybe we'll want this later\nold.pars <- trellis.par.get()\n\n#trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 12:14))\n\ntrellis.par.set(superpose.symbol = list(pch = c(1,3), col = 1))\n\n# Optionally put things back how they were\n#trellis.par.set(old.pars)\n\n\n\n\nUse \nauto.key\n.\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), groups = vs, auto.key = list(space = 'right'))\n\n\n\n\n\n\ntrellis.par.set(old.pars)\n\n\n\n\ntrellis.par.set(superpose.symbol = list(pch = c(1,3), col = 1))\n\ndotplot(variety ~ yield | site, barley, layout = c(1, 6), aspect = c(0.7), groups = year, auto.key = list(space = 'right'))\n\n\n\n\n\n\ntrellis.par.set(old.pars)\n\n\n\n\nVertical.\n\n\ndotplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'cylinders', ylab = 'gearbox - miles per gallon', layout = c(1,3), aspect = 0.3)\n\n\n\n\n\n\nlibrary(readr)\ndensity <- read_csv('density.csv')\ndensity$Density <- as.numeric(density$Density)\n\ndotplot(reorder(MetropolitanArea, Density) ~ Density, density, type = c('p', 'h'), main = 'Title', xlab = 'Population Density (pop / sq.mi)')\n\n\n\n\n\n\ndotplot(reorder(MetropolitanArea, Density) ~ Density | Region, density, type = c('p', 'h'), strip = FALSE, strip.left = TRUE, layout = c(1, 3), scales = list(y = list(relation = 'free')), main = 'Title', xlab = 'Population Density (pop / sq.mi)')\n\n\n\n\n\n\nStripplot\n\n\nLike \nstripchart()\n.\n\n\nstripplot(mpg, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\nstripplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\nstripplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(1,3))\n\n\n\n\n\n\nstripplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(1,3), groups = vs, auto.key = list(space = 'right'))\n\n\n\n\n\n\nstripplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'cylinders', ylab = 'gearbox - miles per gallon', layout = c(1,3))\n\n\n\n\n\n\nHistogram\n\n\nLike \nhist()\n.\n\n\nhistogram(mpg, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\nhistogram(~mpg | factor(cyl), layout = c(1, 3), main = 'Title', xlab = 'miles per gallon', ylab = 'density')\n\n\n\n\n\n\nDensityplot\n\n\nLike \nplot.density()\n.\n\n\ndensityplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'density')\n\n\n\n\n\n\ndensityplot(~mpg | factor(cyl), layout = c(1, 3), main = 'Title', xlab = 'miles per gallon', ylab = 'density')\n\n\n\n\n\n\nECDFplot\n\n\nlibrary(latticeExtra)\n\necdfplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = '')\n\n\n\n\n\n\nBWplot\n\n\nLike \nboxplot\n.\n\n\nbwplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'density')\n\n\n\n\n\n\nbwplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\nbwplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3))\n\n\n\n\n\n\nbwplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'gearbox - cylinders', ylab = 'miles per gallon', layout = c(3,1))\n\n\n\n\n\n\nQQmath\n\n\nLike \nqqnorm()\n.\n\n\nqqmath(mpg, main = 'Title', ylab = 'miles per gallon')\n\n\n\n\n\n\nXYplot\n\n\nLike \nplot()\n.\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'horsepower', ylab = 'cylinders - miles per gallon', layout = c(1,3))\n\n\n\n\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1))\n\n\n\n\n\n\nXYplot options\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1)\n\n\n\n\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, scales = list(y = list(at = seq(10, 30, 10))))\n\n\n\n\n\n\nmeanmpg <- mean(mpg)\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, panel = function(...) {\n  panel.xyplot(...)\n  panel.abline(h = meanmpg, lty = 'dashed')\n  panel.text(450, meanmpg + 1, 'avg', adj = c(1,  0), cex = 0.7)\n})\n\n\n\n\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, panel = function(x, y, ...) {\n    panel.lmline(x, y)\n    panel.xyplot(x, y, ...)\n})\n\n\n\n\n\n\n\n\npanel.points()\n.\n\n\npanel.lines()\n.\n\n\npanel.segments()\n.\n\n\npanel.arrows()\n.\n\n\npanel.rect()\n.\n\n\npanel.polygon()\n.\n\n\npanel.text()\n.\n\n\npanel.abline()\n.\n\n\npanel.lmline()\n.\n\n\npanel.xyplot()\n.\n\n\npanel.curve()\n.\n\n\npanel.rug()\n.\n\n\npanel.grid()\n.\n\n\npanel.bwplot()\n.\n\n\npanel.histogram()\n.\n\n\npanel.loess()\n.\n\n\npanel.violin()\n.\n\n\npanel.smoothScatter()\n.\n\n\n\u2026\n\n\npar.settings\n.\n\n\n\u2026\n\n\n\n\n\n\n\nlibrary(lattice)\n\ndata(SeatacWeather, package = 'latticeExtra')\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'l', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'p', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'l', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'o', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'r', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'g', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 's', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'S', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'h', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'a', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'smooth', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\nxyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', type = 'o')\n\n\n\n\n\n\nxyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', type = 'o', pch = 16, lty = 'dashed')\n\n\n\n\n\n\nxyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\ndata(USAge.df, package = 'latticeExtra')\n\nxyplot(Population ~ Age | factor(Year), USAge.df, groups = Sex, type = c('l', 'g'), auto.key = list(points = FALSE, lines = TRUE, columns = 2), aspect = 'xy', ylab = 'Population (millions)', subset = Year %in% seq(1905, 1975, by = 10))\n\n\n\n\n\n\nxyplot(Population ~ Year | factor(Age), USAge.df, groups = Sex, type = 'l', strip = FALSE, strip.left = TRUE, layout = c(1, 3), ylab = 'Population (millions)', auto.key = list(lines = TRUE, points = FALSE, columns = 2), subset = Age %in% c(0, 10, 20))\n\n\n\n\n\n\ndata(USCancerRates, package = 'latticeExtra')\n\nxyplot(rate.male ~ rate.female | state, USCancerRates, aspect = 'iso', pch = '.', cex = 2, index.cond = function(x, y) { median(y - x, na.rm = TRUE) }, scales = list(log = 2, at = c(75, 150, 300, 600)), panel = function(...) { \n  panel.grid(h = -1, v = -1)\n  panel.abline(0, 1)\n  panel.xyplot(...)\n  },\n  xlab = 'a',\n  ylab = 'b')\n\n\n\n\n\n\ndata(biocAccess, package = 'latticeExtra')\n\nbaxy <- xyplot(log10(counts) ~ hour | month + weekday, biocAccess, type = c('p', 'a'), as.table = TRUE, pch = '.', cex = 2, col.line = 'black')\n\nbaxy\n\n\n\n\n\n\nlibrary(latticeExtra)\nuseOuterStrips(baxy)\n\n\n\n\n\n\nxyplot(sunspot.year, aspect = 'xy', strip = FALSE, strip.left = TRUE, cut = list(number = 4, overlap = 0.05))\n\n\n\n\n\n\ndata(biocAccess, package = 'latticeExtra')\n\nssd <- stl(ts(biocAccess$counts[1:(24 * 30 *2)], frequency = 24), 'periodic')\n\nxyplot(ssd, main = 'Title', xlab = 'Time (Days)')\n\n\n\n\n\n\nSplom\n\n\nsplom(mtcars[c(1, 3, 6)], groups = cyl, data = mtcars, panel = panel.superpose, key = list(title = 'Three Cylinder Options', columns = 3, points = list(text = list(c('4 Cylinder', '6 Cylinder', '8 Cylinder')))))\n\n\n\n\n\n\ntrellis.par.set(superpose.symbol = list(pch = c(1,3, 22), col = 1, alpha = 0.5))\n\nsplom(~data.frame(mpg, disp, hp, drat, wt, qsec), data = mtcars, groups = cyl, pscales = 0, varnames = c('miles\\nper\\ngallon', 'displacement\\n(cu.in(', 'horsepower', 'rear\\naxle\\nratio', 'weight', '1/4\\nmile\\ntime'), auto.key = list(columns = 3, title = 'Title'))\n\n\n\n\n\n\ntrellis.par.set(old.pars)\n\n\n\n\nsplom(USArrests)\n\n\n\n\n\n\nsplom(~USArrests[c(3,1,2,4)] | state.region, pscales = 0, type = c('g', 'p', 'smooth'))\n\n\n\n\n\n\nParallel plot\n\n\nFor multivariate continuous data.\n\n\nparallelplot(~iris[1:4])\n\n\n\n\n\n\nparallelplot(~iris[1:4], horizontal.axis = FALSE)\n\n\n\n\n\n\nparallelplot(~iris[1:4], scales = list(x = list(rot = 90)))\n\n\n\n\n\n\nparallelplot(~iris[1:4] | Species, iris)\n\n\n\n\n\n\nparallelplot(~iris[1:4], iris, groups = Species,\n             horizontal.axis = FALSE, scales = list(x = list(rot = 90)))\n\n\n\n\n\n\nTrivariate plots\n\n\nLike \nimage()\n, \ncontour()\n, \nfilled.contour()\n, \npersp()\n, \nsymbols()\n.\n\n\n\n\nlevelplot()\n.\n\n\ncontourplot()\n.\n\n\ncloud()\n.\n\n\nwireframe()\n.\n\n\n\n\nAdditional Packages\n\u00b6\n\n\nThe \nsm\n Package (density)\n\u00b6\n\n\nlibrary(sm)\n\n\n\n\nDensity plot\n\n\n# create value labels\ncyl.f <- factor(cyl, levels = c(4, 6, 8), labels = c('4 cyl', '6 cyl', '8 cyl'))\n\n# plot densities\nsm.density.compare(mpg, cyl, xlab = 'miles per gallon')\n\ntitle(main = 'Title')\n\n# add legend via mouse click\ncolfill <- c(2:(2 + length(levels(cyl.f))))\nlegend(25, 0.19, levels(cyl.f), fill = colfill) \n\n\n\n\n\n\nThe \ncar\n Package (scatter)\n\u00b6\n\n\nlibrary(car)\n\n\n\n\nScatter plot\n\n\nscatterplot(mpg ~ wt | cyl, data = mtcars,    xlab = 'weight', ylab = 'miles per gallon', labels = row.names(mtcars)) \n\n\n\n\n\n\nSplom\n\n\nscatterplotMatrix( ~mpg + disp + drat + wt | cyl, data = mtcars, main = 'Title')\n\n\n\n\n\n\nscatterplotMatrix == spm\n.\n\n\nspm( ~mpg + disp + drat + wt | cyl, data = mtcars, main = 'Title')\n\n\n\n\n\n\nThe \nvioplot\n Package (boxplot)\n\u00b6\n\n\nlibrary(vioplot)\n\n\n\n\nViolin boxplot\n\n\nx1 <- mpg[mtcars$cyl == 4]\nx2 <- mpg[mtcars$cyl == 6]\nx3 <- mpg[mtcars$cyl == 8]\n\nvioplot(x1, x2, x3, names = c('4 cyl', '6 cyl', '8 cyl'), col = 'green')\n\ntitle('Title')\n\n\n\n\n\n\nThe \nvcd\n Package (count, correlation, mosaic)\n\u00b6\n\n\nlibrary(vcd)\n\n\n\n\nThe package provides a variety of methods for visualizing multivariate\n\ncategorical data.\n\n\nCount\n\n\ncounts <- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2\n\n\n\nmosaic(counts, shade = TRUE, legend = TRUE) \n\n\n\n\n\n\nCorrelation\n\n\ncounts <- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2\n\n\n\nassoc(counts, shade = TRUE)\n\n\n\n\n\n\nMosaic\n\n\nucb <- data.frame(UCBAdmissions)\nucb <- within(ucb, Accept <- factor(Admit, levels = c('Rejected', 'Admitted')))\n\nlibrary(vcd); library(grid)\n\ndoubledecker(xtabs(Freq~ Dept + Gender + Accept, data = ucb), gp = gpar(fill = c('grey90', 'steelblue')))\n\n\n\n\n\n\ndata(Fertility, package = 'AER')\n\ndoubledecker(morekids ~ age, data = Fertility, gp = gpar(fill = c('grey90', 'green')), spacing = spacing_equal(0))\n\n\n\n\n\n\ndoubledecker(morekids ~ gender1 + gender2, data = Fertility, gp = gpar(fill = c('grey90', 'green')))\n\n\n\n\n\n\ndoubledecker(morekids ~ age + gender1 + gender2, data = Fertility, gp = gpar(fill = c('grey90', 'green')), spacing = spacing_dimequal(c(0.1, 0, 0, 0)))\n\n\n\n\n\n\nThe \nhexbin\n Package (scatter)\n\u00b6\n\n\nlibrary(hexbin)\n\n\n\n\nScatter plot\n\n\n# new data\ndata(NHANES)\n\n# compare\nplot(Serum.Iron ~ Transferin, NHANES, main = 'Title', xlab = 'Transferin', ylab = 'Iron')\n\n\n\n\n\n\n# with\nhexbinplot(Serum.Iron ~ Transferin, NHANES, main = 'Title', xlab = 'Transferin', ylab = 'Iron')\n\n\n\n\n\n\nhexbinplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\nx <- rnorm(1000)\ny <- rnorm(1000)\n\nbin <- hexbin(x, y, xbins = 50)\nplot(bin, main = 'Title') \n\n\n\n\n\n\nx <- rnorm(1000)\ny <- rnorm(1000)\n\nplot(x, y, main = 'Title', col =  rgb(0, 100, 0, 50, maxColorValue = 255), pch = 16)\n\n\n\n\n\n\ndata(Diamonds, package = 'Stat2Data')\n\na = hexbin(Diamonds$PricePerCt, Diamonds$Carat, xbins = 40)\n\nlibrary(RColorBrewer)\n\nplot(a)\n\n\n\n\n\n\nColors.\n\n\nrf <- colorRampPalette(rev(brewer.pal(12, 'Set3')))\n\nhexbinplot(Diamonds$PricePerCt ~ Diamonds$Carat, colramp = rf)\n\n\n\n\n\n\nMix \nlattice\n and \nhexbin\n\n\ndata(gvhd10, package = 'latticeExtra')\n\nxyplot(asinh(SSC.H) ~ asinh(FL2.H), gvhd10, aspect = 1, panel = panel.hexbinplot, .aspect.ratio = 1, trans = sqrt)\n\n\n\n\n\n\nxyplot(asinh(SSC.H) ~ asinh(FL2.H) | Days, gvhd10, aspect = 1, panel = panel.hexbinplot, .aspect.ratio = 1, trans =sqrt)\n\n\n\n\n\n\nThe \ncar\n Package (scatter)\n\u00b6\n\n\nlibrary(car)\n\n\n\n\nScatter plot\n\n\nscatterplotMatrix(~mpg + disp + drat + wt | cyl, data = mtcars,\n   main = 'Three Cylinder Options')\n\n\n\n\n\n\nThe \nscatterplot3d\n Package\n\u00b6\n\n\nlibrary(scatterplot3d)\n\n\n\n\nScatter plot\n\n\nscatterplot3d(wt, disp, mpg, main = 'Title')\n\n\n\n\n\n\nscatterplot3d(wt, disp, mpg, pch = 16, highlight.3d = TRUE, type = 'h', main = 'Title')\n\n\n\n\n\n\ns3d <- scatterplot3d(wt, disp, mpg, pch = 16, highlight.3d = TRUE, type = 'h', main = '   Title')\n\nfit <- lm(mpg ~ wt + disp)\n\ns3d$plane3d(fit)\n\n\n\n\n\n\nThe \nrgl\n Package (interactive)\n\u00b6\n\n\nlibrary(rgl)\n\n\n\n\nInteractive plot\n\n\nThe plot will open a new window.\n\n\nplot3d(wt, disp, mpg, col = 'red', size = 3)\n\n\n\n\nThe \ncluster\n Package (dendrogram)\n\u00b6\n\n\nlibrary(cluster)\n\n\n\n\nDendrogram\n\n\nUse the \niris\n dataset.\n\n\nsubset <- sample(1:150, 20)\ncS <- as.character(Sp <- iris$Species[subset])\ncS\n\n\n\n\n##  [1] \"setosa\"     \"versicolor\" \"setosa\"     \"virginica\"  \"virginica\" \n##  [6] \"setosa\"     \"setosa\"     \"setosa\"     \"virginica\"  \"setosa\"    \n## [11] \"versicolor\" \"versicolor\" \"virginica\"  \"setosa\"     \"versicolor\"\n## [16] \"versicolor\" \"setosa\"     \"virginica\"  \"versicolor\" \"versicolor\"\n\n\n\ncS[Sp == 'setosa'] <- 'S'\ncS[Sp == 'versicolor'] <- 'V'\ncS[Sp == 'virginica'] <- 'g'\n\nai <- agnes(iris[subset, 1:4])\n\nplot(ai, label = cS)\n\n\n\n\n\n\nThe \nextracat\n Package (splom)\n\u00b6\n\n\nlibrary(extracat)\n\n\n\n\nSplom\n\n\nFor missing values. Binary matrix with reordering and filtering of rows\n\nand columns. The x-axis shows the frequency of NA. The y-axis shows the\n\nmarginal distribution of NA.\n\n\n# example 1\ndata(CHAIN, package = 'mi')\n\nvisna(CHAIN, sort = 'b')\n\n\n\n\n\n\nsummary(CHAIN)\n\n\n\n\n##    log_virus           age            income          healthy     \n##  Min.   : 0.000   Min.   :21.00   Min.   : 1.000   Min.   :16.67  \n##  1st Qu.: 0.000   1st Qu.:37.00   1st Qu.: 2.000   1st Qu.:35.00  \n##  Median : 0.000   Median :43.00   Median : 3.000   Median :45.37  \n##  Mean   : 4.324   Mean   :42.56   Mean   : 3.377   Mean   :44.40  \n##  3rd Qu.: 9.105   3rd Qu.:48.00   3rd Qu.: 5.000   3rd Qu.:54.89  \n##  Max.   :13.442   Max.   :70.00   Max.   :10.000   Max.   :70.11  \n##  NA's   :179      NA's   :24      NA's   :38       NA's   :24     \n##      mental           damage        treatment     \n##  Min.   :0.0000   Min.   :1.000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:0.0000  \n##  Median :0.0000   Median :4.000   Median :1.0000  \n##  Mean   :0.2717   Mean   :3.578   Mean   :0.8602  \n##  3rd Qu.:1.0000   3rd Qu.:5.000   3rd Qu.:2.0000  \n##  Max.   :1.0000   Max.   :5.000   Max.   :2.0000  \n##  NA's   :24       NA's   :63      NA's   :24\n\n\n\n# example 2\ndata(oly12, package = 'VGAMdata')\n\noly12d <- oly12[, names(oly12) != 'DOB']\noly12a <- oly12\n\nnames(oly12a) <- abbreviate(names(oly12), 3)\n\nvisna(oly12a, sort = 'b')\n\n\n\n\n\n\n# example 3\ndata(freetrade, package = 'Amelia')\n\nfreetrade <- within(freetrade, land1 <- reorder(country, tariff, function(x) sum(is.na(x))))\n\nfluctile(xtabs(is.na(tariff) ~ land1 + year, data = freetrade))\n\n\n\n\n\n\n## viewport[base]\n\n\n\n# example 4\ndata(Pima.tr2, package = 'MASS')\n\nvisna(Pima.tr2, sort = 'b')\n\n\n\n\n\n\nThe \nash\n Package (density)\n\u00b6\n\n\nlibrary(ash)\n\n\n\n\nDensity plot\n\n\nplot(ash1(bin1(mtcars$mpg, nbin = 50)), type = 'l')\n\n\n\n\n## [1] \"ash estimate nonzero outside interval ab\"\n\n\n\n\n\nThe \nKernSmooth\n Package (density)\n\u00b6\n\n\nlibrary(KernSmooth)\n\n\n\n\nDensity plot\n\n\nwith(mtcars, {\n  hist(mpg, freq = FALSE, main = '', col = 'bisque2', ylab = '')\n  lines(density(mpg), lwd = 2)\n  ks1 <- bkde(mpg, bandwidth = dpik(mpg))\n  lines(ks1, col = 'red', lty = 5, lwd = 2)})\n\n\n\n\n\n\nThe \ncorrplot\n Package (correlation)\n\u00b6\n\n\nlibrary(corrplot)\n\n\n\n\nSplom\n\n\n# Create a correlation matrix for the dataset (9-14 are the '2' variables only)\ncorrelations <- cor(mtcars)\n\ncorrplot(correlations)",
            "title": "Plot Snippets for Exploratory (and some Explanatory) Analyses"
        },
        {
            "location": "/Plot_snippets_-_Basics/#data-type-dataset",
            "text": "",
            "title": "Data Type &amp; Dataset"
        },
        {
            "location": "/Plot_snippets_-_Basics/#data-types",
            "text": "continuous vs categorical (or discrete).  continuous: float, x-y-z, 3D, map coordinates, trianguar, lat-long, \n    polar, degree-distance, angle-vector.  categorical: integer, binary, dichotomic, dummy, factor, \n    ordinal (ordered).   Continuous variable characteristics:   asymmetry.  outliers.  multimodality.  gaps, missing values.  heaping, redundance.  rounding, integer.  impossibilities, anomalies.  errors.  \u2026   Categorical variable characteristics:   unexpected pattern of results.  uneven distribution.  extra categories.  unbalanced experiments.  large numbers of categories.  NA, errors, missings\u2026  nominal: no fixed order.  ordinal: fixed order (scale of 1 to 5).  discrete: counts, integers.  dependencies, correlation, associations.  causal relationships, outliers, groups, clusters, gaps, barriers, \n    conditional relationship.  \u2026   Univariate main plots:   histogram.  density.  qqmath chart.  box & whickers chart.  bar chart.  dot.   Bivariate main plots:   xy chart.  qq chart.   Trivariate main plots:   cloud.  wireframe.  countour.  level.   Multivariate main plots:   sploms.  parallel charts (coordinate).   Specialized plots:   frequencies, crosstabs: bar charts, mosaic plots, association plots.  correlations: sploms, pairs, correlograms.  t-tests, non-parrametric tests of group differences: box plot, \n    density plot.  regression: scatter plot.  ANOVA: box plots, line plots.",
            "title": "Data Types"
        },
        {
            "location": "/Plot_snippets_-_Basics/#functions",
            "text": "Create a new variable  iris2 <- within(iris, area <- Petal.Width*Petal.Length)\nhead(iris2, 3)  ##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species area\n## 1          5.1         3.5          1.4         0.2  setosa 0.28\n## 2          4.9         3.0          1.4         0.2  setosa 0.28\n## 3          4.7         3.2          1.3         0.2  setosa 0.26  area <- with(iris, area <- Petal.Width*Petal.Length)\nhead(area, 3)  ## [1] 0.28 0.28 0.26",
            "title": "Functions"
        },
        {
            "location": "/Plot_snippets_-_Basics/#dataset",
            "text": "For most examples, we use the  mtcars  dataset.  Prepare the dataset.  attach(mtcars)  Get data attached to a package (an example).  data(gvhd10, package = 'latticeExtra')",
            "title": "Dataset"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-basic-package",
            "text": "",
            "title": "The Basic Package"
        },
        {
            "location": "/Plot_snippets_-_Basics/#basic-plots-options-parameters",
            "text": "Standardize the parameters (an example)  # color and tick mark text orientation\npar(col = 'black', las = 1)  Grid and layout  One plot.  plot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')   A grid of plots.  par(mfrow = c(2, 1))\n\nplot(mpg, hp, ylab = 'horsepower', xlab = 'miles per gallon')\nboxplot(mpg ~ cyl, xlab = 'mile per gallon', ylab = 'number of cylinders', horizontal = TRUE)\n\npar(mfrow = c(1, 2))\n\nplot(mpg, hp, ylab = 'horsepower', xlab = 'miles per gallon')\nboxplot(mpg ~ cyl, xlab = 'mile per gallon', ylab = 'number of cylinders', horizontal = TRUE)   par(mfrow = c(1, 1))  Other grids.  layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))\n\nplot(mpg, xlab = 'observations', ylab = 'miles per gallon')\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\nboxplot(mpg ~ cyl, ylab = 'mile per gallon', xlab = 'number of cylinders')   # view\nmatrix(c(1,2,1,3), 2, 2, byrow = TRUE)  ##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    3  layout(matrix(c(1,2,1,3), 2, 2, byrow = TRUE))\n\nhist(wt)\nhist(mpg)\nhist(disp)   layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE),  widths = c(3,1), heights = c(1,2))\n\nhist(wt)\nhist(mpg)\nhist(disp)   nf <- layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths = lcm(12), heights = lcm(6))\nlayout.show(nf)   plot(mpg, xlab = 'observations', ylab = 'miles per gallon')\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\nboxplot(mpg ~ cyl, ylab = 'mile per gallon', xlab = 'number of cylinders')   Gridview with additional packages.  library(vcd)  mplot(A, B, C)   See the  lattice  and  latticeExtra  packages for built-in \nfacet/gridview.  ggplot2  as well.  Plot and add ablines  plot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\n# abline(h = yvalues, v = xvalues)\nabline(lm(mpg ~ hp))\n\n# main = 'Title' or...\ntitle('Title')   plot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\nabline(h = c(20, 25))\nabline(v = c(50, 150))\nabline(v = seq(200, 300, 50), lty = 2, col = 'blue')   Add a legend  boxplot(mpg ~ cyl, main = 'Title',\n   yaxt = 'n', xlab = 'mile per gallon', horizontal = TRUE, col = terrain.colors(3))\n\nlegend('topright', inset = 0.05, title = 'number of cylinders', c('4','6','8'), fill = terrain.colors(3), horiz = TRUE)   Save  mygraph <- plot(hp, mpg, main = 'Title', xlab = 'horsepower', ylab = 'miles per gallon')\n\npdf('mygraph.pdf')\npng('mygraph.png')\njpeg('mygraph.jpg')\nbmp('mygraph.bmp')\npostscript('mygraph.ps')  View in a new window  Typing the function will open a new window to render the plot.   windows()  for Windows.  X11()  for Linux.  quartz()  for OS X.    # open the new windows\nwindows()\n\nplot(hp, mpg, main = 'Title', xlab = 'horsepower', ylab = 'miles per gallon')  Enrich the plot, add text  plot(hp, mpg,\n     main = 'Title', col.main = 'blue',\n     sub = 'figure 1', col.sub = 'blue',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     col.lab = 'red', cex.lab = 0.9,\n     xlim = c(50, 350),\n     ylim = c(0, 40))\n\ntext(100, 10, 'text 1') # x and y coordinate\nmtext('text 2', 4, line = 0.5) # pos = 1 (bottom), 2 (left), 3 (top), 4 (right); line (margin)   With  locator() , use the mouse; with 1 for 1 click, 2 for\u2026 Find the \ncoordinates to be entered in the code. For example (after two clicks):  > locator(2)\n$x\n[1] 212.5308 293.7854\n\n$y\n[1] 33.34040 31.87281  plot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\ntext(hp, mpg, row.names(mtcars), cex = 0.7, pos = 4, col = 'red')   Enrich the plot, add symbols  plot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\nsymbols(250, 20, squares = 1, add = TRUE, inches = 0.1, fg = 'red')\nsymbols(250, 25, circles = 1, add = TRUE, inches = 0.1, fg = 'red')   #rectangles\n#stars\n#thermometers\n#boxplots  Combine plots; change  pch =  &  col =  par(mfrow = c(2,2))\n\n# 1\nplot(hp, mpg,\n     main = 'P1',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 1,\n     col = 'black')\n\n# 2\nplot(hp, mpg,\n     main = 'P2',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 3,\n     col = 'blue',\n     cex = 0.5)\n\n# 3\nplot(hp, mpg,\n     main = 'P3',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 5,\n     col = 'red',\n     cex = 2)\n\n# 4\nplot(hp, mpg,\n     main = 'P4',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 7,\n     col = 'green')   # reverse\npar(mfrow = c(1,1))  Change  col =   Change  pch =   par(fig = c(0,0.8,0,0.8))\n\nplot(mtcars$wt, mtcars$mpg, xlab = 'Car Weight',   ylab = 'miles Per Gallon')\n\npar(fig = c(0,0.8,0.55,1), new = TRUE)\n\nboxplot(mtcars$wt, horizontal = TRUE, axes = FALSE)\n\npar(fig = c(0.65,1,0,0.8), new = TRUE)\n\nboxplot(mtcars$mpg, axes = FALSE)\n\nmtext('Enhanced Scatterplot', side = 3, outer = TRUE, line = -3)   # reverse\npar(mfrow = c(1,1))  Change  type = ; without dots  x <- c(1:5); y <- x\n\npar(pch = 22, col = 'red') # plotting symbol and color\n\npar(mfrow = c(2,4)) # all plots on one page\nopts = c('p','l','o','b','c','s','S','h')\n\nfor (i in 1:length(opts)) {\n  heading = paste('type =',opts[i])\n  plot(x, y, type = 'n', main = heading)\n  lines(x, y, type = opts[i])\n}   # reverse\npar(mfrow = c(1,1), col = 'black')  Change  type = ; with dots  x <- c(1:5); y <- x\n\npar(pch = 22, col = 'blue') # plotting symbol and color\n\npar(mfrow = c(2,4)) # all plots on one page\nopts = c('p','l','o','b','c','s','S','h')\n\nfor (i in 1:length(opts)) {\n  heading = paste('type =',opts[i])\n  plot(x, y, main = heading)\n  lines(x, y, type = opts[i])\n}   # reverse\npar(mfrow = c(1,1), col = 'black')  Add or modify the axes  plot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     xaxt = 'n',\n     yaxt = 'n')\n\naxis(1, at = c(100, 200, 300), labels = NULL, pos = 15, lty = 'dashed', col = 'green', las = 2, tck = -0.05)\n\naxis(4, at = c(20, 30), labels = c('bt', 'up'), pos = 125, lty = 'dashed', col = 'blue', las = 2, tck = -0.05)   # reverse\npar(las = 1)  Add layers to the first plot  plot(mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\n# add lines\nlines(mpg[1:10], type = 'l', col = 'green')",
            "title": "Basic Plots, Options &amp; Parameters"
        },
        {
            "location": "/Plot_snippets_-_Basics/#univariate-plots",
            "text": "Plot; continuous  plot(mpg, main = 'Title', xlab = 'observations', ylab = 'miles per gallon')   Plot; categorical  plot(cyl, main = 'Title', xlab = 'observations', ylab = 'cylinders')   QQnorm; continuous  qqnorm(mpg, main = 'Title', xlab = 'observations', ylab = 'cylinders')   QQnorm; categorical  qqnorm(cyl, main = 'Title', xlab = 'observations', ylab = 'cylinders')   Stripchart; continuous  stripchart(mpg, main = 'Title', xlab = 'miles per gallon')   Stripchart; categorical  stripchart(cyl, main = 'Title', xlab = 'cylinders')   Barplot (vertical); continuous  barplot(mpg[1:10], main = 'Title', xlab = 'observations', ylab = 'miles per gallon')   Barplot (horizontal); categorical  barplot(cyl[1:10], main = 'Title', horiz = TRUE, xlab = 'cylinders', ylab = 'observations')   Barplots options  Group with  table() .  counts <- table(cyl)\ncounts  ## cyl\n##  4  6  8 \n## 11  7 14  barplot(counts, main = 'Title', horiz = TRUE, xlab = 'count', names.arg = c('4 Cyl', '6 Cyl', '8 Cyl'))   counts <- table(vs, gear)\ncounts  ##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1  barplot(counts, main = 'Title', xlab = 'gearbox', col = c('darkblue', 'red'), legend = rownames(counts))    counts <- table(vs, gear)\ncounts  ##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1  barplot(counts, main = 'Title', xlab='gearbox', col = c('darkblue', 'red'), legend =  rownames(counts), beside = TRUE)   Group with  aggregate() .  aggregate(mtcars, by = list(cyl, vs), FUN = mean, na.rm = TRUE)  ##   Group.1 Group.2      mpg cyl   disp       hp     drat       wt     qsec\n## 1       4       0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000\n## 2       6       0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667\n## 3       8       0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214\n## 4       4       1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100\n## 5       6       1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500\n##   vs        am     gear     carb\n## 1  0 1.0000000 5.000000 2.000000\n## 2  0 1.0000000 4.333333 4.666667\n## 3  0 0.1428571 3.285714 3.500000\n## 4  1 0.7000000 4.000000 1.500000\n## 5  1 0.0000000 3.500000 2.500000  par(las = 2) # make label text perpendicular to axis\n\npar(mar = c(5, 8, 4, 2)) # increase y-axis margin.\n\ncounts <- table(mtcars$gear)\nbarplot(counts, main = 'Car Distribution', horiz = TRUE, names.arg = c('3 Gears', '4 Gears', '5   Gears'), cex.names = 0.8)   # reverse\npar(las = 1)  Colors.  library(RColorBrewer)\n\npar(mfrow = c(2, 1))\n\nbarplot(iris$Petal.Length)\nbarplot(table(iris$Species, iris$Sepal.Length), col = brewer.pal(3, 'Set1'))   par(mfrow = c(1, 1))  Pie Chart  Avoid!  Dotchart; continuous  dotchart(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'observations')   Dotchart; categorical  dotchart(cyl, main = 'Title', xlab = 'cylinders', ylab = 'observations')   Dotchart options  dotchart(mpg,labels = row.names(mtcars), cex = 0.7, main = 'Title', xlab = 'miles per gallon')   # sort by mpg\nx <- mtcars[order(mpg),]\n\n# must be factors\nx$cyl <- factor(x$cyl)\nx$color[x$cyl == 4] <- 'red'\nx$color[x$cyl == 6] <- 'blue'\nx$color[x$cyl == 8] <- 'darkgreen'\n\ndotchart(x$mpg, labels = row.names(x), cex = 0.7, groups = x$cyl, main = 'Title',  xlab = 'miles per gallon', gcolor = 'black', color = x$color)   More with the  hmisc  package and  panel.dotplot()  and in the  lattice \npackage section.  Boxplot; continuous  boxplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'observations')   Stem; continuous  stem(mpg)  ## \n##   The decimal point is at the |\n## \n##   10 | 44\n##   12 | 3\n##   14 | 3702258\n##   16 | 438\n##   18 | 17227\n##   20 | 00445\n##   22 | 88\n##   24 | 4\n##   26 | 03\n##   28 | \n##   30 | 44\n##   32 | 49  Histogram; continuous  hist(mpg, main = 'Title', xlab = 'miles per gallon - bins', ylab = 'count')   Histogram; categorical  hist(cyl, main = 'Title', xlab = 'cylinders - bins', ylab = 'count')   Histogram options  hist(mpg, breaks = 12, col = 'red')   x <- mpg\n\nh <- hist(x, breaks = 10, main = 'Title', xlab = 'miles per gallon')\n\nxfit <- seq(min(x), max(x),length = 40)\nyfit <- dnorm(xfit, mean = mean(x), sd = sd(x))\nyfit <- yfit*diff(h$mids[1:2])*length(x)\n\nlines(xfit, yfit, col = 'blue', lwd = 2)   Colors.  library(RColorBrewer)\n\npar(mfrow = c(2, 3))\n\nhist(VADeaths, breaks = 10, col = brewer.pal(3, 'Set3'), main = '3, Set3')\nhist(VADeaths, breaks = 4, col = brewer.pal(3, 'Set2'), main = '3, Set2')\nhist(VADeaths, breaks = 8, col = brewer.pal(3, 'Set1'), main = '3, Set1')\nhist(VADeaths, breaks = 2, col = brewer.pal(8, 'Set3'), main = '8, Set3')\nhist(VADeaths, breaks = 10, col = brewer.pal(8, 'Greys'), main = '8, Greys')\nhist(VADeaths, breaks = 10, col = brewer.pal(8, 'Greens'), main = '8, Greens')   par(mfrow = c(1, 1))  Density Plot; continuous  plot(density(mpg), main = 'Title')   plot(density(mpg), main = 'Title')\n\npolygon(density(mpg), col = 'red', border = 'blue')    d1 <- density(mtcars$mpg)\nplot(d1)\nrug(mtcars$mpg)\n\nlines(density(mtcars$mpg, d1$bw/2), col = 'green')\nlines(density(mtcars$mpg, d1$bw/5), col = 'blue')",
            "title": "Univariate Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#bivariate-multivariate-plots",
            "text": "Plot, continuous/continuous  plot(mpg, hp, main = 'Title', xlab = 'miles per gallon', ylab = 'horsepowers')   Plot, continuous/categorical  plot(mpg, cyl, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   Plot options  plot(wt, mpg, main = 'Title', xlab = 'weight', ylab = 'miles per gallon ')\n\nabline(lm(mpg ~ wt), col = 'red') # regression\nlines(lowess(wt, mpg), col = 'blue') # lowess line   SmoothScatter; continuous/continuous  smoothScatter(mpg, hp, main = 'Title', xlab = 'miles per gallon', ylab = 'horsepowers')   Sunflowerplot; categorical/categorical  Special symbols at each location: one observation = one dot; more \nobservations = cross, star, etc.  sunflowerplot(gear, cyl, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')   Boxplot  boxplot(mpg ~ cyl, main = 'Title',   xlab = 'cylinders', ylab = 'miles per gallon')    Colors.  library(RColorBrewer)\n\npar(mfrow = c(1, 2))\n\nboxplot(iris$Sepal.Length, col = 'red')\nboxplot(iris$Sepal.Length ~ iris$Species, col = topo.colors(3))   par(mfrow = c(1, 1))  library(dplyr)\n\ndata(Pima.tr2, package = 'MASS')\n\nPimaV <- select(Pima.tr2, glu:age)\nboxplot(scale(PimaV), pch = 16, outcol = 'red')   Boxplot options  four <- subset(mpg, cyl == 4)\nsix <- subset(mpg, cyl == 6)\neight <- subset(mpg, cyl == 8)\n\nboxplot(four, six, eight, main = 'Title', ylab = 'miles per gallon')\n\naxis(1, at = c(1, 2, 3), labels = c('4 Cyl', '6 Cyl', '8 Cyl'))   Dotchart  counts <- table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  dotchart(counts, main = 'Title', xlab = 'count', ylab = 'cylinders/gearbox')   counts <- table(cyl, gear)\ncounts  ##    gear\n## cyl  3  4  5\n##   4  1  8  2\n##   6  2  4  1\n##   8 12  0  2  dotchart(counts, main = 'Title', xlab = 'count', ylab = 'gearbox/cylinders')   Barplot with its options  Vertical or horizontal. The legend as well can be horizontal or \nvertical.  counts <- table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  barplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 20), col = terrain.colors(3))\n\nlegend('topleft', inset = .04, title = 'gearbox',\n   c('3','4','5'), fill = terrain.colors(3), horiz = TRUE)   counts <- table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  barplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 25), col = terrain.colors(3), legend = rownames(counts))   counts <- table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  barplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 20), col = terrain.colors(3), legend = rownames(counts), beside = TRUE)   Spineplot  \u2018Count\u2019 = blocks; categorical (with factors).  cyl2 <- as.factor(cyl) # mandatory for the y\ngear2 <- as.factor(gear)\n\nspineplot(gear2, cyl2, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')   Count = blocks; continuous.  spineplot(mpg, cyl2, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   Mosaicplot  Count = blocks.  counts <- table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  mosaicplot(counts, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')",
            "title": "Bivariate (Multivariate) Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#multivariate-plots",
            "text": "Pairs  pairs( ~mpg + disp + hp)   Coplot  coplot(mpg ~ hp | wt)   Correlograms  library(corrgram)\n\ncorrgram(mtcars, order = TRUE, lower.panel = panel.shade, upper.panel=panel.pie, text.panel = panel.txt, main = 'Car Milage Data in PC2/PC1 Order')   Plot a dataset with colors  library(RColorBrewer)\n\nplot(iris, col = brewer.pal(3, 'Set1'))   Stars  The star branches are explanatory; be careful with the interpretation! \nWell-advised for visual and pattern exploration.  mtcars[1:4, c(1, 4, 6)]  ##                 mpg  hp    wt\n## Mazda RX4      21.0 110 2.620\n## Mazda RX4 Wag  21.0 110 2.875\n## Datsun 710     22.8  93 2.320\n## Hornet 4 Drive 21.4 110 3.215  stars(mtcars[1:4, c(1, 4, 6)])   Trivariate plots   image() .  contour() .  filled.contour() .  persp() .  symbols() .",
            "title": "Multivariate Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#times-series",
            "text": "Add packages:  zoo  and  xts .  Basics  plot(AirPassengers, type = 'l')   Change the  type =  y1 <- rnorm(100)\n\npar(mfrow = c(2, 1))\n\nplot(y1, type = 'p', main = 'p vs l')\nplot(y1, type = 'l')   plot(y1, type = 'l', main = 'l vs h')\nplot(y1, type = 'h')   plot(y1, type = 'l', lty = 3, main = 'l 3 vs o')\nplot(y1, type = 'o')   plot(y1, type = 'b', main = 'b vs c')\nplot(y1, type = 'c')   plot(y1, type = 's', main = 's vs S')\nplot(y1, type = 'S')   # reverse\npar(mfrow = c(1, 1))  Add a box  y1 <- rnorm(100)\ny2 <- rnorm(100)\n\npar(mfrow = (c(2, 1)))\n\nplot(y1, type = 'l', axes = FALSE, xlab = '', ylab = '', main = '')\n\nbox(col = 'gray')\n\nlines(x = c(20, 20, 40, 40), y = c(-7, max(y1), max(y1), -7), lwd = 3, col = 'gray')\n\nplot(y2, type = 'l', axes = FALSE, xlab = '', ylab = '', main = '')\n\nbox(col = 'gray')\n\nlines(x = c(20, 20, 40, 40), y = c(7, min(y2), min(y2), 7), lwd = 3, col = 'gray')   # reverse\npar(mfrow = c(1,1))  Add lines and text within the plot  y1 <- rnorm(100)\n\n# x goes from 0 to 100\n# xaxt = 'n' remove the x ticks\nplot(y1, type = 'l', lwd = 2, lty = 'longdash', main = 'Title', ylab = 'y', xlab = 'time', xaxt = 'n')\n\nabline(h = 0, lty = 'longdash')\n\nabline(v = 20, lty = 'longdash')\nabline(v = 50, lty = 'longdash')\nabline(v = 95, lty = 'longdash')\n\ntext(17, 1.5, srt = 90, adj = 0, labels = 'Tag 1', cex = 0.8)\ntext(47, 1.5, srt = 90, adj = 0, labels = 'Tag a', cex = 0.8)\ntext(92, 1.5, srt = 90, adj = 0, labels = 'Tag alpha', cex = 0.8)   A comprehensive example  # new data\nhead(Orange)  ##   Tree  age circumference\n## 1    1  118            30\n## 2    1  484            58\n## 3    1  664            87\n## 4    1 1004           115\n## 5    1 1231           120\n## 6    1 1372           142  # convert factor to numeric for convenience\nOrange$Tree <- as.numeric(Orange$Tree)\nntrees <- max(Orange$Tree)\n\n# get the range for the x and y axis\nxrange <- range(Orange$age)\nyrange <- range(Orange$circumference)\n\n# set up the plot\nplot(xrange, yrange, type = 'n', xlab = 'Age (days)',\n   ylab = 'Circumference (mm)' )\ncolors <- rainbow(ntrees)\nlinetype <- c(1:ntrees)\nplotchar <- seq(18, 18 + ntrees, 1)\n\n# add lines\nfor (i in 1:ntrees) {\n  tree <- subset(Orange, Tree == i)\n  lines(tree$age, tree$circumference, type = 'b', lwd = 1.5,\n    lty = linetype[i], col = colors[i], pch = plotchar[i])\n}\n\n# add a title and subtitle\ntitle('Tree Growth', 'example of line plot')\n\n# add a legend\nlegend(xrange[1], yrange[2], 1:ntrees, cex = 0.8, col = colors,\n   pch = plotchar, lty = linetype, title = 'Tree')   Change  lty =",
            "title": "Times Series"
        },
        {
            "location": "/Plot_snippets_-_Basics/#regressions-and-residual-plots",
            "text": "# first\nregr <- lm(mpg ~ hp)\n\nsummary(regr)  ## \n## Call:\n## lm(formula = mpg ~ hp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7121 -2.1122 -0.8854  1.5819  8.2360 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 30.09886    1.63392  18.421  < 2e-16 ***\n## hp          -0.06823    0.01012  -6.742 1.79e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.863 on 30 degrees of freedom\n## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 \n## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07  plot(mpg ~ hp)\nabline(regr)   par(mfrow = c(2, 2))\n\n# then\nplot(regr)   # reverse\npar(mfrow = c(1, 1))",
            "title": "Regressions and Residual Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-lattice-and-latticeextra-packages",
            "text": "library(lattice)",
            "title": "The lattice and latticeExtra Packages"
        },
        {
            "location": "/Plot_snippets_-_Basics/#coloring",
            "text": "# Show the default settings\nshow.settings()   # Save the default theme\nmytheme <- trellis.par.get()\n\n# Turn the B&W\ntrellis.par.set(canonical.theme(color = FALSE))\nshow.settings()",
            "title": "Coloring"
        },
        {
            "location": "/Plot_snippets_-_Basics/#documentation",
            "text": "National Park Service, Advanced \n    Graphics (Lattice)  Treillis \n    Plots",
            "title": "Documentation"
        },
        {
            "location": "/Plot_snippets_-_Basics/#a-note-on-reordering-the-levels-factors",
            "text": "# start\ncyl <- mtcars$cyl\ncyl <- as.factor(cyl)\ncyl  ##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n## Levels: 4 6 8  levels(cyl)  ## [1] \"4\" \"6\" \"8\"  # option 1\ncyl <- factor(cyl, levels = c('8', '6', '4'))\n# or levels = 3:1\n# or levels = letters[3:1]\nlevels(cyl)  ## [1] \"8\" \"6\" \"4\"  cyl <- mtcars$cyl\ncyl <- as.factor(cyl)\n# option 2\ncyl <- reorder(cyl, new.order = 3:1)\nlevels(cyl)  ## [1] \"8\" \"6\" \"4\"  library(lattice)\n\n# normalized x-axis for comparison\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2))   # free x-axis\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2), scales = list(x = 'free'))   # or\nbc.titanic <- barchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2), scales = list(x = 'free'))\n\nbc.titanic   # add bg grid\nupdate(bc.titanic, panel = function(...) {\n  panel.grid(h = 0, v = -1)\n  panel.barchart(...)\n})   # remove lines\nupdate(bc.titanic, panel = function(...) {\n  panel.barchart(..., border = 'transparent')\n})   # or\nupdate(bc.titanic, border = 'transparent')   Titanic1 <- as.data.frame(as.table(Titanic[, , 'Adult' ,]))\nTitanic1  ##    Class    Sex Survived Freq\n## 1    1st   Male       No  118\n## 2    2nd   Male       No  154\n## 3    3rd   Male       No  387\n## 4   Crew   Male       No  670\n## 5    1st Female       No    4\n## 6    2nd Female       No   13\n## 7    3rd Female       No   89\n## 8   Crew Female       No    3\n## 9    1st   Male      Yes   57\n## 10   2nd   Male      Yes   14\n## 11   3rd   Male      Yes   75\n## 12  Crew   Male      Yes  192\n## 13   1st Female      Yes  140\n## 14   2nd Female      Yes   80\n## 15   3rd Female      Yes   76\n## 16  Crew Female      Yes   20  barchart(Class ~ Freq | Sex, Titanic1, groups = Survived, stack = TRUE, auto.key = list(title = 'Survived', columns = 2))   Titanic2 <- reshape(Titanic1, direction = 'wide', v.names = 'Freq', idvar = c('Class', 'Sex'), timevar = 'Survived')\n\nnames(Titanic2) <- c('Class', 'Sex', 'Dead', 'Alive')\n\nbarchart(Class ~ Dead + Alive | Sex, Titanic2, stack = TRUE, auto.key = list(columns = 2))",
            "title": "A note on reordering the levels (factors)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#uni-bi-multivariate-plots",
            "text": "Barchart  Like  barplot() .  # y ~ x\nbarchart(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')   # y ~ x\nbarchart(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', horizontal = FALSE)   barchart(VADeaths, groups = FALSE, layout = c(1, 4), aspect = 0.7, reference =FALSE, main = 'Title', xlab = 'rate per 100')   data(postdoc, package = 'latticeExtra')\n\nbarchart(prop.table(postdoc, margin = 1), xlab = 'Proportion', auto.key = list(adj = 1))   Change  layout = c(x, y, page)  barchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'horsepowers', ylab = 'cylinders - miles per gallon', layout = c(1,3))   barchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'cylinders - horsepowers', ylab = 'miles per gallon', layout = c(3,1))   Change  aspect = 1  1  for square.  barchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1)   Colors  barchart(mpg ~ hp, group = cyl, auto.key = list(space = 'right'), main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')    shingle() ; control the ranges.  equal.count() ; grid.   Dotplot  Like  dotchart() .  dotplot(mpg, main = 'Title', xlab = 'miles per gallon')   dotplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(3,1))   dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3)   dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3, origin = 0)   dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3, origin = 0, type = c('p', 'h'))   Set  auto.key .  # maybe we'll want this later\nold.pars <- trellis.par.get()\n\n#trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 12:14))\n\ntrellis.par.set(superpose.symbol = list(pch = c(1,3), col = 1))\n\n# Optionally put things back how they were\n#trellis.par.set(old.pars)  Use  auto.key .  dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), groups = vs, auto.key = list(space = 'right'))   trellis.par.set(old.pars)  trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 1))\n\ndotplot(variety ~ yield | site, barley, layout = c(1, 6), aspect = c(0.7), groups = year, auto.key = list(space = 'right'))   trellis.par.set(old.pars)  Vertical.  dotplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'cylinders', ylab = 'gearbox - miles per gallon', layout = c(1,3), aspect = 0.3)   library(readr)\ndensity <- read_csv('density.csv')\ndensity$Density <- as.numeric(density$Density)\n\ndotplot(reorder(MetropolitanArea, Density) ~ Density, density, type = c('p', 'h'), main = 'Title', xlab = 'Population Density (pop / sq.mi)')   dotplot(reorder(MetropolitanArea, Density) ~ Density | Region, density, type = c('p', 'h'), strip = FALSE, strip.left = TRUE, layout = c(1, 3), scales = list(y = list(relation = 'free')), main = 'Title', xlab = 'Population Density (pop / sq.mi)')   Stripplot  Like  stripchart() .  stripplot(mpg, main = 'Title', xlab = 'miles per gallon')   stripplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   stripplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(1,3))   stripplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(1,3), groups = vs, auto.key = list(space = 'right'))   stripplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'cylinders', ylab = 'gearbox - miles per gallon', layout = c(1,3))   Histogram  Like  hist() .  histogram(mpg, main = 'Title', xlab = 'miles per gallon')   histogram(~mpg | factor(cyl), layout = c(1, 3), main = 'Title', xlab = 'miles per gallon', ylab = 'density')   Densityplot  Like  plot.density() .  densityplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'density')   densityplot(~mpg | factor(cyl), layout = c(1, 3), main = 'Title', xlab = 'miles per gallon', ylab = 'density')   ECDFplot  library(latticeExtra)\n\necdfplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = '')   BWplot  Like  boxplot .  bwplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'density')   bwplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   bwplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3))   bwplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'gearbox - cylinders', ylab = 'miles per gallon', layout = c(3,1))   QQmath  Like  qqnorm() .  qqmath(mpg, main = 'Title', ylab = 'miles per gallon')   XYplot  Like  plot() .  xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'horsepower', ylab = 'cylinders - miles per gallon', layout = c(1,3))   xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1))   XYplot options  xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1)   xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, scales = list(y = list(at = seq(10, 30, 10))))   meanmpg <- mean(mpg)\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, panel = function(...) {\n  panel.xyplot(...)\n  panel.abline(h = meanmpg, lty = 'dashed')\n  panel.text(450, meanmpg + 1, 'avg', adj = c(1,  0), cex = 0.7)\n})   xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, panel = function(x, y, ...) {\n    panel.lmline(x, y)\n    panel.xyplot(x, y, ...)\n})    panel.points() .  panel.lines() .  panel.segments() .  panel.arrows() .  panel.rect() .  panel.polygon() .  panel.text() .  panel.abline() .  panel.lmline() .  panel.xyplot() .  panel.curve() .  panel.rug() .  panel.grid() .  panel.bwplot() .  panel.histogram() .  panel.loess() .  panel.violin() .  panel.smoothScatter() .  \u2026  par.settings .  \u2026    library(lattice)\n\ndata(SeatacWeather, package = 'latticeExtra')\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'l', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'p', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'l', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'o', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'r', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'g', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 's', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'S', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'h', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'a', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'smooth', lty = 1, col = 'black')   xyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')   xyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', type = 'o')   xyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', type = 'o', pch = 16, lty = 'dashed')   xyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')   data(USAge.df, package = 'latticeExtra')\n\nxyplot(Population ~ Age | factor(Year), USAge.df, groups = Sex, type = c('l', 'g'), auto.key = list(points = FALSE, lines = TRUE, columns = 2), aspect = 'xy', ylab = 'Population (millions)', subset = Year %in% seq(1905, 1975, by = 10))   xyplot(Population ~ Year | factor(Age), USAge.df, groups = Sex, type = 'l', strip = FALSE, strip.left = TRUE, layout = c(1, 3), ylab = 'Population (millions)', auto.key = list(lines = TRUE, points = FALSE, columns = 2), subset = Age %in% c(0, 10, 20))   data(USCancerRates, package = 'latticeExtra')\n\nxyplot(rate.male ~ rate.female | state, USCancerRates, aspect = 'iso', pch = '.', cex = 2, index.cond = function(x, y) { median(y - x, na.rm = TRUE) }, scales = list(log = 2, at = c(75, 150, 300, 600)), panel = function(...) { \n  panel.grid(h = -1, v = -1)\n  panel.abline(0, 1)\n  panel.xyplot(...)\n  },\n  xlab = 'a',\n  ylab = 'b')   data(biocAccess, package = 'latticeExtra')\n\nbaxy <- xyplot(log10(counts) ~ hour | month + weekday, biocAccess, type = c('p', 'a'), as.table = TRUE, pch = '.', cex = 2, col.line = 'black')\n\nbaxy   library(latticeExtra)\nuseOuterStrips(baxy)   xyplot(sunspot.year, aspect = 'xy', strip = FALSE, strip.left = TRUE, cut = list(number = 4, overlap = 0.05))   data(biocAccess, package = 'latticeExtra')\n\nssd <- stl(ts(biocAccess$counts[1:(24 * 30 *2)], frequency = 24), 'periodic')\n\nxyplot(ssd, main = 'Title', xlab = 'Time (Days)')   Splom  splom(mtcars[c(1, 3, 6)], groups = cyl, data = mtcars, panel = panel.superpose, key = list(title = 'Three Cylinder Options', columns = 3, points = list(text = list(c('4 Cylinder', '6 Cylinder', '8 Cylinder')))))   trellis.par.set(superpose.symbol = list(pch = c(1,3, 22), col = 1, alpha = 0.5))\n\nsplom(~data.frame(mpg, disp, hp, drat, wt, qsec), data = mtcars, groups = cyl, pscales = 0, varnames = c('miles\\nper\\ngallon', 'displacement\\n(cu.in(', 'horsepower', 'rear\\naxle\\nratio', 'weight', '1/4\\nmile\\ntime'), auto.key = list(columns = 3, title = 'Title'))   trellis.par.set(old.pars)  splom(USArrests)   splom(~USArrests[c(3,1,2,4)] | state.region, pscales = 0, type = c('g', 'p', 'smooth'))   Parallel plot  For multivariate continuous data.  parallelplot(~iris[1:4])   parallelplot(~iris[1:4], horizontal.axis = FALSE)   parallelplot(~iris[1:4], scales = list(x = list(rot = 90)))   parallelplot(~iris[1:4] | Species, iris)   parallelplot(~iris[1:4], iris, groups = Species,\n             horizontal.axis = FALSE, scales = list(x = list(rot = 90)))   Trivariate plots  Like  image() ,  contour() ,  filled.contour() ,  persp() ,  symbols() .   levelplot() .  contourplot() .  cloud() .  wireframe() .",
            "title": "Uni-, Bi-, Multivariate Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#additional-packages",
            "text": "",
            "title": "Additional Packages"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-sm-package-density",
            "text": "library(sm)  Density plot  # create value labels\ncyl.f <- factor(cyl, levels = c(4, 6, 8), labels = c('4 cyl', '6 cyl', '8 cyl'))\n\n# plot densities\nsm.density.compare(mpg, cyl, xlab = 'miles per gallon')\n\ntitle(main = 'Title')\n\n# add legend via mouse click\ncolfill <- c(2:(2 + length(levels(cyl.f))))\nlegend(25, 0.19, levels(cyl.f), fill = colfill)",
            "title": "The sm Package (density)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-car-package-scatter",
            "text": "library(car)  Scatter plot  scatterplot(mpg ~ wt | cyl, data = mtcars,    xlab = 'weight', ylab = 'miles per gallon', labels = row.names(mtcars))    Splom  scatterplotMatrix( ~mpg + disp + drat + wt | cyl, data = mtcars, main = 'Title')   scatterplotMatrix == spm .  spm( ~mpg + disp + drat + wt | cyl, data = mtcars, main = 'Title')",
            "title": "The car Package (scatter)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-vioplot-package-boxplot",
            "text": "library(vioplot)  Violin boxplot  x1 <- mpg[mtcars$cyl == 4]\nx2 <- mpg[mtcars$cyl == 6]\nx3 <- mpg[mtcars$cyl == 8]\n\nvioplot(x1, x2, x3, names = c('4 cyl', '6 cyl', '8 cyl'), col = 'green')\n\ntitle('Title')",
            "title": "The vioplot Package (boxplot)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-vcd-package-count-correlation-mosaic",
            "text": "library(vcd)  The package provides a variety of methods for visualizing multivariate \ncategorical data.  Count  counts <- table(gear, cyl)\ncounts  ##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2  mosaic(counts, shade = TRUE, legend = TRUE)    Correlation  counts <- table(gear, cyl)\ncounts  ##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2  assoc(counts, shade = TRUE)   Mosaic  ucb <- data.frame(UCBAdmissions)\nucb <- within(ucb, Accept <- factor(Admit, levels = c('Rejected', 'Admitted')))\n\nlibrary(vcd); library(grid)\n\ndoubledecker(xtabs(Freq~ Dept + Gender + Accept, data = ucb), gp = gpar(fill = c('grey90', 'steelblue')))   data(Fertility, package = 'AER')\n\ndoubledecker(morekids ~ age, data = Fertility, gp = gpar(fill = c('grey90', 'green')), spacing = spacing_equal(0))   doubledecker(morekids ~ gender1 + gender2, data = Fertility, gp = gpar(fill = c('grey90', 'green')))   doubledecker(morekids ~ age + gender1 + gender2, data = Fertility, gp = gpar(fill = c('grey90', 'green')), spacing = spacing_dimequal(c(0.1, 0, 0, 0)))",
            "title": "The vcd Package (count, correlation, mosaic)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-hexbin-package-scatter",
            "text": "library(hexbin)  Scatter plot  # new data\ndata(NHANES)\n\n# compare\nplot(Serum.Iron ~ Transferin, NHANES, main = 'Title', xlab = 'Transferin', ylab = 'Iron')   # with\nhexbinplot(Serum.Iron ~ Transferin, NHANES, main = 'Title', xlab = 'Transferin', ylab = 'Iron')   hexbinplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')   x <- rnorm(1000)\ny <- rnorm(1000)\n\nbin <- hexbin(x, y, xbins = 50)\nplot(bin, main = 'Title')    x <- rnorm(1000)\ny <- rnorm(1000)\n\nplot(x, y, main = 'Title', col =  rgb(0, 100, 0, 50, maxColorValue = 255), pch = 16)   data(Diamonds, package = 'Stat2Data')\n\na = hexbin(Diamonds$PricePerCt, Diamonds$Carat, xbins = 40)\n\nlibrary(RColorBrewer)\n\nplot(a)   Colors.  rf <- colorRampPalette(rev(brewer.pal(12, 'Set3')))\n\nhexbinplot(Diamonds$PricePerCt ~ Diamonds$Carat, colramp = rf)   Mix  lattice  and  hexbin  data(gvhd10, package = 'latticeExtra')\n\nxyplot(asinh(SSC.H) ~ asinh(FL2.H), gvhd10, aspect = 1, panel = panel.hexbinplot, .aspect.ratio = 1, trans = sqrt)   xyplot(asinh(SSC.H) ~ asinh(FL2.H) | Days, gvhd10, aspect = 1, panel = panel.hexbinplot, .aspect.ratio = 1, trans =sqrt)",
            "title": "The hexbin Package (scatter)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-car-package-scatter_1",
            "text": "library(car)  Scatter plot  scatterplotMatrix(~mpg + disp + drat + wt | cyl, data = mtcars,\n   main = 'Three Cylinder Options')",
            "title": "The car Package (scatter)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-scatterplot3d-package",
            "text": "library(scatterplot3d)  Scatter plot  scatterplot3d(wt, disp, mpg, main = 'Title')   scatterplot3d(wt, disp, mpg, pch = 16, highlight.3d = TRUE, type = 'h', main = 'Title')   s3d <- scatterplot3d(wt, disp, mpg, pch = 16, highlight.3d = TRUE, type = 'h', main = '   Title')\n\nfit <- lm(mpg ~ wt + disp)\n\ns3d$plane3d(fit)",
            "title": "The scatterplot3d Package"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-rgl-package-interactive",
            "text": "library(rgl)  Interactive plot  The plot will open a new window.  plot3d(wt, disp, mpg, col = 'red', size = 3)",
            "title": "The rgl Package (interactive)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-cluster-package-dendrogram",
            "text": "library(cluster)  Dendrogram  Use the  iris  dataset.  subset <- sample(1:150, 20)\ncS <- as.character(Sp <- iris$Species[subset])\ncS  ##  [1] \"setosa\"     \"versicolor\" \"setosa\"     \"virginica\"  \"virginica\" \n##  [6] \"setosa\"     \"setosa\"     \"setosa\"     \"virginica\"  \"setosa\"    \n## [11] \"versicolor\" \"versicolor\" \"virginica\"  \"setosa\"     \"versicolor\"\n## [16] \"versicolor\" \"setosa\"     \"virginica\"  \"versicolor\" \"versicolor\"  cS[Sp == 'setosa'] <- 'S'\ncS[Sp == 'versicolor'] <- 'V'\ncS[Sp == 'virginica'] <- 'g'\n\nai <- agnes(iris[subset, 1:4])\n\nplot(ai, label = cS)",
            "title": "The cluster Package (dendrogram)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-extracat-package-splom",
            "text": "library(extracat)  Splom  For missing values. Binary matrix with reordering and filtering of rows \nand columns. The x-axis shows the frequency of NA. The y-axis shows the \nmarginal distribution of NA.  # example 1\ndata(CHAIN, package = 'mi')\n\nvisna(CHAIN, sort = 'b')   summary(CHAIN)  ##    log_virus           age            income          healthy     \n##  Min.   : 0.000   Min.   :21.00   Min.   : 1.000   Min.   :16.67  \n##  1st Qu.: 0.000   1st Qu.:37.00   1st Qu.: 2.000   1st Qu.:35.00  \n##  Median : 0.000   Median :43.00   Median : 3.000   Median :45.37  \n##  Mean   : 4.324   Mean   :42.56   Mean   : 3.377   Mean   :44.40  \n##  3rd Qu.: 9.105   3rd Qu.:48.00   3rd Qu.: 5.000   3rd Qu.:54.89  \n##  Max.   :13.442   Max.   :70.00   Max.   :10.000   Max.   :70.11  \n##  NA's   :179      NA's   :24      NA's   :38       NA's   :24     \n##      mental           damage        treatment     \n##  Min.   :0.0000   Min.   :1.000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:0.0000  \n##  Median :0.0000   Median :4.000   Median :1.0000  \n##  Mean   :0.2717   Mean   :3.578   Mean   :0.8602  \n##  3rd Qu.:1.0000   3rd Qu.:5.000   3rd Qu.:2.0000  \n##  Max.   :1.0000   Max.   :5.000   Max.   :2.0000  \n##  NA's   :24       NA's   :63      NA's   :24  # example 2\ndata(oly12, package = 'VGAMdata')\n\noly12d <- oly12[, names(oly12) != 'DOB']\noly12a <- oly12\n\nnames(oly12a) <- abbreviate(names(oly12), 3)\n\nvisna(oly12a, sort = 'b')   # example 3\ndata(freetrade, package = 'Amelia')\n\nfreetrade <- within(freetrade, land1 <- reorder(country, tariff, function(x) sum(is.na(x))))\n\nfluctile(xtabs(is.na(tariff) ~ land1 + year, data = freetrade))   ## viewport[base]  # example 4\ndata(Pima.tr2, package = 'MASS')\n\nvisna(Pima.tr2, sort = 'b')",
            "title": "The extracat Package (splom)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-ash-package-density",
            "text": "library(ash)  Density plot  plot(ash1(bin1(mtcars$mpg, nbin = 50)), type = 'l')  ## [1] \"ash estimate nonzero outside interval ab\"",
            "title": "The ash Package (density)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-kernsmooth-package-density",
            "text": "library(KernSmooth)  Density plot  with(mtcars, {\n  hist(mpg, freq = FALSE, main = '', col = 'bisque2', ylab = '')\n  lines(density(mpg), lwd = 2)\n  ks1 <- bkde(mpg, bandwidth = dpik(mpg))\n  lines(ks1, col = 'red', lty = 5, lwd = 2)})",
            "title": "The KernSmooth Package (density)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-corrplot-package-correlation",
            "text": "library(corrplot)  Splom  # Create a correlation matrix for the dataset (9-14 are the '2' variables only)\ncorrelations <- cor(mtcars)\n\ncorrplot(correlations)",
            "title": "The corrplot Package (correlation)"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/",
            "text": "Documentation\n\n\nDataset\n\n\nThe \nggplot2\n Package\n\n\nSECTION 1\n\n\n1, Introduction\n\n\n2, Data\n\n\n3, Aesthetics\n\n\n4, Geometries\n\n\n5, qplot and wrap-up\n\n\nSECTION 2\n\n\n1, Statistics\n\n\n2, Coordinates and Facets\n\n\n3, Themes\n\n\n4, Best Practices\n\n\n5, Case Study\n\n\nSECTION 3\n\n\nSECTION 4 - Cheat List\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\n\n\nggplot2\n\n\nggthemes\n\n\nr4stats\n\n\n\n\nDataset\n\u00b6\n\n\nFor most examples, we use the \nmtcars\n, \ndiamonds\n, \niris\n,\n\n\nChickWeight\n, \nrecess\n, \nfish\n, \nVocab\n, \nTitanic\n, \nmamsleep\n,\n\n\nbarley\n, \nadult\n datasets.\n\n\nThe \nggplot2\n Package\n\u00b6\n\n\nlibrary(ggplot2)\n\n\n\n\nImport additional packages.\n\n\nlibrary(digest)\nlibrary(grid)\nlibrary(gtable)\nlibrary(MASS)\nlibrary(plyr)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(stats)\nlibrary(tidyr)\n\n\n\n\nFor this project, import additional packages.\n\n\nlibrary(ggthemes)\nlibrary(RColorBrewer)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(car)\nlibrary(Hmisc)\nlibrary(dplyr)\n\n\n\n\nSuggested additional packages\u2026\n\n\nSECTION 1\n\u00b6\n\n\n1, Introduction\n\u00b6\n\n\nExploring \nggplot2\n, part 1\n\n\n# basic plot\nggplot(mtcars, aes(x = cyl, y = mpg)) +\n  geom_point()\n\n\n\n\n\n\nExploring \nggplot2\n, part 2\n\n\n# cyl is a factor\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_point()\n\n\n\n\n\n\nExploring \nggplot2\n, part 3\n\n\n# scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\n\n\n# add color\nggplot(mtcars, aes(x = wt, y = mpg, col = disp)) +\n  geom_point()\n\n\n\n\n\n\n# change size\nggplot(mtcars, aes(x = wt, y = mpg, size = disp)) +\n  geom_point()\n\n\n\n\n\n\nExploring \nggplot2\n, part 4\n\n\n# Add geom_point() with +\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_point()\n\n\n\n\n\n\n# Add geom_point() and geom_smooth() with +\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_point() + geom_smooth()\n\n\n\n\n\n\nExploring \nggplot2\n, part 5\n\n\n# only the smooth line\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_smooth()\n\n\n\n\n\n\n# change col\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) + \n  geom_point()\n\n\n\n\n\n\n# change the alpha\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point(alpha = 0.4)\n\n\n\n\n\n\nExploring \nggplot2\n, part 6\n\n\n# 2 facets for comparison\nlibrary(gridExtra)\n\ndata(father.son, package = 'UsingR')\n\na <- ggplot(father.son, aes(fheight, sheight)) +\n  geom_point() +\n  geom_smooth(method = 'lm', colour = 'red') +\n  geom_abline(slope = 1, intercept = 0)\n\nb <- ggplot(father.son, aes(fheight, sheight)) +\n  geom_point() +\n  geom_smooth(method = 'lm', colour = 'red', se = FALSE) +\n  stat_smooth()\n\ngrid.arrange(a, b, nrow = 1)\n\n\n\n\n\n\n# load more data\ndata(oly12, package = 'VGAMdata')\n\n# 2 facets for comparison\nggplot(oly12, aes(Height, Weight)) +\n  geom_point(size = 1) +\n  facet_wrap(~Sex, ncol = 1)\n\n\n\n\n\n\n# create a new variable inside de data frame\noly12S <- within(oly12, oly12$Sport <- abbreviate(oly12$Sport, 12))\n\n# multiple facets or splom\nggplot(oly12S, aes(Height, Weight)) +\n  geom_point(size = 1) +\n  facet_wrap(~Sport) +\n  ggtitle('Weight and Height by Sport') \n\n\n\n\n\n\nUnderstanding the grammar, part 1\n\n\n# create the object containing the data and aes layers\ndia_plot <- ggplot(diamonds, aes(x = Carat, y = PricePerCt))\n\n# add a geom layer\ndia_plot + \n  geom_point()\n\n\n\n\n\n\n# add the same geom layer, but with aes() inside\ndia_plot +\n  geom_point(aes(col = Clarity))\n\n\n\n\n\n\nUnderstanding the grammar, part 2\n\n\nset.seed(1)\n\n# create the object containing the data and aes layers\ndia_plot <- ggplot(diamonds, aes(x = Carat, y = PricePerCt))\n\n# add geom_point() with alpha set to 0.2\ndia_plot <- dia_plot +\n  geom_point(alpha = 0.2)\n\ndia_plot\n\n\n\n\n\n\n# plot dia_plot with additional geom_smooth() with se set to FALSE\ndia_plot +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n2, Data\n\u00b6\n\n\nBase package and \nggplot2\n, part 1 - plot\n\n\n# basic plot\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)\n\n\n\n\n\n\n# change cyl inside mtcars to a factor\nmtcars$cyl <- as.factor(mtcars$cyl)\n\n# make the same plot as in the first instruction\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)\n\n\n\n\n\n\nBase package and \nggplot2\n, part 2 - lm\n\n\ntransfer to other\n\n\n# Basic plot\nmtcars$cyl <- as.factor(mtcars$cyl)\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)\n\n# use lm() to calculate a linear model and save it as carModel\ncarModel <- lm(mpg ~ wt, data = mtcars)\n\n# Call abline() with carModel as first argument and lty as second\nabline(carModel, lty = 2)\n\n# plot each subset efficiently with lapply\nlapply(mtcars$cyl, function(x) {\n  abline(lm(mpg ~ wt, mtcars, subset = (cyl == x)), col = x)\n  })\n\n\n\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n## \n## [[8]]\n## NULL\n## \n## [[9]]\n## NULL\n## \n## [[10]]\n## NULL\n## \n## [[11]]\n## NULL\n## \n## [[12]]\n## NULL\n## \n## [[13]]\n## NULL\n## \n## [[14]]\n## NULL\n## \n## [[15]]\n## NULL\n## \n## [[16]]\n## NULL\n## \n## [[17]]\n## NULL\n## \n## [[18]]\n## NULL\n## \n## [[19]]\n## NULL\n## \n## [[20]]\n## NULL\n## \n## [[21]]\n## NULL\n## \n## [[22]]\n## NULL\n## \n## [[23]]\n## NULL\n## \n## [[24]]\n## NULL\n## \n## [[25]]\n## NULL\n## \n## [[26]]\n## NULL\n## \n## [[27]]\n## NULL\n## \n## [[28]]\n## NULL\n## \n## [[29]]\n## NULL\n## \n## [[30]]\n## NULL\n## \n## [[31]]\n## NULL\n## \n## [[32]]\n## NULL\n\n\n\n# draw the legend of the plot\nlegend(x = 5, y = 33, legend = levels(mtcars$cyl), col = 1:3, pch = 1, bty = 'n')\n\n\n\n\n\n\nBase package and \nggplot2\n, part 3\n\n\n# scatter plot\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) + \n  geom_point()\n\n\n\n\n\n\n# include the lines of the linear models, per cyl\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\n# include a lm for the entire dataset in its whole\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE) +\n  geom_smooth(aes(group = 1), method = 'lm', se = FALSE, linetype = 2)\n\n\n\n\n\n\nVariables to visuals, part 1\n\n\niris.tidy <- iris %>%\n  gather(key, Value, -Species) %>%\n  separate(key, c('Part', 'Measure'), '\\\\.')\n\n# create 2 facets\nggplot(iris.tidy, aes(x = Species, y = Value, col = Part)) +\n  geom_jitter() + facet_grid(. ~ Measure)\n\n\n\n\n\n\nVariables to visuals, part 2\n\n\n# Add a new column, Flower, to iris that contains unique ids\niris$Flower <- 1:nrow(iris)\n\niris.wide <- iris %>%\n  gather(key, value, -Species, -Flower) %>%\n  separate(key, c('Part', 'Measure'), '\\\\.') %>%\n  spread(Measure, value)\n\n# create 3 facets\nggplot(iris.wide, aes(x = Length, y = Width, col = Part)) + \n  geom_jitter() +\n  facet_grid(. ~ Species)\n\n\n\n\n\n\n3, Aesthetics\n\u00b6\n\n\nAll about aesthetics, part 1\n\n\n# map cyl to y\nggplot(mtcars, aes(x = mpg, y = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to x\nggplot(mtcars, aes(y = mpg, x = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to col\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# change shape and size of the points\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(shape = 1, size = 4)\n\n\n\n\n\n\nAll about aesthetics, part 2\n\n\n# map cyl to fill\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# Change shape, size and alpha of the points in the above plot\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(shape = 16, size = 6, alpha = 0.6)\n\n\n\n\n\n\nAll about aesthetics, part 3\n\n\n# map cyl to size\nggplot(mtcars, aes(x = wt, y = mpg, size = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to alpha\nggplot(mtcars, aes(x = wt, y = mpg, alpha = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to shape \nggplot(mtcars, aes(x = wt, y = mpg, shape = cyl, label = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to labels\nggplot(mtcars, aes(x = wt, y = mpg, label = cyl)) +\n  geom_text()\n\n\n\n\n\n\nAll about attributes, part 1\n\n\n# define a hexadecimal color\nmy_color <- '#123456'\n\n# set the color aesthetic \nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# set the color aesthetic and attribute \nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(col = my_color)\n\n\n\n\n\n\n# set the fill aesthetic and color, size and shape attributes\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(size = 10, shape = 23, col = my_color)\n\n\n\n\n\n\nAll about attributes, part 2\n\n\n# draw points with alpha 0.5\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n# raw points with shape 24 and color yellow\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(shape = 24, col = 'yellow')\n\n\n\n\n\n\n# draw text with label x, color red and size 10\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_text(label = 'x', col = 'red', size = 10)\n\n\n\n\n\n\nGoing all out\n\n\n# Map mpg onto x, qsec onto y and factor(cyl) onto col\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl))) +\n  geom_point()\n\n\n\n\n\n\n# Add mapping: factor(am) onto shape\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am))) +\n  geom_point()\n\n\n\n\n\n\n# Add mapping: (hp/wt) onto size\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am), size = hp/wt)) +\n  geom_point()\n\n\n\n\n\n\n# Add mapping: rownames(mtcars) onto label\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am), size = hp/wt)) +\n  geom_text(aes(label = rownames(mtcars)))\n\n\n\n\n\n\nPosition\n\n\n# base layers\ncyl.am <- ggplot(mtcars, aes(x = factor(cyl), fill = factor(am)))\n\n# add geom (position = 'stack'' by default)\ncyl.am + \n  geom_bar(position = 'stack')\n\n\n\n\n\n\n# show proportion\ncyl.am + \n  geom_bar(position = 'fill')\n\n\n\n\n\n\n# dodging\ncyl.am + \n  geom_bar(position = 'dodge')\n\n\n\n\n\n\n# clean up the axes with scale_ functions\nval = c('#E41A1C', '#377EB8')\nlab = c('Manual', 'Automatic')\n\ncyl.am + geom_bar(position = 'dodge', ) +\n  scale_x_discrete('Cylinders') +\n  scale_y_continuous('Number') +\n  scale_fill_manual('Transmission', values = val, labels = lab)\n\n\n\n\n\n\nSetting a dummy aesthetic\n\n\n# add a new column called group\nmtcars$group <- 0\n\n# create jittered plot of mtcars: mpg onto x, group onto y\nggplot(mtcars, aes(x = mpg, y = group)) +   geom_jitter()\n\n\n\n\n\n\n# change the y aesthetic limits\nggplot(mtcars, aes(x = mpg, y = group)) +\n  geom_jitter() +\n  scale_y_continuous(limits = c(-2, 2))\n\n\n\n\n\n\nOverplotting 1 - Point shape and transparency\n\n\n# basic scatter plot: wt on x-axis and mpg on y-axis; map cyl to col\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4)\n\n\n\n\n\n\n# hollow circles - an improvement\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4, shape = 1)\n\n\n\n\n\n\n# add transparency - very nice\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4, shape = 1, alpha = 0.6)\n\n\n\n\n\n\nOverplotting 2 - alpha with large datasets\n\n\n# scatter plot: carat (x), price (y), clarity (col)\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point()\n\n\n\n\n\n\n# adjust for overplotting\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n# scatter plot: clarity (x), carat (y), price (col)\nggplot(diamonds, aes(x = Clarity, y = Carat, col = PricePerCt)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n# dot plot with jittering\nggplot(diamonds, aes(x = Clarity, y = Carat, col = PricePerCt)) +\n  geom_point(alpha = 0.5, position = 'jitter')\n\n\n\n\n\n\n4, Geometries\n\u00b6\n\n\nScatter plots and jittering (1)\n\n\n# plot the cyl on the x-axis and wt on the y-axis\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_point()\n\n\n\n\n\n\n# Use geom_jitter() instead of geom_point()\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_jitter()\n\n\n\n\n\n\n# Define the position object using position_jitter(): posn.j\nposn.j <-  position_jitter(0.1)\n\n# Use posn.j in geom_point()\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_point(position = posn.j)\n\n\n\n\n\n\nScatter plots and jittering (2)\n\n\n# scatter plot of vocabulary (y) against education (x). Use geom_point()\nggplot(Vocab, aes(x = education, y = vocabulary)) + \n  geom_point()\n\n\n\n\n\n\n# use geom_jitter() instead of geom_point()\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter()\n\n\n\n\n\n\n# set alpha to a very low 0.2\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2)\n\n\n\n\n\n\n# set the shape to 1\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2, shape = 1)\n\n\n\n\n\n\nHistograms\n\n\n# univariate histogram\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram()\n\n\n\n\n\n\n# change the bin width to 1\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n# change the y aesthetic to density\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1)\n\n\n\n\n\n\n# custom color code\nmyBlue <- '#377EB8'\n\n# Change the fill color to myBlue\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = myBlue)\n\n\n\n\n\n\nPosition\n\n\nmtcars$am <- as.factor(mtcars$am)\n\n# bar plot of cyl, filled according to am\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar()\n\n\n\n\n\n\n# change the position argument to stack\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'stack')\n\n\n\n\n\n\n# change the position argument to fill\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'fill')\n\n\n\n\n\n\n# change the position argument to dodge\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'dodge')\n\n\n\n\n\n\nOverlapping bar plots\n\n\n# bar plot of cyl, filled according to am\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar()\n\n\n\n\n\n\n# change the position argument to 'dodge'\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'dodge')\n\n\n\n\n\n\n# define posn_d with position_dodge()\nposn_d <- position_dodge(0.2)\n\n# change the position argument to posn_d\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = posn_d)\n\n\n\n\n\n\n# use posn_d as position and adjust alpha to 0.6\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = posn_d, alpha = 0.6)\n\n\n\n\n\n\nOverlapping histograms\n\n\n# histogram, add coloring defined by cyl \nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n# change position to identity \nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(binwidth = 1, position = 'identity')\n\n\n\n\n\n\n# change geom to freqpoly (position is identity by default) \nggplot(mtcars, aes(mpg, col = cyl)) +\n  geom_freqpoly(binwidth = 1)\n\n\n\n\n\n\nFacets or splom histograms\n\n\n# load the package\nlibrary(reshape2)\n\n# load new data\ndata(uniranks, package = 'GDAdata')\n\n# name the variables\nnames(uniranks)[c(5, 6, 8, 8, 10, 11, 13)] <- c('AvTeach', 'NSSTeach', 'SpendperSt', 'StudentStaffR', 'Careers', 'VAddScore', 'NSSFeedb')\n\n# reshape the data frame\nur2 <- melt(uniranks[, c(3, 5:13)], id.vars = 'UniGroup', variable.name = 'uniV', value.name = 'uniX')\n\n\n\n\n# Splom\nggplot(ur2, aes(uniX)) +\n  geom_histogram() +\n  xlab('') +\n  ylab('') +\n  facet_grid(UniGroup ~ uniV, scales = 'free_x')\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\ndata(Pima.tr2, package = 'MASS')\n\nh1 <- ggplot(Pima.tr2, aes(glu)) + geom_histogram()\nh2 <- ggplot(Pima.tr2, aes(bp)) + geom_histogram()\nh3 <- ggplot(Pima.tr2, aes(skin)) + geom_histogram()\nh4 <- ggplot(Pima.tr2, aes(bmi)) + geom_histogram()\nh5 <- ggplot(Pima.tr2, aes(ped)) + geom_histogram()\nh6 <- ggplot(Pima.tr2, aes(age)) + geom_histogram()\n\ngrid.arrange(h1, h2, h3, h4, h5, h6, nrow = 2)\n\n\n\n\n\n\nBar plots with color ramp, part 1\n\n\n# Example of how to use a brewed color palette\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar() + \n  scale_fill_brewer(palette = 'Set1')\n\n\n\n\n\n\nVocab$education <- as.factor(Vocab$education)\nVocab$vocabulary <- as.factor(Vocab$vocabulary)\n\n# Plot education on x and vocabulary on fill\n# Use the default brewed color palette\nggplot(Vocab, aes(x = education, fill = vocabulary)) + geom_bar(position = 'fill') + scale_fill_brewer(palette = 'Set3')\n\n\n\n\n\n\nBar plots with color ramp, part 2\n\n\n# Definition of a set of blue colors\nblues <- brewer.pal(9, 'Blues')\n\n# Make a color range using colorRampPalette() and the set of blues\nblue_range <- colorRampPalette(blues)\n\n# Use blue_range to adjust the color of the bars, use scale_fill_manual()\nggplot(Vocab, aes(x = education, fill = vocabulary)) + \n  geom_bar(position = 'fill') +\n  scale_fill_manual(values = blue_range(11))\n\n\n\n\n\n\nOverlapping histograms (2)\n\n\n# histogram\nggplot(mtcars, aes(mpg)) + geom_histogram(binwidth = 1)\n\n\n\n\n\n\n# expand the histogram to fill using am\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n# change the position argument to 'dodge'\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'dodge', binwidth = 1)\n\n\n\n\n\n\n# change the position argument to 'fill'\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'fill', binwidth = 1)\n\n\n\n\n\n\n# change the position argument to 'identity' and set alpha to 0.4\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'identity', binwidth = 1, alpha = 0.4)\n\n\n\n\n\n\n# change fill to cyl\nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(position = 'identity', binwidth = 1, alpha = 0.4)\n\n\n\n\n\n\nLine plots\n\n\n# plot unemploy as a function of date using a line plot\nggplot(economics, aes(x = date, y = unemploy)) +\n  geom_line()  \n\n\n\n\n\n\n# adjust plot to represent the fraction of total population that is unemployed\nggplot(economics, aes(x = date, y = unemploy/pop)) +\n  geom_line()\n\n\n\n\n\n\nPeriods of recession\n\n\n# draw the recess periods\nggplot(economics, aes(x = date, y = unemploy/pop)) +\n  geom_line() +\n  geom_rect(data = recess, inherit.aes = FALSE, aes(xmin = begin, xmax = end, ymin = -Inf, ymax = +Inf), fill = 'red', alpha = 0.2)\n\n\n\n\n\n\nMultiple time series, part 1\n\n\n# use gather to go from fish to fish.tidy.\nfish.tidy <- gather(fish, Species, Capture, -Year)\n\n\n\n\nMultiple time series, part 2\n\n\n# plot\nggplot(fish.tidy, aes(x = Year, y = Capture, col = Species)) +\n  geom_line()\n\n\n\n\n\n\n5, qplot and wrap-up\n\u00b6\n\n\nUsing \nqplot\n\n\n# the old way\nplot(mpg ~ wt, data = mtcars)\n\n\n\n\n\n\n# using ggplot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(shape = 1)\n\n\n\n\n\n\n# Using qplot\nqplot(wt, mpg, data = mtcars)\n\n\n\n\n\n\nUsing aesthetics\n\n\n# Categorical: cyl\nqplot(wt, mpg, data = mtcars, size = cyl)\n\n\n\n\n\n\n# gear\nqplot(wt, mpg, data = mtcars, size = gear)\n\n\n\n\n\n\n# Continuous: hp\nqplot(wt, mpg, data = mtcars, col = hp)\n\n\n\n\n\n\n# qsec\nqplot(wt, mpg, data = mtcars, size = qsec)\n\n\n\n\n\n\nChoosing geoms, part 1\n\n\n# qplot() with x only\nqplot(factor(cyl), data = mtcars)\n\n\n\n\n\n\n# qplot() with x and y\nqplot(factor(cyl), factor(vs), data = mtcars)\n\n\n\n\n\n\n# qplot() with geom set to jitter manually\nqplot(factor(cyl), factor(vs), data = mtcars, geom = 'jitter')\n\n\n\n\n\n\nChoosing geoms, part 2 - dotplot\n\n\n# make a dot plot with ggplot\nggplot(mtcars, aes(cyl, wt, fill = am)) + \n  geom_dotplot(stackdir = 'center', binaxis = 'y')\n\n\n\n\n\n\n# qplot with geom 'dotplot', binaxis = 'y' and stackdir = 'center'\nqplot(as.numeric(cyl), wt, data = mtcars, fill = am, geom = 'dotplot', stackdir = 'center', binaxis = 'y')\n\n\n\n\n\n\nChicken weight\n\n\n# base\nggplot(ChickWeight, aes(x = Time, y = weight)) +\n  geom_line(aes(group = Chick))\n\n\n\n\n\n\n# color\nggplot(ChickWeight, aes(x = Time, y = weight, col = Diet)) +\n  geom_line(aes(group = Chick))\n\n\n\n\n\n\n# lines\nggplot(ChickWeight, aes(x = Time, y = weight, col = Diet)) +\n  geom_line(aes(group = Chick), alpha = 0.3) +\n  geom_smooth(lwd = 2, se = FALSE)\n\n\n\n\n\n\nTitanic\n\n\n# Use ggplot() for the first instruction\nggplot(titanic, aes(x = factor(Pclass), fill = factor(Sex))) +\n  geom_bar(position = 'dodge')\n\n\n\n\n\n\n# Use ggplot() for the second instruction\nggplot(titanic, aes(x = factor(Pclass), fill = factor(Sex))) +\n  geom_bar(position = 'dodge') +\n  facet_grid('. ~ Survived')\n\n\n\n\n\n\n# position jitter\nposn.j <- position_jitter(0.5, 0)\n\n# Use ggplot() for the last instruction\nggplot(titanic, aes(x = factor(Pclass), y = Age, col = factor(Sex))) +\n  geom_jitter(size = 3, alpha = 0.5, position = posn.j) +\n  facet_grid('. ~ Survived')\n\n\n\n\n\n\nSECTION 2\n\u00b6\n\n\n1, Statistics\n\u00b6\n\n\nSmoothing\n\n\n# scatter plot with LOESS smooth with a CI ribbon\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n# scatter plot with LOESS smooth without CI\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n# scatter plot with an OLS linear model\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n# scatter plot with an OLS linear model without points\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\nGrouping variables\n\n\n# cyl as a factor variable\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\n# set the group aesthetic\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl), group = 1)) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = F)\n\n\n\n\n\n\n# add a second smooth layer in which the group aesthetic is set\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'lm', se = FALSE, aes(group = 1))\n\n\n\n\n\n\nModifying \nstat_smooth\n\n\n# change the LOESS span\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 0.7, method = 'auto')\n\n\n\n\n\n\n# method = 'auto' is by default\n\n\n\n\n# set the model to the default LOESS and use a span of 0.7\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1), col = 'black', span = 0.7)\n\n\n\n\n\n\n# set col to 'All', inside the aes layer\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1, col = 'All cyl'), span = 0.7)\n\n\n\n\n\n\n# add `scale_color_manual` to change the colors\nmyColors <- c(brewer.pal(3, 'Dark2'), 'black')\n\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1, col = 'All cyl'), span = 0.7) +\n  scale_color_manual('Cylinders', values = myColors)\n\n\n\n\n\n\nModifying \nstat_smooth\n (2)\n\n\n# jittered scatter plot, add a linear model (lm) smooth\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2) +\n  stat_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\n# only lm, colored by year\nggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) +\n  stat_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\n# set a color brewer palette\nggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) + \n  stat_smooth(method = 'lm', se = FALSE) +\n  scale_color_brewer('Accent')\n\n\n\n\n\n\n# change col and group, specify alpha, size and geom, and add scale_color_gradient\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + \n  stat_smooth(method = 'lm', se = FALSE, alpha = 0.6, size = 2, geom = 'path') +\n  scale_color_brewer('Blues') +\n  scale_color_gradientn(colors = brewer.pal(9, 'YlOrRd'))\n\n\n\n\n\n\nQuantiles\n\n\n# use stat_quantile instead of stat_smooth\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) +  stat_quantile(alpha = 0.6, size = 2) + \n  scale_color_gradientn(colors = brewer.pal(9,'YlOrRd'))\n\n\n\n\n\n\n# set quantile to 0.5\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + \n  stat_quantile(alpha = 0.6, size = 2, quantiles = c(0.5)) + \n  scale_color_gradientn(colors = brewer.pal(9,'YlOrRd'))\n\n\n\n\n\n\nSum\n\n\n# plot with linear and loess model\np <- ggplot(Vocab, aes(x = education, y = vocabulary)) + \n  stat_smooth(method = 'loess', aes(col = 'red'), se = F) + \n  stat_smooth(method = 'lm', aes(col = 'blue'), se = F) + \n  scale_color_discrete('Model', labels = c('red' = 'LOESS', 'blue' = 'lm'))\n\np\n\n\n\n\n\n\n# add stat_sum (by overall proportion)\np + \n  stat_sum()\n\n\n\n\n\n\n#aes(group = 1)\n\n\n\n\n# set size range\np + \n  stat_sum() + \n  scale_size(range = c(1,10))\n\n\n\n\n\n\n# proportional within years of education; set group aesthetic\np + \n  stat_sum(aes(group = education))\n\n\n\n\n\n\n# set the n\np + \n  stat_sum(aes(group = education, size = ..n..))\n\n\n\n\n\n\nPreparations\n\n\n# convert cyl and am to factors\nmtcars$cyl <- as.factor(mtcars$cyl)\nmtcars$am <- as.factor(mtcars$am)\n\n# define positions\nposn.d <- position_dodge(width = 0.1)\nposn.jd <- position_jitterdodge(jitter.width = 0.1, dodge.width = 0.2)\nposn.j <- position_jitter(width = 0.2)\n\n# base layers\nwt.cyl.am <- ggplot(mtcars, aes(x = cyl, y = wt, col = am, group = am, fill = am))\n\n\n\n\nPlotting variations\n\n\n# base layer\nwt.cyl.am <- ggplot(mtcars, aes(x = cyl,  y = wt, col = am, fill = am, group = am))\n\n\n\n\n# jittered, dodged scatter plot with transparent points\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6)\n\n\n\n\n\n\n# mean and sd\nwt.cyl.am +\n  geom_point(position = posn.jd, alpha = 0.6) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = posn.d)\n\n\n\n\n\n\n# mean and 95% CI\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6) + \n  stat_summary(fun.data = mean_cl_normal, position = posn.d)\n\n\n\n\n\n\n# mean and SD with T-tipped error bars\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6) + \n  stat_summary(geom = 'point', fun.y = mean, position = posn.d) + \n  stat_summary(geom = 'errorbar', fun.data = mean_sdl, fun.args = list(mult = 1), width = 0.1, position = posn.d)\n\n\n\n\n\n\n2, Coordinates and Facets\n\u00b6\n\n\nZooming In\n\n\n# basic\np <- ggplot(mtcars, aes(x = wt, y = hp, col = am)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\n# add scale_x_continuous\np + \n  scale_x_continuous(limits = c(3, 6), expand = c(0,0))\n\n\n\n\n\n\n# zoom in\np + \n  coord_cartesian(xlim = c(3, 6))\n\n\n\n\n\n\nAspect Ratio\n\n\n# scatter plot\nbase.plot <- ggplot(iris, aes(y = Sepal.Width, x = Sepal.Length, col = Species)) + \n  geom_jitter() + \n  geom_smooth(method = 'lm', se = FALSE)\n\n\n\n\n# default aspect ratio\n# fix aspect ratio (1:1)\nbase.plot + \n  coord_equal()\n\n\n\n\n\n\nbase.plot + \n  coord_fixed()\n\n\n\n\n\n\nPie Charts\n\n\n# stacked bar plot\nthin.bar <- ggplot(mtcars, aes(x = 1, fill = cyl)) + \n  geom_bar()\n\nthin.bar\n\n\n\n\n\n\n# convert thin.bar to pie chart\nthin.bar + \n  coord_polar(theta = 'y')\n\n\n\n\n\n\n# create stacked bar plot\nwide.bar <- ggplot(mtcars, aes(x = 1, fill = cyl)) + \n  geom_bar(width = 1)\n\nwide.bar\n\n\n\n\n\n\n# Convert wide.bar to pie chart\nwide.bar + coord_polar(theta = 'y')\n\n\n\n\n\n\nFacets: the basics\n\n\n# scatter plot\np <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point()\n\n\n\n\n# separate rows according am\n# facet_grid(rows ~ cols)\np + \n  facet_grid(am ~ .)\n\n\n\n\n\n\n# separate columns according to cyl\n# facet_grid(rows ~ cols)\np + facet_grid(. ~ cyl)\n\n\n\n\n\n\n# separate by both columns and rows \n# facet_grid(rows ~ cols)\np + \n  facet_grid(am ~ cyl)\n\n\n\n\n\n\nMany variables\n\n\n# create the `cyl_am` col and `myCol` vector\nmtcars$cyl_am <- paste(mtcars$cyl, mtcars$am, sep = '_')\n\nmyCol <- rbind(brewer.pal(9, 'Blues')[c(3,6,8)],\n               brewer.pal(9, 'Reds')[c(3,6,8)])\n\n\n\n\n# scatter plot, add color scale\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) + \n  geom_point() + \n  scale_color_manual(values = myCol)\n\n\n\n\n\n\n# facet according on rows and columns\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) +\n  geom_point() + \n  scale_color_manual(values = myCol) + \n  facet_grid(gear ~ vs)\n\n\n\n\n\n\n# add more variables\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am, size = disp)) + \n  geom_point() + \n  scale_color_manual(values = myCol) + \n  facet_grid(gear ~ vs)\n\n\n\n\n\n\nDropping levels\n\n\n# scatter plot\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point()\n\n\n\n\n\n\n# facet rows according to `vore`\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point() + \n  facet_grid(vore ~ .)\n\n\n\n\n\n\n# specify scale and space arguments to free up rows\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point() + \n  facet_grid(vore ~ ., scale = 'free_y', space = 'free_y')\n\n\n\n\n\n\n3, Themes\n\u00b6\n\n\nRectangles\n\n\n# separate columns according to cyl\n# facet_grid(rows ~ cols)\nmtcars$cyl <- c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4)\n\nmtcars$Cylinders <- factor(mtcars$cyl)\n\nz <- ggplot(mtcars, aes(x = wt, y = mpg, col = Cylinders)) + \n  geom_point(size = 2, alpha = 0.7) + \n  facet_grid(. ~ cyl) + \n  labs(x = 'Weight (lb/1000)', y = 'Miles/(US) gallon') + \n  geom_smooth(method = 'lm', se = FALSE) +\n  theme_base() +\n  scale_colour_economist()\nz\n\n\n\n\n\n\n# change the plot background color to myPink (#FEE0D2)\nmyPink <- '#FEE0D2'\n\nz + \n  theme(plot.background = element_rect(fill = myPink))\n\n\n\n\n\n\n# adjust the border to be a black line of size 3\nz + \n  theme(plot.background = element_rect(fill = myPink, color = 'black', size = 3))\n\n\n\n\n\n\n# adjust the border to be a black line of size 3\nz + \n  theme(plot.background = element_rect(color = 'black', size = 3))\n\n\n\n\n\n\n# set panel.background, legend.key, legend.background and strip.background to element_blank()\nz + \n  theme(plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank())\n\n\n\n\n\n\nLines\n\n\n# Extend z with theme() and three arguments\nz +\n    theme(panel.grid = element_blank(), axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'))\n\n\n\n\n\n\nText\n\n\n# extend z with theme() function and four arguments\nmyRed <- '#99000D'\n\nz +\n    theme(strip.text = element_text(size = 16, color = myRed), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'))\n\n\n\n\n\n\nLegends\n\n\n# move legend by position\nz + \n  theme(legend.position = c(0.85, 0.85))\n\n\n\n\n\n\n# change direction\nz + \n  theme(legend.direction = 'horizontal')\n\n\n\n\n\n\n# change location by name\nz + \n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n# remove legend entirely\nz + \n  theme(legend.position = 'none')\n\n\n\n\n\n\nPositions\n\n\n# increase spacing between facets\nz + \n  theme(panel.margin.x = unit(2, 'cm'))\n\n\n\n\n\n\n# add code to remove any excess plot margin space\nz + \n  theme(panel.margin.x = unit(2, 'cm'), plot.margin = unit(c(0,0,0,0), 'cm'))\n\n\n\n\n\n\nUpdate Themestheme update\n\n\n# theme layer saved as an object, theme_pink\ntheme_pink <- theme(panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank(), plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.grid = element_blank(), axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'), strip.text = element_text(size = 16, color = myRed), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'), legend.position = 'none')\n\n\n\n\nz2 <- z\n\n# apply theme_pink to z2\nz2 + \n  theme_pink\n\n\n\n\n\n\n# change code so that old theme is saved as old\nold <- theme_update(panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank(), plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.grid = element_blank(),axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'), strip.text = element_text(size = 16, color = myRed), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'), legend.position = 'none')\n\n\n\n\n# display the plot z2\ntheme_set(theme_pink)\n\nz2 + \n  theme_pink\n\n\n\n\n\n\n# restore the old plot\ntheme_set(old)\n\nz2\n\n\n\n\n\n\nExploring ggthemes\n\n\n# apply theme_tufte\n# set the theme with theme_set\ntheme_set(theme_tufte())\n\n# or apply it in the ggplot command\nz2 + \n  theme_tufte()\n\n\n\n\n\n\n# apply theme_tufte, modified\n# set the theme with theme_set\ntheme_set(theme_tufte() + \n  theme(legend.position = c(0.9, 0.9), axis.title = element_text(face = 'italic', size = 12),  legend.title = element_text(face = 'italic', size = 12)))\n\n# or apply it in the ggplot command\nz2 + \n  theme_tufte() +\n  theme(legend.position = c(0.9, 0.9),        axis.title = element_text(face = 'italic', size = 12), legend.title = element_text(face = 'italic', size = 12))\n\n\n\n\n\n\n# apply theme_igray\n# set the theme with `theme_set`\ntheme_set(theme_igray())\n\n# or apply it in the ggplot command\nz2 + \n  theme_igray()\n\n\n\n\n\n\n# apply `theme_igray`, modified\n# set the theme with `theme_set`\ntheme_set(theme_igray() + \n  theme(legend.position = c(0.9, 0.9), legend.key = element_blank(), legend.background = element_rect(fill = 'grey90')))\n\nz2 + \n  # Or apply it in the ggplot command\n  theme_igray() +\n  theme(legend.position = c(0.9, 0.9),\n        legend.key = element_blank(),\n        legend.background = element_rect(fill = 'grey90'))\n\n\n\n\n\n\n4, Best Practices\n\u00b6\n\n\nBar Plots (1)\n\n\n# base layers\nm <- ggplot(mtcars, aes(x = cyl, y = wt))\n\n\n\n\n# dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar', fill = 'skyblue') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1)\n\n\n\n\n\n\nBar Plots (2)\n\n\n# base layers\nm <- ggplot(mtcars, aes(x = cyl,y = wt, col = am, fill = am))\n\n\n\n\n# dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1)\n\n\n\n\n\n\n# set position dodge in each `stat` function\nm + \n  stat_summary(fun.y = mean, geom = 'bar', position = 'dodge') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1, position = 'dodge')\n\n\n\n\n\n\n# set your dodge `posn` manually\nposn.d <- position_dodge(0.9)\n\n\n\n\n# redraw dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar', position = posn.d) +  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1, position = posn.d)\n\n\n\n\n\n\nBar Plots (3)\n\n\n# base layers\nmtcars.cyl <- mtcars %>% group_by(cyl) %>% summarise(wt.avg = mean(wt))\nmtcars.cyl\n\n\n\n\n## # A tibble: 3 \u00d7 2\n##     cyl   wt.avg\n##   <dbl>    <dbl>\n## 1     4 2.285727\n## 2     6 3.117143\n## 3     8 3.999214\n\n\n\nm <- ggplot(mtcars.cyl, aes(x = cyl, y = wt.avg))\nm\n\n\n\n\n\n\n# draw bar plot\nm + \n  geom_bar(stat = 'identity', fill = 'skyblue')\n\n\n\n\n\n\nPie Charts (1)\n\n\n# bar chart to pie chart\nggplot(mtcars, aes(x = cyl, fill = am)) + geom_bar(position = 'fill')\n\n\n\n\n\n\nggplot(mtcars, aes(x = cyl, fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl)\n\n\n\n\n\n\nggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl)\n\n\n\n\n\n\nggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl) + coord_polar(theta = 'y')\n\n\n\n\n\n\nggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill', width = 1) + facet_grid(. ~ cyl) + coord_polar(theta = 'y')\n\n\n\n\n\n\nParallel coordinate plot\n\n\n# parallel coordinates plot using `GGally`\n# all columns except `am` (`am` column is the 9th)\ngroup_by_am <- 9\nmy_names_am <- (1:11)[-group_by_am]\n\n\n\n\n# parallel plot; each variable plotted as a z-score transformation\nggparcoord(mtcars, columns = my_names_am, groupColumn = group_by_am, alpha = 0.8)\n\n\n\n\n\n\n# scaled between 0-1 and most discriminating variable first\nggparcoord(mtcars, columns = my_names_am, groupColumn = group_by_am, alpha = 0.8, scale = 'uniminmax', order = 'anyClass')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species') # xlab, ylab, scale_x_discrete, them\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'uniminmax')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'globalminmax')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', mapping = aes(size = 1))\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', alphaLines = 0.3)\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'center')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scaleSummary = 'median', missing = 'exclude')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', order = 'allClass') # or custom filter\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'std')\n\n\n\n\n\n\nSplom\n\n\nlibrary(dplyr)\n\ndata(Pima.tr2, package = 'MASS')\nPimaV <- select(Pima.tr2, glu:age)\n\nggpairs(PimaV, diag = list(continuous = 'density'), axisLabels = 'show')\n\n\n\n\n\n\nHeat Maps\n\n\n# create color palette\nmyColors <- brewer.pal(9, 'Reds')\n\n\n\n\n# heat map\nggplot(barley, aes(x = year, y = variety, fill = yield)) + \n  geom_tile()\n\n\n\n\n\n\n# add facet_wrap(~ variable); not like facet_grid(. ~ variable)\nggplot(barley, aes(x = year, y = variety, fill = yield)) + \n  geom_tile() + \n  facet_wrap( ~ site, ncol = 1)\n\n\n\n\n\n\n# \nggplot(barley, aes(x = year, y = variety, fill = yield)) + geom_tile() + facet_wrap( ~ site, ncol = 1) + \n  scale_fill_gradientn(colors = myColors)\n\n\n\n\n\n\nHeat Maps Alternatives (1)\n\n\n# line plots\nggplot(barley, aes(x = year, y = yield, col = variety, group = variety)) + geom_line() + \n  facet_wrap(facets = ~ site, nrow = 1)\n\n\n\n\n\n\nHeat Maps Alternatives (2)\n\n\n# overlapping ribbon plot\nggplot(barley, aes(x = year, y = yield, col = site, group = site, fill = site)) + geom_line() + \n  stat_summary(fun.y = mean, geom = 'line') + \n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'ribbon', col = NA, alpha = 0.1)\n\n\n\n\n\n\n5, Case Study\n\u00b6\n\n\nSort and order\n\n\n# reorder\ndata(Cars93, package = 'MASS')\n\nCars93 <- within(Cars93, TypeWt <- reorder(Type, Weight, mean))\n\nCars93 <- within(Cars93, Type1 <- factor(Type, levels = c('Small', 'Sporty', 'Compact', 'Midsize', 'Large', 'Van')))\n\nwith(Cars93, table(TypeWt, Type1))\n\n\n\n\n##          Type1\n## TypeWt    Small Sporty Compact Midsize Large Van\n##   Small      21      0       0       0     0   0\n##   Sporty      0     14       0       0     0   0\n##   Compact     0      0      16       0     0   0\n##   Midsize     0      0       0      22     0   0\n##   Large       0      0       0       0    11   0\n##   Van         0      0       0       0     0   9\n\n\n\nggplot(Cars93, aes(TypeWt, 100/MPG.city)) + \n  geom_boxplot() + \n  ylab('Gallons per 100 miles') + \n  xlab('Car type')\n\n\n\n\n\n\nCars93 <- within(Cars93, {\n  levels(Type1) <- c('Small', 'Large', 'Midsize', 'Small', 'Sporty', 'Large')\n})\n\nggplot(Cars93, aes(TypeWt, 100/MPG.city)) + \n  geom_boxplot() + \n  ylab('Gallons per 100 miles') + \n  xlab('Car type')\n\n\n\n\n\n\nEnsemble plots\n\n\nlibrary(gridExtra)\n\ndata(Fertility, package = 'AER')\n\np0 <- ggplot(Fertility) + geom_histogram(binwidth = 1) + ylab('')\np1 <- p0 + aes(x = age)\np2 <- p0 + aes(x = work) + xlab('Weeks worked in 1979')\n\nk <- ggplot(Fertility) + \n  geom_bar() + ylab('') + \n  ylim(0, 250000)\n\np3 <- k + aes(x = morekids) + \n  xlab('has more children')\np4 <- k + aes(x = gender1) + \n  xlab('first child')\np5 <- k + aes(x = gender2) + \n  xlab('second child')\np6 <- k + aes(x = afam) + \n  xlab('African-American')\np7 <- k + aes(x = hispanic) + \n  xlab('Hispanic')\np8 <- k + aes(x = other) + \n  xlab('other race')\n\ngrid.arrange(arrangeGrob(p1, p2, ncol = 2, widths = c(3, 3)), arrangeGrob(p3, p4, p5, p6, p7, p8, ncol = 6), nrow = 2, heights = c(1.25, 1))\n\n\n\n\n\n\nExploring Data\n\n\n# histogram\nggplot(adult, aes(x = SRAGE_P)) + \n  geom_histogram()\n\n\n\n\n\n\n# histogram\nggplot(adult, aes(x = BMI_P)) + \n  geom_histogram()\n\n\n\n\n\n\n# color, default binwidth\nggplot(adult,aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\nData Cleaning\n\n\n# remove individual aboves 84\nadult <- adult[adult$SRAGE_P <= 84, ] \n\n# remove individuals with a BMI below 16 and above or equal to 52\nadult <- adult[adult$BMI_P >= 16 & adult$BMI_P < 52, ]\n\n# relabel race\nadult$RACEHPR2 <- factor(adult$RACEHPR2, labels = c('Latino', 'Asian', 'African American', 'White'))\n\n# relabel the BMI categories variable\nadult$RBMI <- factor(adult$RBMI, labels = c('Under-weight', 'Normal-weight', 'Over-weight', 'Obese'))\n\n\n\n\nMultiple Histograms\n\n\n# color palette BMI_fill\nBMI_fill <- scale_fill_brewer('BMI Category', palette = 'Reds')\n\n\n\n\n# histogram, add BMI_fill and customizations\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(binwidth = 1) + \n  BMI_fill + facet_grid(RBMI ~ .) + \n  theme_classic()\n\n\n\n\n\n\nAlternatives\n\n\n# count histogram\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(binwidth = 1) +\n  BMI_fill\n\n\n\n\n\n\n# density histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1) +\n  BMI_fill\n\n\n\n\n\n\n# faceted count histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(binwidth = 1) +\n  BMI_fill + facet_grid(RBMI ~ .)\n\n\n\n\n\n\n# faceted density histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1) +\n  BMI_fill + facet_grid(RBMI ~ .)\n\n\n\n\n\n\n# density histogram with `position = 'fill'`\nggplot(adult, aes (x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1, position = 'fill') +\n  BMI_fill\n\n\n\n\n\n\n# accurate histogram\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..count../sum(..count..)), binwidth = 1, position = 'fill') +\n  BMI_fill\n\n\n\n\n\n\nDo Things Manually\n\n\n# an attempt to facet the accurate frequency histogram from before (failed)\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..count../sum(..count..)), binwidth = 1, position = 'fill') +\n  BMI_fill +\n  facet_grid(RBMI ~ .)\n\n\n\n\n\n\n# create DF with `table()`\nDF <- table(adult$RBMI, adult$SRAGE_P)\n\n# use apply on DF to get frequency of each group\nDF_freq <- apply(DF, 2, function(x) x/sum(x))\n\n# melt on DF to create DF_melted\nDF_melted <- melt(DF_freq)\n\n# change names of DF_melted\nnames(DF_melted) <- c('FILL', 'X', 'value')\n\n\n\n\n# add code to make this a faceted plot\nggplot(DF_melted, aes(x = X, y = value, fill = FILL)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  BMI_fill + \n  facet_grid(FILL ~ .)\n\n\n\n\n\n\nMerimeko/Mosaic Plot\n\n\n# The initial contingency table\nDF <- as.data.frame.matrix(table(adult$SRAGE_P, adult$RBMI))\n\n# Add the columns groupsSum, xmax and xmin. Remove groupSum again.\nDF$groupSum <- rowSums(DF)\nDF$xmax <- cumsum(DF$groupSum)\nDF$xmin <- DF$xmax - DF$groupSum\n# The groupSum column needs to be removed, don't remove this line\nDF$groupSum <- NULL\n\n# Copy row names to variable X\nDF$X <- row.names(DF)\n\n# Melt the dataset\nDF_melted <- melt(DF, id.vars = c('X', 'xmin', 'xmax'), variable.name = 'FILL')\n\n# dplyr call to calculate ymin and ymax - don't change\nDF_melted <- DF_melted %>% \n  group_by(X) %>% \n  mutate(ymax = cumsum(value/sum(value)),\n         ymin = ymax - value/sum(value))\n\n# Plot rectangles - don't change.\nggplot(DF_melted, aes(ymin = ymin, \n                 ymax = ymax,\n                 xmin = xmin, \n                 xmax = xmax, \n                 fill = FILL)) + \n  geom_rect(colour = 'white') +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  BMI_fill +\n  theme_tufte()\n\n\n\n\n\n\nAdding statistics\n\n\n# perform chi.sq test (`RBMI` and `SRAGE_P`)\nresults <- chisq.test(table(adult$RBMI, adult$SRAGE_P))\n\n# melt results$residuals and store as resid\nresid <- melt(results$residuals)\n\n# change names of resid\nnames(resid) <- c('FILL', 'X', 'residual')\n\n# merge the two datasets\nDF_all <- merge(DF_melted, resid)\n\n\n\n\n# update plot command\nggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) +\n  geom_rect() +\n  scale_fill_gradient2() +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  theme_tufte()\n\n\n\n\n\n\nAdding text\n\n\n# position for labels on x axis\nDF_all$xtext <- DF_all$xmin + (DF_all$xmax - DF_all$xmin) / 2\n\n# position for labels on y axis\nindex <- DF_all$xmax == max(DF_all$xmax)\nDF_all$ytext <- DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2\n\n\n\n\n# plot\nggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) + \n  geom_rect(col = 'white') +\n  # geom_text for ages (i.e. the x axis)\n  geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) +\n  # geom_text for BMI (i.e. the fill axis)\n  geom_text(aes(x = max(xmax), y = ytext, label = FILL), size = 3, hjust = 1, show.legend  = FALSE) +\n  scale_fill_gradient2() +\n  theme_tufte() +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\nGeneralizations\n\n\n# script generalized into a function\nmosaicGG <- function(data, X, FILL) {\n  # Proportions in raw data\n  DF <- as.data.frame.matrix(table(data[[X]], data[[FILL]]))\n  DF$groupSum <- rowSums(DF)\n  DF$xmax <- cumsum(DF$groupSum)\n  DF$xmin <- DF$xmax - DF$groupSum\n  DF$X <- row.names(DF)\n  DF$groupSum <- NULL\n  DF_melted <- melt(DF, id = c('X', 'xmin', 'xmax'), variable.name = 'FILL')\n\n  DF_melted <- DF_melted %>% \n    group_by(X) %>% \n    mutate(ymax = cumsum(value/sum(value)),\n           ymin = ymax - value/sum(value))\n\n  # Chi-sq test\n  results <- chisq.test(table(data[[FILL]], data[[X]])) # fill and then x\n  resid <- melt(results$residuals)\n  names(resid) <- c('FILL', 'X', 'residual')\n  # Merge data\n  DF_all <- merge(DF_melted, resid)\n   # Positions for labels\n  DF_all$xtext <- DF_all$xmin + (DF_all$xmax - DF_all$xmin)/2\n  index <- DF_all$xmax == max(DF_all$xmax)\n  DF_all$ytext <- DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2\n\n  # plot\n  g <- ggplot(DF_all, aes(ymin = ymin,  ymax = ymax, xmin = xmin, \n                          xmax = xmax, fill = residual)) + \n    geom_rect(col = 'white') +\n    geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) + geom_text(aes(x = max(xmax),  y = ytext, label = FILL), size = 3, hjust = 1, show.legend = FALSE) +\n    scale_fill_gradient2('Residuals') +\n    scale_x_continuous('Individuals', expand = c(0,0)) +\n    scale_y_continuous('Proportion', expand = c(0,0)) +\n    theme_tufte() +\n    theme(legend.position = 'bottom')\n  print(g)\n}\n\n\n\n\n# BMI described by age (in x)\nmosaicGG(adult, 'SRAGE_P','RBMI')\n\n\n\n\n\n\n# poverty described by age (in x)\nmosaicGG(adult, 'SRAGE_P', 'POVLL')\n\n\n\n\n\n\n# `am` described by `cyl` (in x)\nmosaicGG(mtcars, 'cyl', 'am')\n\n\n\n\n\n\n# `Vocab` described by education\nmosaicGG(Vocab, 'education', 'vocabulary')\n\n\n\n\n\n\nSECTION 3\n\u00b6\n\n\nSECTION 4 - Cheat List\n\u00b6\n\n\nggplot(data, aes(x = , y = ), col = , fill = , size = , labels = , alpha\n\n= , shape = , line = , position = \u2018jitter\u2019)\n\n\n+ geom_point()\n\n+ geom_point(aes(), col = , position = posn.j)\n\n+ geom_jitter()\n\n+ facet_grid(. ~ x) # y ~ x\n\n+ scale_x_continous('Sepal Length', limits = c(2, 8), breaks = seq(2, 8, 3))\n\n+ scale_color_discrete('Species', labels = c('a', 'b', 'c'))\n\n+ labs(x = , y = , col = )\n\n\n\nposn.j <- position_jitter(width = 0.1)\n\n\nData\n\n\n\n\ndiamonds\n, prices of 50,000 round cut diamonds.\n\n\neconomics\n, economics_long, US economic time series.\n\n\nfaithfuld\n, 2d density estimate of Old Faithful data.\n\n\nluv_colours\n, colors().\n\n\nmidwest\n, midwest demographics.\n\n\nmpg\n, fuel economy data from 1999 and 2008 for 38 popular models\n\n    of car.\n\n\nmsleep\n, an updated and expanded version of the mammals\n\n    sleep dataset.\n\n\npresidential\n, terms of 11 presidents from Eisenhower to Obama.\n\n\nseals\n, vector field of seal movements.\n\n\ntxhousing\n, Housing sales in TX.\n\n\n\n\nAesthetics\n\n\n\n\nx-axis.\n\n\ny-asix.\n\n\ncolor.\n\n\nfill.\n\n\nsize (points, lines).\n\n\nlabels.\n\n\nalpha.\n\n\nshape (points).\n\n\nlinetype (lines).\n\n\naes\n, Define aesth.etic mappings.\n\n\naes_\n (aes_q, aes_string), Define aesthetic mappings from\n\n    strings, or quoted calls and formulas.\n\n\naes_all\n, Given a character vector, create a set of\n\n    identity mappings.\n\n\naes_auto\n, Automatic aesthetic mapping.\n\n\naes_colour_fill_alpha\n (color, colour, fill), Colour related\n\n    aesthetics: colour, fill and alpha.\n\n\naes_group_order\n (group), Aesthetics: group.\n\n    aes_linetype_size_shape (linetype, shape, size), Differentiation\n\n    related aesthetics: linetype, size, shape.\n\n\naes_position\n (x, xend, xmax, xmin, y, yend, ymax, ymin), Position\n\n    related aesthetics: x, y, xmin, xmax, ymin, ymax, xend, yend.\n\n\n\n\nPosition\n\n\n\n\nposition_dodge\n, Adjust position by dodging overlaps to the side.\n\n\nposition_fill\n (position_stack), Stack overlapping objects on top\n\n    of one another.\n\n\nposition_identity\n, Don\u2019t adjust position\n\n\nposition_nudge\n, Nudge points.\n\n\nposition_jitter\n, Jitter points to avoid overplotting.\n\n\nposition_jitterdodge\n, Adjust position by simultaneously dodging\n\n    and jittering.\n\n\n\n\nScales\n\n\n\n\nexpand_limits\n, Expand the plot limits with data.\n\n\nguides\n, Set guides for each scale.\n\n\nguide_legend\n, Legend guide.\n\n\nguide_colourbar\n (guide_colorbar), Continuous colour bar guide.\n\n\nlims\n (xlim, ylim), Convenience functions to set the axis limits.\n\n\nscale_alpha\n (scale_alpha_continuous, scale_alpha_discrete),\n\n    Alpha scales.\n\n\nscale_colour_brewer\n (scale_color_brewer,\n\n    scale_color_distiller, scale_colour_distiller,\n\n    scale_fill_brewer, scale_fill_distiller), Sequential, diverging\n\n    and qualitative colour scales from colorbrewer.org\n\n\nscale_colour_gradient\n (scale_color_continuous,\n\n    scale_color_gradient, scale_color_gradient2,\n\n    scale_color_gradientn, scale_colour_continuous,\n\n    scale_colour_date, scale_colour_datetime,\n\n    scale_colour_gradient2, scale_colour_gradientn,\n\n    scale_fill_continuous, scale_fill_date, scale_fill_datetime,\n\n    scale_fill_gradient,\n\n    scale_fill_gradient2, scale_fill_gradientn).\n\n\nscale_colour_grey\n (scale_color_grey, scale_fill_grey),\n\n    Sequential grey colour scale.\n\n\nscale_colour_hue\n (scale_color_discrete, scale_color_hue,\n\n    scale_colour_discrete, scale_fill_discrete, scale_fill_hue),\n\n    Qualitative colour scale with evenly spaced hues.\n\n\nscale_identity\n (scale_alpha_identity, scale_color_identity,\n\n    scale_colour_identity, scale_fill_identity,\n\n    scale_linetype_identity, scale_shape_identity,\n\n    scale_size_identity), Use values without scaling.\n\n\nscale_manual\n (scale_alpha_manual, scale_color_manual,\n\n    scale_colour_manual, scale_fill_manual, scale_linetype_manual,\n\n    scale_shape_manual, scale_size_manual), Create your own\n\n    discrete scale.\n\n\nscale_linetype\n (scale_linetype_continuous,\n\n    scale_linetype_discrete), Scale for line patterns.\n\n\nscale_shape\n (scale_shape_continuous, scale_shape_discrete),\n\n    Scale for shapes, aka glyphs.\n\n\nscale_size\n (scale_radius, scale_size_area,\n\n    scale_size_continuous, scale_size_date, scale_size_datetime,\n\n    scale_size_discrete), Scale size (area or radius).\n\n\nscale_x_discrete\n (scale_y_discrete), Discrete position.\n\n\nlabs\n (ggtitle, xlab, ylab), Change axis labels and legend titles.\n\n\nupdate_labels\n, Update axis/legend labels.\n\n\n\n\nGeometries\n\n\n\n\npoint.\n\n\nline.\n\n\nhistogram.\n\n\nbar.\n\n\nboxplot.\n\n\ngeom_abline\n (geom_hline, geom_vline), Lines: horizontal,\n\n    vertical, and specified by slope and intercept.\n\n\ngeom_bar\n (stat_count), Bars, rectangles with bases on x-axis\n\n\ngeom_bin2d\n (stat_bin2d, stat_bin_2d), Add heatmap of 2d\n\n    bin counts.\n\n\ngeom_blank\n, Blank, draws nothing.\n\n\ngeom_boxplot\n (stat_boxplot), Box and whiskers plot.\n\n\ngeom_contour\n (stat_contour), Display contours of a 3d surface\n\n    in 2d.\n\n\ngeom_count\n(stat_sum), Count the number of observations at\n\n    each location.\n\n\ngeom_crossbar\n (geom_errorbar, geom_linerange, geom_pointrange),\n\n    Vertical intervals: lines, crossbars & errorbars.\n\n\ngeom_density\n (stat_density), Display a smooth density estimate.\n\n\ngeom_density_2d\n (geom_density2d, stat_density2d,\n\n    stat_density_2d), Contours from a 2d density estimate.\n\n\ngeom_dotplot\n, Dot plot\n\n\ngeom_errorbarh\n, Horizontal error bars.\n\n\ngeom_freqpoly\n (geom_histogram, stat_bin), Histograms and\n\n    frequency polygons.\n\n\ngeom_hex\n (stat_bin_hex, stat_binhex), Hexagon binning.\n\n\ngeom_jitter\n, Points, jittered to reduce overplotting.\n\n\ngeom_label\n (geom_text), Textual annotations.\n\n\ngeom_map\n, Polygons from a reference map.\n\n\ngeom_path\n (geom_line, geom_step), Connect observations.\n\n\ngeom_point\n, Points, as for a scatterplot.\n\n\ngeom_polygon\n, Polygon, a filled path.\n\n\ngeom_quantile\n (stat_quantile), Add quantile lines from a\n\n    quantile regression.\n\n\ngeom_raster\n (geom_rect, geom_tile), Draw rectangles.\n\n\ngeom_ribbon\n (geom_area), Ribbons and area plots.\n\n\ngeom_rug\n, Marginal rug plots.\n\n\ngeom_segment\n (geom_curve), Line segments and curves.\n\n\ngeom_smooth\n (stat_smooth), Add a smoothed conditional mean.\n\n\ngeom_violin\n (stat_ydensity), Violin plot.\n\n\n\n\nFacets\n\n\n\n\ncolumns.\n\n\nrows.\n\n\nfacet_grid\n, Lay out panels in a grid.\n\n\nfacet_null\n, Facet specification: a single panel.\n\n\nfacet_wrap\n, Wrap a 1d ribbon of panels into 2d.\n\n\nlabeller\n, Generic labeller function for facets.\n\n\nlabel_bquote\n, Backquoted labeller.\n\n\n\n\nAnnotation\n\n\n\n\nannotate\n, Create an annotation layer.\n\n\nannotation_custom\n, Annotation: Custom grob.\n\n\nannotation_logticks\n, Annotation: log tick marks.\n\n\nannotation_map\n, Annotation: maps.\n\n\nannotation_raster\n, Annotation: High-performance\n\n    rectangular tiling.\n\n\nborders\n, Create a layer of map borders.\n\n\n\n\nFortify\n\n\n\n\nfortify\n, Fortify a model with data.\n\n\nfortify-multcomp\n (fortify.cld, fortify.confint.glht, fortify.glht,\n\n    fortify.summary.glht), Fortify methods for objects produced by.\n\n\nfortify.lm\n, Supplement the data fitted to a linear model with\n\n    model fit statistics.\n\n\nfortify.map\n, Fortify method for map objects.\n\n\nfortify.sp\n (fortify.Line, fortify.Lines, fortify.Polygon,\n\n    fortify.Polygons, fortify.SpatialLinesDataFrame,\n\n    fortify.SpatialPolygons, fortify.SpatialPolygonsDataFrame), Fortify\n\n    method for classes from the sp package.\n\n\nmap_data\n, Create a data frame of map data.\n\n\n\n\nStatistics\n\n\n\n\nbinning.\n\n\nsmoothing.\n\n\ndescriptive.\n\n\ninferential.\n\n\nstat_ecdf\n, Empirical Cumulative Density Function.\n\n\nstat_ellipse\n, Plot data ellipses.\n\n\nstat_function\n, Superimpose a function.\n\n\nstat_identity\n, Identity statistic.\n\n\nstat_qq\n (geom_qq), Calculation for quantile-quantile plot.\n\n\nstat_summary_2d\n (stat_summary2d, stat_summary_hex), Bin and\n\n    summarise in 2d (rectangle & hexagons)\n\n\nstat_unique\n, Remove duplicates.\n\n\nCoordinates.\n\n\ncartesian.\n\n\nfixes.\n\n\npolar.\n\n\nlimites.\n\n\ncoord_cartesian\n, Cartesian coordinates.\n\n\ncoord_fixed\n (coord_equal), Cartesian coordinates with fixed\n\n    relationship between x and y scales.\n\n\ncoord_flip\n, Flipped cartesian coordinates.\n\n\ncoord_map\n (coord_quickmap), Map projections.\n\n\ncoord_polar\n, Polar coordinates.\n\n\ncoord_trans\n, Transformed cartesian coordinate system.\n\n\n\n\nThemes\n\n\n\n\ntheme_bw\n\n\ntheme_grey\n\n\ntheme_classic\n\n\ntheme_minimal\n\n\nggthemes",
            "title": "Plot Snippets - ggplot2"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#dataset",
            "text": "For most examples, we use the  mtcars ,  diamonds ,  iris ,  ChickWeight ,  recess ,  fish ,  Vocab ,  Titanic ,  mamsleep ,  barley ,  adult  datasets.",
            "title": "Dataset"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#the-ggplot2-package",
            "text": "library(ggplot2)  Import additional packages.  library(digest)\nlibrary(grid)\nlibrary(gtable)\nlibrary(MASS)\nlibrary(plyr)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(stats)\nlibrary(tidyr)  For this project, import additional packages.  library(ggthemes)\nlibrary(RColorBrewer)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(car)\nlibrary(Hmisc)\nlibrary(dplyr)  Suggested additional packages\u2026",
            "title": "The ggplot2 Package"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#section-1",
            "text": "",
            "title": "SECTION 1"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#1-introduction",
            "text": "Exploring  ggplot2 , part 1  # basic plot\nggplot(mtcars, aes(x = cyl, y = mpg)) +\n  geom_point()   Exploring  ggplot2 , part 2  # cyl is a factor\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_point()   Exploring  ggplot2 , part 3  # scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()   # add color\nggplot(mtcars, aes(x = wt, y = mpg, col = disp)) +\n  geom_point()   # change size\nggplot(mtcars, aes(x = wt, y = mpg, size = disp)) +\n  geom_point()   Exploring  ggplot2 , part 4  # Add geom_point() with +\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_point()   # Add geom_point() and geom_smooth() with +\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_point() + geom_smooth()   Exploring  ggplot2 , part 5  # only the smooth line\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_smooth()   # change col\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) + \n  geom_point()   # change the alpha\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point(alpha = 0.4)   Exploring  ggplot2 , part 6  # 2 facets for comparison\nlibrary(gridExtra)\n\ndata(father.son, package = 'UsingR')\n\na <- ggplot(father.son, aes(fheight, sheight)) +\n  geom_point() +\n  geom_smooth(method = 'lm', colour = 'red') +\n  geom_abline(slope = 1, intercept = 0)\n\nb <- ggplot(father.son, aes(fheight, sheight)) +\n  geom_point() +\n  geom_smooth(method = 'lm', colour = 'red', se = FALSE) +\n  stat_smooth()\n\ngrid.arrange(a, b, nrow = 1)   # load more data\ndata(oly12, package = 'VGAMdata')\n\n# 2 facets for comparison\nggplot(oly12, aes(Height, Weight)) +\n  geom_point(size = 1) +\n  facet_wrap(~Sex, ncol = 1)   # create a new variable inside de data frame\noly12S <- within(oly12, oly12$Sport <- abbreviate(oly12$Sport, 12))\n\n# multiple facets or splom\nggplot(oly12S, aes(Height, Weight)) +\n  geom_point(size = 1) +\n  facet_wrap(~Sport) +\n  ggtitle('Weight and Height by Sport')    Understanding the grammar, part 1  # create the object containing the data and aes layers\ndia_plot <- ggplot(diamonds, aes(x = Carat, y = PricePerCt))\n\n# add a geom layer\ndia_plot + \n  geom_point()   # add the same geom layer, but with aes() inside\ndia_plot +\n  geom_point(aes(col = Clarity))   Understanding the grammar, part 2  set.seed(1)\n\n# create the object containing the data and aes layers\ndia_plot <- ggplot(diamonds, aes(x = Carat, y = PricePerCt))\n\n# add geom_point() with alpha set to 0.2\ndia_plot <- dia_plot +\n  geom_point(alpha = 0.2)\n\ndia_plot   # plot dia_plot with additional geom_smooth() with se set to FALSE\ndia_plot +\n  geom_smooth(se = FALSE)",
            "title": "1, Introduction"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#2-data",
            "text": "Base package and  ggplot2 , part 1 - plot  # basic plot\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)   # change cyl inside mtcars to a factor\nmtcars$cyl <- as.factor(mtcars$cyl)\n\n# make the same plot as in the first instruction\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)   Base package and  ggplot2 , part 2 - lm  transfer to other  # Basic plot\nmtcars$cyl <- as.factor(mtcars$cyl)\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)\n\n# use lm() to calculate a linear model and save it as carModel\ncarModel <- lm(mpg ~ wt, data = mtcars)\n\n# Call abline() with carModel as first argument and lty as second\nabline(carModel, lty = 2)\n\n# plot each subset efficiently with lapply\nlapply(mtcars$cyl, function(x) {\n  abline(lm(mpg ~ wt, mtcars, subset = (cyl == x)), col = x)\n  })  ## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n## \n## [[8]]\n## NULL\n## \n## [[9]]\n## NULL\n## \n## [[10]]\n## NULL\n## \n## [[11]]\n## NULL\n## \n## [[12]]\n## NULL\n## \n## [[13]]\n## NULL\n## \n## [[14]]\n## NULL\n## \n## [[15]]\n## NULL\n## \n## [[16]]\n## NULL\n## \n## [[17]]\n## NULL\n## \n## [[18]]\n## NULL\n## \n## [[19]]\n## NULL\n## \n## [[20]]\n## NULL\n## \n## [[21]]\n## NULL\n## \n## [[22]]\n## NULL\n## \n## [[23]]\n## NULL\n## \n## [[24]]\n## NULL\n## \n## [[25]]\n## NULL\n## \n## [[26]]\n## NULL\n## \n## [[27]]\n## NULL\n## \n## [[28]]\n## NULL\n## \n## [[29]]\n## NULL\n## \n## [[30]]\n## NULL\n## \n## [[31]]\n## NULL\n## \n## [[32]]\n## NULL  # draw the legend of the plot\nlegend(x = 5, y = 33, legend = levels(mtcars$cyl), col = 1:3, pch = 1, bty = 'n')   Base package and  ggplot2 , part 3  # scatter plot\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) + \n  geom_point()   # include the lines of the linear models, per cyl\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE)   # include a lm for the entire dataset in its whole\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE) +\n  geom_smooth(aes(group = 1), method = 'lm', se = FALSE, linetype = 2)   Variables to visuals, part 1  iris.tidy <- iris %>%\n  gather(key, Value, -Species) %>%\n  separate(key, c('Part', 'Measure'), '\\\\.')\n\n# create 2 facets\nggplot(iris.tidy, aes(x = Species, y = Value, col = Part)) +\n  geom_jitter() + facet_grid(. ~ Measure)   Variables to visuals, part 2  # Add a new column, Flower, to iris that contains unique ids\niris$Flower <- 1:nrow(iris)\n\niris.wide <- iris %>%\n  gather(key, value, -Species, -Flower) %>%\n  separate(key, c('Part', 'Measure'), '\\\\.') %>%\n  spread(Measure, value)\n\n# create 3 facets\nggplot(iris.wide, aes(x = Length, y = Width, col = Part)) + \n  geom_jitter() +\n  facet_grid(. ~ Species)",
            "title": "2, Data"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#3-aesthetics",
            "text": "All about aesthetics, part 1  # map cyl to y\nggplot(mtcars, aes(x = mpg, y = cyl)) +\n  geom_point()   # map cyl to x\nggplot(mtcars, aes(y = mpg, x = cyl)) +\n  geom_point()   # map cyl to col\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point()   # change shape and size of the points\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(shape = 1, size = 4)   All about aesthetics, part 2  # map cyl to fill\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point()   # Change shape, size and alpha of the points in the above plot\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(shape = 16, size = 6, alpha = 0.6)   All about aesthetics, part 3  # map cyl to size\nggplot(mtcars, aes(x = wt, y = mpg, size = cyl)) +\n  geom_point()   # map cyl to alpha\nggplot(mtcars, aes(x = wt, y = mpg, alpha = cyl)) +\n  geom_point()   # map cyl to shape \nggplot(mtcars, aes(x = wt, y = mpg, shape = cyl, label = cyl)) +\n  geom_point()   # map cyl to labels\nggplot(mtcars, aes(x = wt, y = mpg, label = cyl)) +\n  geom_text()   All about attributes, part 1  # define a hexadecimal color\nmy_color <- '#123456'\n\n# set the color aesthetic \nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point()   # set the color aesthetic and attribute \nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(col = my_color)   # set the fill aesthetic and color, size and shape attributes\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(size = 10, shape = 23, col = my_color)   All about attributes, part 2  # draw points with alpha 0.5\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(alpha = 0.5)   # raw points with shape 24 and color yellow\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(shape = 24, col = 'yellow')   # draw text with label x, color red and size 10\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_text(label = 'x', col = 'red', size = 10)   Going all out  # Map mpg onto x, qsec onto y and factor(cyl) onto col\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl))) +\n  geom_point()   # Add mapping: factor(am) onto shape\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am))) +\n  geom_point()   # Add mapping: (hp/wt) onto size\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am), size = hp/wt)) +\n  geom_point()   # Add mapping: rownames(mtcars) onto label\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am), size = hp/wt)) +\n  geom_text(aes(label = rownames(mtcars)))   Position  # base layers\ncyl.am <- ggplot(mtcars, aes(x = factor(cyl), fill = factor(am)))\n\n# add geom (position = 'stack'' by default)\ncyl.am + \n  geom_bar(position = 'stack')   # show proportion\ncyl.am + \n  geom_bar(position = 'fill')   # dodging\ncyl.am + \n  geom_bar(position = 'dodge')   # clean up the axes with scale_ functions\nval = c('#E41A1C', '#377EB8')\nlab = c('Manual', 'Automatic')\n\ncyl.am + geom_bar(position = 'dodge', ) +\n  scale_x_discrete('Cylinders') +\n  scale_y_continuous('Number') +\n  scale_fill_manual('Transmission', values = val, labels = lab)   Setting a dummy aesthetic  # add a new column called group\nmtcars$group <- 0\n\n# create jittered plot of mtcars: mpg onto x, group onto y\nggplot(mtcars, aes(x = mpg, y = group)) +   geom_jitter()   # change the y aesthetic limits\nggplot(mtcars, aes(x = mpg, y = group)) +\n  geom_jitter() +\n  scale_y_continuous(limits = c(-2, 2))   Overplotting 1 - Point shape and transparency  # basic scatter plot: wt on x-axis and mpg on y-axis; map cyl to col\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4)   # hollow circles - an improvement\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4, shape = 1)   # add transparency - very nice\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4, shape = 1, alpha = 0.6)   Overplotting 2 - alpha with large datasets  # scatter plot: carat (x), price (y), clarity (col)\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point()   # adjust for overplotting\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point(alpha = 0.5)   # scatter plot: clarity (x), carat (y), price (col)\nggplot(diamonds, aes(x = Clarity, y = Carat, col = PricePerCt)) +\n  geom_point(alpha = 0.5)   # dot plot with jittering\nggplot(diamonds, aes(x = Clarity, y = Carat, col = PricePerCt)) +\n  geom_point(alpha = 0.5, position = 'jitter')",
            "title": "3, Aesthetics"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#4-geometries",
            "text": "Scatter plots and jittering (1)  # plot the cyl on the x-axis and wt on the y-axis\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_point()   # Use geom_jitter() instead of geom_point()\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_jitter()   # Define the position object using position_jitter(): posn.j\nposn.j <-  position_jitter(0.1)\n\n# Use posn.j in geom_point()\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_point(position = posn.j)   Scatter plots and jittering (2)  # scatter plot of vocabulary (y) against education (x). Use geom_point()\nggplot(Vocab, aes(x = education, y = vocabulary)) + \n  geom_point()   # use geom_jitter() instead of geom_point()\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter()   # set alpha to a very low 0.2\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2)   # set the shape to 1\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2, shape = 1)   Histograms  # univariate histogram\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram()   # change the bin width to 1\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 1)   # change the y aesthetic to density\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1)   # custom color code\nmyBlue <- '#377EB8'\n\n# Change the fill color to myBlue\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = myBlue)   Position  mtcars$am <- as.factor(mtcars$am)\n\n# bar plot of cyl, filled according to am\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar()   # change the position argument to stack\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'stack')   # change the position argument to fill\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'fill')   # change the position argument to dodge\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'dodge')   Overlapping bar plots  # bar plot of cyl, filled according to am\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar()   # change the position argument to 'dodge'\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'dodge')   # define posn_d with position_dodge()\nposn_d <- position_dodge(0.2)\n\n# change the position argument to posn_d\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = posn_d)   # use posn_d as position and adjust alpha to 0.6\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = posn_d, alpha = 0.6)   Overlapping histograms  # histogram, add coloring defined by cyl \nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(binwidth = 1)   # change position to identity \nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(binwidth = 1, position = 'identity')   # change geom to freqpoly (position is identity by default) \nggplot(mtcars, aes(mpg, col = cyl)) +\n  geom_freqpoly(binwidth = 1)   Facets or splom histograms  # load the package\nlibrary(reshape2)\n\n# load new data\ndata(uniranks, package = 'GDAdata')\n\n# name the variables\nnames(uniranks)[c(5, 6, 8, 8, 10, 11, 13)] <- c('AvTeach', 'NSSTeach', 'SpendperSt', 'StudentStaffR', 'Careers', 'VAddScore', 'NSSFeedb')\n\n# reshape the data frame\nur2 <- melt(uniranks[, c(3, 5:13)], id.vars = 'UniGroup', variable.name = 'uniV', value.name = 'uniX')  # Splom\nggplot(ur2, aes(uniX)) +\n  geom_histogram() +\n  xlab('') +\n  ylab('') +\n  facet_grid(UniGroup ~ uniV, scales = 'free_x')   library(ggplot2)\nlibrary(gridExtra)\ndata(Pima.tr2, package = 'MASS')\n\nh1 <- ggplot(Pima.tr2, aes(glu)) + geom_histogram()\nh2 <- ggplot(Pima.tr2, aes(bp)) + geom_histogram()\nh3 <- ggplot(Pima.tr2, aes(skin)) + geom_histogram()\nh4 <- ggplot(Pima.tr2, aes(bmi)) + geom_histogram()\nh5 <- ggplot(Pima.tr2, aes(ped)) + geom_histogram()\nh6 <- ggplot(Pima.tr2, aes(age)) + geom_histogram()\n\ngrid.arrange(h1, h2, h3, h4, h5, h6, nrow = 2)   Bar plots with color ramp, part 1  # Example of how to use a brewed color palette\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar() + \n  scale_fill_brewer(palette = 'Set1')   Vocab$education <- as.factor(Vocab$education)\nVocab$vocabulary <- as.factor(Vocab$vocabulary)\n\n# Plot education on x and vocabulary on fill\n# Use the default brewed color palette\nggplot(Vocab, aes(x = education, fill = vocabulary)) + geom_bar(position = 'fill') + scale_fill_brewer(palette = 'Set3')   Bar plots with color ramp, part 2  # Definition of a set of blue colors\nblues <- brewer.pal(9, 'Blues')\n\n# Make a color range using colorRampPalette() and the set of blues\nblue_range <- colorRampPalette(blues)\n\n# Use blue_range to adjust the color of the bars, use scale_fill_manual()\nggplot(Vocab, aes(x = education, fill = vocabulary)) + \n  geom_bar(position = 'fill') +\n  scale_fill_manual(values = blue_range(11))   Overlapping histograms (2)  # histogram\nggplot(mtcars, aes(mpg)) + geom_histogram(binwidth = 1)   # expand the histogram to fill using am\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(binwidth = 1)   # change the position argument to 'dodge'\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'dodge', binwidth = 1)   # change the position argument to 'fill'\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'fill', binwidth = 1)   # change the position argument to 'identity' and set alpha to 0.4\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'identity', binwidth = 1, alpha = 0.4)   # change fill to cyl\nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(position = 'identity', binwidth = 1, alpha = 0.4)   Line plots  # plot unemploy as a function of date using a line plot\nggplot(economics, aes(x = date, y = unemploy)) +\n  geom_line()     # adjust plot to represent the fraction of total population that is unemployed\nggplot(economics, aes(x = date, y = unemploy/pop)) +\n  geom_line()   Periods of recession  # draw the recess periods\nggplot(economics, aes(x = date, y = unemploy/pop)) +\n  geom_line() +\n  geom_rect(data = recess, inherit.aes = FALSE, aes(xmin = begin, xmax = end, ymin = -Inf, ymax = +Inf), fill = 'red', alpha = 0.2)   Multiple time series, part 1  # use gather to go from fish to fish.tidy.\nfish.tidy <- gather(fish, Species, Capture, -Year)  Multiple time series, part 2  # plot\nggplot(fish.tidy, aes(x = Year, y = Capture, col = Species)) +\n  geom_line()",
            "title": "4, Geometries"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#5-qplot-and-wrap-up",
            "text": "Using  qplot  # the old way\nplot(mpg ~ wt, data = mtcars)   # using ggplot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(shape = 1)   # Using qplot\nqplot(wt, mpg, data = mtcars)   Using aesthetics  # Categorical: cyl\nqplot(wt, mpg, data = mtcars, size = cyl)   # gear\nqplot(wt, mpg, data = mtcars, size = gear)   # Continuous: hp\nqplot(wt, mpg, data = mtcars, col = hp)   # qsec\nqplot(wt, mpg, data = mtcars, size = qsec)   Choosing geoms, part 1  # qplot() with x only\nqplot(factor(cyl), data = mtcars)   # qplot() with x and y\nqplot(factor(cyl), factor(vs), data = mtcars)   # qplot() with geom set to jitter manually\nqplot(factor(cyl), factor(vs), data = mtcars, geom = 'jitter')   Choosing geoms, part 2 - dotplot  # make a dot plot with ggplot\nggplot(mtcars, aes(cyl, wt, fill = am)) + \n  geom_dotplot(stackdir = 'center', binaxis = 'y')   # qplot with geom 'dotplot', binaxis = 'y' and stackdir = 'center'\nqplot(as.numeric(cyl), wt, data = mtcars, fill = am, geom = 'dotplot', stackdir = 'center', binaxis = 'y')   Chicken weight  # base\nggplot(ChickWeight, aes(x = Time, y = weight)) +\n  geom_line(aes(group = Chick))   # color\nggplot(ChickWeight, aes(x = Time, y = weight, col = Diet)) +\n  geom_line(aes(group = Chick))   # lines\nggplot(ChickWeight, aes(x = Time, y = weight, col = Diet)) +\n  geom_line(aes(group = Chick), alpha = 0.3) +\n  geom_smooth(lwd = 2, se = FALSE)   Titanic  # Use ggplot() for the first instruction\nggplot(titanic, aes(x = factor(Pclass), fill = factor(Sex))) +\n  geom_bar(position = 'dodge')   # Use ggplot() for the second instruction\nggplot(titanic, aes(x = factor(Pclass), fill = factor(Sex))) +\n  geom_bar(position = 'dodge') +\n  facet_grid('. ~ Survived')   # position jitter\nposn.j <- position_jitter(0.5, 0)\n\n# Use ggplot() for the last instruction\nggplot(titanic, aes(x = factor(Pclass), y = Age, col = factor(Sex))) +\n  geom_jitter(size = 3, alpha = 0.5, position = posn.j) +\n  facet_grid('. ~ Survived')",
            "title": "5, qplot and wrap-up"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#section-2",
            "text": "",
            "title": "SECTION 2"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#1-statistics",
            "text": "Smoothing  # scatter plot with LOESS smooth with a CI ribbon\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth()   # scatter plot with LOESS smooth without CI\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(se = FALSE)   # scatter plot with an OLS linear model\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = 'lm')   # scatter plot with an OLS linear model without points\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_smooth(method = 'lm', se = FALSE)   Grouping variables  # cyl as a factor variable\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE)   # set the group aesthetic\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl), group = 1)) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = F)   # add a second smooth layer in which the group aesthetic is set\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'lm', se = FALSE, aes(group = 1))   Modifying  stat_smooth  # change the LOESS span\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 0.7, method = 'auto')   # method = 'auto' is by default  # set the model to the default LOESS and use a span of 0.7\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1), col = 'black', span = 0.7)   # set col to 'All', inside the aes layer\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1, col = 'All cyl'), span = 0.7)   # add `scale_color_manual` to change the colors\nmyColors <- c(brewer.pal(3, 'Dark2'), 'black')\n\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1, col = 'All cyl'), span = 0.7) +\n  scale_color_manual('Cylinders', values = myColors)   Modifying  stat_smooth  (2)  # jittered scatter plot, add a linear model (lm) smooth\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2) +\n  stat_smooth(method = 'lm', se = FALSE)   # only lm, colored by year\nggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) +\n  stat_smooth(method = 'lm', se = FALSE)   # set a color brewer palette\nggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) + \n  stat_smooth(method = 'lm', se = FALSE) +\n  scale_color_brewer('Accent')   # change col and group, specify alpha, size and geom, and add scale_color_gradient\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + \n  stat_smooth(method = 'lm', se = FALSE, alpha = 0.6, size = 2, geom = 'path') +\n  scale_color_brewer('Blues') +\n  scale_color_gradientn(colors = brewer.pal(9, 'YlOrRd'))   Quantiles  # use stat_quantile instead of stat_smooth\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) +  stat_quantile(alpha = 0.6, size = 2) + \n  scale_color_gradientn(colors = brewer.pal(9,'YlOrRd'))   # set quantile to 0.5\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + \n  stat_quantile(alpha = 0.6, size = 2, quantiles = c(0.5)) + \n  scale_color_gradientn(colors = brewer.pal(9,'YlOrRd'))   Sum  # plot with linear and loess model\np <- ggplot(Vocab, aes(x = education, y = vocabulary)) + \n  stat_smooth(method = 'loess', aes(col = 'red'), se = F) + \n  stat_smooth(method = 'lm', aes(col = 'blue'), se = F) + \n  scale_color_discrete('Model', labels = c('red' = 'LOESS', 'blue' = 'lm'))\n\np   # add stat_sum (by overall proportion)\np + \n  stat_sum()   #aes(group = 1)  # set size range\np + \n  stat_sum() + \n  scale_size(range = c(1,10))   # proportional within years of education; set group aesthetic\np + \n  stat_sum(aes(group = education))   # set the n\np + \n  stat_sum(aes(group = education, size = ..n..))   Preparations  # convert cyl and am to factors\nmtcars$cyl <- as.factor(mtcars$cyl)\nmtcars$am <- as.factor(mtcars$am)\n\n# define positions\nposn.d <- position_dodge(width = 0.1)\nposn.jd <- position_jitterdodge(jitter.width = 0.1, dodge.width = 0.2)\nposn.j <- position_jitter(width = 0.2)\n\n# base layers\nwt.cyl.am <- ggplot(mtcars, aes(x = cyl, y = wt, col = am, group = am, fill = am))  Plotting variations  # base layer\nwt.cyl.am <- ggplot(mtcars, aes(x = cyl,  y = wt, col = am, fill = am, group = am))  # jittered, dodged scatter plot with transparent points\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6)   # mean and sd\nwt.cyl.am +\n  geom_point(position = posn.jd, alpha = 0.6) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = posn.d)   # mean and 95% CI\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6) + \n  stat_summary(fun.data = mean_cl_normal, position = posn.d)   # mean and SD with T-tipped error bars\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6) + \n  stat_summary(geom = 'point', fun.y = mean, position = posn.d) + \n  stat_summary(geom = 'errorbar', fun.data = mean_sdl, fun.args = list(mult = 1), width = 0.1, position = posn.d)",
            "title": "1, Statistics"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#2-coordinates-and-facets",
            "text": "Zooming In  # basic\np <- ggplot(mtcars, aes(x = wt, y = hp, col = am)) + \n  geom_point() + \n  geom_smooth()  # add scale_x_continuous\np + \n  scale_x_continuous(limits = c(3, 6), expand = c(0,0))   # zoom in\np + \n  coord_cartesian(xlim = c(3, 6))   Aspect Ratio  # scatter plot\nbase.plot <- ggplot(iris, aes(y = Sepal.Width, x = Sepal.Length, col = Species)) + \n  geom_jitter() + \n  geom_smooth(method = 'lm', se = FALSE)  # default aspect ratio\n# fix aspect ratio (1:1)\nbase.plot + \n  coord_equal()   base.plot + \n  coord_fixed()   Pie Charts  # stacked bar plot\nthin.bar <- ggplot(mtcars, aes(x = 1, fill = cyl)) + \n  geom_bar()\n\nthin.bar   # convert thin.bar to pie chart\nthin.bar + \n  coord_polar(theta = 'y')   # create stacked bar plot\nwide.bar <- ggplot(mtcars, aes(x = 1, fill = cyl)) + \n  geom_bar(width = 1)\n\nwide.bar   # Convert wide.bar to pie chart\nwide.bar + coord_polar(theta = 'y')   Facets: the basics  # scatter plot\np <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point()  # separate rows according am\n# facet_grid(rows ~ cols)\np + \n  facet_grid(am ~ .)   # separate columns according to cyl\n# facet_grid(rows ~ cols)\np + facet_grid(. ~ cyl)   # separate by both columns and rows \n# facet_grid(rows ~ cols)\np + \n  facet_grid(am ~ cyl)   Many variables  # create the `cyl_am` col and `myCol` vector\nmtcars$cyl_am <- paste(mtcars$cyl, mtcars$am, sep = '_')\n\nmyCol <- rbind(brewer.pal(9, 'Blues')[c(3,6,8)],\n               brewer.pal(9, 'Reds')[c(3,6,8)])  # scatter plot, add color scale\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) + \n  geom_point() + \n  scale_color_manual(values = myCol)   # facet according on rows and columns\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) +\n  geom_point() + \n  scale_color_manual(values = myCol) + \n  facet_grid(gear ~ vs)   # add more variables\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am, size = disp)) + \n  geom_point() + \n  scale_color_manual(values = myCol) + \n  facet_grid(gear ~ vs)   Dropping levels  # scatter plot\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point()   # facet rows according to `vore`\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point() + \n  facet_grid(vore ~ .)   # specify scale and space arguments to free up rows\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point() + \n  facet_grid(vore ~ ., scale = 'free_y', space = 'free_y')",
            "title": "2, Coordinates and Facets"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#3-themes",
            "text": "Rectangles  # separate columns according to cyl\n# facet_grid(rows ~ cols)\nmtcars$cyl <- c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4)\n\nmtcars$Cylinders <- factor(mtcars$cyl)\n\nz <- ggplot(mtcars, aes(x = wt, y = mpg, col = Cylinders)) + \n  geom_point(size = 2, alpha = 0.7) + \n  facet_grid(. ~ cyl) + \n  labs(x = 'Weight (lb/1000)', y = 'Miles/(US) gallon') + \n  geom_smooth(method = 'lm', se = FALSE) +\n  theme_base() +\n  scale_colour_economist()\nz   # change the plot background color to myPink (#FEE0D2)\nmyPink <- '#FEE0D2'\n\nz + \n  theme(plot.background = element_rect(fill = myPink))   # adjust the border to be a black line of size 3\nz + \n  theme(plot.background = element_rect(fill = myPink, color = 'black', size = 3))   # adjust the border to be a black line of size 3\nz + \n  theme(plot.background = element_rect(color = 'black', size = 3))   # set panel.background, legend.key, legend.background and strip.background to element_blank()\nz + \n  theme(plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank())   Lines  # Extend z with theme() and three arguments\nz +\n    theme(panel.grid = element_blank(), axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'))   Text  # extend z with theme() function and four arguments\nmyRed <- '#99000D'\n\nz +\n    theme(strip.text = element_text(size = 16, color = myRed), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'))   Legends  # move legend by position\nz + \n  theme(legend.position = c(0.85, 0.85))   # change direction\nz + \n  theme(legend.direction = 'horizontal')   # change location by name\nz + \n  theme(legend.position = 'bottom')   # remove legend entirely\nz + \n  theme(legend.position = 'none')   Positions  # increase spacing between facets\nz + \n  theme(panel.margin.x = unit(2, 'cm'))   # add code to remove any excess plot margin space\nz + \n  theme(panel.margin.x = unit(2, 'cm'), plot.margin = unit(c(0,0,0,0), 'cm'))   Update Themestheme update  # theme layer saved as an object, theme_pink\ntheme_pink <- theme(panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank(), plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.grid = element_blank(), axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'), strip.text = element_text(size = 16, color = myRed), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'), legend.position = 'none')  z2 <- z\n\n# apply theme_pink to z2\nz2 + \n  theme_pink   # change code so that old theme is saved as old\nold <- theme_update(panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank(), plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.grid = element_blank(),axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'), strip.text = element_text(size = 16, color = myRed), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'), legend.position = 'none')  # display the plot z2\ntheme_set(theme_pink)\n\nz2 + \n  theme_pink   # restore the old plot\ntheme_set(old)\n\nz2   Exploring ggthemes  # apply theme_tufte\n# set the theme with theme_set\ntheme_set(theme_tufte())\n\n# or apply it in the ggplot command\nz2 + \n  theme_tufte()   # apply theme_tufte, modified\n# set the theme with theme_set\ntheme_set(theme_tufte() + \n  theme(legend.position = c(0.9, 0.9), axis.title = element_text(face = 'italic', size = 12),  legend.title = element_text(face = 'italic', size = 12)))\n\n# or apply it in the ggplot command\nz2 + \n  theme_tufte() +\n  theme(legend.position = c(0.9, 0.9),        axis.title = element_text(face = 'italic', size = 12), legend.title = element_text(face = 'italic', size = 12))   # apply theme_igray\n# set the theme with `theme_set`\ntheme_set(theme_igray())\n\n# or apply it in the ggplot command\nz2 + \n  theme_igray()   # apply `theme_igray`, modified\n# set the theme with `theme_set`\ntheme_set(theme_igray() + \n  theme(legend.position = c(0.9, 0.9), legend.key = element_blank(), legend.background = element_rect(fill = 'grey90')))\n\nz2 + \n  # Or apply it in the ggplot command\n  theme_igray() +\n  theme(legend.position = c(0.9, 0.9),\n        legend.key = element_blank(),\n        legend.background = element_rect(fill = 'grey90'))",
            "title": "3, Themes"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#4-best-practices",
            "text": "Bar Plots (1)  # base layers\nm <- ggplot(mtcars, aes(x = cyl, y = wt))  # dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar', fill = 'skyblue') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1)   Bar Plots (2)  # base layers\nm <- ggplot(mtcars, aes(x = cyl,y = wt, col = am, fill = am))  # dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1)   # set position dodge in each `stat` function\nm + \n  stat_summary(fun.y = mean, geom = 'bar', position = 'dodge') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1, position = 'dodge')   # set your dodge `posn` manually\nposn.d <- position_dodge(0.9)  # redraw dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar', position = posn.d) +  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1, position = posn.d)   Bar Plots (3)  # base layers\nmtcars.cyl <- mtcars %>% group_by(cyl) %>% summarise(wt.avg = mean(wt))\nmtcars.cyl  ## # A tibble: 3 \u00d7 2\n##     cyl   wt.avg\n##   <dbl>    <dbl>\n## 1     4 2.285727\n## 2     6 3.117143\n## 3     8 3.999214  m <- ggplot(mtcars.cyl, aes(x = cyl, y = wt.avg))\nm   # draw bar plot\nm + \n  geom_bar(stat = 'identity', fill = 'skyblue')   Pie Charts (1)  # bar chart to pie chart\nggplot(mtcars, aes(x = cyl, fill = am)) + geom_bar(position = 'fill')   ggplot(mtcars, aes(x = cyl, fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl)   ggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl)   ggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl) + coord_polar(theta = 'y')   ggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill', width = 1) + facet_grid(. ~ cyl) + coord_polar(theta = 'y')   Parallel coordinate plot  # parallel coordinates plot using `GGally`\n# all columns except `am` (`am` column is the 9th)\ngroup_by_am <- 9\nmy_names_am <- (1:11)[-group_by_am]  # parallel plot; each variable plotted as a z-score transformation\nggparcoord(mtcars, columns = my_names_am, groupColumn = group_by_am, alpha = 0.8)   # scaled between 0-1 and most discriminating variable first\nggparcoord(mtcars, columns = my_names_am, groupColumn = group_by_am, alpha = 0.8, scale = 'uniminmax', order = 'anyClass')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species') # xlab, ylab, scale_x_discrete, them   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'uniminmax')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'globalminmax')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', mapping = aes(size = 1))   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', alphaLines = 0.3)   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'center')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scaleSummary = 'median', missing = 'exclude')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', order = 'allClass') # or custom filter   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'std')   Splom  library(dplyr)\n\ndata(Pima.tr2, package = 'MASS')\nPimaV <- select(Pima.tr2, glu:age)\n\nggpairs(PimaV, diag = list(continuous = 'density'), axisLabels = 'show')   Heat Maps  # create color palette\nmyColors <- brewer.pal(9, 'Reds')  # heat map\nggplot(barley, aes(x = year, y = variety, fill = yield)) + \n  geom_tile()   # add facet_wrap(~ variable); not like facet_grid(. ~ variable)\nggplot(barley, aes(x = year, y = variety, fill = yield)) + \n  geom_tile() + \n  facet_wrap( ~ site, ncol = 1)   # \nggplot(barley, aes(x = year, y = variety, fill = yield)) + geom_tile() + facet_wrap( ~ site, ncol = 1) + \n  scale_fill_gradientn(colors = myColors)   Heat Maps Alternatives (1)  # line plots\nggplot(barley, aes(x = year, y = yield, col = variety, group = variety)) + geom_line() + \n  facet_wrap(facets = ~ site, nrow = 1)   Heat Maps Alternatives (2)  # overlapping ribbon plot\nggplot(barley, aes(x = year, y = yield, col = site, group = site, fill = site)) + geom_line() + \n  stat_summary(fun.y = mean, geom = 'line') + \n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'ribbon', col = NA, alpha = 0.1)",
            "title": "4, Best Practices"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#5-case-study",
            "text": "Sort and order  # reorder\ndata(Cars93, package = 'MASS')\n\nCars93 <- within(Cars93, TypeWt <- reorder(Type, Weight, mean))\n\nCars93 <- within(Cars93, Type1 <- factor(Type, levels = c('Small', 'Sporty', 'Compact', 'Midsize', 'Large', 'Van')))\n\nwith(Cars93, table(TypeWt, Type1))  ##          Type1\n## TypeWt    Small Sporty Compact Midsize Large Van\n##   Small      21      0       0       0     0   0\n##   Sporty      0     14       0       0     0   0\n##   Compact     0      0      16       0     0   0\n##   Midsize     0      0       0      22     0   0\n##   Large       0      0       0       0    11   0\n##   Van         0      0       0       0     0   9  ggplot(Cars93, aes(TypeWt, 100/MPG.city)) + \n  geom_boxplot() + \n  ylab('Gallons per 100 miles') + \n  xlab('Car type')   Cars93 <- within(Cars93, {\n  levels(Type1) <- c('Small', 'Large', 'Midsize', 'Small', 'Sporty', 'Large')\n})\n\nggplot(Cars93, aes(TypeWt, 100/MPG.city)) + \n  geom_boxplot() + \n  ylab('Gallons per 100 miles') + \n  xlab('Car type')   Ensemble plots  library(gridExtra)\n\ndata(Fertility, package = 'AER')\n\np0 <- ggplot(Fertility) + geom_histogram(binwidth = 1) + ylab('')\np1 <- p0 + aes(x = age)\np2 <- p0 + aes(x = work) + xlab('Weeks worked in 1979')\n\nk <- ggplot(Fertility) + \n  geom_bar() + ylab('') + \n  ylim(0, 250000)\n\np3 <- k + aes(x = morekids) + \n  xlab('has more children')\np4 <- k + aes(x = gender1) + \n  xlab('first child')\np5 <- k + aes(x = gender2) + \n  xlab('second child')\np6 <- k + aes(x = afam) + \n  xlab('African-American')\np7 <- k + aes(x = hispanic) + \n  xlab('Hispanic')\np8 <- k + aes(x = other) + \n  xlab('other race')\n\ngrid.arrange(arrangeGrob(p1, p2, ncol = 2, widths = c(3, 3)), arrangeGrob(p3, p4, p5, p6, p7, p8, ncol = 6), nrow = 2, heights = c(1.25, 1))   Exploring Data  # histogram\nggplot(adult, aes(x = SRAGE_P)) + \n  geom_histogram()   # histogram\nggplot(adult, aes(x = BMI_P)) + \n  geom_histogram()   # color, default binwidth\nggplot(adult,aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(binwidth = 1)   Data Cleaning  # remove individual aboves 84\nadult <- adult[adult$SRAGE_P <= 84, ] \n\n# remove individuals with a BMI below 16 and above or equal to 52\nadult <- adult[adult$BMI_P >= 16 & adult$BMI_P < 52, ]\n\n# relabel race\nadult$RACEHPR2 <- factor(adult$RACEHPR2, labels = c('Latino', 'Asian', 'African American', 'White'))\n\n# relabel the BMI categories variable\nadult$RBMI <- factor(adult$RBMI, labels = c('Under-weight', 'Normal-weight', 'Over-weight', 'Obese'))  Multiple Histograms  # color palette BMI_fill\nBMI_fill <- scale_fill_brewer('BMI Category', palette = 'Reds')  # histogram, add BMI_fill and customizations\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(binwidth = 1) + \n  BMI_fill + facet_grid(RBMI ~ .) + \n  theme_classic()   Alternatives  # count histogram\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(binwidth = 1) +\n  BMI_fill   # density histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1) +\n  BMI_fill   # faceted count histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(binwidth = 1) +\n  BMI_fill + facet_grid(RBMI ~ .)   # faceted density histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1) +\n  BMI_fill + facet_grid(RBMI ~ .)   # density histogram with `position = 'fill'`\nggplot(adult, aes (x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1, position = 'fill') +\n  BMI_fill   # accurate histogram\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..count../sum(..count..)), binwidth = 1, position = 'fill') +\n  BMI_fill   Do Things Manually  # an attempt to facet the accurate frequency histogram from before (failed)\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..count../sum(..count..)), binwidth = 1, position = 'fill') +\n  BMI_fill +\n  facet_grid(RBMI ~ .)   # create DF with `table()`\nDF <- table(adult$RBMI, adult$SRAGE_P)\n\n# use apply on DF to get frequency of each group\nDF_freq <- apply(DF, 2, function(x) x/sum(x))\n\n# melt on DF to create DF_melted\nDF_melted <- melt(DF_freq)\n\n# change names of DF_melted\nnames(DF_melted) <- c('FILL', 'X', 'value')  # add code to make this a faceted plot\nggplot(DF_melted, aes(x = X, y = value, fill = FILL)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  BMI_fill + \n  facet_grid(FILL ~ .)   Merimeko/Mosaic Plot  # The initial contingency table\nDF <- as.data.frame.matrix(table(adult$SRAGE_P, adult$RBMI))\n\n# Add the columns groupsSum, xmax and xmin. Remove groupSum again.\nDF$groupSum <- rowSums(DF)\nDF$xmax <- cumsum(DF$groupSum)\nDF$xmin <- DF$xmax - DF$groupSum\n# The groupSum column needs to be removed, don't remove this line\nDF$groupSum <- NULL\n\n# Copy row names to variable X\nDF$X <- row.names(DF)\n\n# Melt the dataset\nDF_melted <- melt(DF, id.vars = c('X', 'xmin', 'xmax'), variable.name = 'FILL')\n\n# dplyr call to calculate ymin and ymax - don't change\nDF_melted <- DF_melted %>% \n  group_by(X) %>% \n  mutate(ymax = cumsum(value/sum(value)),\n         ymin = ymax - value/sum(value))\n\n# Plot rectangles - don't change.\nggplot(DF_melted, aes(ymin = ymin, \n                 ymax = ymax,\n                 xmin = xmin, \n                 xmax = xmax, \n                 fill = FILL)) + \n  geom_rect(colour = 'white') +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  BMI_fill +\n  theme_tufte()   Adding statistics  # perform chi.sq test (`RBMI` and `SRAGE_P`)\nresults <- chisq.test(table(adult$RBMI, adult$SRAGE_P))\n\n# melt results$residuals and store as resid\nresid <- melt(results$residuals)\n\n# change names of resid\nnames(resid) <- c('FILL', 'X', 'residual')\n\n# merge the two datasets\nDF_all <- merge(DF_melted, resid)  # update plot command\nggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) +\n  geom_rect() +\n  scale_fill_gradient2() +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  theme_tufte()   Adding text  # position for labels on x axis\nDF_all$xtext <- DF_all$xmin + (DF_all$xmax - DF_all$xmin) / 2\n\n# position for labels on y axis\nindex <- DF_all$xmax == max(DF_all$xmax)\nDF_all$ytext <- DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2  # plot\nggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) + \n  geom_rect(col = 'white') +\n  # geom_text for ages (i.e. the x axis)\n  geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) +\n  # geom_text for BMI (i.e. the fill axis)\n  geom_text(aes(x = max(xmax), y = ytext, label = FILL), size = 3, hjust = 1, show.legend  = FALSE) +\n  scale_fill_gradient2() +\n  theme_tufte() +\n  theme(legend.position = 'bottom')   Generalizations  # script generalized into a function\nmosaicGG <- function(data, X, FILL) {\n  # Proportions in raw data\n  DF <- as.data.frame.matrix(table(data[[X]], data[[FILL]]))\n  DF$groupSum <- rowSums(DF)\n  DF$xmax <- cumsum(DF$groupSum)\n  DF$xmin <- DF$xmax - DF$groupSum\n  DF$X <- row.names(DF)\n  DF$groupSum <- NULL\n  DF_melted <- melt(DF, id = c('X', 'xmin', 'xmax'), variable.name = 'FILL')\n\n  DF_melted <- DF_melted %>% \n    group_by(X) %>% \n    mutate(ymax = cumsum(value/sum(value)),\n           ymin = ymax - value/sum(value))\n\n  # Chi-sq test\n  results <- chisq.test(table(data[[FILL]], data[[X]])) # fill and then x\n  resid <- melt(results$residuals)\n  names(resid) <- c('FILL', 'X', 'residual')\n  # Merge data\n  DF_all <- merge(DF_melted, resid)\n   # Positions for labels\n  DF_all$xtext <- DF_all$xmin + (DF_all$xmax - DF_all$xmin)/2\n  index <- DF_all$xmax == max(DF_all$xmax)\n  DF_all$ytext <- DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2\n\n  # plot\n  g <- ggplot(DF_all, aes(ymin = ymin,  ymax = ymax, xmin = xmin, \n                          xmax = xmax, fill = residual)) + \n    geom_rect(col = 'white') +\n    geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) + geom_text(aes(x = max(xmax),  y = ytext, label = FILL), size = 3, hjust = 1, show.legend = FALSE) +\n    scale_fill_gradient2('Residuals') +\n    scale_x_continuous('Individuals', expand = c(0,0)) +\n    scale_y_continuous('Proportion', expand = c(0,0)) +\n    theme_tufte() +\n    theme(legend.position = 'bottom')\n  print(g)\n}  # BMI described by age (in x)\nmosaicGG(adult, 'SRAGE_P','RBMI')   # poverty described by age (in x)\nmosaicGG(adult, 'SRAGE_P', 'POVLL')   # `am` described by `cyl` (in x)\nmosaicGG(mtcars, 'cyl', 'am')   # `Vocab` described by education\nmosaicGG(Vocab, 'education', 'vocabulary')",
            "title": "5, Case Study"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#section-3",
            "text": "",
            "title": "SECTION 3"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#section-4-cheat-list",
            "text": "ggplot(data, aes(x = , y = ), col = , fill = , size = , labels = , alpha \n= , shape = , line = , position = \u2018jitter\u2019)  + geom_point()\n\n+ geom_point(aes(), col = , position = posn.j)\n\n+ geom_jitter()\n\n+ facet_grid(. ~ x) # y ~ x\n\n+ scale_x_continous('Sepal Length', limits = c(2, 8), breaks = seq(2, 8, 3))\n\n+ scale_color_discrete('Species', labels = c('a', 'b', 'c'))\n\n+ labs(x = , y = , col = )  posn.j <- position_jitter(width = 0.1)  Data   diamonds , prices of 50,000 round cut diamonds.  economics , economics_long, US economic time series.  faithfuld , 2d density estimate of Old Faithful data.  luv_colours , colors().  midwest , midwest demographics.  mpg , fuel economy data from 1999 and 2008 for 38 popular models \n    of car.  msleep , an updated and expanded version of the mammals \n    sleep dataset.  presidential , terms of 11 presidents from Eisenhower to Obama.  seals , vector field of seal movements.  txhousing , Housing sales in TX.   Aesthetics   x-axis.  y-asix.  color.  fill.  size (points, lines).  labels.  alpha.  shape (points).  linetype (lines).  aes , Define aesth.etic mappings.  aes_  (aes_q, aes_string), Define aesthetic mappings from \n    strings, or quoted calls and formulas.  aes_all , Given a character vector, create a set of \n    identity mappings.  aes_auto , Automatic aesthetic mapping.  aes_colour_fill_alpha  (color, colour, fill), Colour related \n    aesthetics: colour, fill and alpha.  aes_group_order  (group), Aesthetics: group. \n    aes_linetype_size_shape (linetype, shape, size), Differentiation \n    related aesthetics: linetype, size, shape.  aes_position  (x, xend, xmax, xmin, y, yend, ymax, ymin), Position \n    related aesthetics: x, y, xmin, xmax, ymin, ymax, xend, yend.   Position   position_dodge , Adjust position by dodging overlaps to the side.  position_fill  (position_stack), Stack overlapping objects on top \n    of one another.  position_identity , Don\u2019t adjust position  position_nudge , Nudge points.  position_jitter , Jitter points to avoid overplotting.  position_jitterdodge , Adjust position by simultaneously dodging \n    and jittering.   Scales   expand_limits , Expand the plot limits with data.  guides , Set guides for each scale.  guide_legend , Legend guide.  guide_colourbar  (guide_colorbar), Continuous colour bar guide.  lims  (xlim, ylim), Convenience functions to set the axis limits.  scale_alpha  (scale_alpha_continuous, scale_alpha_discrete), \n    Alpha scales.  scale_colour_brewer  (scale_color_brewer, \n    scale_color_distiller, scale_colour_distiller, \n    scale_fill_brewer, scale_fill_distiller), Sequential, diverging \n    and qualitative colour scales from colorbrewer.org  scale_colour_gradient  (scale_color_continuous, \n    scale_color_gradient, scale_color_gradient2, \n    scale_color_gradientn, scale_colour_continuous, \n    scale_colour_date, scale_colour_datetime, \n    scale_colour_gradient2, scale_colour_gradientn, \n    scale_fill_continuous, scale_fill_date, scale_fill_datetime, \n    scale_fill_gradient, \n    scale_fill_gradient2, scale_fill_gradientn).  scale_colour_grey  (scale_color_grey, scale_fill_grey), \n    Sequential grey colour scale.  scale_colour_hue  (scale_color_discrete, scale_color_hue, \n    scale_colour_discrete, scale_fill_discrete, scale_fill_hue), \n    Qualitative colour scale with evenly spaced hues.  scale_identity  (scale_alpha_identity, scale_color_identity, \n    scale_colour_identity, scale_fill_identity, \n    scale_linetype_identity, scale_shape_identity, \n    scale_size_identity), Use values without scaling.  scale_manual  (scale_alpha_manual, scale_color_manual, \n    scale_colour_manual, scale_fill_manual, scale_linetype_manual, \n    scale_shape_manual, scale_size_manual), Create your own \n    discrete scale.  scale_linetype  (scale_linetype_continuous, \n    scale_linetype_discrete), Scale for line patterns.  scale_shape  (scale_shape_continuous, scale_shape_discrete), \n    Scale for shapes, aka glyphs.  scale_size  (scale_radius, scale_size_area, \n    scale_size_continuous, scale_size_date, scale_size_datetime, \n    scale_size_discrete), Scale size (area or radius).  scale_x_discrete  (scale_y_discrete), Discrete position.  labs  (ggtitle, xlab, ylab), Change axis labels and legend titles.  update_labels , Update axis/legend labels.   Geometries   point.  line.  histogram.  bar.  boxplot.  geom_abline  (geom_hline, geom_vline), Lines: horizontal, \n    vertical, and specified by slope and intercept.  geom_bar  (stat_count), Bars, rectangles with bases on x-axis  geom_bin2d  (stat_bin2d, stat_bin_2d), Add heatmap of 2d \n    bin counts.  geom_blank , Blank, draws nothing.  geom_boxplot  (stat_boxplot), Box and whiskers plot.  geom_contour  (stat_contour), Display contours of a 3d surface \n    in 2d.  geom_count (stat_sum), Count the number of observations at \n    each location.  geom_crossbar  (geom_errorbar, geom_linerange, geom_pointrange), \n    Vertical intervals: lines, crossbars & errorbars.  geom_density  (stat_density), Display a smooth density estimate.  geom_density_2d  (geom_density2d, stat_density2d, \n    stat_density_2d), Contours from a 2d density estimate.  geom_dotplot , Dot plot  geom_errorbarh , Horizontal error bars.  geom_freqpoly  (geom_histogram, stat_bin), Histograms and \n    frequency polygons.  geom_hex  (stat_bin_hex, stat_binhex), Hexagon binning.  geom_jitter , Points, jittered to reduce overplotting.  geom_label  (geom_text), Textual annotations.  geom_map , Polygons from a reference map.  geom_path  (geom_line, geom_step), Connect observations.  geom_point , Points, as for a scatterplot.  geom_polygon , Polygon, a filled path.  geom_quantile  (stat_quantile), Add quantile lines from a \n    quantile regression.  geom_raster  (geom_rect, geom_tile), Draw rectangles.  geom_ribbon  (geom_area), Ribbons and area plots.  geom_rug , Marginal rug plots.  geom_segment  (geom_curve), Line segments and curves.  geom_smooth  (stat_smooth), Add a smoothed conditional mean.  geom_violin  (stat_ydensity), Violin plot.   Facets   columns.  rows.  facet_grid , Lay out panels in a grid.  facet_null , Facet specification: a single panel.  facet_wrap , Wrap a 1d ribbon of panels into 2d.  labeller , Generic labeller function for facets.  label_bquote , Backquoted labeller.   Annotation   annotate , Create an annotation layer.  annotation_custom , Annotation: Custom grob.  annotation_logticks , Annotation: log tick marks.  annotation_map , Annotation: maps.  annotation_raster , Annotation: High-performance \n    rectangular tiling.  borders , Create a layer of map borders.   Fortify   fortify , Fortify a model with data.  fortify-multcomp  (fortify.cld, fortify.confint.glht, fortify.glht, \n    fortify.summary.glht), Fortify methods for objects produced by.  fortify.lm , Supplement the data fitted to a linear model with \n    model fit statistics.  fortify.map , Fortify method for map objects.  fortify.sp  (fortify.Line, fortify.Lines, fortify.Polygon, \n    fortify.Polygons, fortify.SpatialLinesDataFrame, \n    fortify.SpatialPolygons, fortify.SpatialPolygonsDataFrame), Fortify \n    method for classes from the sp package.  map_data , Create a data frame of map data.   Statistics   binning.  smoothing.  descriptive.  inferential.  stat_ecdf , Empirical Cumulative Density Function.  stat_ellipse , Plot data ellipses.  stat_function , Superimpose a function.  stat_identity , Identity statistic.  stat_qq  (geom_qq), Calculation for quantile-quantile plot.  stat_summary_2d  (stat_summary2d, stat_summary_hex), Bin and \n    summarise in 2d (rectangle & hexagons)  stat_unique , Remove duplicates.  Coordinates.  cartesian.  fixes.  polar.  limites.  coord_cartesian , Cartesian coordinates.  coord_fixed  (coord_equal), Cartesian coordinates with fixed \n    relationship between x and y scales.  coord_flip , Flipped cartesian coordinates.  coord_map  (coord_quickmap), Map projections.  coord_polar , Polar coordinates.  coord_trans , Transformed cartesian coordinate system.   Themes   theme_bw  theme_grey  theme_classic  theme_minimal  ggthemes",
            "title": "SECTION 4 - Cheat List"
        },
        {
            "location": "/Plot_snippets_-_ggvis/",
            "text": "Documentation\n\n\nDataset\n\n\nThe \nggvis\n Package\n\n\n1, The Grammar of Graphics\n\n\n2, Lines and Syntax\n\n\n3, Transformations\n\n\n4, Interactivity and Layers\n\n\n5, Customizing Axes, Legends, and\n\n    Scales\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\nggvis\n generates html outputs. Graphics presented here are images:\n\n    .png files; .gif files when specified.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\n\n\nggvis\n Overview\n\n\nggvis\n Cookbook\n\n\n\n\nDataset\n\u00b6\n\n\nFor most examples, we use the \nmtcars\n, \npressure\n, \nfaithful\n datasets.\n\n\nThe \nggvis\n Package\n\u00b6\n\n\nlibrary(ggvis)\n\n\n\n\n1, The Grammar of Graphics\n\u00b6\n\n\nStart to explore \n\n\n# change the code below to plot the disp variable of mtcars on the x axis\nmtcars %>%\n  ggvis(~disp, ~mpg) %>%\n  layer_points()\n\n\n\n\n\n\nggvis\n and its capabilities \n\n\n# Change the code below to make a graph with red points\nmtcars %>%\n  ggvis(~wt, ~mpg, fill := \"red\") %>%\n  layer_points()\n\n\n\n\n\n\n# Change the code below draw smooths instead of points\nmtcars %>%\n  ggvis(~wt, ~mpg) %>%\n  layer_smooths()\n\n\n\n\n\n\n# Change the code below to make a graph containing both points and a smoothed summary line\nmtcars %>%\n  ggvis(~wt, ~mpg) %>%\n  layer_points() %>%\n  layer_smooths()\n\n\n\n\n\n\nggvis\n grammar ~ graphics grammar\n\n\n# Make a scatterplot of the pressure dataset\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_points\n\n\n\n\n\n\n# Adapt the code you wrote for the first challenge: show bars instead of points\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_bars\n\n\n\n\n\n\n# Adapt the code you wrote for the first challenge: show lines instead of points\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines\n\n\n\n\n\n\n# Adapt the code you wrote for the first challenge: map the fill property to the temperature variable\npressure %>%\n  ggvis(~temperature, ~pressure, fill = ~temperature) %>%\n  layer_points\n\n\n\n\n\n\n# Extend the code you wrote for the previous challenge: map the size property to the pressure variable\npressure %>%\n  ggvis(~temperature, ~pressure, fill = ~temperature, size = ~pressure) %>%\n  layer_points\n\n\n\n\n\n\n4 essential components of a graph\n\n\nfaithful %>%\n    ggvis(~waiting, ~eruptions, fill := \"red\") %>%\n    layer_points() %>%\n    add_axis(\"y\", title = \"Duration of eruption (m)\",\n             values = c(2, 3, 4, 5), subdivide = 9) %>%\n    add_axis(\"x\", title = \"Time since previous eruption (m)\")\n\n\n\n\n\n\n2, Lines and Syntax\n\u00b6\n\n\nThree operators: \n%>%\n, \n=\n and \n:=\n\n\nlayer_points(ggvis(faithful, ~waiting, ~eruptions))\n\n\n\n\n\n\n# Rewrite the code with the pipe operator     \nfaithful %>%\n  ggvis(~waiting, ~eruptions) %>%\n  layer_points()\n\n\n\n\n\n\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_points()\n\n\n\n\n\n\n# Modify this graph to map the size property to the pressure variable\npressure %>%\n  ggvis(~temperature, ~pressure, size = ~pressure) %>%\n  layer_points()\n\n\n\n\n\n\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_points()\n\n\n\n\n\n\n# Modify this graph by setting the size property\npressure %>%\n  ggvis(~temperature, ~pressure, size := 100) %>%\n  layer_points()\n\n\n\n\n\n\npressure %>% \n  ggvis(~temperature, ~pressure, fill = \"red\") %>%\n  layer_points()\n\n\n\n\n\n\n# Fix this code to set the fill property to red\npressure %>% \n  ggvis(~temperature, ~pressure, fill := \"red\") %>%\n  layer_points()\n\n\n\n\n\n\nReferring to different objects\n\n\nred <- \"green\"\npressure$red <- pressure$temperature\n\npressure %>%\n  ggvis(~temperature, ~pressure, fill := red) %>%\n  layer_points()\n\n\n\n\n\n\nProperties for points\n\n\n# Change the code to set the fills using pressure$black\npressure %>%\n  ggvis(~temperature, ~pressure, fill := ~'black') %>%\n  layer_points()\n\n\n\n\n\n\n# Plot the faithful data as described in the second instruction\nfaithful %>% \n  ggvis(~waiting, ~eruptions, size = ~eruptions, opacity := 0.5, fill := \"blue\", stroke := \"black\") %>% \n  layer_points()\n\n\n\n\n\n\n# Plot the faithful data as described in the third instruction\nfaithful %>% \n  ggvis(~waiting, ~eruptions, size := 100, fill := \"red\", fillOpacity = ~eruptions, stroke := \"red\", shape := \"cross\") %>%\n  layer_points()\n\n\n\n\n\n\nProperties for lines\n\n\n# Change the code below to use the lines mark\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines()\n\n\n\n\n\n\n# Set the properties described in the second instruction in the graph below\npressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"red\", strokeWidth := 2, strokeDash := 6) %>%\n  layer_lines()\n\n\n\n\n\n\nDisplay model fits\n\n\nlayer_lines()\n will always connect the points in your plot from the\n\nleftmost point to the rightmost point. This can be undesirable if you\n\nare trying to plot a specific shape.\n\n\nlayer_paths()\n: this mark connects the points in the order that they\n\nappear in the data set. So the paths mark will connect the point that\n\ncorresponds to the first row of the data to the point that corresponds\n\nto the second row of data, and so on - no matter where those points\n\nappear in the graph.\n\n\n# change the third line of code to plot a map of Texas\nlibrary(maps)\nlibrary(ggplot2)\n\ntexas <- ggplot2::map_data(\"state\", region = \"texas\")\n\ntexas %>%\n  ggvis(~long, ~lat) %>%\n  layer_paths()\n\n\n\n\n\n\n# Same plot, but set the fill property of the texas map to dark orange\ntexas %>%\n  ggvis(~long, ~lat, fill := \"darkorange\") %>%\n  layer_paths()\n\n\n\n\n\n\ncompute_smooth()\n to simplify model fits\n\n\ncompute_model_prediction()\n is a useful function to use with line\n\ngraphs. It takes a data frame as input and returns a new data frame as\n\noutput. The new data frame will contain the x and y values of a line\n\nfitted to the data in the original data frame.\n\n\nGenerate the x and y coordinates for a LOESS smooth line.\n\n\nfaithful %>%\n  compute_model_prediction(eruptions ~ waiting, model = \"lm\") %>%\n  head(10)\n\n\n\n\n##       pred_    resp_\n## 1  43.00000 1.377986\n## 2  43.67089 1.428724\n## 3  44.34177 1.479461\n## 4  45.01266 1.530199\n## 5  45.68354 1.580937\n## 6  46.35443 1.631674\n## 7  47.02532 1.682412\n## 8  47.69620 1.733150\n## 9  48.36709 1.783888\n## 10 49.03797 1.834625\n\n\n\n# Compute the x and y coordinates for a loess smooth line that predicts mpg with the wt\nmtcars %>%\n  compute_smooth(mpg ~ wt) %>%\n  head(10)\n\n\n\n\n##       pred_    resp_\n## 1  1.513000 32.08897\n## 2  1.562506 31.68786\n## 3  1.612013 31.28163\n## 4  1.661519 30.87037\n## 5  1.711025 30.45419\n## 6  1.760532 30.03318\n## 7  1.810038 29.60745\n## 8  1.859544 29.17711\n## 9  1.909051 28.74224\n## 10 1.958557 28.30017\n\n\n\n#model = \"loess\" is set by default\n\n\n\n\n3, Transformations\n\u00b6\n\n\nHistograms (1)\n\n\n# Build a histogram of the waiting variable of the faithful data set.\nfaithful %>%\n  ggvis(~waiting) %>%\n  layer_histograms()\n\n\n\n\n\n\n# Build the same histogram, but with a binwidth (width argument) of 5 units\nfaithful %>%\n  ggvis(~waiting) %>%\n  layer_histograms(width = 5)\n\n\n\n\n\n\nHistograms (2)\n\n\nfaithful %>%\n  ggvis(~waiting) %>%\n  layer_histograms(width = 5)\n\n\n\n\n\n\n# Transform the code: just compute the bins instead of plotting a histogram\nfaithful %>%\n  compute_bin(~waiting, width = 5)\n\n\n\n\n##    count_ x_ xmin_ xmax_ width_\n## 1      13 45  42.5  47.5      5\n## 2      24 50  47.5  52.5      5\n## 3      29 55  52.5  57.5      5\n## 4      21 60  57.5  62.5      5\n## 5      13 65  62.5  67.5      5\n## 6      13 70  67.5  72.5      5\n## 7      42 75  72.5  77.5      5\n## 8      58 80  77.5  82.5      5\n## 9      38 85  82.5  87.5      5\n## 10     17 90  87.5  92.5      5\n## 11      4 95  92.5  97.5      5\n\n\n\n# Combine the solution to the first challenge with layer_rects() to build a histogram\nfaithful %>%\n  compute_bin(~waiting, width = 5) %>%\n  ggvis(x = ~xmin_, x2 = ~xmax_, y = 0, y2 = ~count_) %>%\n  layer_rects()\n\n\n\n\n\n\nDensity plots\n\n\n# Combine compute_density() with layer_lines() to make a density plot of the waiting variable.\nfaithful %>%\n  compute_density(~waiting) %>%\n  ggvis(~pred_,~resp_) %>%\n  layer_lines()\n\n\n\n\n\n\n# Build a density plot directly using layer_densities. Use the correct variables and properties.\nfaithful %>%\n  ggvis(~waiting) %>%\n  layer_densities(fill := \"green\")\n\n\n\n\n\n\nShortcuts\n\n\n# Complete the code to plot a bar graph of the cyl factor.\nmtcars %>%\n  ggvis(~factor(cyl)) %>%\n  layer_bars()\n\n\n\n\n\n\n# Adapt the solution to the first challenge to just calculate the count values. No plotting!\nmtcars %>%\n  compute_count(~factor(cyl))\n\n\n\n\n##   count_ x_\n## 1     11  4\n## 2      7  6\n## 3     14  8\n\n\n\nggvis\n and \ngroup_by\n\n\nmtcars %>%\n  group_by(am) %>%\n  ggvis(~mpg, ~wt, stroke = ~factor(am)) %>%\n  layer_smooths()\n\n\n\n\n\n\n# Change the code to plot a unique smooth line for each value of the cyl variable.\nmtcars %>%\n  group_by(cyl) %>%\n  ggvis(~mpg, ~wt, stroke = ~factor(cyl)) %>%\n  layer_smooths()\n\n\n\n\n\n\n# Adapt the graph to contain a separate density for each value of cyl.\nmtcars %>%\n  group_by(cyl) %>%\n  ggvis(~mpg) %>%\n  layer_densities()\n\n\n\n\n\n\n# Copy and alter the solution to the second challenge to map the fill property to a categorical version of cyl.\nmtcars %>%\n  group_by(cyl) %>%\n  ggvis(~mpg) %>%\n  layer_densities(fill = ~factor(cyl))\n\n\n\n\n\n\ngroup_by()\n versus \ninteraction()\n\n\nmtcars %>%\n  group_by(cyl) %>%\n  ggvis(~mpg) %>%\n  layer_densities(fill = ~factor(cyl))\n\n\n\n\n\n\n# Alter the graph: separate density for each unique combination of 'cyl' and 'am'.\nmtcars %>%\n  group_by(cyl,am) %>%\n  ggvis(~mpg, fill = ~factor(cyl)) %>%\n  layer_densities()\n\n#factor(cyl),factor(am)\n\n\n\n\n\n\nmtcars %>%\n  group_by(cyl, am) %>%\n  ggvis(~mpg, fill = ~factor(cyl)) %>%\n  layer_densities()\n\n\n\n\n\n\n# Update the graph to map `fill` to the unique combinations of the grouping variables.\nmtcars %>%\n  group_by(cyl, am) %>%\n  ggvis(~mpg, fill = ~interaction(cyl,am)) %>% \n  layer_densities()\n\n\n\n\n\n\nChaining is a virtue\n\n\nmtcars %>%\n    group_by(cyl, am) %>%\n    ggvis(~mpg, fill = ~interaction(cyl, am)) %>%\n    layer_densities()\n\n\n\n\n\n\nThis call is exactly equivalent to the following piece of code that is\n\nvery hard to read:\n\n\nlayer_densities(ggvis(group_by(mtcars, cyl, am), ~mpg, fill = ~interaction(cyl, am)))\n\n\n\n\n\n\n4, Interactivity and Layers\n\u00b6\n\n\nThe basics of interactive plots\n\n\n# Run this code and inspect the output. Follow the link in the instructions for the interactive version\nfaithful %>% \n  ggvis(~waiting, ~eruptions, fillOpacity := 0.5, \n        shape := input_select(label = \"Choose shape:\", choices = c(\"circle\", \"square\", \"cross\", \"diamond\", \"triangle-up\", \"triangle-down\"))) %>% \n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\n# Copy the first code chunk and alter the code to make the fill property interactive using a select box\nfaithful %>%\n  ggvis(~waiting, ~eruptions, fillOpacity := 0.5, shape := input_select(label = \"Choose shape:\", choices = c(\"circle\", \"square\", \"cross\", \"diamond\", \"triangle-up\", \"triangle-down\")), fill := input_select(label = \"Choose color:\", choices = c(\"black\",\"red\",\"blue\",\"green\"))) %>%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\n# Add radio buttons to control the fill of the plot\nmtcars %>% \n  ggvis(~mpg, ~wt, fill := input_radiobuttons(label = \"Choose color:\", choices = c(\"black\",\"red\",\"blue\",\"green\"))) %>%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\nInput widgets in more detail\n\n\nmtcars %>%\n  ggvis(~mpg, ~wt, fill := input_radiobuttons(label = \"Choose color:\", choices = c(\"black\", \"red\", \"blue\", \"green\"))) %>%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\n# Change the radiobuttons widget to a text widget \nmtcars %>%\n  ggvis(~mpg, ~wt, fill := input_text(label = \"Choose color:\", value = c(\"black\"))) %>%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\nmtcars %>%\n  ggvis(~mpg, ~wt) %>%\n  layer_points()\n\n\n\n\n\n\n# Map the fill property to a select box that returns variable names\nmtcars %>%\n  ggvis(~mpg, ~wt, fill = input_select(label = \"Choose fill variable:\", choices = names(mtcars), map = as.name)) %>%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\nInput widgets in more detail (2)\n\n\n# Map the fill property to a select box that returns variable names\nmtcars %>%\n  ggvis(~mpg, ~wt, fill = input_select(label = \"Choose fill variable:\", choices = names(mtcars), map = as.name)) %>%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\nControl parameters and values\n\n\n# Map the bindwidth to a numeric field (\"Choose a binwidth:\")\nmtcars %>%\n  ggvis(~mpg) %>%\n  layer_histograms(width = input_numeric(value = 1, label = \"Choose a binwidth:\"))\n\n\n\n\n.gif file:\n\n\n\n\n# Map the binwidth to a slider bar (\"Choose a binwidth:\") with the correct specifications\nmtcars %>%\n  ggvis(~mpg) %>%\n  layer_histograms(width = input_slider(min = 1, max = 20, label = \"Choose a binwidth:\"))\n\n\n\n\n.gif file:\n\n\n\n\nMulti-layered plots and their properties\n\n\n# Add a layer of points to the graph below.\npressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"skyblue\") %>% layer_lines() %>%\n  layer_points()\n\n\n\n\n\n\n# Copy and adapt the solution to the first instruction below so that only the lines layer uses a skyblue stroke.\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines(stroke := \"skyblue\") %>%\n  layer_points()\n\n\n\n\n\n\n# Rewrite the code below so that only the points layer uses the shape property.\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines(stroke := \"skyblue\") %>%\n  layer_points(shape := \"triangle-up\")\n\n\n\n\n\n\n# Refactor the code for the graph below to make it as concise as possible\npressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"skyblue\", strokeOpacity := 0.5, strokeWidth := 5) %>%\n  layer_lines() %>%\n  layer_points(fill = ~temperature, shape := \"triangle-up\", size := 300)\n\n\n\n\n\n\nMulti-layered plots and their properties (2)\n\n\n# Rewrite the code below so that only the points layer uses the shape property.\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines(stroke := \"skyblue\") %>%\n  layer_points(shape := \"triangle-up\")\n\n\n\n\n\n\n# Refactor the code for the graph below to make it as concise as possible\npressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"skyblue\",\n        strokeOpacity := 0.5, strokeWidth := 5) %>%\n  layer_lines() %>%\n  layer_points(fill = ~temperature, shape := \"triangle-up\", size := 300)\n\n\n\n\n\n\nThere is no limit on the number of layers!\n\n\n# Create a graph containing a scatterplot, a linear model and a smooth line.\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_points() %>%\n  layer_lines(stroke := \"black\", opacity := 0.5) %>%\n  layer_model_predictions(model = \"lm\", stroke := \"navy\") %>%\n  layer_smooths(stroke := \"skyblue\")\n\n\n\n\n\n\nTaking local and global to the next level\n\n\npressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"darkred\") %>%\n  layer_lines(stroke := \"orange\", strokeDash := 5, strokeWidth := 5) %>%\n  layer_points(shape := \"circle\", size := 100, fill := \"lightgreen\") %>%\n  layer_smooths()\n\n\n\n\n\n\n5, Customizing Axes, Legends, and Scales\n\u00b6\n\n\nAxes\n\n\n# add the title of the x axis: \"Time since previous eruption (m)\"\nfaithful %>% \n  ggvis(~waiting, ~eruptions) %>% \n  layer_points() %>% \n  add_axis(\"y\", title = \"Duration of eruption (m)\") %>%\n  add_axis(\"x\", title = \"Time since previous eruption (m)\")\n\n\n\n\n\n\n# Add to the code to place a labelled tick mark at 50, 60, 70, 80, 90 on the x axis.\nfaithful %>% \n  ggvis(~waiting, ~eruptions) %>% \n  layer_points() %>% \n  add_axis(\"y\", title = \"Duration of eruption (m)\", values = c(2, 3, 4, 5), subdivide = 9) %>% \n  add_axis(\"x\", title = \"Time since previous eruption (m)\", values = c(50, 60, 70, 80, 90), subdivide = 9)\n\n\n\n\n\n\n# Add to the code below to change the location of the y axis\nfaithful %>% \n  ggvis(~waiting, ~eruptions) %>% \n  layer_points() %>%\n  add_axis(\"x\", orient = \"top\") %>% add_axis(\"y\", orient = \"right\")\n\n\n\n\n\n\nLegends\n\n\n# Add a legend to the plot below: use the correct title and orientation\nfaithful %>% \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions))) %>% \n  layer_points() %>%\n  add_legend(\"fill\", title = \"~ duration (m)\", orient = \"left\")\n\n\n\n\n\n\n#add_legend(vis, scales = NULL, orient = \"right\", title = NULL, format = NULL, values = NULL, properties = NULL)\n\n# Use add_legend() to combine the legends in the plot below. Adjust its properties as instructed.\nfaithful %>% \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), \n        size = ~round(eruptions))  %>%\n  layer_points() %>%\n  add_legend(c(\"fill\", \"shape\", \"size\"), title = \"~ duration (m)\", values = c(2,3,4,5))\n\n\n\n\n\n\nLegends (2)\n\n\n# Fix the legend\nfaithful %>% \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), \n        size = ~round(eruptions)) %>% \n    layer_points() %>% \n    add_legend(c(\"fill\", \"shape\", \"size\"), title = \"~ duration (m)\")\n\n\n\n\n\n\nScale types\n\n\n# Add to the code below to make the stroke color range from \"darkred\" to \"orange\".\nmtcars %>% \n  ggvis(~wt, ~mpg, fill = ~disp, stroke = ~disp, strokeWidth := 2) %>%\n  layer_points() %>%\n  scale_numeric(\"fill\", range = c(\"red\", \"yellow\")) %>% scale_numeric(\"stroke\", range = c(\"darkred\", \"orange\"))\n\n\n\n\n\n\n# Change the graph below to make the fill colors range from green to beige.\nmtcars %>% ggvis(~wt, ~mpg, fill = ~hp) %>%\n  layer_points() %>% scale_numeric(\"fill\", range = c(\"green\", \"beige\"))\n\n\n\n\n\n\n# Create a scale that will map `factor(cyl)` to a new range of colors: purple, blue, and green. \nmtcars %>% ggvis(~wt, ~mpg, fill = ~factor(cyl)) %>%\n  layer_points() %>% scale_nominal(\"fill\", range = c(\"purple\", \"blue\", \"green\"))\n\n\n\n\n\n\nAdjust any visual property\n\n\n# Add a scale that limits the range of opacity from 0.2 to 1. \nmtcars %>%\n  ggvis(x = ~wt, y = ~mpg, fill = ~factor(cyl), opacity = ~hp) %>%\n  layer_points() %>%\n  scale_numeric(\"opacity\", range = c(0.2,1))\n\n\n\n\n\n\n# Add a second scale that will expand the x axis to cover data values from 0 to 6.\nmtcars %>%\n  ggvis(~wt, ~mpg, fill = ~disp) %>%\n  layer_points() %>%\n  scale_numeric(\"y\", domain = c(0, NA)) %>%\n  scale_numeric(\"x\", domain = c(0, 6))\n\n\n\n\n\n\nAdjust any visual property (2)\n\n\n# Add a second scale to set domain for x\nmtcars %>%\n  ggvis(~wt, ~mpg, fill = ~disp) %>%\n  layer_points() %>%\n  scale_numeric(\"y\", domain = c(0, NA)) %>%\n  scale_numeric(\"x\", domain = c(0, 6))\n\n\n\n\n\n\n\u201c\n=\n\u201d versus \u201c\n:=\n\u201c\n\n\n# Set the fill value to the color variable instead of mapping it, and see what happens\nmtcars$color <- c(\"red\", \"teal\", \"#cccccc\", \"tan\")\n\nmtcars %>%\n  ggvis(x = ~wt, y = ~mpg, fill = ~color) %>%\n  layer_points()\n\n\n\n\n\n\nmtcars %>%\n  ggvis(x = ~wt, y = ~mpg, fill := ~color) %>%\n  layer_points()",
            "title": "Plot Snippets - ggvis"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#dataset",
            "text": "For most examples, we use the  mtcars ,  pressure ,  faithful  datasets.",
            "title": "Dataset"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#the-ggvis-package",
            "text": "library(ggvis)",
            "title": "The ggvis Package"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#1-the-grammar-of-graphics",
            "text": "Start to explore   # change the code below to plot the disp variable of mtcars on the x axis\nmtcars %>%\n  ggvis(~disp, ~mpg) %>%\n  layer_points()   ggvis  and its capabilities   # Change the code below to make a graph with red points\nmtcars %>%\n  ggvis(~wt, ~mpg, fill := \"red\") %>%\n  layer_points()   # Change the code below draw smooths instead of points\nmtcars %>%\n  ggvis(~wt, ~mpg) %>%\n  layer_smooths()   # Change the code below to make a graph containing both points and a smoothed summary line\nmtcars %>%\n  ggvis(~wt, ~mpg) %>%\n  layer_points() %>%\n  layer_smooths()   ggvis  grammar ~ graphics grammar  # Make a scatterplot of the pressure dataset\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_points   # Adapt the code you wrote for the first challenge: show bars instead of points\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_bars   # Adapt the code you wrote for the first challenge: show lines instead of points\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines   # Adapt the code you wrote for the first challenge: map the fill property to the temperature variable\npressure %>%\n  ggvis(~temperature, ~pressure, fill = ~temperature) %>%\n  layer_points   # Extend the code you wrote for the previous challenge: map the size property to the pressure variable\npressure %>%\n  ggvis(~temperature, ~pressure, fill = ~temperature, size = ~pressure) %>%\n  layer_points   4 essential components of a graph  faithful %>%\n    ggvis(~waiting, ~eruptions, fill := \"red\") %>%\n    layer_points() %>%\n    add_axis(\"y\", title = \"Duration of eruption (m)\",\n             values = c(2, 3, 4, 5), subdivide = 9) %>%\n    add_axis(\"x\", title = \"Time since previous eruption (m)\")",
            "title": "1, The Grammar of Graphics"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#2-lines-and-syntax",
            "text": "Three operators:  %>% ,  =  and  :=  layer_points(ggvis(faithful, ~waiting, ~eruptions))   # Rewrite the code with the pipe operator     \nfaithful %>%\n  ggvis(~waiting, ~eruptions) %>%\n  layer_points()   pressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_points()   # Modify this graph to map the size property to the pressure variable\npressure %>%\n  ggvis(~temperature, ~pressure, size = ~pressure) %>%\n  layer_points()   pressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_points()   # Modify this graph by setting the size property\npressure %>%\n  ggvis(~temperature, ~pressure, size := 100) %>%\n  layer_points()   pressure %>% \n  ggvis(~temperature, ~pressure, fill = \"red\") %>%\n  layer_points()   # Fix this code to set the fill property to red\npressure %>% \n  ggvis(~temperature, ~pressure, fill := \"red\") %>%\n  layer_points()   Referring to different objects  red <- \"green\"\npressure$red <- pressure$temperature\n\npressure %>%\n  ggvis(~temperature, ~pressure, fill := red) %>%\n  layer_points()   Properties for points  # Change the code to set the fills using pressure$black\npressure %>%\n  ggvis(~temperature, ~pressure, fill := ~'black') %>%\n  layer_points()   # Plot the faithful data as described in the second instruction\nfaithful %>% \n  ggvis(~waiting, ~eruptions, size = ~eruptions, opacity := 0.5, fill := \"blue\", stroke := \"black\") %>% \n  layer_points()   # Plot the faithful data as described in the third instruction\nfaithful %>% \n  ggvis(~waiting, ~eruptions, size := 100, fill := \"red\", fillOpacity = ~eruptions, stroke := \"red\", shape := \"cross\") %>%\n  layer_points()   Properties for lines  # Change the code below to use the lines mark\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines()   # Set the properties described in the second instruction in the graph below\npressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"red\", strokeWidth := 2, strokeDash := 6) %>%\n  layer_lines()   Display model fits  layer_lines()  will always connect the points in your plot from the \nleftmost point to the rightmost point. This can be undesirable if you \nare trying to plot a specific shape.  layer_paths() : this mark connects the points in the order that they \nappear in the data set. So the paths mark will connect the point that \ncorresponds to the first row of the data to the point that corresponds \nto the second row of data, and so on - no matter where those points \nappear in the graph.  # change the third line of code to plot a map of Texas\nlibrary(maps)\nlibrary(ggplot2)\n\ntexas <- ggplot2::map_data(\"state\", region = \"texas\")\n\ntexas %>%\n  ggvis(~long, ~lat) %>%\n  layer_paths()   # Same plot, but set the fill property of the texas map to dark orange\ntexas %>%\n  ggvis(~long, ~lat, fill := \"darkorange\") %>%\n  layer_paths()   compute_smooth()  to simplify model fits  compute_model_prediction()  is a useful function to use with line \ngraphs. It takes a data frame as input and returns a new data frame as \noutput. The new data frame will contain the x and y values of a line \nfitted to the data in the original data frame.  Generate the x and y coordinates for a LOESS smooth line.  faithful %>%\n  compute_model_prediction(eruptions ~ waiting, model = \"lm\") %>%\n  head(10)  ##       pred_    resp_\n## 1  43.00000 1.377986\n## 2  43.67089 1.428724\n## 3  44.34177 1.479461\n## 4  45.01266 1.530199\n## 5  45.68354 1.580937\n## 6  46.35443 1.631674\n## 7  47.02532 1.682412\n## 8  47.69620 1.733150\n## 9  48.36709 1.783888\n## 10 49.03797 1.834625  # Compute the x and y coordinates for a loess smooth line that predicts mpg with the wt\nmtcars %>%\n  compute_smooth(mpg ~ wt) %>%\n  head(10)  ##       pred_    resp_\n## 1  1.513000 32.08897\n## 2  1.562506 31.68786\n## 3  1.612013 31.28163\n## 4  1.661519 30.87037\n## 5  1.711025 30.45419\n## 6  1.760532 30.03318\n## 7  1.810038 29.60745\n## 8  1.859544 29.17711\n## 9  1.909051 28.74224\n## 10 1.958557 28.30017  #model = \"loess\" is set by default",
            "title": "2, Lines and Syntax"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#3-transformations",
            "text": "Histograms (1)  # Build a histogram of the waiting variable of the faithful data set.\nfaithful %>%\n  ggvis(~waiting) %>%\n  layer_histograms()   # Build the same histogram, but with a binwidth (width argument) of 5 units\nfaithful %>%\n  ggvis(~waiting) %>%\n  layer_histograms(width = 5)   Histograms (2)  faithful %>%\n  ggvis(~waiting) %>%\n  layer_histograms(width = 5)   # Transform the code: just compute the bins instead of plotting a histogram\nfaithful %>%\n  compute_bin(~waiting, width = 5)  ##    count_ x_ xmin_ xmax_ width_\n## 1      13 45  42.5  47.5      5\n## 2      24 50  47.5  52.5      5\n## 3      29 55  52.5  57.5      5\n## 4      21 60  57.5  62.5      5\n## 5      13 65  62.5  67.5      5\n## 6      13 70  67.5  72.5      5\n## 7      42 75  72.5  77.5      5\n## 8      58 80  77.5  82.5      5\n## 9      38 85  82.5  87.5      5\n## 10     17 90  87.5  92.5      5\n## 11      4 95  92.5  97.5      5  # Combine the solution to the first challenge with layer_rects() to build a histogram\nfaithful %>%\n  compute_bin(~waiting, width = 5) %>%\n  ggvis(x = ~xmin_, x2 = ~xmax_, y = 0, y2 = ~count_) %>%\n  layer_rects()   Density plots  # Combine compute_density() with layer_lines() to make a density plot of the waiting variable.\nfaithful %>%\n  compute_density(~waiting) %>%\n  ggvis(~pred_,~resp_) %>%\n  layer_lines()   # Build a density plot directly using layer_densities. Use the correct variables and properties.\nfaithful %>%\n  ggvis(~waiting) %>%\n  layer_densities(fill := \"green\")   Shortcuts  # Complete the code to plot a bar graph of the cyl factor.\nmtcars %>%\n  ggvis(~factor(cyl)) %>%\n  layer_bars()   # Adapt the solution to the first challenge to just calculate the count values. No plotting!\nmtcars %>%\n  compute_count(~factor(cyl))  ##   count_ x_\n## 1     11  4\n## 2      7  6\n## 3     14  8  ggvis  and  group_by  mtcars %>%\n  group_by(am) %>%\n  ggvis(~mpg, ~wt, stroke = ~factor(am)) %>%\n  layer_smooths()   # Change the code to plot a unique smooth line for each value of the cyl variable.\nmtcars %>%\n  group_by(cyl) %>%\n  ggvis(~mpg, ~wt, stroke = ~factor(cyl)) %>%\n  layer_smooths()   # Adapt the graph to contain a separate density for each value of cyl.\nmtcars %>%\n  group_by(cyl) %>%\n  ggvis(~mpg) %>%\n  layer_densities()   # Copy and alter the solution to the second challenge to map the fill property to a categorical version of cyl.\nmtcars %>%\n  group_by(cyl) %>%\n  ggvis(~mpg) %>%\n  layer_densities(fill = ~factor(cyl))   group_by()  versus  interaction()  mtcars %>%\n  group_by(cyl) %>%\n  ggvis(~mpg) %>%\n  layer_densities(fill = ~factor(cyl))   # Alter the graph: separate density for each unique combination of 'cyl' and 'am'.\nmtcars %>%\n  group_by(cyl,am) %>%\n  ggvis(~mpg, fill = ~factor(cyl)) %>%\n  layer_densities()\n\n#factor(cyl),factor(am)   mtcars %>%\n  group_by(cyl, am) %>%\n  ggvis(~mpg, fill = ~factor(cyl)) %>%\n  layer_densities()   # Update the graph to map `fill` to the unique combinations of the grouping variables.\nmtcars %>%\n  group_by(cyl, am) %>%\n  ggvis(~mpg, fill = ~interaction(cyl,am)) %>% \n  layer_densities()   Chaining is a virtue  mtcars %>%\n    group_by(cyl, am) %>%\n    ggvis(~mpg, fill = ~interaction(cyl, am)) %>%\n    layer_densities()   This call is exactly equivalent to the following piece of code that is \nvery hard to read:  layer_densities(ggvis(group_by(mtcars, cyl, am), ~mpg, fill = ~interaction(cyl, am)))",
            "title": "3, Transformations"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#4-interactivity-and-layers",
            "text": "The basics of interactive plots  # Run this code and inspect the output. Follow the link in the instructions for the interactive version\nfaithful %>% \n  ggvis(~waiting, ~eruptions, fillOpacity := 0.5, \n        shape := input_select(label = \"Choose shape:\", choices = c(\"circle\", \"square\", \"cross\", \"diamond\", \"triangle-up\", \"triangle-down\"))) %>% \n  layer_points()  .gif file:   # Copy the first code chunk and alter the code to make the fill property interactive using a select box\nfaithful %>%\n  ggvis(~waiting, ~eruptions, fillOpacity := 0.5, shape := input_select(label = \"Choose shape:\", choices = c(\"circle\", \"square\", \"cross\", \"diamond\", \"triangle-up\", \"triangle-down\")), fill := input_select(label = \"Choose color:\", choices = c(\"black\",\"red\",\"blue\",\"green\"))) %>%\n  layer_points()  .gif file:   # Add radio buttons to control the fill of the plot\nmtcars %>% \n  ggvis(~mpg, ~wt, fill := input_radiobuttons(label = \"Choose color:\", choices = c(\"black\",\"red\",\"blue\",\"green\"))) %>%\n  layer_points()  .gif file:   Input widgets in more detail  mtcars %>%\n  ggvis(~mpg, ~wt, fill := input_radiobuttons(label = \"Choose color:\", choices = c(\"black\", \"red\", \"blue\", \"green\"))) %>%\n  layer_points()  .gif file:   # Change the radiobuttons widget to a text widget \nmtcars %>%\n  ggvis(~mpg, ~wt, fill := input_text(label = \"Choose color:\", value = c(\"black\"))) %>%\n  layer_points()  .gif file:   mtcars %>%\n  ggvis(~mpg, ~wt) %>%\n  layer_points()   # Map the fill property to a select box that returns variable names\nmtcars %>%\n  ggvis(~mpg, ~wt, fill = input_select(label = \"Choose fill variable:\", choices = names(mtcars), map = as.name)) %>%\n  layer_points()  .gif file:   Input widgets in more detail (2)  # Map the fill property to a select box that returns variable names\nmtcars %>%\n  ggvis(~mpg, ~wt, fill = input_select(label = \"Choose fill variable:\", choices = names(mtcars), map = as.name)) %>%\n  layer_points()  .gif file:   Control parameters and values  # Map the bindwidth to a numeric field (\"Choose a binwidth:\")\nmtcars %>%\n  ggvis(~mpg) %>%\n  layer_histograms(width = input_numeric(value = 1, label = \"Choose a binwidth:\"))  .gif file:   # Map the binwidth to a slider bar (\"Choose a binwidth:\") with the correct specifications\nmtcars %>%\n  ggvis(~mpg) %>%\n  layer_histograms(width = input_slider(min = 1, max = 20, label = \"Choose a binwidth:\"))  .gif file:   Multi-layered plots and their properties  # Add a layer of points to the graph below.\npressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"skyblue\") %>% layer_lines() %>%\n  layer_points()   # Copy and adapt the solution to the first instruction below so that only the lines layer uses a skyblue stroke.\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines(stroke := \"skyblue\") %>%\n  layer_points()   # Rewrite the code below so that only the points layer uses the shape property.\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines(stroke := \"skyblue\") %>%\n  layer_points(shape := \"triangle-up\")   # Refactor the code for the graph below to make it as concise as possible\npressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"skyblue\", strokeOpacity := 0.5, strokeWidth := 5) %>%\n  layer_lines() %>%\n  layer_points(fill = ~temperature, shape := \"triangle-up\", size := 300)   Multi-layered plots and their properties (2)  # Rewrite the code below so that only the points layer uses the shape property.\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_lines(stroke := \"skyblue\") %>%\n  layer_points(shape := \"triangle-up\")   # Refactor the code for the graph below to make it as concise as possible\npressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"skyblue\",\n        strokeOpacity := 0.5, strokeWidth := 5) %>%\n  layer_lines() %>%\n  layer_points(fill = ~temperature, shape := \"triangle-up\", size := 300)   There is no limit on the number of layers!  # Create a graph containing a scatterplot, a linear model and a smooth line.\npressure %>%\n  ggvis(~temperature, ~pressure) %>%\n  layer_points() %>%\n  layer_lines(stroke := \"black\", opacity := 0.5) %>%\n  layer_model_predictions(model = \"lm\", stroke := \"navy\") %>%\n  layer_smooths(stroke := \"skyblue\")   Taking local and global to the next level  pressure %>%\n  ggvis(~temperature, ~pressure, stroke := \"darkred\") %>%\n  layer_lines(stroke := \"orange\", strokeDash := 5, strokeWidth := 5) %>%\n  layer_points(shape := \"circle\", size := 100, fill := \"lightgreen\") %>%\n  layer_smooths()",
            "title": "4, Interactivity and Layers"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#5-customizing-axes-legends-and-scales",
            "text": "Axes  # add the title of the x axis: \"Time since previous eruption (m)\"\nfaithful %>% \n  ggvis(~waiting, ~eruptions) %>% \n  layer_points() %>% \n  add_axis(\"y\", title = \"Duration of eruption (m)\") %>%\n  add_axis(\"x\", title = \"Time since previous eruption (m)\")   # Add to the code to place a labelled tick mark at 50, 60, 70, 80, 90 on the x axis.\nfaithful %>% \n  ggvis(~waiting, ~eruptions) %>% \n  layer_points() %>% \n  add_axis(\"y\", title = \"Duration of eruption (m)\", values = c(2, 3, 4, 5), subdivide = 9) %>% \n  add_axis(\"x\", title = \"Time since previous eruption (m)\", values = c(50, 60, 70, 80, 90), subdivide = 9)   # Add to the code below to change the location of the y axis\nfaithful %>% \n  ggvis(~waiting, ~eruptions) %>% \n  layer_points() %>%\n  add_axis(\"x\", orient = \"top\") %>% add_axis(\"y\", orient = \"right\")   Legends  # Add a legend to the plot below: use the correct title and orientation\nfaithful %>% \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions))) %>% \n  layer_points() %>%\n  add_legend(\"fill\", title = \"~ duration (m)\", orient = \"left\")   #add_legend(vis, scales = NULL, orient = \"right\", title = NULL, format = NULL, values = NULL, properties = NULL)\n\n# Use add_legend() to combine the legends in the plot below. Adjust its properties as instructed.\nfaithful %>% \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), \n        size = ~round(eruptions))  %>%\n  layer_points() %>%\n  add_legend(c(\"fill\", \"shape\", \"size\"), title = \"~ duration (m)\", values = c(2,3,4,5))   Legends (2)  # Fix the legend\nfaithful %>% \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), \n        size = ~round(eruptions)) %>% \n    layer_points() %>% \n    add_legend(c(\"fill\", \"shape\", \"size\"), title = \"~ duration (m)\")   Scale types  # Add to the code below to make the stroke color range from \"darkred\" to \"orange\".\nmtcars %>% \n  ggvis(~wt, ~mpg, fill = ~disp, stroke = ~disp, strokeWidth := 2) %>%\n  layer_points() %>%\n  scale_numeric(\"fill\", range = c(\"red\", \"yellow\")) %>% scale_numeric(\"stroke\", range = c(\"darkred\", \"orange\"))   # Change the graph below to make the fill colors range from green to beige.\nmtcars %>% ggvis(~wt, ~mpg, fill = ~hp) %>%\n  layer_points() %>% scale_numeric(\"fill\", range = c(\"green\", \"beige\"))   # Create a scale that will map `factor(cyl)` to a new range of colors: purple, blue, and green. \nmtcars %>% ggvis(~wt, ~mpg, fill = ~factor(cyl)) %>%\n  layer_points() %>% scale_nominal(\"fill\", range = c(\"purple\", \"blue\", \"green\"))   Adjust any visual property  # Add a scale that limits the range of opacity from 0.2 to 1. \nmtcars %>%\n  ggvis(x = ~wt, y = ~mpg, fill = ~factor(cyl), opacity = ~hp) %>%\n  layer_points() %>%\n  scale_numeric(\"opacity\", range = c(0.2,1))   # Add a second scale that will expand the x axis to cover data values from 0 to 6.\nmtcars %>%\n  ggvis(~wt, ~mpg, fill = ~disp) %>%\n  layer_points() %>%\n  scale_numeric(\"y\", domain = c(0, NA)) %>%\n  scale_numeric(\"x\", domain = c(0, 6))   Adjust any visual property (2)  # Add a second scale to set domain for x\nmtcars %>%\n  ggvis(~wt, ~mpg, fill = ~disp) %>%\n  layer_points() %>%\n  scale_numeric(\"y\", domain = c(0, NA)) %>%\n  scale_numeric(\"x\", domain = c(0, 6))   \u201c = \u201d versus \u201c := \u201c  # Set the fill value to the color variable instead of mapping it, and see what happens\nmtcars$color <- c(\"red\", \"teal\", \"#cccccc\", \"tan\")\n\nmtcars %>%\n  ggvis(x = ~wt, y = ~mpg, fill = ~color) %>%\n  layer_points()   mtcars %>%\n  ggvis(x = ~wt, y = ~mpg, fill := ~color) %>%\n  layer_points()",
            "title": "5, Customizing Axes, Legends, and Scales"
        },
        {
            "location": "/Plot_snippets_-_Colours/",
            "text": "Default colours\n\n\nBasic colours\n\n\nRColorBrewer examples\n\n\nBuilding a palette\n\n\nGrabing colours\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDefault colours\n\u00b6\n\n\n# Set\npalette('default')\npalette()\n\n\n\n\n## [1] \"black\"   \"red\"     \"green3\"  \"blue\"    \"cyan\"    \"magenta\" \"yellow\" \n## [8] \"gray\"\n\n\n\n# Show\npar(mfrow = c(1, 2))\n\nn <- 8\npie(rep(1, n), col = FALSE, main = 'colourless')\nn <- 8\npie(rep(1, n), col = palette(), main = 'default')\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nBasic colours\n\u00b6\n\n\n# 8 types times 9 tones\nn <- 9\nrainbow(n, s = 1, v = 1, start = 0, end = max(1, n - 1)/n, alpha = 1)\n\n\n\n\n## [1] \"#FF0000FF\" \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\"\n## [7] \"#0000FFFF\" \"#AA00FFFF\" \"#FF00AAFF\"\n\n\n\nm <- (1:n)/n\ngray(m)\n\n\n\n\n## [1] \"#1C1C1C\" \"#393939\" \"#555555\" \"#717171\" \"#8E8E8E\" \"#AAAAAA\" \"#C6C6C6\"\n## [8] \"#E3E3E3\" \"#FFFFFF\"\n\n\n\nhsv(m, s = 1, v = 1, alpha = 1)\n\n\n\n\n## [1] \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" \"#0000FFFF\"\n## [7] \"#AA00FFFF\" \"#FF00AAFF\" \"#FF0000FF\"\n\n\n\nblues9\n\n\n\n\n## [1] \"#F7FBFF\" \"#DEEBF7\" \"#C6DBEF\" \"#9ECAE1\" \"#6BAED6\" \"#4292C6\" \"#2171B5\"\n## [8] \"#08519C\" \"#08306B\"\n\n\n\nheat.colors(n, alpha = 1)\n\n\n\n\n## [1] \"#FF0000FF\" \"#FF2A00FF\" \"#FF5500FF\" \"#FF8000FF\" \"#FFAA00FF\" \"#FFD500FF\"\n## [7] \"#FFFF00FF\" \"#FFFF40FF\" \"#FFFFBFFF\"\n\n\n\nterrain.colors(n, alpha = 1)\n\n\n\n\n## [1] \"#00A600FF\" \"#3EBB00FF\" \"#8BD000FF\" \"#E6E600FF\" \"#E8C32EFF\" \"#EBB25EFF\"\n## [7] \"#EDB48EFF\" \"#F0C9C0FF\" \"#F2F2F2FF\"\n\n\n\ncm.colors(n, alpha = 1)\n\n\n\n\n## [1] \"#80FFFFFF\" \"#9FFFFFFF\" \"#BFFFFFFF\" \"#DFFFFFFF\" \"#FFFFFFFF\" \"#FFDFFFFF\"\n## [7] \"#FFBFFFFF\" \"#FF9FFFFF\" \"#FF80FFFF\"\n\n\n\ntopo.colors(n, alpha = 1)\n\n\n\n\n## [1] \"#4C00FFFF\" \"#004CFFFF\" \"#00E5FFFF\" \"#00FF4DFF\" \"#4DFF00FF\" \"#E6FF00FF\"\n## [7] \"#FFFF00FF\" \"#FFDE59FF\" \"#FFE0B3FF\"\n\n\n\n# Show\npar(mfrow = c(2, 2))\n\npie(rep(1, n), col = rainbow(n, alpha = 1), main = 'rainbow')\npie(rep(1, n), col = gray(m), main = 'gray')\npie(rep(1, n), col = hsv(m, alpha = 1), main = 'hsv')\npie(rep(1, n), col = blues9, main = 'blues9')\n\n\n\n\n\n\npie(rep(1, n), col = heat.colors(n, alpha = 1), main = 'heat')\npie(rep(1, n), col = terrain.colors(n, alpha = 1), main = 'terrain')\npie(rep(1, n), col = cm.colors(n), main = 'cm.colors')\npie(rep(1, n), col = topo.colors(n, alpha = 1), main = 'topo')\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nRColorBrewer examples\n\u00b6\n\n\nlibrary(RColorBrewer)\n\n# Show all\ndisplay.brewer.all()\n\n\n\n\n\n\n# Pick a palette\nn <- 8\ncolors <- brewer.pal(n, \"BuPu\")\ncolors\n\n\n\n\n## [1] \"#F7FCFD\" \"#E0ECF4\" \"#BFD3E6\" \"#9EBCDA\" \"#8C96C6\" \"#8C6BB1\" \"#88419D\"\n## [8] \"#6E016B\"\n\n\n\npar(mfrow = c(1, 2))\npie(rep(1, n), col = colors, main = 'Sequential RdPu')\n\n# Interpolate these colors\npal <- colorRampPalette(brewer.pal(n, 'RdPu'))\npal(8)\n\n\n\n\n## [1] \"#FFF7F3\" \"#FDE0DD\" \"#FCC5C0\" \"#FA9FB5\" \"#F768A1\" \"#DD3497\" \"#AE017E\"\n## [8] \"#7A0177\"\n\n\n\npie(rep(1, n), col = pal(8), main = 'Interpolated RdPu')\n\n\n\n\n\n\n# Apply\ndata(volcano)\npar(mfrow = c(2, 1))\nimage(volcano, col = pal(8))\nimage(volcano, col = pal(30))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n# Show samples\npar(mfrow = c(2, 2))\n\nn = 9\npie(rep(1, n), col = brewer.pal(n, 'RdPu'), main = 'Sequential RdPu')\nn = 9\npie(rep(1, n), col = brewer.pal(n, 'Set1'), main = 'Qualitative Set1')\nn = 12\npie(rep(1, n), col = brewer.pal(n, 'Paired'), main = 'Qualitative Paired')\nn = 11\npie(rep(1, n), col = brewer.pal(n, 'RdBu'), main = 'Divergent RdBu')\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n# Show\nn = 8\ndarkcols <- brewer.pal(n, 'Dark2')\npie(rep(1, n), col = darkcols, main = 'Dark2')\n\n\n\n\n\n\nBuilding a palette\n\u00b6\n\n\n# All\nhead(colors())\n\n\n\n\n## [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n## [5] \"antiquewhite2\" \"antiquewhite3\"\n\n\n\nlength(colors()) # 657\n\n\n\n\n## [1] 657\n\n\n\n# Create\nmycols <- colors()[c(8, 5, 30, 53, 118, 72)] #\n# or\n# mycols <- c('aquamarine', 'antiquewhite2', 'blue4', 'chocolate1', 'deeppink2', 'cyan4')\n\n# Show\nn = 6\npie(rep(1, n), col = mycols, main = 'mycols')\n\n\n\n\n\n\n# Generate randomly\ncl <- colors(distinct = TRUE)\nset.seed(15887) # to set random generator seed\nmycols2 <- sample(cl, 7)\n\n# Show\nn = 7\npie(rep(1, n), col = mycols2, main = 'mycols2 (random)')\n\n\n\n\n\n\nGrabing colours\n\u00b6\n\n\n\n\nGrab Website Colors\n\n\nRGB Color Codes Chart",
            "title": "Plot Snippets - Colours"
        },
        {
            "location": "/Plot_snippets_-_Colours/#basic-colours",
            "text": "# 8 types times 9 tones\nn <- 9\nrainbow(n, s = 1, v = 1, start = 0, end = max(1, n - 1)/n, alpha = 1)  ## [1] \"#FF0000FF\" \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\"\n## [7] \"#0000FFFF\" \"#AA00FFFF\" \"#FF00AAFF\"  m <- (1:n)/n\ngray(m)  ## [1] \"#1C1C1C\" \"#393939\" \"#555555\" \"#717171\" \"#8E8E8E\" \"#AAAAAA\" \"#C6C6C6\"\n## [8] \"#E3E3E3\" \"#FFFFFF\"  hsv(m, s = 1, v = 1, alpha = 1)  ## [1] \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" \"#0000FFFF\"\n## [7] \"#AA00FFFF\" \"#FF00AAFF\" \"#FF0000FF\"  blues9  ## [1] \"#F7FBFF\" \"#DEEBF7\" \"#C6DBEF\" \"#9ECAE1\" \"#6BAED6\" \"#4292C6\" \"#2171B5\"\n## [8] \"#08519C\" \"#08306B\"  heat.colors(n, alpha = 1)  ## [1] \"#FF0000FF\" \"#FF2A00FF\" \"#FF5500FF\" \"#FF8000FF\" \"#FFAA00FF\" \"#FFD500FF\"\n## [7] \"#FFFF00FF\" \"#FFFF40FF\" \"#FFFFBFFF\"  terrain.colors(n, alpha = 1)  ## [1] \"#00A600FF\" \"#3EBB00FF\" \"#8BD000FF\" \"#E6E600FF\" \"#E8C32EFF\" \"#EBB25EFF\"\n## [7] \"#EDB48EFF\" \"#F0C9C0FF\" \"#F2F2F2FF\"  cm.colors(n, alpha = 1)  ## [1] \"#80FFFFFF\" \"#9FFFFFFF\" \"#BFFFFFFF\" \"#DFFFFFFF\" \"#FFFFFFFF\" \"#FFDFFFFF\"\n## [7] \"#FFBFFFFF\" \"#FF9FFFFF\" \"#FF80FFFF\"  topo.colors(n, alpha = 1)  ## [1] \"#4C00FFFF\" \"#004CFFFF\" \"#00E5FFFF\" \"#00FF4DFF\" \"#4DFF00FF\" \"#E6FF00FF\"\n## [7] \"#FFFF00FF\" \"#FFDE59FF\" \"#FFE0B3FF\"  # Show\npar(mfrow = c(2, 2))\n\npie(rep(1, n), col = rainbow(n, alpha = 1), main = 'rainbow')\npie(rep(1, n), col = gray(m), main = 'gray')\npie(rep(1, n), col = hsv(m, alpha = 1), main = 'hsv')\npie(rep(1, n), col = blues9, main = 'blues9')   pie(rep(1, n), col = heat.colors(n, alpha = 1), main = 'heat')\npie(rep(1, n), col = terrain.colors(n, alpha = 1), main = 'terrain')\npie(rep(1, n), col = cm.colors(n), main = 'cm.colors')\npie(rep(1, n), col = topo.colors(n, alpha = 1), main = 'topo')   par(mfrow = c(1, 1))",
            "title": "Basic colours"
        },
        {
            "location": "/Plot_snippets_-_Colours/#rcolorbrewer-examples",
            "text": "library(RColorBrewer)\n\n# Show all\ndisplay.brewer.all()   # Pick a palette\nn <- 8\ncolors <- brewer.pal(n, \"BuPu\")\ncolors  ## [1] \"#F7FCFD\" \"#E0ECF4\" \"#BFD3E6\" \"#9EBCDA\" \"#8C96C6\" \"#8C6BB1\" \"#88419D\"\n## [8] \"#6E016B\"  par(mfrow = c(1, 2))\npie(rep(1, n), col = colors, main = 'Sequential RdPu')\n\n# Interpolate these colors\npal <- colorRampPalette(brewer.pal(n, 'RdPu'))\npal(8)  ## [1] \"#FFF7F3\" \"#FDE0DD\" \"#FCC5C0\" \"#FA9FB5\" \"#F768A1\" \"#DD3497\" \"#AE017E\"\n## [8] \"#7A0177\"  pie(rep(1, n), col = pal(8), main = 'Interpolated RdPu')   # Apply\ndata(volcano)\npar(mfrow = c(2, 1))\nimage(volcano, col = pal(8))\nimage(volcano, col = pal(30))   par(mfrow = c(1, 1))  # Show samples\npar(mfrow = c(2, 2))\n\nn = 9\npie(rep(1, n), col = brewer.pal(n, 'RdPu'), main = 'Sequential RdPu')\nn = 9\npie(rep(1, n), col = brewer.pal(n, 'Set1'), main = 'Qualitative Set1')\nn = 12\npie(rep(1, n), col = brewer.pal(n, 'Paired'), main = 'Qualitative Paired')\nn = 11\npie(rep(1, n), col = brewer.pal(n, 'RdBu'), main = 'Divergent RdBu')   par(mfrow = c(1, 1))  # Show\nn = 8\ndarkcols <- brewer.pal(n, 'Dark2')\npie(rep(1, n), col = darkcols, main = 'Dark2')",
            "title": "RColorBrewer examples"
        },
        {
            "location": "/Plot_snippets_-_Colours/#building-a-palette",
            "text": "# All\nhead(colors())  ## [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n## [5] \"antiquewhite2\" \"antiquewhite3\"  length(colors()) # 657  ## [1] 657  # Create\nmycols <- colors()[c(8, 5, 30, 53, 118, 72)] #\n# or\n# mycols <- c('aquamarine', 'antiquewhite2', 'blue4', 'chocolate1', 'deeppink2', 'cyan4')\n\n# Show\nn = 6\npie(rep(1, n), col = mycols, main = 'mycols')   # Generate randomly\ncl <- colors(distinct = TRUE)\nset.seed(15887) # to set random generator seed\nmycols2 <- sample(cl, 7)\n\n# Show\nn = 7\npie(rep(1, n), col = mycols2, main = 'mycols2 (random)')",
            "title": "Building a palette"
        },
        {
            "location": "/Plot_snippets_-_Colours/#grabing-colours",
            "text": "Grab Website Colors  RGB Color Codes Chart",
            "title": "Grabing colours"
        },
        {
            "location": "/GoogleVis/",
            "text": "CONTENT\n\n\nThe googleVis package\n\n\nSuppress the message\n\n\nPrinting\n\n\nEmbedding a chart/map into a static website\n\n\nOther static websites\n\n\nDynamic websites\n\n\n\n\n\n\nCharts\n\n\ngvisMotionChart\n\n\nWithin an HTML iframe\n\n\nInformation from the object\n\n\n\n\n\n\nMaps\n\n\ngvisGeoChart\n\n\ngvisMap or Google Maps\n\n\n\n\n\n\nEditor\n\n\nTables with gvisTable\n\n\nDashboards with gvisMerge\n\n\nOptions\n\n\nApostrophes\n\n\n\n\n\n\n\n\nForeword\n\n\nNotes, snippets, and results.\n\n\n\n\nThe \ngoogleVis\n package\n\u00b6\n\n\nThe package provides an interface to Google\u2019s chart tools, allowing users to create interactive charts based on data frames. It included maps.\n\n\nThe interactive maps are displayed in a browser. We can plot a complete set of interactive graphs and embed them into a web page.\n\n\nSome motion charts cannot be displays in tablets and mobile phones (using HTML5) because they are rendered with Flash; Flash has to be installed on a PC.\n\n\n\n\nExamples\n.\n\n\nCharts: line, bar, column, area, stepped area, combo, scatter, bubble, customizing, candlestick (or boxplot), pie, gauge, annotation, Sankey, histogram, and motion (GapMinder-like)\n\n\nMaps: intensity, geo, choropleth, marker, Google Maps.\n\n\nTable, organizational chart, tree map, calendar, timeline, merging.\n\n\nMotion charts and some maps only work in Flash, not in HTML5 as with tablets and mobile phones).\n\n\n\n\n\n\nGallery\n.\n\n\nDocumentation\n.\n\n\nAs above.\n\n\n\n\n\n\nIntroduction\n.\n\n\nRoles\n.\n\n\nTrendlines\n.\n\n\nMarkdown\n.\n\n\nIn R, run a demo with \ndemo(googleVis)\n.\n\n\n\n\nAlways cite the package:\n\n\ncitation(\"googleVis\")\n\n\n\n\nSuppress the message\n\u00b6\n\n\nWe normally load the library\u2026\n\n\nlibrary(googleVis)\n\n\n\n\n\u2026but we can also suppress the message when loading the package.\n\n\nsuppressPackageStartupMessages(library(googleVis))\n\n\n\n\nPrinting\n\u00b6\n\n\n\n\nGenerate the chart.\n\n\n\n\nlibrary(googleVis)\n\nhead(Fruits, 3)\n\nM <- gvisMotionChart(Fruits,\n                     idvar='Fruit',\n                     timevar='Year',\n                     options=list(width=400, height=350))\n\n# xvar =\n# yvar =\n# colorvar =\n# sizevar =\n# date.format =\n\n\n\n\n\n\nEmulate the chart\u2026\n\n\n\n\n# In the browser, with a tag underneath\nplot(M)\n\n# In the console (code only), with a tag underneath\nM\n\n# Header part\n# Basic html and formatting tags\nprint(M, tag='header')\n\n# Actual Google visualization code\n# Can be copy-paste in a markdown or html document\nprint(M, tag='chart')\n\n# Header + visualization code = what we see in the browser\n\n# Components of the chart\nprint(M, tag='jsChart')\n\n# Basic chart caption and html footer (what is underneath)\nprint(M, tag='caption')\n\n# Save it locally\nprint(M, file=\"GoogleVis/M.html\")\n# Or\n#cat(M$html$chart, file = \"GoogleVis/M.html\")\n\n\n\n\nEmbedding a chart/map into a static website\n\u00b6\n\n\nThe chart can be standing alone as an image file; even be embedded within a text.\n\n\n\n\nIn RStudio, set the working directory with \nsetwd(' ')\n.\n\n\nSet the option to print the output in a html file.\n\n\n\n\n{r, message=FALSE}\nlibrary(googleVis)\n\nop <- options(gvis.plot.tag='chart')\n\n\n\n\n\n\nSet options back to original options.\n\n\n\n\noptions(op)\n\n\n\n\n\n\nGenerate the chart.\n\n\n\n\nlibrary(googleVis)\n\n# Set the option to print here (alternative)\noptions(gvis.plot.tag='chart')\n\n# Create the chart\nM <- gvisMotionChart(Fruits,\n                     'Fruit',\n                     'Year',\n                     options=list(width=400, height=350))\n\n# Do not print the chart in the browser\n#plot(M) # in the browser\n\n# Print the chart as a html file\ncat(M$html$chart, file = \"M.html\")\n\n\n\n\n\n\nThe file has several parts: JavaScript and HTML. The part that displays the chart is in the end.\n\n\n\n\n<!-- divChart -->\n\n<div id=\"MotionChartID2cd02805862d\" \n     style=\"width: 400; height: 350;\">\n</div>\n\n\n\n\n\n\nFollowing this bullet, in the Markdown document, paste the M.html code. First, open the HTML file to copy the code.\n\n\nModify the markup language with more styling parameters.\n\n\n\n\n<div id=\"MotionChartID2cd02805862d\" \nstyle=\"width: 400; height: 350; float:left;\";>\n</div>\n\n\n\n\n\n\nThe HTML snippet must follow the JavaScript snippet in the document. Move the HTML snippet within a text (the text can separate both snippets). \n\n\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n<div id=\"MotionChartID2cd02805862d\" \nstyle=\"width: 400; height: 350; float:left;\";>\n</div>\nDuis aute irure dolor...\n\n\n\n\nHere is the result:\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataMotionChartID2cd02805862d () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Apples\",\n2008,\n\"West\",\n98,\n78,\n20,\n\"2008-12-31\"\n],\n[\n\"Apples\",\n2009,\n\"West\",\n111,\n79,\n32,\n\"2009-12-31\"\n],\n[\n\"Apples\",\n2010,\n\"West\",\n89,\n76,\n13,\n\"2010-12-31\"\n],\n[\n\"Oranges\",\n2008,\n\"East\",\n96,\n81,\n15,\n\"2008-12-31\"\n],\n[\n\"Bananas\",\n2008,\n\"East\",\n85,\n76,\n9,\n\"2008-12-31\"\n],\n[\n\"Oranges\",\n2009,\n\"East\",\n93,\n80,\n13,\n\"2009-12-31\"\n],\n[\n\"Bananas\",\n2009,\n\"East\",\n94,\n78,\n16,\n\"2009-12-31\"\n],\n[\n\"Oranges\",\n2010,\n\"East\",\n98,\n91,\n7,\n\"2010-12-31\"\n],\n[\n\"Bananas\",\n2010,\n\"East\",\n81,\n71,\n10,\n\"2010-12-31\"\n] \n];\ndata.addColumn('string','Fruit');\ndata.addColumn('number','Year');\ndata.addColumn('string','Location');\ndata.addColumn('number','Sales');\ndata.addColumn('number','Expenses');\ndata.addColumn('number','Profit');\ndata.addColumn('string','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMotionChartID2cd02805862d() {\nvar data = gvisDataMotionChartID2cd02805862d();\nvar options = {};\noptions[\"width\"] = 400;\noptions[\"height\"] = 350;\noptions[\"state\"] = \"\";\n\n\n    var chart = new google.visualization.MotionChart(\n    document.getElementById('MotionChartID2cd02805862d')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"motionchart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMotionChartID2cd02805862d);\n})();\nfunction displayChartMotionChartID2cd02805862d() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. \n\n\n\n\n\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.\n\n\nOther static websites\n\u00b6\n\n\nThere is a procedure for embedding graphics in:\n\n\n\n\nWordPress,\n\n\nGoogle Sites,\n\n\nBlogger,\n\n\nGoogle Code wiki pages.\n\n\nWikipedia,\n\n\nothers websites.\n\n\n\n\nDynamic websites\n\u00b6\n\n\n\n\nThe \nR.rsp\n package allows the integration of R code into html code. \n\n\nThe \nrApache\n and \nbrew\n packages support web application development using R and the Apache HTTP server.\n\n\nRook is a lightweight web server interface for R.\n\n\nThe \nshiny\n package builds interactive web application with R.\n\n\n\n\nCharts\n\u00b6\n\n\nConsult the examples (further above); charts are similar to what we find in other packages. \n\n\ngvisMotionChart\n is exclusive to \ngoogleVis\n.\n\n\ngvisMotionChart\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nhead(Fruits, 3)\n\n\n\n\n# Output\n   Fruit Year Location Sales Expenses Profit       Date\n1 Apples 2008     West    98       78     20 2008-12-31\n2 Apples 2009     West   111       79     32 2009-12-31\n3 Apples 2010     West    89       76     13 2010-12-31\n\n\n\n\nM <- gvisMotionChart(Fruits,\n                     idvar='Fruit',\n                     timevar='Year',\n                     options=list(width=400, height=350))\n\n# xvar =\n# yvar =\n# colorvar =\n# sizevar =\n# date.format =\n\nplot(M)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataMotionChartID336e668836ab () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Apples\",\n2008,\n\"West\",\n98,\n78,\n20,\n\"2008-12-31\"\n],\n[\n\"Apples\",\n2009,\n\"West\",\n111,\n79,\n32,\n\"2009-12-31\"\n],\n[\n\"Apples\",\n2010,\n\"West\",\n89,\n76,\n13,\n\"2010-12-31\"\n],\n[\n\"Oranges\",\n2008,\n\"East\",\n96,\n81,\n15,\n\"2008-12-31\"\n],\n[\n\"Bananas\",\n2008,\n\"East\",\n85,\n76,\n9,\n\"2008-12-31\"\n],\n[\n\"Oranges\",\n2009,\n\"East\",\n93,\n80,\n13,\n\"2009-12-31\"\n],\n[\n\"Bananas\",\n2009,\n\"East\",\n94,\n78,\n16,\n\"2009-12-31\"\n],\n[\n\"Oranges\",\n2010,\n\"East\",\n98,\n91,\n7,\n\"2010-12-31\"\n],\n[\n\"Bananas\",\n2010,\n\"East\",\n81,\n71,\n10,\n\"2010-12-31\"\n] \n];\ndata.addColumn('string','Fruit');\ndata.addColumn('number','Year');\ndata.addColumn('string','Location');\ndata.addColumn('number','Sales');\ndata.addColumn('number','Expenses');\ndata.addColumn('number','Profit');\ndata.addColumn('string','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMotionChartID336e668836ab() {\nvar data = gvisDataMotionChartID336e668836ab();\nvar options = {};\noptions[\"width\"] = 400;\noptions[\"height\"] = 350;\noptions[\"state\"] = \"\";\n\n\n    var chart = new google.visualization.MotionChart(\n    document.getElementById('MotionChartID336e668836ab')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"motionchart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMotionChartID336e668836ab);\n})();\nfunction displayChartMotionChartID336e668836ab() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWithin an HTML \niframe\n\u00b6\n\n\nAfter we generate the HTML chart or map, save the object as a HTML file.\n\n\nlibrary(htmlwidgets) \nlibrary(DT) \n\nsaveWidget(object, \"googleVis.html\")\n\n\n\n\nAdd this HTML snippet to the Markdown/HTML document.\n\n\n<iframe seamless src=\"/googleVis.html\" width=600px \nheight=400px ></iframe>\n\n\n\n\n\n\n\n\n\nHere is another example with a Leaflet file with the \nleaflet\n package.\n\n\n\n\n\nInformation from the object\n\u00b6\n\n\nM$type\nM$chartid\n\n\n\n\n\"MotionChart\"\n\"MotionChartID336e668836ab\"\n\n\n\n\nMaps\n\u00b6\n\n\ngvisGeoChart\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nhead(Exports, 3)\n\n\n\n\n# Output\n        Country Profit Online\n1       Germany      3   TRUE\n2        Brazil      4  FALSE\n3 United States      5   TRUE\n\n\n\n\nGeo <- gvisGeoChart(Exports, \n                    locationvar = \"Country\",\n                    colorvar = \"Profit\",\n                    sizevar = \"\", # size of markers\n                    hovervar = \"\", # text\n                    options = list(projection=\"kavrayskiy-vii\"))\n\n# locationvar can be lat:long or address, country name, region, state, city\n\nplot(Geo)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID49617fef373 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49617fef373() {\nvar data = gvisDataGeoChartID49617fef373();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\noptions[\"projection\"] = \"kavrayskiy-vii\";\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49617fef373')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49617fef373);\n})();\nfunction displayChartGeoChartID49617fef373() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhead(CityPopularity, 3)\n\n\n\n\n# Output\n      City Popularity\n1 New York        200\n2   Boston        300\n3    Miami        400\n\n\n\n\nGeo2 <- gvisGeoChart(CityPopularity, \n                     locationvar='City',\n                     colorvar='Popularity',\n                     options=list(region='US',\n                                  height=350,\n                                  displayMode='markers', \n                                  colorAxis=\"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\"))\n\nplot(Geo2)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID4961741b9385 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"New York\",\n200\n],\n[\n\"Boston\",\n300\n],\n[\n\"Miami\",\n400\n],\n[\n\"Chicago\",\n500\n],\n[\n\"Los Angeles\",\n600\n],\n[\n\"Houston\",\n700\n] \n];\ndata.addColumn('string','City');\ndata.addColumn('number','Popularity');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID4961741b9385() {\nvar data = gvisDataGeoChartID4961741b9385();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 350;\noptions[\"region\"] = \"US\";\noptions[\"displayMode\"] = \"markers\";\noptions[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']};\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID4961741b9385')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID4961741b9385);\n})();\nfunction displayChartGeoChartID4961741b9385() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCityPopularity3 <- data.frame(City = c('Montreal', 'Toronto'),\n                              Popularity = c(400, 200))\n\nGeo3 <- gvisGeoChart(CityPopularity3, \n                     locationvar='City',\n                     colorvar='Popularity',\n                     options=list(region = 'CA',\n                                  height=350,\n                                  displayMode='markers', \n                                  colorAxis=\"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\"))\n\nplot(Geo3)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID49614f24acff () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Montreal\",\n700\n],\n[\n\"Toronto\",\n200\n] \n];\ndata.addColumn('string','City');\ndata.addColumn('number','Popularity');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49614f24acff() {\nvar data = gvisDataGeoChartID49614f24acff();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 350;\noptions[\"region\"] = \"CA\";\noptions[\"displayMode\"] = \"markers\";\noptions[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']};\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49614f24acff')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49614f24acff);\n})();\nfunction displayChartGeoChartID49614f24acff() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhead(Andrew, 3)\n\n\n\n\n# Output\n        Date/Time UTC  Lat  Long Pressure_mb Speed_kt            Category\n1 1992-08-16 18:00:00 10.8 -35.5        1010       25 Tropical Depression\n2 1992-08-17 00:00:00 11.2 -37.4        1009       30 Tropical Depression\n3 1992-08-17 06:00:00 11.7 -39.6        1008       30 Tropical Depression\n     LatLong                                              Tip\n1 10.8:-35.5 Tropical Depression<BR>Pressure=1010<BR>Speed=25\n2 11.2:-37.4 Tropical Depression<BR>Pressure=1009<BR>Speed=30\n3 11.7:-39.6 Tropical Depression<BR>Pressure=1008<BR>Speed=30\n\n\n\n\nGeoMarker <- gvisGeoChart(Andrew,\n                          \"LatLong\", \n                          sizevar='Speed_kt',\n                          colorvar=\"Pressure_mb\", \n                          options=list(region=\"US\"))\n\nplot(GeoMarker)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID2cd02f295f7 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n10.8,\n-35.5,\n1010,\n25\n],\n[\n11.2,\n-37.4,\n1009,\n30\n],\n[\n11.7,\n-39.6,\n1008,\n30\n],\n[\n12.3,\n-42,\n1006,\n35\n],\n[\n13.1,\n-44.2,\n1003,\n35\n],\n[\n13.6,\n-46.2,\n1002,\n40\n],\n[\n14.1,\n-48,\n1001,\n45\n],\n[\n14.6,\n-49.9,\n1000,\n45\n],\n[\n15.4,\n-51.8,\n1000,\n45\n],\n[\n16.3,\n-53.5,\n1001,\n45\n],\n[\n17.2,\n-55.3,\n1002,\n45\n],\n[\n18,\n-56.9,\n1005,\n45\n],\n[\n18.8,\n-58.3,\n1007,\n45\n],\n[\n19.8,\n-59.3,\n1011,\n40\n],\n[\n20.7,\n-60,\n1013,\n40\n],\n[\n21.7,\n-60.7,\n1015,\n40\n],\n[\n22.5,\n-61.5,\n1014,\n40\n],\n[\n23.2,\n-62.4,\n1014,\n45\n],\n[\n23.9,\n-63.3,\n1010,\n45\n],\n[\n24.4,\n-64.2,\n1007,\n50\n],\n[\n24.8,\n-64.9,\n1004,\n50\n],\n[\n25.3,\n-65.9,\n1000,\n55\n],\n[\n25.6,\n-67,\n994,\n60\n],\n[\n25.8,\n-68.3,\n981,\n70\n],\n[\n25.7,\n-69.7,\n969,\n80\n],\n[\n25.6,\n-71.1,\n961,\n90\n],\n[\n25.5,\n-72.5,\n947,\n105\n],\n[\n25.4,\n-74.2,\n933,\n120\n],\n[\n25.4,\n-75.8,\n922,\n135\n],\n[\n25.4,\n-77.5,\n930,\n125\n],\n[\n25.4,\n-79.3,\n937,\n120\n],\n[\n25.6,\n-81.2,\n951,\n110\n],\n[\n25.8,\n-83.1,\n947,\n115\n],\n[\n26.2,\n-85,\n943,\n115\n],\n[\n26.6,\n-86.7,\n948,\n115\n],\n[\n27.2,\n-88.2,\n946,\n115\n],\n[\n27.8,\n-89.6,\n941,\n120\n],\n[\n28.5,\n-90.5,\n937,\n120\n],\n[\n29.2,\n-91.3,\n955,\n115\n],\n[\n30.1,\n-91.7,\n973,\n80\n],\n[\n30.9,\n-91.6,\n991,\n50\n],\n[\n31.5,\n-91.1,\n995,\n35\n],\n[\n32.1,\n-90.5,\n997,\n30\n],\n[\n32.8,\n-89.6,\n998,\n30\n],\n[\n33.6,\n-88.4,\n999,\n25\n],\n[\n34.4,\n-86.7,\n1000,\n20\n],\n[\n35.4,\n-84,\n1000,\n20\n] \n];\ndata.addColumn('number','Latitude');\ndata.addColumn('number','Longitude');\ndata.addColumn('number','Pressure_mb');\ndata.addColumn('number','Speed_kt');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID2cd02f295f7() {\nvar data = gvisDataGeoChartID2cd02f295f7();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\noptions[\"region\"] = \"US\";\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID2cd02f295f7')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID2cd02f295f7);\n})();\nfunction displayChartGeoChartID2cd02f295f7() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngvisMap\n or Google Maps\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nAndrewMap <- gvisMap(Andrew,\n                     locationvar=\"LatLong\" ,\n                     tipvar=\"Tip\", #  text displayed over the tip icon\n                     options=list(showTip=TRUE, \n                                  showLine=TRUE, \n                                  enableScrollWheel=TRUE,\n                                  mapType='terrain', \n                                  useMapTypeControl=TRUE))\n\nplot(AndrewMap)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataMapID496129f7ada () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n10.8,\n-35.5,\n\"Tropical Depression<BR>Pressure=1010<BR>Speed=25\"\n],\n[\n11.2,\n-37.4,\n\"Tropical Depression<BR>Pressure=1009<BR>Speed=30\"\n],\n[\n11.7,\n-39.6,\n\"Tropical Depression<BR>Pressure=1008<BR>Speed=30\"\n],\n[\n12.3,\n-42,\n\"Tropical Storm<BR>Pressure=1006<BR>Speed=35\"\n],\n[\n13.1,\n-44.2,\n\"Tropical Storm<BR>Pressure=1003<BR>Speed=35\"\n],\n[\n13.6,\n-46.2,\n\"Tropical Storm<BR>Pressure=1002<BR>Speed=40\"\n],\n[\n14.1,\n-48,\n\"Tropical Storm<BR>Pressure=1001<BR>Speed=45\"\n],\n[\n14.6,\n-49.9,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=45\"\n],\n[\n15.4,\n-51.8,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=45\"\n],\n[\n16.3,\n-53.5,\n\"Tropical Storm<BR>Pressure=1001<BR>Speed=45\"\n],\n[\n17.2,\n-55.3,\n\"Tropical Storm<BR>Pressure=1002<BR>Speed=45\"\n],\n[\n18,\n-56.9,\n\"Tropical Storm<BR>Pressure=1005<BR>Speed=45\"\n],\n[\n18.8,\n-58.3,\n\"Tropical Storm<BR>Pressure=1007<BR>Speed=45\"\n],\n[\n19.8,\n-59.3,\n\"Tropical Storm<BR>Pressure=1011<BR>Speed=40\"\n],\n[\n20.7,\n-60,\n\"Tropical Storm<BR>Pressure=1013<BR>Speed=40\"\n],\n[\n21.7,\n-60.7,\n\"Tropical Storm<BR>Pressure=1015<BR>Speed=40\"\n],\n[\n22.5,\n-61.5,\n\"Tropical Storm<BR>Pressure=1014<BR>Speed=40\"\n],\n[\n23.2,\n-62.4,\n\"Tropical Storm<BR>Pressure=1014<BR>Speed=45\"\n],\n[\n23.9,\n-63.3,\n\"Tropical Storm<BR>Pressure=1010<BR>Speed=45\"\n],\n[\n24.4,\n-64.2,\n\"Tropical Storm<BR>Pressure=1007<BR>Speed=50\"\n],\n[\n24.8,\n-64.9,\n\"Tropical Storm<BR>Pressure=1004<BR>Speed=50\"\n],\n[\n25.3,\n-65.9,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=55\"\n],\n[\n25.6,\n-67,\n\"Tropical Storm<BR>Pressure=994<BR>Speed=60\"\n],\n[\n25.8,\n-68.3,\n\"Hurricane<BR>Pressure=981<BR>Speed=70\"\n],\n[\n25.7,\n-69.7,\n\"Hurricane<BR>Pressure=969<BR>Speed=80\"\n],\n[\n25.6,\n-71.1,\n\"Hurricane<BR>Pressure=961<BR>Speed=90\"\n],\n[\n25.5,\n-72.5,\n\"Hurricane<BR>Pressure=947<BR>Speed=105\"\n],\n[\n25.4,\n-74.2,\n\"Hurricane<BR>Pressure=933<BR>Speed=120\"\n],\n[\n25.4,\n-75.8,\n\"Hurricane<BR>Pressure=922<BR>Speed=135\"\n],\n[\n25.4,\n-77.5,\n\"Hurricane<BR>Pressure=930<BR>Speed=125\"\n],\n[\n25.4,\n-79.3,\n\"Hurricane<BR>Pressure=937<BR>Speed=120\"\n],\n[\n25.6,\n-81.2,\n\"Hurricane<BR>Pressure=951<BR>Speed=110\"\n],\n[\n25.8,\n-83.1,\n\"Hurricane<BR>Pressure=947<BR>Speed=115\"\n],\n[\n26.2,\n-85,\n\"Hurricane<BR>Pressure=943<BR>Speed=115\"\n],\n[\n26.6,\n-86.7,\n\"Hurricane<BR>Pressure=948<BR>Speed=115\"\n],\n[\n27.2,\n-88.2,\n\"Hurricane<BR>Pressure=946<BR>Speed=115\"\n],\n[\n27.8,\n-89.6,\n\"Hurricane<BR>Pressure=941<BR>Speed=120\"\n],\n[\n28.5,\n-90.5,\n\"Hurricane<BR>Pressure=937<BR>Speed=120\"\n],\n[\n29.2,\n-91.3,\n\"Hurricane<BR>Pressure=955<BR>Speed=115\"\n],\n[\n30.1,\n-91.7,\n\"Tropical Storm<BR>Pressure=973<BR>Speed=80\"\n],\n[\n30.9,\n-91.6,\n\"Tropical Storm<BR>Pressure=991<BR>Speed=50\"\n],\n[\n31.5,\n-91.1,\n\"Tropical Depression<BR>Pressure=995<BR>Speed=35\"\n],\n[\n32.1,\n-90.5,\n\"Tropical Depression<BR>Pressure=997<BR>Speed=30\"\n],\n[\n32.8,\n-89.6,\n\"Tropical Depression<BR>Pressure=998<BR>Speed=30\"\n],\n[\n33.6,\n-88.4,\n\"Tropical Depression<BR>Pressure=999<BR>Speed=25\"\n],\n[\n34.4,\n-86.7,\n\"Tropical Depression<BR>Pressure=1000<BR>Speed=20\"\n],\n[\n35.4,\n-84,\n\"Tropical Depression<BR>Pressure=1000<BR>Speed=20\"\n] \n];\ndata.addColumn('number','Latitude');\ndata.addColumn('number','Longitude');\ndata.addColumn('string','Tip');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMapID496129f7ada() {\nvar data = gvisDataMapID496129f7ada();\nvar options = {};\noptions[\"showTip\"] = true;\noptions[\"showLine\"] = true;\noptions[\"enableScrollWheel\"] = true;\noptions[\"mapType\"] = \"terrain\";\noptions[\"useMapTypeControl\"] = true;\n\n\n    var chart = new google.visualization.Map(\n    document.getElementById('MapID496129f7ada')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"map\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMapID496129f7ada);\n})();\nfunction displayChartMapID496129f7ada() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEditor\n\u00b6\n\n\nThis is a fantastic option that let us start with one chart or map and change everything with a menu.\n\n\n\n\n\n\n\n\n\n\n\nEditor <- gvisGeoChart(Exports, \n                       locationvar=\"Country\",\n                       colorvar=\"Profit\",\n                       options=list(gvis.editor='Edit me!'))\n\nplot(Editor)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID49613aa5c54a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49613aa5c54a() {\nvar data = gvisDataGeoChartID49613aa5c54a();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\n\n\n    chartGeoChartID49613aa5c54a = new google.visualization.ChartWrapper({\n    dataTable: data,       \n    chartType: 'GeoChart',\n    containerId: 'GeoChartID49613aa5c54a',\n    options: options\n    });\n    chartGeoChartID49613aa5c54a.draw();\n\n\n}\n\n  function openEditorGeoChartID49613aa5c54a() {\n  var editor = new google.visualization.ChartEditor();\n  google.visualization.events.addListener(editor, 'ok',\n  function() { \n  chartGeoChartID49613aa5c54a = editor.getChartWrapper();  \n  chartGeoChartID49613aa5c54a.draw(document.getElementById('GeoChartID49613aa5c54a')); \n  }); \n  editor.openDialog(chartGeoChartID49613aa5c54a);\n  }\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"charteditor\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49613aa5c54a);\n})();\nfunction displayChartGeoChartID49613aa5c54a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTables with \ngvisTable\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nhead(Stock, 3)\n\n\n\n\n# Output\n        Date  Device Value Title Annotation\n1 2008-01-01 Pencils  3000  <NA>       <NA>\n2 2008-01-02 Pencils 14045  <NA>       <NA>\n3 2008-01-03 Pencils  5502  <NA>       <NA>\n\n\n\n\nTable <- gvisTable(Stock, \n                   formats=list(Value=\"#,###\"))\n\nplot(Table)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataTableID2cd03b735929 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\nnew Date(2008,0,1),\n\"Pencils\",\n3000,\nnull,\nnull\n],\n[\nnew Date(2008,0,2),\n\"Pencils\",\n14045,\nnull,\nnull\n],\n[\nnew Date(2008,0,3),\n\"Pencils\",\n5502,\nnull,\nnull\n],\n[\nnew Date(2008,0,4),\n\"Pencils\",\n75284,\nnull,\nnull\n],\n[\nnew Date(2008,0,5),\n\"Pencils\",\n41476,\n\"Bought pencils\",\n\"Bought 200k pencils\"\n],\n[\nnew Date(2008,0,6),\n\"Pencils\",\n333222,\nnull,\nnull\n],\n[\nnew Date(2008,0,1),\n\"Pens\",\n40645,\nnull,\nnull\n],\n[\nnew Date(2008,0,2),\n\"Pens\",\n20374,\nnull,\nnull\n],\n[\nnew Date(2008,0,3),\n\"Pens\",\n50766,\nnull,\nnull\n],\n[\nnew Date(2008,0,4),\n\"Pens\",\n14334,\n\"Out of stock\",\n\"Ran out of stock of pens at 4pm\"\n],\n[\nnew Date(2008,0,5),\n\"Pens\",\n66467,\nnull,\nnull\n],\n[\nnew Date(2008,0,6),\n\"Pens\",\n39463,\nnull,\nnull\n] \n];\ndata.addColumn('date','Date');\ndata.addColumn('string','Device');\ndata.addColumn('number','Value');\ndata.addColumn('string','Title');\ndata.addColumn('string','Annotation');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartTableID2cd03b735929() {\nvar data = gvisDataTableID2cd03b735929();\nvar options = {};\noptions[\"allowHtml\"] = true;\n\n  var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"});\n  dataFormat1.format(data, 2);\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID2cd03b735929')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID2cd03b735929);\n})();\nfunction displayChartTableID2cd03b735929() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhead(Population, 3)\n\n\n\n\n# Output\n                                                                                 Rank       Country Population % of World Population\n1    1         China 1339940000                0.1950\n2    2         India 1188650000                0.1730\n3    3 United States  310438000                0.0452\n                                                                                                                                                                     Flag\n1 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\">\n2                                                       <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\">\n3                               <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\">\n  Mode       Date\n1 TRUE 2010-10-09\n2 TRUE 2010-10-09\n3 TRUE 2010-10-09\n\n\n\n\nPopTable <- gvisTable(Population, \n                      formats=list(Population=\"#,###\",\n                                   '% of World Population'='#.#%'),\n                      options=list(page='enable'))\n\nplot(PopTable)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataTableID2cd013050362 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"1\",\n\"China\",\n1339940000,\n0.195,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"2\",\n\"India\",\n1188650000,\n0.173,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"3\",\n\"United States\",\n310438000,\n0.0452,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"4\",\n\"Indonesia\",\n237556363,\n0.0346,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Indonesia.svg/22px-Flag_of_Indonesia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"5\",\n\"Brazil\",\n193626000,\n0.0282,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Brazil.svg/22px-Flag_of_Brazil.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"6\",\n\"Pakistan\",\n170745000,\n0.0248,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Flag_of_Pakistan.svg/22px-Flag_of_Pakistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"7\",\n\"Bangladesh\",\n164425000,\n0.0239,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Flag_of_Bangladesh.svg/22px-Flag_of_Bangladesh.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"8\",\n\"Nigeria\",\n158259000,\n0.023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Flag_of_Nigeria.svg/22px-Flag_of_Nigeria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"9\",\n\"Russia\",\n141927297,\n0.0206,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Russia.svg/22px-Flag_of_Russia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"10\",\n\"Japan\",\n127390000,\n0.0185,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Flag_of_Japan.svg/22px-Flag_of_Japan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"11\",\n\"Mexico\",\n108396211,\n0.0158,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Flag_of_Mexico.svg/22px-Flag_of_Mexico.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"12\",\n\"Philippines\",\n94013200,\n0.0137,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_the_Philippines.svg/22px-Flag_of_the_Philippines.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"13\",\n\"Vietnam\",\n85846997,\n0.0125,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Vietnam.svg/22px-Flag_of_Vietnam.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"14\",\n\"Ethiopia\",\n84976000,\n0.0124,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Flag_of_Ethiopia.svg/22px-Flag_of_Ethiopia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"15\",\n\"Germany\",\n81802257,\n0.0119,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/22px-Flag_of_Germany.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"16\",\n\"Egypt\",\n79135000,\n0.0115,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Egypt.svg/22px-Flag_of_Egypt.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"17\",\n\"Iran\",\n75078000,\n0.0109,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Flag_of_Iran.svg/22px-Flag_of_Iran.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"18\",\n\"Turkey\",\n72561312,\n0.0106,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Flag_of_Turkey.svg/22px-Flag_of_Turkey.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"19\",\n\"Dem. Rep. of Congo\",\n67827000,\n0.0099,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Democratic_Republic_of_the_Congo.svg/22px-Flag_of_the_Democratic_Republic_of_the_Congo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"20\",\n\"Thailand\",\n67070000,\n0.01,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Flag_of_Thailand.svg/22px-Flag_of_Thailand.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"21\",\n\"France\",\n65447374,\n0.0095,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/22px-Flag_of_France.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"22\",\n\"United Kingdom\",\n62008049,\n0.009,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/22px-Flag_of_the_United_Kingdom.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"23\",\n\"Italy\",\n60402499,\n0.0088,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/03/Flag_of_Italy.svg/22px-Flag_of_Italy.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"24\",\n\"Myanmar\",\n50496000,\n0.0073,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Flag_of_Myanmar.svg/22px-Flag_of_Myanmar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"25\",\n\"South Africa\",\n49991300,\n0.0073,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Flag_of_South_Africa.svg/22px-Flag_of_South_Africa.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"26\",\n\"South Korea\",\n49773145,\n0.0072,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Flag_of_South_Korea.svg/22px-Flag_of_South_Korea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"27\",\n\"Spain\",\n46072834,\n0.0067,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Spain.svg/22px-Flag_of_Spain.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"28\",\n\"Ukraine\",\n45871738,\n0.0067,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Ukraine.svg/22px-Flag_of_Ukraine.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"29\",\n\"Colombia\",\n45655000,\n0.0066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Colombia.svg/22px-Flag_of_Colombia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"30\",\n\"Tanzania\",\n45040000,\n0.0066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tanzania.svg/22px-Flag_of_Tanzania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"31\",\n\"Sudan\",\n43192000,\n0.0063,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Sudan.svg/22px-Flag_of_Sudan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"32\",\n\"Argentina\",\n40518951,\n0.0059,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/22px-Flag_of_Argentina.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"33\",\n\"Kenya\",\n38610097,\n0.0056,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Kenya.svg/22px-Flag_of_Kenya.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"34\",\n\"Poland\",\n38167329,\n0.0056,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Flag_of_Poland.svg/22px-Flag_of_Poland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"35\",\n\"Algeria\",\n35423000,\n0.0052,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Algeria.svg/22px-Flag_of_Algeria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"36\",\n\"Canada\",\n34272000,\n0.005,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Canada.svg/22px-Flag_of_Canada.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"37\",\n\"Uganda\",\n33796000,\n0.0049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Flag_of_Uganda.svg/22px-Flag_of_Uganda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"38\",\n\"Morocco\",\n31944000,\n0.0046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Morocco.svg/22px-Flag_of_Morocco.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"39\",\n\"Iraq\",\n31467000,\n0.0046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Flag_of_Iraq.svg/22px-Flag_of_Iraq.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"40\",\n\"Nepal\",\n29853000,\n0.0043,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Flag_of_Nepal.svg/16px-Flag_of_Nepal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"41\",\n\"Peru\",\n29461933,\n0.0043,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Peru.svg/22px-Flag_of_Peru.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"42\",\n\"Afghanistan\",\n29117000,\n0.0042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Afghanistan.svg/22px-Flag_of_Afghanistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"43\",\n\"Venezuela\",\n28958000,\n0.0042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Venezuela.svg/22px-Flag_of_Venezuela.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"44\",\n\"Malaysia\",\n28250500,\n0.0041,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Flag_of_Malaysia.svg/22px-Flag_of_Malaysia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"45\",\n\"Uzbekistan\",\n27794000,\n0.004,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Uzbekistan.svg/22px-Flag_of_Uzbekistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"46\",\n\"Saudi Arabia\",\n27136977,\n0.0039,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Flag_of_Saudi_Arabia.svg/22px-Flag_of_Saudi_Arabia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"47\",\n\"Ghana\",\n24333000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Ghana.svg/22px-Flag_of_Ghana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"48\",\n\"Yemen\",\n24256000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Yemen.svg/22px-Flag_of_Yemen.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"49\",\n\"North Korea\",\n23991000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Flag_of_North_Korea.svg/22px-Flag_of_North_Korea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"50\",\n\"Mozambique\",\n23406000,\n0.0034,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Mozambique.svg/22px-Flag_of_Mozambique.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"52\",\n\"Syria\",\n22505000,\n0.0033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Flag_of_Syria.svg/22px-Flag_of_Syria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"53\",\n\"Australia\",\n22483305,\n0.0033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Flag_of_Australia.svg/22px-Flag_of_Australia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"54\",\n\"Cote d'Ivoire\",\n21571000,\n0.0031,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Flag_of_Cote_d%27Ivoire.svg/22px-Flag_of_Cote_d%27Ivoire.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"55\",\n\"Romania\",\n21466174,\n0.0031,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Romania.svg/22px-Flag_of_Romania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"56\",\n\"Sri Lanka\",\n20410000,\n0.003,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Sri_Lanka.svg/22px-Flag_of_Sri_Lanka.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"57\",\n\"Madagascar\",\n20146000,\n0.0029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Madagascar.svg/22px-Flag_of_Madagascar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"58\",\n\"Cameroon\",\n19958000,\n0.0029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Cameroon.svg/22px-Flag_of_Cameroon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"59\",\n\"Angola\",\n18993000,\n0.0028,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Angola.svg/22px-Flag_of_Angola.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"60\",\n\"Chile\",\n17140000,\n0.0025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Flag_of_Chile.svg/22px-Flag_of_Chile.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"61\",\n\"Netherlands\",\n16619500,\n0.00242,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/22px-Flag_of_the_Netherlands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"62\",\n\"Burkina Faso\",\n16287000,\n0.0024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Burkina_Faso.svg/22px-Flag_of_Burkina_Faso.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"63\",\n\"Kazakhstan\",\n16197000,\n0.0024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kazakhstan.svg/22px-Flag_of_Kazakhstan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"64\",\n\"Niger\",\n15891000,\n0.0023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Flag_of_Niger.svg/22px-Flag_of_Niger.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"65\",\n\"Malawi\",\n15692000,\n0.0023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Flag_of_Malawi.svg/22px-Flag_of_Malawi.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"66\",\n\"Mali\",\n14517176,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Mali.svg/22px-Flag_of_Mali.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"67\",\n\"Guatemala\",\n14377000,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Flag_of_Guatemala.svg/22px-Flag_of_Guatemala.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"68\",\n\"Ecuador\",\n14259000,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Flag_of_Ecuador.svg/22px-Flag_of_Ecuador.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"69\",\n\"Cambodia\",\n13395682,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Flag_of_Cambodia.svg/22px-Flag_of_Cambodia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"70\",\n\"Zambia\",\n13257000,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Zambia.svg/22px-Flag_of_Zambia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"71\",\n\"Senegal\",\n12861000,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Flag_of_Senegal.svg/22px-Flag_of_Senegal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"72\",\n\"Zimbabwe\",\n12644000,\n0.0018,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Flag_of_Zimbabwe.svg/22px-Flag_of_Zimbabwe.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"73\",\n\"Chad\",\n11506000,\n0.0017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Flag_of_Chad.svg/22px-Flag_of_Chad.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"74\",\n\"Greece\",\n11306183,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Greece.svg/22px-Flag_of_Greece.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"75\",\n\"Cuba\",\n11204000,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Flag_of_Cuba.svg/22px-Flag_of_Cuba.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"76\",\n\"Belgium\",\n10827519,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Belgium_%28civil%29.svg/22px-Flag_of_Belgium_%28civil%29.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"77\",\n\"Portugal\",\n10636888,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Portugal.svg/22px-Flag_of_Portugal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"78\",\n\"Czech Republic\",\n10512397,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_Czech_Republic.svg/22px-Flag_of_the_Czech_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"79\",\n\"Tunisia\",\n10432500,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Tunisia.svg/22px-Flag_of_Tunisia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"80\",\n\"Guinea\",\n10324000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Flag_of_Guinea.svg/22px-Flag_of_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"81\",\n\"Rwanda\",\n10277000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Rwanda.svg/22px-Flag_of_Rwanda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"82\",\n\"Dominican Republic\",\n10225000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_the_Dominican_Republic.svg/22px-Flag_of_the_Dominican_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"83\",\n\"Haiti\",\n10188000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Haiti.svg/22px-Flag_of_Haiti.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"84\",\n\"Bolivia\",\n10031000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Bolivia.svg/22px-Flag_of_Bolivia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"85\",\n\"Hungary\",\n10013628,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/Flag_of_Hungary.svg/22px-Flag_of_Hungary.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"86\",\n\"Serbia\",\n9856000,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Flag_of_Serbia.svg/22px-Flag_of_Serbia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"87\",\n\"Belarus\",\n9467700,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Flag_of_Belarus.svg/22px-Flag_of_Belarus.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"88\",\n\"Sweden\",\n9393648,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Sweden.svg/22px-Flag_of_Sweden.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"89\",\n\"Somalia\",\n9359000,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Flag_of_Somalia.svg/22px-Flag_of_Somalia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"90\",\n\"Benin\",\n9212000,\n0.0013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Benin.svg/22px-Flag_of_Benin.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"91\",\n\"Azerbaijan\",\n8997400,\n0.0013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Azerbaijan.svg/22px-Flag_of_Azerbaijan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"92\",\n\"Burundi\",\n8519000,\n0.0012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Flag_of_Burundi.svg/22px-Flag_of_Burundi.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"93\",\n\"Austria\",\n8372930,\n0.0012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_Austria.svg/22px-Flag_of_Austria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"94\",\n\"Switzerland\",\n7782900,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Switzerland.svg/20px-Flag_of_Switzerland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"95\",\n\"Israel\",\n7640800,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Israel.svg/22px-Flag_of_Israel.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"96\",\n\"Honduras\",\n7616000,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Flag_of_Honduras.svg/22px-Flag_of_Honduras.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"97\",\n\"Bulgaria\",\n7576751,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Bulgaria.svg/22px-Flag_of_Bulgaria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"98\",\n\"Tajikistan\",\n7075000,\n0.00103,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Tajikistan.svg/22px-Flag_of_Tajikistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"100\",\n\"Papua New Guinea\",\n6888000,\n0.001,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Flag_of_Papua_New_Guinea.svg/22px-Flag_of_Papua_New_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"101\",\n\"Togo\",\n6780000,\n0.00099,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Flag_of_Togo.svg/22px-Flag_of_Togo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"102\",\n\"Libya\",\n6546000,\n0.00095,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Libya.svg/22px-Flag_of_Libya.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"103\",\n\"Jordan\",\n6472000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Flag_of_Jordan.svg/22px-Flag_of_Jordan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"104\",\n\"Paraguay\",\n6460000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Paraguay.svg/22px-Flag_of_Paraguay.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"105\",\n\"Laos\",\n6436000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Laos.svg/22px-Flag_of_Laos.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"106\",\n\"El Salvador\",\n6194000,\n9e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_El_Salvador.svg/22px-Flag_of_El_Salvador.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"107\",\n\"Sierra Leone\",\n5836000,\n0.00085,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Sierra_Leone.svg/22px-Flag_of_Sierra_Leone.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"108\",\n\"Nicaragua\",\n5822000,\n0.00085,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Nicaragua.svg/22px-Flag_of_Nicaragua.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"109\",\n\"Kyrgyzstan\",\n5550000,\n0.00081,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Flag_of_Kyrgyzstan.svg/22px-Flag_of_Kyrgyzstan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"110\",\n\"Denmark\",\n5543819,\n8e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Denmark.svg/22px-Flag_of_Denmark.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"111\",\n\"Slovakia\",\n5429763,\n0.00079,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Flag_of_Slovakia.svg/22px-Flag_of_Slovakia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"112\",\n\"Finland\",\n5370000,\n0.00078,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Finland.svg/22px-Flag_of_Finland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"113\",\n\"Eritrea\",\n5224000,\n0.00076,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Flag_of_Eritrea.svg/22px-Flag_of_Eritrea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"114\",\n\"Turkmenistan\",\n5177000,\n0.00075,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Turkmenistan.svg/22px-Flag_of_Turkmenistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"115\",\n\"Singapore\",\n5076700,\n0.00074,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Singapore.svg/22px-Flag_of_Singapore.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"116\",\n\"Norway\",\n4906500,\n0.00071,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Flag_of_Norway.svg/22px-Flag_of_Norway.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"117\",\n\"United Arab Emirates\",\n4707000,\n0.00068,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_United_Arab_Emirates.svg/22px-Flag_of_the_United_Arab_Emirates.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"118\",\n\"Costa Rica\",\n4640000,\n0.00068,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/Flag_of_Costa_Rica.svg/22px-Flag_of_Costa_Rica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"119\",\n\"Central African Republic\",\n4506000,\n0.00066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Central_African_Republic.svg/22px-Flag_of_the_Central_African_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"120\",\n\"Ireland\",\n4470700,\n0.00064,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Flag_of_Ireland.svg/22px-Flag_of_Ireland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"121\",\n\"Georgia\",\n4436000,\n0.00065,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Georgia.svg/22px-Flag_of_Georgia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"122\",\n\"Croatia\",\n4435056,\n0.00065,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Croatia.svg/22px-Flag_of_Croatia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"123\",\n\"New Zealand\",\n4393000,\n0.00064,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Flag_of_New_Zealand.svg/22px-Flag_of_New_Zealand.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"124\",\n\"Lebanon\",\n4255000,\n0.00062,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Flag_of_Lebanon.svg/22px-Flag_of_Lebanon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"125\",\n\"Liberia\",\n4102000,\n6e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Flag_of_Liberia.svg/22px-Flag_of_Liberia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"127\",\n\"Palestinian territories\",\n3935249,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/22px-Flag_of_Palestine.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"128\",\n\"Bosnia and Herzegovina\",\n3760000,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Flag_of_Bosnia_and_Herzegovina.svg/22px-Flag_of_Bosnia_and_Herzegovina.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"129\",\n\"Republic of the Congo\",\n3759000,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Republic_of_the_Congo.svg/22px-Flag_of_the_Republic_of_the_Congo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"130\",\n\"Moldova\",\n3563800,\n0.00052,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Moldova.svg/22px-Flag_of_Moldova.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"131\",\n\"Uruguay\",\n3372000,\n0.00049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Uruguay.svg/22px-Flag_of_Uruguay.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"132\",\n\"Mauritania\",\n3366000,\n0.00049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Flag_of_Mauritania.svg/22px-Flag_of_Mauritania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"133\",\n\"Lithuania\",\n3329227,\n0.00048,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Lithuania.svg/22px-Flag_of_Lithuania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"134\",\n\"Panama\",\n3322576,\n0.00048,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Flag_of_Panama.svg/22px-Flag_of_Panama.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"135\",\n\"Armenia\",\n3238000,\n0.00047,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Flag_of_Armenia.svg/22px-Flag_of_Armenia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"136\",\n\"Albania\",\n3195000,\n0.00046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/36/Flag_of_Albania.svg/22px-Flag_of_Albania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"137\",\n\"Kuwait\",\n3051000,\n0.00044,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Flag_of_Kuwait.svg/22px-Flag_of_Kuwait.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"138\",\n\"Oman\",\n2905000,\n0.00042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Oman.svg/22px-Flag_of_Oman.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"139\",\n\"Mongolia\",\n2776500,\n4e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Mongolia.svg/22px-Flag_of_Mongolia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"140\",\n\"Jamaica\",\n2730000,\n4e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Jamaica.svg/22px-Flag_of_Jamaica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"141\",\n\"Latvia\",\n2236300,\n0.00033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Latvia.svg/22px-Flag_of_Latvia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"142\",\n\"Namibia\",\n2212000,\n0.00032,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Namibia.svg/22px-Flag_of_Namibia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"143\",\n\"Lesotho\",\n2084000,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Flag_of_Lesotho.svg/22px-Flag_of_Lesotho.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"144\",\n\"Slovenia\",\n2065720,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Flag_of_Slovenia.svg/22px-Flag_of_Slovenia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"145\",\n\"Republic of Macedonia\",\n2048620,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Flag_of_Macedonia.svg/22px-Flag_of_Macedonia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"146\",\n\"Botswana\",\n1978000,\n0.00029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_Botswana.svg/22px-Flag_of_Botswana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"147\",\n\"Gambia\",\n1751000,\n0.00025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_The_Gambia.svg/22px-Flag_of_The_Gambia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"148\",\n\"Qatar\",\n1696563,\n0.00025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Flag_of_Qatar.svg/22px-Flag_of_Qatar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"149\",\n\"Guinea-Bissau\",\n1647000,\n0.00024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Guinea-Bissau.svg/22px-Flag_of_Guinea-Bissau.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"150\",\n\"Gabon\",\n1501000,\n0.00022,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Flag_of_Gabon.svg/22px-Flag_of_Gabon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"151\",\n\"Trinidad and Tobago\",\n1344000,\n2e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Trinidad_and_Tobago.svg/22px-Flag_of_Trinidad_and_Tobago.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"152\",\n\"Estonia\",\n1340127,\n0.00019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Flag_of_Estonia.svg/22px-Flag_of_Estonia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"153\",\n\"Mauritius\",\n1297000,\n0.00019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Mauritius.svg/22px-Flag_of_Mauritius.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"154\",\n\"Swaziland\",\n1202000,\n0.00017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Flag_of_Swaziland.svg/22px-Flag_of_Swaziland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"155\",\n\"East Timor\",\n1171000,\n0.00017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Flag_of_East_Timor.svg/22px-Flag_of_East_Timor.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"156\",\n\"Djibouti\",\n879000,\n0.00013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_Djibouti.svg/22px-Flag_of_Djibouti.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"157\",\n\"Fiji\",\n854000,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Fiji.svg/22px-Flag_of_Fiji.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"158\",\n\"Bahrain\",\n807000,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Bahrain.svg/22px-Flag_of_Bahrain.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"159\",\n\"Cyprus\",\n801851,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Cyprus.svg/22px-Flag_of_Cyprus.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"160\",\n\"Guyana\",\n761000,\n0.00011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_Guyana.svg/22px-Flag_of_Guyana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"161\",\n\"Bhutan\",\n708000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Flag_of_Bhutan.svg/22px-Flag_of_Bhutan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"162\",\n\"Equatorial Guinea\",\n693000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Equatorial_Guinea.svg/22px-Flag_of_Equatorial_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"163\",\n\"Comoros\",\n691000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Flag_of_the_Comoros.svg/22px-Flag_of_the_Comoros.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"164\",\n\"Montenegro\",\n626000,\n9e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Montenegro.svg/22px-Flag_of_Montenegro.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"166\",\n\"Solomon Islands\",\n536000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Flag_of_the_Solomon_Islands.svg/22px-Flag_of_the_Solomon_Islands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"167\",\n\"Western Sahara\",\n530000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Flag_of_Western_Sahara.svg/22px-Flag_of_Western_Sahara.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"168\",\n\"Suriname\",\n524000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Flag_of_Suriname.svg/22px-Flag_of_Suriname.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"169\",\n\"Cape Verde\",\n513000,\n7e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Cape_Verde.svg/22px-Flag_of_Cape_Verde.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"170\",\n\"Luxembourg\",\n502207,\n7e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Flag_of_Luxembourg.svg/22px-Flag_of_Luxembourg.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"171\",\n\"Malta\",\n416333,\n6e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Malta.svg/22px-Flag_of_Malta.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"172\",\n\"Brunei\",\n407000,\n6e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Brunei.svg/22px-Flag_of_Brunei.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"173\",\n\"Bahamas\",\n346000,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Flag_of_the_Bahamas.svg/22px-Flag_of_the_Bahamas.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"174\",\n\"Belize\",\n322100,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Flag_of_Belize.svg/22px-Flag_of_Belize.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"175\",\n\"Iceland\",\n318006,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Iceland.svg/22px-Flag_of_Iceland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"176\",\n\"Maldives\",\n314000,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Maldives.svg/22px-Flag_of_Maldives.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"177\",\n\"Barbados\",\n257000,\n4e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Flag_of_Barbados.svg/22px-Flag_of_Barbados.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"178\",\n\"Vanuatu\",\n246000,\n4e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Vanuatu.svg/22px-Flag_of_Vanuatu.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"181\",\n\"Samoa\",\n179000,\n3e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Samoa.svg/22px-Flag_of_Samoa.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"182\",\n\"Saint Lucia\",\n174000,\n3e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Saint_Lucia.svg/22px-Flag_of_Saint_Lucia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"183\",\n\"Sao Tome and Principe\",\n165000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Sao_Tome_and_Principe.svg/22px-Flag_of_Sao_Tome_and_Principe.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"184\",\n\"Federated States of Micronesia\",\n111000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Federated_States_of_Micronesia.svg/22px-Flag_of_Federated_States_of_Micronesia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"186\",\n\"Saint Vincent and the Grenadines\",\n109000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Flag_of_Saint_Vincent_and_the_Grenadines.svg/22px-Flag_of_Saint_Vincent_and_the_Grenadines.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"188\",\n\"Grenada\",\n104000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Grenada.svg/22px-Flag_of_Grenada.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"189\",\n\"Tonga\",\n104000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Tonga.svg/22px-Flag_of_Tonga.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"190\",\n\"Kiribati\",\n100000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kiribati.svg/22px-Flag_of_Kiribati.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"192\",\n\"Antigua and Barbuda\",\n89000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Antigua_and_Barbuda.svg/22px-Flag_of_Antigua_and_Barbuda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"194\",\n\"Seychelles\",\n85000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Seychelles.svg/22px-Flag_of_the_Seychelles.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"195\",\n\"Andorra\",\n84082,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Andorra.svg/22px-Flag_of_Andorra.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"198\",\n\"Dominica\",\n67000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Flag_of_Dominica.svg/22px-Flag_of_Dominica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"200\",\n\"Marshall Islands\",\n63000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Flag_of_the_Marshall_Islands.svg/22px-Flag_of_the_Marshall_Islands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"204\",\n\"Saint Kitts and Nevis\",\n52000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Saint_Kitts_and_Nevis.svg/22px-Flag_of_Saint_Kitts_and_Nevis.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"206\",\n\"Liechtenstein\",\n35904,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Flag_of_Liechtenstein.svg/22px-Flag_of_Liechtenstein.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"207\",\n\"Monaco\",\n33000,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/22px-Flag_of_Monaco.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"209\",\n\"San Marino\",\n31794,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Flag_of_San_Marino.svg/22px-Flag_of_San_Marino.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"213\",\n\"Palau\",\n20000,\n3e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Palau.svg/22px-Flag_of_Palau.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"215\",\n\"Tuvalu\",\n10000,\n1e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tuvalu.svg/22px-Flag_of_Tuvalu.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"216\",\n\"Nauru\",\n10000,\n1e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Flag_of_Nauru.svg/22px-Flag_of_Nauru.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"222\",\n\"Vatican City\",\n800,\n2e-07,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_the_Vatican_City.svg/20px-Flag_of_the_Vatican_City.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n] \n];\ndata.addColumn('string','Rank');\ndata.addColumn('string','Country');\ndata.addColumn('number','Population');\ndata.addColumn('number','% of World Population');\ndata.addColumn('string','Flag');\ndata.addColumn('boolean','Mode');\ndata.addColumn('date','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartTableID2cd013050362() {\nvar data = gvisDataTableID2cd013050362();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"page\"] = \"enable\";\n\n  var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"});\n  dataFormat1.format(data, 2);\n  var dataFormat2 = new google.visualization.NumberFormat({pattern:\"#.#%\"});\n  dataFormat2.format(data, 3);\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID2cd013050362')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID2cd013050362);\n})();\nfunction displayChartTableID2cd013050362() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDashboards with \ngvisMerge\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nG <- gvisGeoChart(Exports,\n                  locationvar=\"Country\",\n                  colorvar=\"Profit\",\n                  options=list(width=300, height=200))\n\nT <- gvisTable(Exports,\n               options=list(width=300, height=370))\n\n\n\n\nGT <- gvisMerge(G, T, horizontal=FALSE)\n\nplot(GT)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID49611ffdda49 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n\n// jsData \nfunction gvisDataTableID496155ab963a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3,\ntrue\n],\n[\n\"Brazil\",\n4,\nfalse\n],\n[\n\"United States\",\n5,\ntrue\n],\n[\n\"France\",\n4,\ntrue\n],\n[\n\"Hungary\",\n3,\nfalse\n],\n[\n\"India\",\n2,\ntrue\n],\n[\n\"Iceland\",\n1,\nfalse\n],\n[\n\"Norway\",\n4,\ntrue\n],\n[\n\"Spain\",\n5,\ntrue\n],\n[\n\"Turkey\",\n1,\nfalse\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addColumn('boolean','Online');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49611ffdda49() {\nvar data = gvisDataGeoChartID49611ffdda49();\nvar options = {};\noptions[\"width\"] = 300;\noptions[\"height\"] = 200;\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49611ffdda49')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n\n// jsDrawChart\nfunction drawChartTableID496155ab963a() {\nvar data = gvisDataTableID496155ab963a();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"width\"] = 300;\noptions[\"height\"] = 370;\n\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID496155ab963a')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49611ffdda49);\n})();\nfunction displayChartGeoChartID49611ffdda49() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID496155ab963a);\n})();\nfunction displayChartTableID496155ab963a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG <- gvisGeoChart(Exports,\n                  locationvar=\"Country\",\n                  colorvar=\"Profit\",\n                  options=list(width=300, height=370))\n\n\n\n\nGT <- gvisMerge(G, T, horizontal=TRUE)\n\nplot(GT)\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID114f1fa5c040 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n\n// jsData \nfunction gvisDataTableID114f79fb112f () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3,\ntrue\n],\n[\n\"Brazil\",\n4,\nfalse\n],\n[\n\"United States\",\n5,\ntrue\n],\n[\n\"France\",\n4,\ntrue\n],\n[\n\"Hungary\",\n3,\nfalse\n],\n[\n\"India\",\n2,\ntrue\n],\n[\n\"Iceland\",\n1,\nfalse\n],\n[\n\"Norway\",\n4,\ntrue\n],\n[\n\"Spain\",\n5,\ntrue\n],\n[\n\"Turkey\",\n1,\nfalse\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addColumn('boolean','Online');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID114f1fa5c040() {\nvar data = gvisDataGeoChartID114f1fa5c040();\nvar options = {};\noptions[\"width\"] = 300;\noptions[\"height\"] = 370;\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID114f1fa5c040')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n\n// jsDrawChart\nfunction drawChartTableID114f79fb112f() {\nvar data = gvisDataTableID114f79fb112f();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"width\"] = 200;\noptions[\"height\"] = 270;\n\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID114f79fb112f')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID114f1fa5c040);\n})();\nfunction displayChartGeoChartID114f1fa5c040() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID114f79fb112f);\n})();\nfunction displayChartTableID114f79fb112f() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\ndf <- data.frame(country=c(\"US\", \"GB\", \"BR\"),\n                 val1=c(1,3,4),\n                 val2=c(23,12,32))\n\nLine <- gvisLineChart(df,\n                      xvar=\"country\",\n                      yvar=c(\"val1\",\"val2\"),\n                      options=list(\n                        title=\"Hello World\",\n                        titleTextStyle=\"{color:'red',\n                                         fontName:'Courier',\n                                         fontSize:16}\",\n                        backgroundColor=\"#D3D3D3\",\n                        vAxis=\"{gridlines:{color:'red', count:3}}\",\n                        hAxis=\"{title:'Country',\n                                titleTextStyle:{color:'blue'}}\",\n                        series=\"[{color:'green',\n                                  targetAxisIndex: 0},\n                                 {color: 'orange',targetAxisIndex:1}]\",\n                        vAxes=\"[{title:'val1'}, {title:'val2'}]\",\n                        legend=\"bottom\",\n                        curveType=\"function\",\n                        width=500,\n                        height=300))\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataLineChartID584f5d6f811a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"US\",\n1,\n23\n],\n[\n\"GB\",\n3,\n12\n],\n[\n\"BR\",\n4,\n32\n] \n];\ndata.addColumn('string','country');\ndata.addColumn('number','val1');\ndata.addColumn('number','val2');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartLineChartID584f5d6f811a() {\nvar data = gvisDataLineChartID584f5d6f811a();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"title\"] = \"Hello World\";\noptions[\"titleTextStyle\"] = {color:'red',\n                                         fontName:'Courier',\n                                         fontSize:16};\noptions[\"backgroundColor\"] = \"#D3D3D3\";\noptions[\"vAxis\"] = {gridlines:{color:'red', count:3}};\noptions[\"hAxis\"] = {title:'Country',\n                                titleTextStyle:{color:'blue'}};\noptions[\"series\"] = [{color:'green',\n                                  targetAxisIndex: 0},\n                                 {color: 'orange',targetAxisIndex:1}];\noptions[\"vAxes\"] = [{title:'val1'}, {title:'val2'}];\noptions[\"legend\"] = \"bottom\";\noptions[\"curveType\"] = \"function\";\noptions[\"width\"] = 500;\noptions[\"height\"] = 300;\n\n\n    var chart = new google.visualization.LineChart(\n    document.getElementById('LineChartID584f5d6f811a')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"corechart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartLineChartID584f5d6f811a);\n})();\nfunction displayChartLineChartID584f5d6f811a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApostrophes\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\ndf <- data.frame(\"Year\"=c(2009,2010),\n                 \"Lloyd\\\\'s\"=c(86.1, 93.3),\n                 \"Munich Re\\\\'s R/I\"=c(95.3, 100.5), check.names=FALSE)\n\nR <- gvisColumnChart(df, \n                     options=list(vAxis='{baseline:0}',\n                                  title=\"Combined Ratio %\",\n                                  legend=\"{position:'bottom'}\"))\n\ncat(R$html$chart, file = \"GoogleVis/R.html\") # save\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataColumnChartID336e54fe1b42 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n2009,\n86.1,\n95.3\n],\n[\n2010,\n93.3,\n100.5\n] \n];\ndata.addColumn('number','Year');\ndata.addColumn('number','Lloyd\\'s');\ndata.addColumn('number','Munich Re\\'s R/I');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartColumnChartID336e54fe1b42() {\nvar data = gvisDataColumnChartID336e54fe1b42();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"vAxis\"] = {baseline:0};\noptions[\"title\"] = \"Combined Ratio %\";\noptions[\"legend\"] = {position:'bottom'};\n\n\n    var chart = new google.visualization.ColumnChart(\n    document.getElementById('ColumnChartID336e54fe1b42')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"corechart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartColumnChartID336e54fe1b42);\n})();\nfunction displayChartColumnChartID336e54fe1b42() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "GoogleVis"
        },
        {
            "location": "/GoogleVis/#suppress-the-message",
            "text": "We normally load the library\u2026  library(googleVis)  \u2026but we can also suppress the message when loading the package.  suppressPackageStartupMessages(library(googleVis))",
            "title": "Suppress the message"
        },
        {
            "location": "/GoogleVis/#printing",
            "text": "Generate the chart.   library(googleVis)\n\nhead(Fruits, 3)\n\nM <- gvisMotionChart(Fruits,\n                     idvar='Fruit',\n                     timevar='Year',\n                     options=list(width=400, height=350))\n\n# xvar =\n# yvar =\n# colorvar =\n# sizevar =\n# date.format =   Emulate the chart\u2026   # In the browser, with a tag underneath\nplot(M)\n\n# In the console (code only), with a tag underneath\nM\n\n# Header part\n# Basic html and formatting tags\nprint(M, tag='header')\n\n# Actual Google visualization code\n# Can be copy-paste in a markdown or html document\nprint(M, tag='chart')\n\n# Header + visualization code = what we see in the browser\n\n# Components of the chart\nprint(M, tag='jsChart')\n\n# Basic chart caption and html footer (what is underneath)\nprint(M, tag='caption')\n\n# Save it locally\nprint(M, file=\"GoogleVis/M.html\")\n# Or\n#cat(M$html$chart, file = \"GoogleVis/M.html\")",
            "title": "Printing"
        },
        {
            "location": "/GoogleVis/#embedding-a-chartmap-into-a-static-website",
            "text": "The chart can be standing alone as an image file; even be embedded within a text.   In RStudio, set the working directory with  setwd(' ') .  Set the option to print the output in a html file.   {r, message=FALSE}\nlibrary(googleVis)\n\nop <- options(gvis.plot.tag='chart')   Set options back to original options.   options(op)   Generate the chart.   library(googleVis)\n\n# Set the option to print here (alternative)\noptions(gvis.plot.tag='chart')\n\n# Create the chart\nM <- gvisMotionChart(Fruits,\n                     'Fruit',\n                     'Year',\n                     options=list(width=400, height=350))\n\n# Do not print the chart in the browser\n#plot(M) # in the browser\n\n# Print the chart as a html file\ncat(M$html$chart, file = \"M.html\")   The file has several parts: JavaScript and HTML. The part that displays the chart is in the end.   <!-- divChart -->\n\n<div id=\"MotionChartID2cd02805862d\" \n     style=\"width: 400; height: 350;\">\n</div>   Following this bullet, in the Markdown document, paste the M.html code. First, open the HTML file to copy the code.  Modify the markup language with more styling parameters.   <div id=\"MotionChartID2cd02805862d\" \nstyle=\"width: 400; height: 350; float:left;\";>\n</div>   The HTML snippet must follow the JavaScript snippet in the document. Move the HTML snippet within a text (the text can separate both snippets).    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n<div id=\"MotionChartID2cd02805862d\" \nstyle=\"width: 400; height: 350; float:left;\";>\n</div>\nDuis aute irure dolor...  Here is the result:   \n\n// jsData \nfunction gvisDataMotionChartID2cd02805862d () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Apples\",\n2008,\n\"West\",\n98,\n78,\n20,\n\"2008-12-31\"\n],\n[\n\"Apples\",\n2009,\n\"West\",\n111,\n79,\n32,\n\"2009-12-31\"\n],\n[\n\"Apples\",\n2010,\n\"West\",\n89,\n76,\n13,\n\"2010-12-31\"\n],\n[\n\"Oranges\",\n2008,\n\"East\",\n96,\n81,\n15,\n\"2008-12-31\"\n],\n[\n\"Bananas\",\n2008,\n\"East\",\n85,\n76,\n9,\n\"2008-12-31\"\n],\n[\n\"Oranges\",\n2009,\n\"East\",\n93,\n80,\n13,\n\"2009-12-31\"\n],\n[\n\"Bananas\",\n2009,\n\"East\",\n94,\n78,\n16,\n\"2009-12-31\"\n],\n[\n\"Oranges\",\n2010,\n\"East\",\n98,\n91,\n7,\n\"2010-12-31\"\n],\n[\n\"Bananas\",\n2010,\n\"East\",\n81,\n71,\n10,\n\"2010-12-31\"\n] \n];\ndata.addColumn('string','Fruit');\ndata.addColumn('number','Year');\ndata.addColumn('string','Location');\ndata.addColumn('number','Sales');\ndata.addColumn('number','Expenses');\ndata.addColumn('number','Profit');\ndata.addColumn('string','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMotionChartID2cd02805862d() {\nvar data = gvisDataMotionChartID2cd02805862d();\nvar options = {};\noptions[\"width\"] = 400;\noptions[\"height\"] = 350;\noptions[\"state\"] = \"\";\n\n\n    var chart = new google.visualization.MotionChart(\n    document.getElementById('MotionChartID2cd02805862d')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"motionchart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMotionChartID2cd02805862d);\n})();\nfunction displayChartMotionChartID2cd02805862d() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter     Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.    \nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.",
            "title": "Embedding a chart/map into a static website"
        },
        {
            "location": "/GoogleVis/#other-static-websites",
            "text": "There is a procedure for embedding graphics in:   WordPress,  Google Sites,  Blogger,  Google Code wiki pages.  Wikipedia,  others websites.",
            "title": "Other static websites"
        },
        {
            "location": "/GoogleVis/#dynamic-websites",
            "text": "The  R.rsp  package allows the integration of R code into html code.   The  rApache  and  brew  packages support web application development using R and the Apache HTTP server.  Rook is a lightweight web server interface for R.  The  shiny  package builds interactive web application with R.",
            "title": "Dynamic websites"
        },
        {
            "location": "/GoogleVis/#charts",
            "text": "Consult the examples (further above); charts are similar to what we find in other packages.   gvisMotionChart  is exclusive to  googleVis .",
            "title": "Charts"
        },
        {
            "location": "/GoogleVis/#gvismotionchart",
            "text": "head(Fruits, 3)  # Output\n   Fruit Year Location Sales Expenses Profit       Date\n1 Apples 2008     West    98       78     20 2008-12-31\n2 Apples 2009     West   111       79     32 2009-12-31\n3 Apples 2010     West    89       76     13 2010-12-31  M <- gvisMotionChart(Fruits,\n                     idvar='Fruit',\n                     timevar='Year',\n                     options=list(width=400, height=350))\n\n# xvar =\n# yvar =\n# colorvar =\n# sizevar =\n# date.format =\n\nplot(M)   \n\n// jsData \nfunction gvisDataMotionChartID336e668836ab () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Apples\",\n2008,\n\"West\",\n98,\n78,\n20,\n\"2008-12-31\"\n],\n[\n\"Apples\",\n2009,\n\"West\",\n111,\n79,\n32,\n\"2009-12-31\"\n],\n[\n\"Apples\",\n2010,\n\"West\",\n89,\n76,\n13,\n\"2010-12-31\"\n],\n[\n\"Oranges\",\n2008,\n\"East\",\n96,\n81,\n15,\n\"2008-12-31\"\n],\n[\n\"Bananas\",\n2008,\n\"East\",\n85,\n76,\n9,\n\"2008-12-31\"\n],\n[\n\"Oranges\",\n2009,\n\"East\",\n93,\n80,\n13,\n\"2009-12-31\"\n],\n[\n\"Bananas\",\n2009,\n\"East\",\n94,\n78,\n16,\n\"2009-12-31\"\n],\n[\n\"Oranges\",\n2010,\n\"East\",\n98,\n91,\n7,\n\"2010-12-31\"\n],\n[\n\"Bananas\",\n2010,\n\"East\",\n81,\n71,\n10,\n\"2010-12-31\"\n] \n];\ndata.addColumn('string','Fruit');\ndata.addColumn('number','Year');\ndata.addColumn('string','Location');\ndata.addColumn('number','Sales');\ndata.addColumn('number','Expenses');\ndata.addColumn('number','Profit');\ndata.addColumn('string','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMotionChartID336e668836ab() {\nvar data = gvisDataMotionChartID336e668836ab();\nvar options = {};\noptions[\"width\"] = 400;\noptions[\"height\"] = 350;\noptions[\"state\"] = \"\";\n\n\n    var chart = new google.visualization.MotionChart(\n    document.getElementById('MotionChartID336e668836ab')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"motionchart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMotionChartID336e668836ab);\n})();\nfunction displayChartMotionChartID336e668836ab() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "gvisMotionChart"
        },
        {
            "location": "/GoogleVis/#within-an-html-iframe",
            "text": "After we generate the HTML chart or map, save the object as a HTML file.  library(htmlwidgets) \nlibrary(DT) \n\nsaveWidget(object, \"googleVis.html\")  Add this HTML snippet to the Markdown/HTML document.  <iframe seamless src=\"/googleVis.html\" width=600px \nheight=400px ></iframe>    Here is another example with a Leaflet file with the  leaflet  package.",
            "title": "Within an HTML iframe"
        },
        {
            "location": "/GoogleVis/#information-from-the-object",
            "text": "M$type\nM$chartid  \"MotionChart\"\n\"MotionChartID336e668836ab\"",
            "title": "Information from the object"
        },
        {
            "location": "/GoogleVis/#maps",
            "text": "",
            "title": "Maps"
        },
        {
            "location": "/GoogleVis/#gvisgeochart",
            "text": "head(Exports, 3)  # Output\n        Country Profit Online\n1       Germany      3   TRUE\n2        Brazil      4  FALSE\n3 United States      5   TRUE  Geo <- gvisGeoChart(Exports, \n                    locationvar = \"Country\",\n                    colorvar = \"Profit\",\n                    sizevar = \"\", # size of markers\n                    hovervar = \"\", # text\n                    options = list(projection=\"kavrayskiy-vii\"))\n\n# locationvar can be lat:long or address, country name, region, state, city\n\nplot(Geo)   \n\n// jsData \nfunction gvisDataGeoChartID49617fef373 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49617fef373() {\nvar data = gvisDataGeoChartID49617fef373();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\noptions[\"projection\"] = \"kavrayskiy-vii\";\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49617fef373')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49617fef373);\n})();\nfunction displayChartGeoChartID49617fef373() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter          head(CityPopularity, 3)  # Output\n      City Popularity\n1 New York        200\n2   Boston        300\n3    Miami        400  Geo2 <- gvisGeoChart(CityPopularity, \n                     locationvar='City',\n                     colorvar='Popularity',\n                     options=list(region='US',\n                                  height=350,\n                                  displayMode='markers', \n                                  colorAxis=\"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\"))\n\nplot(Geo2)   \n\n// jsData \nfunction gvisDataGeoChartID4961741b9385 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"New York\",\n200\n],\n[\n\"Boston\",\n300\n],\n[\n\"Miami\",\n400\n],\n[\n\"Chicago\",\n500\n],\n[\n\"Los Angeles\",\n600\n],\n[\n\"Houston\",\n700\n] \n];\ndata.addColumn('string','City');\ndata.addColumn('number','Popularity');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID4961741b9385() {\nvar data = gvisDataGeoChartID4961741b9385();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 350;\noptions[\"region\"] = \"US\";\noptions[\"displayMode\"] = \"markers\";\noptions[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']};\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID4961741b9385')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID4961741b9385);\n})();\nfunction displayChartGeoChartID4961741b9385() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter          CityPopularity3 <- data.frame(City = c('Montreal', 'Toronto'),\n                              Popularity = c(400, 200))\n\nGeo3 <- gvisGeoChart(CityPopularity3, \n                     locationvar='City',\n                     colorvar='Popularity',\n                     options=list(region = 'CA',\n                                  height=350,\n                                  displayMode='markers', \n                                  colorAxis=\"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\"))\n\nplot(Geo3)   \n\n// jsData \nfunction gvisDataGeoChartID49614f24acff () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Montreal\",\n700\n],\n[\n\"Toronto\",\n200\n] \n];\ndata.addColumn('string','City');\ndata.addColumn('number','Popularity');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49614f24acff() {\nvar data = gvisDataGeoChartID49614f24acff();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 350;\noptions[\"region\"] = \"CA\";\noptions[\"displayMode\"] = \"markers\";\noptions[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']};\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49614f24acff')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49614f24acff);\n})();\nfunction displayChartGeoChartID49614f24acff() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter          head(Andrew, 3)  # Output\n        Date/Time UTC  Lat  Long Pressure_mb Speed_kt            Category\n1 1992-08-16 18:00:00 10.8 -35.5        1010       25 Tropical Depression\n2 1992-08-17 00:00:00 11.2 -37.4        1009       30 Tropical Depression\n3 1992-08-17 06:00:00 11.7 -39.6        1008       30 Tropical Depression\n     LatLong                                              Tip\n1 10.8:-35.5 Tropical Depression<BR>Pressure=1010<BR>Speed=25\n2 11.2:-37.4 Tropical Depression<BR>Pressure=1009<BR>Speed=30\n3 11.7:-39.6 Tropical Depression<BR>Pressure=1008<BR>Speed=30  GeoMarker <- gvisGeoChart(Andrew,\n                          \"LatLong\", \n                          sizevar='Speed_kt',\n                          colorvar=\"Pressure_mb\", \n                          options=list(region=\"US\"))\n\nplot(GeoMarker)   \n\n// jsData \nfunction gvisDataGeoChartID2cd02f295f7 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n10.8,\n-35.5,\n1010,\n25\n],\n[\n11.2,\n-37.4,\n1009,\n30\n],\n[\n11.7,\n-39.6,\n1008,\n30\n],\n[\n12.3,\n-42,\n1006,\n35\n],\n[\n13.1,\n-44.2,\n1003,\n35\n],\n[\n13.6,\n-46.2,\n1002,\n40\n],\n[\n14.1,\n-48,\n1001,\n45\n],\n[\n14.6,\n-49.9,\n1000,\n45\n],\n[\n15.4,\n-51.8,\n1000,\n45\n],\n[\n16.3,\n-53.5,\n1001,\n45\n],\n[\n17.2,\n-55.3,\n1002,\n45\n],\n[\n18,\n-56.9,\n1005,\n45\n],\n[\n18.8,\n-58.3,\n1007,\n45\n],\n[\n19.8,\n-59.3,\n1011,\n40\n],\n[\n20.7,\n-60,\n1013,\n40\n],\n[\n21.7,\n-60.7,\n1015,\n40\n],\n[\n22.5,\n-61.5,\n1014,\n40\n],\n[\n23.2,\n-62.4,\n1014,\n45\n],\n[\n23.9,\n-63.3,\n1010,\n45\n],\n[\n24.4,\n-64.2,\n1007,\n50\n],\n[\n24.8,\n-64.9,\n1004,\n50\n],\n[\n25.3,\n-65.9,\n1000,\n55\n],\n[\n25.6,\n-67,\n994,\n60\n],\n[\n25.8,\n-68.3,\n981,\n70\n],\n[\n25.7,\n-69.7,\n969,\n80\n],\n[\n25.6,\n-71.1,\n961,\n90\n],\n[\n25.5,\n-72.5,\n947,\n105\n],\n[\n25.4,\n-74.2,\n933,\n120\n],\n[\n25.4,\n-75.8,\n922,\n135\n],\n[\n25.4,\n-77.5,\n930,\n125\n],\n[\n25.4,\n-79.3,\n937,\n120\n],\n[\n25.6,\n-81.2,\n951,\n110\n],\n[\n25.8,\n-83.1,\n947,\n115\n],\n[\n26.2,\n-85,\n943,\n115\n],\n[\n26.6,\n-86.7,\n948,\n115\n],\n[\n27.2,\n-88.2,\n946,\n115\n],\n[\n27.8,\n-89.6,\n941,\n120\n],\n[\n28.5,\n-90.5,\n937,\n120\n],\n[\n29.2,\n-91.3,\n955,\n115\n],\n[\n30.1,\n-91.7,\n973,\n80\n],\n[\n30.9,\n-91.6,\n991,\n50\n],\n[\n31.5,\n-91.1,\n995,\n35\n],\n[\n32.1,\n-90.5,\n997,\n30\n],\n[\n32.8,\n-89.6,\n998,\n30\n],\n[\n33.6,\n-88.4,\n999,\n25\n],\n[\n34.4,\n-86.7,\n1000,\n20\n],\n[\n35.4,\n-84,\n1000,\n20\n] \n];\ndata.addColumn('number','Latitude');\ndata.addColumn('number','Longitude');\ndata.addColumn('number','Pressure_mb');\ndata.addColumn('number','Speed_kt');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID2cd02f295f7() {\nvar data = gvisDataGeoChartID2cd02f295f7();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\noptions[\"region\"] = \"US\";\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID2cd02f295f7')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID2cd02f295f7);\n})();\nfunction displayChartGeoChartID2cd02f295f7() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "gvisGeoChart"
        },
        {
            "location": "/GoogleVis/#gvismap-or-google-maps",
            "text": "AndrewMap <- gvisMap(Andrew,\n                     locationvar=\"LatLong\" ,\n                     tipvar=\"Tip\", #  text displayed over the tip icon\n                     options=list(showTip=TRUE, \n                                  showLine=TRUE, \n                                  enableScrollWheel=TRUE,\n                                  mapType='terrain', \n                                  useMapTypeControl=TRUE))\n\nplot(AndrewMap)   \n\n// jsData \nfunction gvisDataMapID496129f7ada () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n10.8,\n-35.5,\n\"Tropical Depression<BR>Pressure=1010<BR>Speed=25\"\n],\n[\n11.2,\n-37.4,\n\"Tropical Depression<BR>Pressure=1009<BR>Speed=30\"\n],\n[\n11.7,\n-39.6,\n\"Tropical Depression<BR>Pressure=1008<BR>Speed=30\"\n],\n[\n12.3,\n-42,\n\"Tropical Storm<BR>Pressure=1006<BR>Speed=35\"\n],\n[\n13.1,\n-44.2,\n\"Tropical Storm<BR>Pressure=1003<BR>Speed=35\"\n],\n[\n13.6,\n-46.2,\n\"Tropical Storm<BR>Pressure=1002<BR>Speed=40\"\n],\n[\n14.1,\n-48,\n\"Tropical Storm<BR>Pressure=1001<BR>Speed=45\"\n],\n[\n14.6,\n-49.9,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=45\"\n],\n[\n15.4,\n-51.8,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=45\"\n],\n[\n16.3,\n-53.5,\n\"Tropical Storm<BR>Pressure=1001<BR>Speed=45\"\n],\n[\n17.2,\n-55.3,\n\"Tropical Storm<BR>Pressure=1002<BR>Speed=45\"\n],\n[\n18,\n-56.9,\n\"Tropical Storm<BR>Pressure=1005<BR>Speed=45\"\n],\n[\n18.8,\n-58.3,\n\"Tropical Storm<BR>Pressure=1007<BR>Speed=45\"\n],\n[\n19.8,\n-59.3,\n\"Tropical Storm<BR>Pressure=1011<BR>Speed=40\"\n],\n[\n20.7,\n-60,\n\"Tropical Storm<BR>Pressure=1013<BR>Speed=40\"\n],\n[\n21.7,\n-60.7,\n\"Tropical Storm<BR>Pressure=1015<BR>Speed=40\"\n],\n[\n22.5,\n-61.5,\n\"Tropical Storm<BR>Pressure=1014<BR>Speed=40\"\n],\n[\n23.2,\n-62.4,\n\"Tropical Storm<BR>Pressure=1014<BR>Speed=45\"\n],\n[\n23.9,\n-63.3,\n\"Tropical Storm<BR>Pressure=1010<BR>Speed=45\"\n],\n[\n24.4,\n-64.2,\n\"Tropical Storm<BR>Pressure=1007<BR>Speed=50\"\n],\n[\n24.8,\n-64.9,\n\"Tropical Storm<BR>Pressure=1004<BR>Speed=50\"\n],\n[\n25.3,\n-65.9,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=55\"\n],\n[\n25.6,\n-67,\n\"Tropical Storm<BR>Pressure=994<BR>Speed=60\"\n],\n[\n25.8,\n-68.3,\n\"Hurricane<BR>Pressure=981<BR>Speed=70\"\n],\n[\n25.7,\n-69.7,\n\"Hurricane<BR>Pressure=969<BR>Speed=80\"\n],\n[\n25.6,\n-71.1,\n\"Hurricane<BR>Pressure=961<BR>Speed=90\"\n],\n[\n25.5,\n-72.5,\n\"Hurricane<BR>Pressure=947<BR>Speed=105\"\n],\n[\n25.4,\n-74.2,\n\"Hurricane<BR>Pressure=933<BR>Speed=120\"\n],\n[\n25.4,\n-75.8,\n\"Hurricane<BR>Pressure=922<BR>Speed=135\"\n],\n[\n25.4,\n-77.5,\n\"Hurricane<BR>Pressure=930<BR>Speed=125\"\n],\n[\n25.4,\n-79.3,\n\"Hurricane<BR>Pressure=937<BR>Speed=120\"\n],\n[\n25.6,\n-81.2,\n\"Hurricane<BR>Pressure=951<BR>Speed=110\"\n],\n[\n25.8,\n-83.1,\n\"Hurricane<BR>Pressure=947<BR>Speed=115\"\n],\n[\n26.2,\n-85,\n\"Hurricane<BR>Pressure=943<BR>Speed=115\"\n],\n[\n26.6,\n-86.7,\n\"Hurricane<BR>Pressure=948<BR>Speed=115\"\n],\n[\n27.2,\n-88.2,\n\"Hurricane<BR>Pressure=946<BR>Speed=115\"\n],\n[\n27.8,\n-89.6,\n\"Hurricane<BR>Pressure=941<BR>Speed=120\"\n],\n[\n28.5,\n-90.5,\n\"Hurricane<BR>Pressure=937<BR>Speed=120\"\n],\n[\n29.2,\n-91.3,\n\"Hurricane<BR>Pressure=955<BR>Speed=115\"\n],\n[\n30.1,\n-91.7,\n\"Tropical Storm<BR>Pressure=973<BR>Speed=80\"\n],\n[\n30.9,\n-91.6,\n\"Tropical Storm<BR>Pressure=991<BR>Speed=50\"\n],\n[\n31.5,\n-91.1,\n\"Tropical Depression<BR>Pressure=995<BR>Speed=35\"\n],\n[\n32.1,\n-90.5,\n\"Tropical Depression<BR>Pressure=997<BR>Speed=30\"\n],\n[\n32.8,\n-89.6,\n\"Tropical Depression<BR>Pressure=998<BR>Speed=30\"\n],\n[\n33.6,\n-88.4,\n\"Tropical Depression<BR>Pressure=999<BR>Speed=25\"\n],\n[\n34.4,\n-86.7,\n\"Tropical Depression<BR>Pressure=1000<BR>Speed=20\"\n],\n[\n35.4,\n-84,\n\"Tropical Depression<BR>Pressure=1000<BR>Speed=20\"\n] \n];\ndata.addColumn('number','Latitude');\ndata.addColumn('number','Longitude');\ndata.addColumn('string','Tip');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMapID496129f7ada() {\nvar data = gvisDataMapID496129f7ada();\nvar options = {};\noptions[\"showTip\"] = true;\noptions[\"showLine\"] = true;\noptions[\"enableScrollWheel\"] = true;\noptions[\"mapType\"] = \"terrain\";\noptions[\"useMapTypeControl\"] = true;\n\n\n    var chart = new google.visualization.Map(\n    document.getElementById('MapID496129f7ada')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"map\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMapID496129f7ada);\n})();\nfunction displayChartMapID496129f7ada() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "gvisMap or Google Maps"
        },
        {
            "location": "/GoogleVis/#editor",
            "text": "This is a fantastic option that let us start with one chart or map and change everything with a menu.     Editor <- gvisGeoChart(Exports, \n                       locationvar=\"Country\",\n                       colorvar=\"Profit\",\n                       options=list(gvis.editor='Edit me!'))\n\nplot(Editor)   \n\n// jsData \nfunction gvisDataGeoChartID49613aa5c54a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49613aa5c54a() {\nvar data = gvisDataGeoChartID49613aa5c54a();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\n\n\n    chartGeoChartID49613aa5c54a = new google.visualization.ChartWrapper({\n    dataTable: data,       \n    chartType: 'GeoChart',\n    containerId: 'GeoChartID49613aa5c54a',\n    options: options\n    });\n    chartGeoChartID49613aa5c54a.draw();\n\n\n}\n\n  function openEditorGeoChartID49613aa5c54a() {\n  var editor = new google.visualization.ChartEditor();\n  google.visualization.events.addListener(editor, 'ok',\n  function() { \n  chartGeoChartID49613aa5c54a = editor.getChartWrapper();  \n  chartGeoChartID49613aa5c54a.draw(document.getElementById('GeoChartID49613aa5c54a')); \n  }); \n  editor.openDialog(chartGeoChartID49613aa5c54a);\n  }\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"charteditor\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49613aa5c54a);\n})();\nfunction displayChartGeoChartID49613aa5c54a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Editor"
        },
        {
            "location": "/GoogleVis/#tables-with-gvistable",
            "text": "head(Stock, 3)  # Output\n        Date  Device Value Title Annotation\n1 2008-01-01 Pencils  3000  <NA>       <NA>\n2 2008-01-02 Pencils 14045  <NA>       <NA>\n3 2008-01-03 Pencils  5502  <NA>       <NA>  Table <- gvisTable(Stock, \n                   formats=list(Value=\"#,###\"))\n\nplot(Table)   \n\n// jsData \nfunction gvisDataTableID2cd03b735929 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\nnew Date(2008,0,1),\n\"Pencils\",\n3000,\nnull,\nnull\n],\n[\nnew Date(2008,0,2),\n\"Pencils\",\n14045,\nnull,\nnull\n],\n[\nnew Date(2008,0,3),\n\"Pencils\",\n5502,\nnull,\nnull\n],\n[\nnew Date(2008,0,4),\n\"Pencils\",\n75284,\nnull,\nnull\n],\n[\nnew Date(2008,0,5),\n\"Pencils\",\n41476,\n\"Bought pencils\",\n\"Bought 200k pencils\"\n],\n[\nnew Date(2008,0,6),\n\"Pencils\",\n333222,\nnull,\nnull\n],\n[\nnew Date(2008,0,1),\n\"Pens\",\n40645,\nnull,\nnull\n],\n[\nnew Date(2008,0,2),\n\"Pens\",\n20374,\nnull,\nnull\n],\n[\nnew Date(2008,0,3),\n\"Pens\",\n50766,\nnull,\nnull\n],\n[\nnew Date(2008,0,4),\n\"Pens\",\n14334,\n\"Out of stock\",\n\"Ran out of stock of pens at 4pm\"\n],\n[\nnew Date(2008,0,5),\n\"Pens\",\n66467,\nnull,\nnull\n],\n[\nnew Date(2008,0,6),\n\"Pens\",\n39463,\nnull,\nnull\n] \n];\ndata.addColumn('date','Date');\ndata.addColumn('string','Device');\ndata.addColumn('number','Value');\ndata.addColumn('string','Title');\ndata.addColumn('string','Annotation');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartTableID2cd03b735929() {\nvar data = gvisDataTableID2cd03b735929();\nvar options = {};\noptions[\"allowHtml\"] = true;\n\n  var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"});\n  dataFormat1.format(data, 2);\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID2cd03b735929')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID2cd03b735929);\n})();\nfunction displayChartTableID2cd03b735929() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter           head(Population, 3)  # Output\n                                                                                 Rank       Country Population % of World Population\n1    1         China 1339940000                0.1950\n2    2         India 1188650000                0.1730\n3    3 United States  310438000                0.0452\n                                                                                                                                                                     Flag\n1 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\">\n2                                                       <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\">\n3                               <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\">\n  Mode       Date\n1 TRUE 2010-10-09\n2 TRUE 2010-10-09\n3 TRUE 2010-10-09  PopTable <- gvisTable(Population, \n                      formats=list(Population=\"#,###\",\n                                   '% of World Population'='#.#%'),\n                      options=list(page='enable'))\n\nplot(PopTable)   \n\n// jsData \nfunction gvisDataTableID2cd013050362 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"1\",\n\"China\",\n1339940000,\n0.195,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"2\",\n\"India\",\n1188650000,\n0.173,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"3\",\n\"United States\",\n310438000,\n0.0452,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"4\",\n\"Indonesia\",\n237556363,\n0.0346,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Indonesia.svg/22px-Flag_of_Indonesia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"5\",\n\"Brazil\",\n193626000,\n0.0282,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Brazil.svg/22px-Flag_of_Brazil.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"6\",\n\"Pakistan\",\n170745000,\n0.0248,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Flag_of_Pakistan.svg/22px-Flag_of_Pakistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"7\",\n\"Bangladesh\",\n164425000,\n0.0239,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Flag_of_Bangladesh.svg/22px-Flag_of_Bangladesh.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"8\",\n\"Nigeria\",\n158259000,\n0.023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Flag_of_Nigeria.svg/22px-Flag_of_Nigeria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"9\",\n\"Russia\",\n141927297,\n0.0206,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Russia.svg/22px-Flag_of_Russia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"10\",\n\"Japan\",\n127390000,\n0.0185,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Flag_of_Japan.svg/22px-Flag_of_Japan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"11\",\n\"Mexico\",\n108396211,\n0.0158,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Flag_of_Mexico.svg/22px-Flag_of_Mexico.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"12\",\n\"Philippines\",\n94013200,\n0.0137,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_the_Philippines.svg/22px-Flag_of_the_Philippines.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"13\",\n\"Vietnam\",\n85846997,\n0.0125,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Vietnam.svg/22px-Flag_of_Vietnam.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"14\",\n\"Ethiopia\",\n84976000,\n0.0124,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Flag_of_Ethiopia.svg/22px-Flag_of_Ethiopia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"15\",\n\"Germany\",\n81802257,\n0.0119,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/22px-Flag_of_Germany.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"16\",\n\"Egypt\",\n79135000,\n0.0115,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Egypt.svg/22px-Flag_of_Egypt.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"17\",\n\"Iran\",\n75078000,\n0.0109,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Flag_of_Iran.svg/22px-Flag_of_Iran.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"18\",\n\"Turkey\",\n72561312,\n0.0106,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Flag_of_Turkey.svg/22px-Flag_of_Turkey.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"19\",\n\"Dem. Rep. of Congo\",\n67827000,\n0.0099,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Democratic_Republic_of_the_Congo.svg/22px-Flag_of_the_Democratic_Republic_of_the_Congo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"20\",\n\"Thailand\",\n67070000,\n0.01,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Flag_of_Thailand.svg/22px-Flag_of_Thailand.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"21\",\n\"France\",\n65447374,\n0.0095,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/22px-Flag_of_France.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"22\",\n\"United Kingdom\",\n62008049,\n0.009,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/22px-Flag_of_the_United_Kingdom.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"23\",\n\"Italy\",\n60402499,\n0.0088,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/03/Flag_of_Italy.svg/22px-Flag_of_Italy.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"24\",\n\"Myanmar\",\n50496000,\n0.0073,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Flag_of_Myanmar.svg/22px-Flag_of_Myanmar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"25\",\n\"South Africa\",\n49991300,\n0.0073,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Flag_of_South_Africa.svg/22px-Flag_of_South_Africa.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"26\",\n\"South Korea\",\n49773145,\n0.0072,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Flag_of_South_Korea.svg/22px-Flag_of_South_Korea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"27\",\n\"Spain\",\n46072834,\n0.0067,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Spain.svg/22px-Flag_of_Spain.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"28\",\n\"Ukraine\",\n45871738,\n0.0067,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Ukraine.svg/22px-Flag_of_Ukraine.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"29\",\n\"Colombia\",\n45655000,\n0.0066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Colombia.svg/22px-Flag_of_Colombia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"30\",\n\"Tanzania\",\n45040000,\n0.0066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tanzania.svg/22px-Flag_of_Tanzania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"31\",\n\"Sudan\",\n43192000,\n0.0063,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Sudan.svg/22px-Flag_of_Sudan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"32\",\n\"Argentina\",\n40518951,\n0.0059,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/22px-Flag_of_Argentina.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"33\",\n\"Kenya\",\n38610097,\n0.0056,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Kenya.svg/22px-Flag_of_Kenya.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"34\",\n\"Poland\",\n38167329,\n0.0056,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Flag_of_Poland.svg/22px-Flag_of_Poland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"35\",\n\"Algeria\",\n35423000,\n0.0052,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Algeria.svg/22px-Flag_of_Algeria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"36\",\n\"Canada\",\n34272000,\n0.005,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Canada.svg/22px-Flag_of_Canada.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"37\",\n\"Uganda\",\n33796000,\n0.0049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Flag_of_Uganda.svg/22px-Flag_of_Uganda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"38\",\n\"Morocco\",\n31944000,\n0.0046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Morocco.svg/22px-Flag_of_Morocco.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"39\",\n\"Iraq\",\n31467000,\n0.0046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Flag_of_Iraq.svg/22px-Flag_of_Iraq.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"40\",\n\"Nepal\",\n29853000,\n0.0043,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Flag_of_Nepal.svg/16px-Flag_of_Nepal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"41\",\n\"Peru\",\n29461933,\n0.0043,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Peru.svg/22px-Flag_of_Peru.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"42\",\n\"Afghanistan\",\n29117000,\n0.0042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Afghanistan.svg/22px-Flag_of_Afghanistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"43\",\n\"Venezuela\",\n28958000,\n0.0042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Venezuela.svg/22px-Flag_of_Venezuela.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"44\",\n\"Malaysia\",\n28250500,\n0.0041,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Flag_of_Malaysia.svg/22px-Flag_of_Malaysia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"45\",\n\"Uzbekistan\",\n27794000,\n0.004,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Uzbekistan.svg/22px-Flag_of_Uzbekistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"46\",\n\"Saudi Arabia\",\n27136977,\n0.0039,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Flag_of_Saudi_Arabia.svg/22px-Flag_of_Saudi_Arabia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"47\",\n\"Ghana\",\n24333000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Ghana.svg/22px-Flag_of_Ghana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"48\",\n\"Yemen\",\n24256000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Yemen.svg/22px-Flag_of_Yemen.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"49\",\n\"North Korea\",\n23991000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Flag_of_North_Korea.svg/22px-Flag_of_North_Korea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"50\",\n\"Mozambique\",\n23406000,\n0.0034,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Mozambique.svg/22px-Flag_of_Mozambique.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"52\",\n\"Syria\",\n22505000,\n0.0033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Flag_of_Syria.svg/22px-Flag_of_Syria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"53\",\n\"Australia\",\n22483305,\n0.0033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Flag_of_Australia.svg/22px-Flag_of_Australia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"54\",\n\"Cote d'Ivoire\",\n21571000,\n0.0031,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Flag_of_Cote_d%27Ivoire.svg/22px-Flag_of_Cote_d%27Ivoire.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"55\",\n\"Romania\",\n21466174,\n0.0031,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Romania.svg/22px-Flag_of_Romania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"56\",\n\"Sri Lanka\",\n20410000,\n0.003,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Sri_Lanka.svg/22px-Flag_of_Sri_Lanka.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"57\",\n\"Madagascar\",\n20146000,\n0.0029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Madagascar.svg/22px-Flag_of_Madagascar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"58\",\n\"Cameroon\",\n19958000,\n0.0029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Cameroon.svg/22px-Flag_of_Cameroon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"59\",\n\"Angola\",\n18993000,\n0.0028,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Angola.svg/22px-Flag_of_Angola.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"60\",\n\"Chile\",\n17140000,\n0.0025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Flag_of_Chile.svg/22px-Flag_of_Chile.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"61\",\n\"Netherlands\",\n16619500,\n0.00242,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/22px-Flag_of_the_Netherlands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"62\",\n\"Burkina Faso\",\n16287000,\n0.0024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Burkina_Faso.svg/22px-Flag_of_Burkina_Faso.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"63\",\n\"Kazakhstan\",\n16197000,\n0.0024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kazakhstan.svg/22px-Flag_of_Kazakhstan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"64\",\n\"Niger\",\n15891000,\n0.0023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Flag_of_Niger.svg/22px-Flag_of_Niger.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"65\",\n\"Malawi\",\n15692000,\n0.0023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Flag_of_Malawi.svg/22px-Flag_of_Malawi.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"66\",\n\"Mali\",\n14517176,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Mali.svg/22px-Flag_of_Mali.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"67\",\n\"Guatemala\",\n14377000,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Flag_of_Guatemala.svg/22px-Flag_of_Guatemala.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"68\",\n\"Ecuador\",\n14259000,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Flag_of_Ecuador.svg/22px-Flag_of_Ecuador.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"69\",\n\"Cambodia\",\n13395682,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Flag_of_Cambodia.svg/22px-Flag_of_Cambodia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"70\",\n\"Zambia\",\n13257000,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Zambia.svg/22px-Flag_of_Zambia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"71\",\n\"Senegal\",\n12861000,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Flag_of_Senegal.svg/22px-Flag_of_Senegal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"72\",\n\"Zimbabwe\",\n12644000,\n0.0018,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Flag_of_Zimbabwe.svg/22px-Flag_of_Zimbabwe.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"73\",\n\"Chad\",\n11506000,\n0.0017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Flag_of_Chad.svg/22px-Flag_of_Chad.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"74\",\n\"Greece\",\n11306183,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Greece.svg/22px-Flag_of_Greece.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"75\",\n\"Cuba\",\n11204000,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Flag_of_Cuba.svg/22px-Flag_of_Cuba.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"76\",\n\"Belgium\",\n10827519,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Belgium_%28civil%29.svg/22px-Flag_of_Belgium_%28civil%29.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"77\",\n\"Portugal\",\n10636888,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Portugal.svg/22px-Flag_of_Portugal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"78\",\n\"Czech Republic\",\n10512397,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_Czech_Republic.svg/22px-Flag_of_the_Czech_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"79\",\n\"Tunisia\",\n10432500,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Tunisia.svg/22px-Flag_of_Tunisia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"80\",\n\"Guinea\",\n10324000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Flag_of_Guinea.svg/22px-Flag_of_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"81\",\n\"Rwanda\",\n10277000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Rwanda.svg/22px-Flag_of_Rwanda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"82\",\n\"Dominican Republic\",\n10225000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_the_Dominican_Republic.svg/22px-Flag_of_the_Dominican_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"83\",\n\"Haiti\",\n10188000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Haiti.svg/22px-Flag_of_Haiti.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"84\",\n\"Bolivia\",\n10031000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Bolivia.svg/22px-Flag_of_Bolivia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"85\",\n\"Hungary\",\n10013628,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/Flag_of_Hungary.svg/22px-Flag_of_Hungary.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"86\",\n\"Serbia\",\n9856000,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Flag_of_Serbia.svg/22px-Flag_of_Serbia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"87\",\n\"Belarus\",\n9467700,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Flag_of_Belarus.svg/22px-Flag_of_Belarus.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"88\",\n\"Sweden\",\n9393648,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Sweden.svg/22px-Flag_of_Sweden.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"89\",\n\"Somalia\",\n9359000,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Flag_of_Somalia.svg/22px-Flag_of_Somalia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"90\",\n\"Benin\",\n9212000,\n0.0013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Benin.svg/22px-Flag_of_Benin.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"91\",\n\"Azerbaijan\",\n8997400,\n0.0013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Azerbaijan.svg/22px-Flag_of_Azerbaijan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"92\",\n\"Burundi\",\n8519000,\n0.0012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Flag_of_Burundi.svg/22px-Flag_of_Burundi.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"93\",\n\"Austria\",\n8372930,\n0.0012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_Austria.svg/22px-Flag_of_Austria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"94\",\n\"Switzerland\",\n7782900,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Switzerland.svg/20px-Flag_of_Switzerland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"95\",\n\"Israel\",\n7640800,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Israel.svg/22px-Flag_of_Israel.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"96\",\n\"Honduras\",\n7616000,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Flag_of_Honduras.svg/22px-Flag_of_Honduras.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"97\",\n\"Bulgaria\",\n7576751,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Bulgaria.svg/22px-Flag_of_Bulgaria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"98\",\n\"Tajikistan\",\n7075000,\n0.00103,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Tajikistan.svg/22px-Flag_of_Tajikistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"100\",\n\"Papua New Guinea\",\n6888000,\n0.001,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Flag_of_Papua_New_Guinea.svg/22px-Flag_of_Papua_New_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"101\",\n\"Togo\",\n6780000,\n0.00099,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Flag_of_Togo.svg/22px-Flag_of_Togo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"102\",\n\"Libya\",\n6546000,\n0.00095,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Libya.svg/22px-Flag_of_Libya.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"103\",\n\"Jordan\",\n6472000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Flag_of_Jordan.svg/22px-Flag_of_Jordan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"104\",\n\"Paraguay\",\n6460000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Paraguay.svg/22px-Flag_of_Paraguay.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"105\",\n\"Laos\",\n6436000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Laos.svg/22px-Flag_of_Laos.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"106\",\n\"El Salvador\",\n6194000,\n9e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_El_Salvador.svg/22px-Flag_of_El_Salvador.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"107\",\n\"Sierra Leone\",\n5836000,\n0.00085,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Sierra_Leone.svg/22px-Flag_of_Sierra_Leone.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"108\",\n\"Nicaragua\",\n5822000,\n0.00085,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Nicaragua.svg/22px-Flag_of_Nicaragua.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"109\",\n\"Kyrgyzstan\",\n5550000,\n0.00081,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Flag_of_Kyrgyzstan.svg/22px-Flag_of_Kyrgyzstan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"110\",\n\"Denmark\",\n5543819,\n8e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Denmark.svg/22px-Flag_of_Denmark.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"111\",\n\"Slovakia\",\n5429763,\n0.00079,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Flag_of_Slovakia.svg/22px-Flag_of_Slovakia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"112\",\n\"Finland\",\n5370000,\n0.00078,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Finland.svg/22px-Flag_of_Finland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"113\",\n\"Eritrea\",\n5224000,\n0.00076,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Flag_of_Eritrea.svg/22px-Flag_of_Eritrea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"114\",\n\"Turkmenistan\",\n5177000,\n0.00075,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Turkmenistan.svg/22px-Flag_of_Turkmenistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"115\",\n\"Singapore\",\n5076700,\n0.00074,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Singapore.svg/22px-Flag_of_Singapore.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"116\",\n\"Norway\",\n4906500,\n0.00071,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Flag_of_Norway.svg/22px-Flag_of_Norway.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"117\",\n\"United Arab Emirates\",\n4707000,\n0.00068,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_United_Arab_Emirates.svg/22px-Flag_of_the_United_Arab_Emirates.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"118\",\n\"Costa Rica\",\n4640000,\n0.00068,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/Flag_of_Costa_Rica.svg/22px-Flag_of_Costa_Rica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"119\",\n\"Central African Republic\",\n4506000,\n0.00066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Central_African_Republic.svg/22px-Flag_of_the_Central_African_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"120\",\n\"Ireland\",\n4470700,\n0.00064,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Flag_of_Ireland.svg/22px-Flag_of_Ireland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"121\",\n\"Georgia\",\n4436000,\n0.00065,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Georgia.svg/22px-Flag_of_Georgia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"122\",\n\"Croatia\",\n4435056,\n0.00065,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Croatia.svg/22px-Flag_of_Croatia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"123\",\n\"New Zealand\",\n4393000,\n0.00064,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Flag_of_New_Zealand.svg/22px-Flag_of_New_Zealand.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"124\",\n\"Lebanon\",\n4255000,\n0.00062,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Flag_of_Lebanon.svg/22px-Flag_of_Lebanon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"125\",\n\"Liberia\",\n4102000,\n6e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Flag_of_Liberia.svg/22px-Flag_of_Liberia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"127\",\n\"Palestinian territories\",\n3935249,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/22px-Flag_of_Palestine.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"128\",\n\"Bosnia and Herzegovina\",\n3760000,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Flag_of_Bosnia_and_Herzegovina.svg/22px-Flag_of_Bosnia_and_Herzegovina.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"129\",\n\"Republic of the Congo\",\n3759000,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Republic_of_the_Congo.svg/22px-Flag_of_the_Republic_of_the_Congo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"130\",\n\"Moldova\",\n3563800,\n0.00052,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Moldova.svg/22px-Flag_of_Moldova.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"131\",\n\"Uruguay\",\n3372000,\n0.00049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Uruguay.svg/22px-Flag_of_Uruguay.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"132\",\n\"Mauritania\",\n3366000,\n0.00049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Flag_of_Mauritania.svg/22px-Flag_of_Mauritania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"133\",\n\"Lithuania\",\n3329227,\n0.00048,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Lithuania.svg/22px-Flag_of_Lithuania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"134\",\n\"Panama\",\n3322576,\n0.00048,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Flag_of_Panama.svg/22px-Flag_of_Panama.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"135\",\n\"Armenia\",\n3238000,\n0.00047,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Flag_of_Armenia.svg/22px-Flag_of_Armenia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"136\",\n\"Albania\",\n3195000,\n0.00046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/36/Flag_of_Albania.svg/22px-Flag_of_Albania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"137\",\n\"Kuwait\",\n3051000,\n0.00044,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Flag_of_Kuwait.svg/22px-Flag_of_Kuwait.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"138\",\n\"Oman\",\n2905000,\n0.00042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Oman.svg/22px-Flag_of_Oman.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"139\",\n\"Mongolia\",\n2776500,\n4e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Mongolia.svg/22px-Flag_of_Mongolia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"140\",\n\"Jamaica\",\n2730000,\n4e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Jamaica.svg/22px-Flag_of_Jamaica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"141\",\n\"Latvia\",\n2236300,\n0.00033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Latvia.svg/22px-Flag_of_Latvia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"142\",\n\"Namibia\",\n2212000,\n0.00032,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Namibia.svg/22px-Flag_of_Namibia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"143\",\n\"Lesotho\",\n2084000,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Flag_of_Lesotho.svg/22px-Flag_of_Lesotho.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"144\",\n\"Slovenia\",\n2065720,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Flag_of_Slovenia.svg/22px-Flag_of_Slovenia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"145\",\n\"Republic of Macedonia\",\n2048620,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Flag_of_Macedonia.svg/22px-Flag_of_Macedonia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"146\",\n\"Botswana\",\n1978000,\n0.00029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_Botswana.svg/22px-Flag_of_Botswana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"147\",\n\"Gambia\",\n1751000,\n0.00025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_The_Gambia.svg/22px-Flag_of_The_Gambia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"148\",\n\"Qatar\",\n1696563,\n0.00025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Flag_of_Qatar.svg/22px-Flag_of_Qatar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"149\",\n\"Guinea-Bissau\",\n1647000,\n0.00024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Guinea-Bissau.svg/22px-Flag_of_Guinea-Bissau.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"150\",\n\"Gabon\",\n1501000,\n0.00022,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Flag_of_Gabon.svg/22px-Flag_of_Gabon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"151\",\n\"Trinidad and Tobago\",\n1344000,\n2e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Trinidad_and_Tobago.svg/22px-Flag_of_Trinidad_and_Tobago.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"152\",\n\"Estonia\",\n1340127,\n0.00019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Flag_of_Estonia.svg/22px-Flag_of_Estonia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"153\",\n\"Mauritius\",\n1297000,\n0.00019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Mauritius.svg/22px-Flag_of_Mauritius.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"154\",\n\"Swaziland\",\n1202000,\n0.00017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Flag_of_Swaziland.svg/22px-Flag_of_Swaziland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"155\",\n\"East Timor\",\n1171000,\n0.00017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Flag_of_East_Timor.svg/22px-Flag_of_East_Timor.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"156\",\n\"Djibouti\",\n879000,\n0.00013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_Djibouti.svg/22px-Flag_of_Djibouti.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"157\",\n\"Fiji\",\n854000,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Fiji.svg/22px-Flag_of_Fiji.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"158\",\n\"Bahrain\",\n807000,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Bahrain.svg/22px-Flag_of_Bahrain.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"159\",\n\"Cyprus\",\n801851,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Cyprus.svg/22px-Flag_of_Cyprus.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"160\",\n\"Guyana\",\n761000,\n0.00011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_Guyana.svg/22px-Flag_of_Guyana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"161\",\n\"Bhutan\",\n708000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Flag_of_Bhutan.svg/22px-Flag_of_Bhutan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"162\",\n\"Equatorial Guinea\",\n693000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Equatorial_Guinea.svg/22px-Flag_of_Equatorial_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"163\",\n\"Comoros\",\n691000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Flag_of_the_Comoros.svg/22px-Flag_of_the_Comoros.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"164\",\n\"Montenegro\",\n626000,\n9e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Montenegro.svg/22px-Flag_of_Montenegro.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"166\",\n\"Solomon Islands\",\n536000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Flag_of_the_Solomon_Islands.svg/22px-Flag_of_the_Solomon_Islands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"167\",\n\"Western Sahara\",\n530000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Flag_of_Western_Sahara.svg/22px-Flag_of_Western_Sahara.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"168\",\n\"Suriname\",\n524000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Flag_of_Suriname.svg/22px-Flag_of_Suriname.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"169\",\n\"Cape Verde\",\n513000,\n7e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Cape_Verde.svg/22px-Flag_of_Cape_Verde.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"170\",\n\"Luxembourg\",\n502207,\n7e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Flag_of_Luxembourg.svg/22px-Flag_of_Luxembourg.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"171\",\n\"Malta\",\n416333,\n6e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Malta.svg/22px-Flag_of_Malta.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"172\",\n\"Brunei\",\n407000,\n6e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Brunei.svg/22px-Flag_of_Brunei.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"173\",\n\"Bahamas\",\n346000,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Flag_of_the_Bahamas.svg/22px-Flag_of_the_Bahamas.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"174\",\n\"Belize\",\n322100,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Flag_of_Belize.svg/22px-Flag_of_Belize.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"175\",\n\"Iceland\",\n318006,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Iceland.svg/22px-Flag_of_Iceland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"176\",\n\"Maldives\",\n314000,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Maldives.svg/22px-Flag_of_Maldives.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"177\",\n\"Barbados\",\n257000,\n4e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Flag_of_Barbados.svg/22px-Flag_of_Barbados.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"178\",\n\"Vanuatu\",\n246000,\n4e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Vanuatu.svg/22px-Flag_of_Vanuatu.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"181\",\n\"Samoa\",\n179000,\n3e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Samoa.svg/22px-Flag_of_Samoa.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"182\",\n\"Saint Lucia\",\n174000,\n3e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Saint_Lucia.svg/22px-Flag_of_Saint_Lucia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"183\",\n\"Sao Tome and Principe\",\n165000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Sao_Tome_and_Principe.svg/22px-Flag_of_Sao_Tome_and_Principe.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"184\",\n\"Federated States of Micronesia\",\n111000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Federated_States_of_Micronesia.svg/22px-Flag_of_Federated_States_of_Micronesia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"186\",\n\"Saint Vincent and the Grenadines\",\n109000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Flag_of_Saint_Vincent_and_the_Grenadines.svg/22px-Flag_of_Saint_Vincent_and_the_Grenadines.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"188\",\n\"Grenada\",\n104000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Grenada.svg/22px-Flag_of_Grenada.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"189\",\n\"Tonga\",\n104000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Tonga.svg/22px-Flag_of_Tonga.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"190\",\n\"Kiribati\",\n100000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kiribati.svg/22px-Flag_of_Kiribati.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"192\",\n\"Antigua and Barbuda\",\n89000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Antigua_and_Barbuda.svg/22px-Flag_of_Antigua_and_Barbuda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"194\",\n\"Seychelles\",\n85000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Seychelles.svg/22px-Flag_of_the_Seychelles.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"195\",\n\"Andorra\",\n84082,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Andorra.svg/22px-Flag_of_Andorra.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"198\",\n\"Dominica\",\n67000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Flag_of_Dominica.svg/22px-Flag_of_Dominica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"200\",\n\"Marshall Islands\",\n63000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Flag_of_the_Marshall_Islands.svg/22px-Flag_of_the_Marshall_Islands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"204\",\n\"Saint Kitts and Nevis\",\n52000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Saint_Kitts_and_Nevis.svg/22px-Flag_of_Saint_Kitts_and_Nevis.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"206\",\n\"Liechtenstein\",\n35904,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Flag_of_Liechtenstein.svg/22px-Flag_of_Liechtenstein.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"207\",\n\"Monaco\",\n33000,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/22px-Flag_of_Monaco.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"209\",\n\"San Marino\",\n31794,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Flag_of_San_Marino.svg/22px-Flag_of_San_Marino.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"213\",\n\"Palau\",\n20000,\n3e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Palau.svg/22px-Flag_of_Palau.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"215\",\n\"Tuvalu\",\n10000,\n1e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tuvalu.svg/22px-Flag_of_Tuvalu.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"216\",\n\"Nauru\",\n10000,\n1e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Flag_of_Nauru.svg/22px-Flag_of_Nauru.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"222\",\n\"Vatican City\",\n800,\n2e-07,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_the_Vatican_City.svg/20px-Flag_of_the_Vatican_City.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n] \n];\ndata.addColumn('string','Rank');\ndata.addColumn('string','Country');\ndata.addColumn('number','Population');\ndata.addColumn('number','% of World Population');\ndata.addColumn('string','Flag');\ndata.addColumn('boolean','Mode');\ndata.addColumn('date','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartTableID2cd013050362() {\nvar data = gvisDataTableID2cd013050362();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"page\"] = \"enable\";\n\n  var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"});\n  dataFormat1.format(data, 2);\n  var dataFormat2 = new google.visualization.NumberFormat({pattern:\"#.#%\"});\n  dataFormat2.format(data, 3);\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID2cd013050362')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID2cd013050362);\n})();\nfunction displayChartTableID2cd013050362() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Tables with gvisTable"
        },
        {
            "location": "/GoogleVis/#dashboards-with-gvismerge",
            "text": "G <- gvisGeoChart(Exports,\n                  locationvar=\"Country\",\n                  colorvar=\"Profit\",\n                  options=list(width=300, height=200))\n\nT <- gvisTable(Exports,\n               options=list(width=300, height=370))  GT <- gvisMerge(G, T, horizontal=FALSE)\n\nplot(GT)   \n\n// jsData \nfunction gvisDataGeoChartID49611ffdda49 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n\n// jsData \nfunction gvisDataTableID496155ab963a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3,\ntrue\n],\n[\n\"Brazil\",\n4,\nfalse\n],\n[\n\"United States\",\n5,\ntrue\n],\n[\n\"France\",\n4,\ntrue\n],\n[\n\"Hungary\",\n3,\nfalse\n],\n[\n\"India\",\n2,\ntrue\n],\n[\n\"Iceland\",\n1,\nfalse\n],\n[\n\"Norway\",\n4,\ntrue\n],\n[\n\"Spain\",\n5,\ntrue\n],\n[\n\"Turkey\",\n1,\nfalse\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addColumn('boolean','Online');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49611ffdda49() {\nvar data = gvisDataGeoChartID49611ffdda49();\nvar options = {};\noptions[\"width\"] = 300;\noptions[\"height\"] = 200;\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49611ffdda49')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n\n// jsDrawChart\nfunction drawChartTableID496155ab963a() {\nvar data = gvisDataTableID496155ab963a();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"width\"] = 300;\noptions[\"height\"] = 370;\n\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID496155ab963a')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49611ffdda49);\n})();\nfunction displayChartGeoChartID49611ffdda49() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID496155ab963a);\n})();\nfunction displayChartTableID496155ab963a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter                       G <- gvisGeoChart(Exports,\n                  locationvar=\"Country\",\n                  colorvar=\"Profit\",\n                  options=list(width=300, height=370))  GT <- gvisMerge(G, T, horizontal=TRUE)\n\nplot(GT)   \n\n// jsData \nfunction gvisDataGeoChartID114f1fa5c040 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n\n// jsData \nfunction gvisDataTableID114f79fb112f () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3,\ntrue\n],\n[\n\"Brazil\",\n4,\nfalse\n],\n[\n\"United States\",\n5,\ntrue\n],\n[\n\"France\",\n4,\ntrue\n],\n[\n\"Hungary\",\n3,\nfalse\n],\n[\n\"India\",\n2,\ntrue\n],\n[\n\"Iceland\",\n1,\nfalse\n],\n[\n\"Norway\",\n4,\ntrue\n],\n[\n\"Spain\",\n5,\ntrue\n],\n[\n\"Turkey\",\n1,\nfalse\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addColumn('boolean','Online');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID114f1fa5c040() {\nvar data = gvisDataGeoChartID114f1fa5c040();\nvar options = {};\noptions[\"width\"] = 300;\noptions[\"height\"] = 370;\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID114f1fa5c040')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n\n// jsDrawChart\nfunction drawChartTableID114f79fb112f() {\nvar data = gvisDataTableID114f79fb112f();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"width\"] = 200;\noptions[\"height\"] = 270;\n\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID114f79fb112f')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID114f1fa5c040);\n})();\nfunction displayChartGeoChartID114f1fa5c040() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID114f79fb112f);\n})();\nfunction displayChartTableID114f79fb112f() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Dashboards with gvisMerge"
        },
        {
            "location": "/GoogleVis/#options",
            "text": "df <- data.frame(country=c(\"US\", \"GB\", \"BR\"),\n                 val1=c(1,3,4),\n                 val2=c(23,12,32))\n\nLine <- gvisLineChart(df,\n                      xvar=\"country\",\n                      yvar=c(\"val1\",\"val2\"),\n                      options=list(\n                        title=\"Hello World\",\n                        titleTextStyle=\"{color:'red',\n                                         fontName:'Courier',\n                                         fontSize:16}\",\n                        backgroundColor=\"#D3D3D3\",\n                        vAxis=\"{gridlines:{color:'red', count:3}}\",\n                        hAxis=\"{title:'Country',\n                                titleTextStyle:{color:'blue'}}\",\n                        series=\"[{color:'green',\n                                  targetAxisIndex: 0},\n                                 {color: 'orange',targetAxisIndex:1}]\",\n                        vAxes=\"[{title:'val1'}, {title:'val2'}]\",\n                        legend=\"bottom\",\n                        curveType=\"function\",\n                        width=500,\n                        height=300))   \n\n// jsData \nfunction gvisDataLineChartID584f5d6f811a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"US\",\n1,\n23\n],\n[\n\"GB\",\n3,\n12\n],\n[\n\"BR\",\n4,\n32\n] \n];\ndata.addColumn('string','country');\ndata.addColumn('number','val1');\ndata.addColumn('number','val2');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartLineChartID584f5d6f811a() {\nvar data = gvisDataLineChartID584f5d6f811a();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"title\"] = \"Hello World\";\noptions[\"titleTextStyle\"] = {color:'red',\n                                         fontName:'Courier',\n                                         fontSize:16};\noptions[\"backgroundColor\"] = \"#D3D3D3\";\noptions[\"vAxis\"] = {gridlines:{color:'red', count:3}};\noptions[\"hAxis\"] = {title:'Country',\n                                titleTextStyle:{color:'blue'}};\noptions[\"series\"] = [{color:'green',\n                                  targetAxisIndex: 0},\n                                 {color: 'orange',targetAxisIndex:1}];\noptions[\"vAxes\"] = [{title:'val1'}, {title:'val2'}];\noptions[\"legend\"] = \"bottom\";\noptions[\"curveType\"] = \"function\";\noptions[\"width\"] = 500;\noptions[\"height\"] = 300;\n\n\n    var chart = new google.visualization.LineChart(\n    document.getElementById('LineChartID584f5d6f811a')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"corechart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartLineChartID584f5d6f811a);\n})();\nfunction displayChartLineChartID584f5d6f811a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Options"
        },
        {
            "location": "/GoogleVis/#apostrophes",
            "text": "df <- data.frame(\"Year\"=c(2009,2010),\n                 \"Lloyd\\\\'s\"=c(86.1, 93.3),\n                 \"Munich Re\\\\'s R/I\"=c(95.3, 100.5), check.names=FALSE)\n\nR <- gvisColumnChart(df, \n                     options=list(vAxis='{baseline:0}',\n                                  title=\"Combined Ratio %\",\n                                  legend=\"{position:'bottom'}\"))\n\ncat(R$html$chart, file = \"GoogleVis/R.html\") # save   \n\n// jsData \nfunction gvisDataColumnChartID336e54fe1b42 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n2009,\n86.1,\n95.3\n],\n[\n2010,\n93.3,\n100.5\n] \n];\ndata.addColumn('number','Year');\ndata.addColumn('number','Lloyd\\'s');\ndata.addColumn('number','Munich Re\\'s R/I');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartColumnChartID336e54fe1b42() {\nvar data = gvisDataColumnChartID336e54fe1b42();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"vAxis\"] = {baseline:0};\noptions[\"title\"] = \"Combined Ratio %\";\noptions[\"legend\"] = {position:'bottom'};\n\n\n    var chart = new google.visualization.ColumnChart(\n    document.getElementById('ColumnChartID336e54fe1b42')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"corechart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartColumnChartID336e54fe1b42);\n})();\nfunction displayChartColumnChartID336e54fe1b42() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Apostrophes"
        },
        {
            "location": "/Tables/",
            "text": "Markdown tables\n\n\nExample 1\n\n\nExample 2\n\n\n\n\n\n\nThe \nxtable\n package\n\n\nExample 1\n\n\n\n\n\n\nThe \nknitr::kable\n function\n\n\nExample 1\n\n\n\n\n\n\nThe \npander::pandoc.table\n\n    function\n\n\nExample 1\n\n\n\n\n\n\nThe \nhtmlTable\n package\n\n\nExample 1\n\n\nExample 2\n\n\nExample 3\n\n\nExample 4\n\n\nExample 5\n\n\nExample 6\n\n\nExample 7\n\n\n\n\n\n\nThe \nztable\n package\n\n\nExample 1\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: \u2018pygments\u2019 syntax, the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nMarkdown tables\n\u00b6\n\n\nExample 1\n\u00b6\n\n\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\n: Demonstration of pipe table syntax.\n\n\n\n\n\nDemonstration of pipe table syntax.\n\n\n\n\n\n\nRight\n\n\nLeft\n\n\nDefault\n\n\nCenter\n\n\n\n\n\n\n\n\n\n\n12\n\n\n12\n\n\n12\n\n\n12\n\n\n\n\n\n\n123\n\n\n123\n\n\n123\n\n\n123\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\nExample 2\n\u00b6\n\n\n: Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+\n\n\n\n\n\nSample grid table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFruit\n\n\nPrice\n\n\nAdvantages\n\n\n\n\n\n\n\n\n\n\nBananas\n\n\n$1.34\n\n\n\n\nbuilt-in wrapper\n\n\nbright color\n\n\n\n\n\n\n\n\nOranges\n\n\n$2.10\n\n\n\n\ncures scurvy\n\n\ntasty\n\n\n\n\n\n\n\n\n\n\n\nThe \nxtable\n package\n\u00b6\n\n\nExample 1\n\u00b6\n\n\nlibrary(xtable)\n\n# given the data in the first row\nprint(xtable(output,\n             caption = 'A test table', \n             align = c('l', 'c', 'r')),\n      type = 'html')\n\n\n\n\nThe \nknitr::kable\n function\n\u00b6\n\n\nExample 1\n\u00b6\n\n\nlibrary(knitr)\n\n# given the data in the first row\nkable(output, \n      caption = 'A test table', \n      align = c('c', 'r'))\n\n\n\n\n\n\nA test table\n\n\n\n\n\n\n\n\n1st header\n\n\n2nd header\n\n\n\n\n\n\n\n\n\n\n1st row\n\n\nContent A\n\n\nContent B\n\n\n\n\n\n\n2nd row\n\n\nContent C\n\n\nContent D\n\n\n\n\n\n\n\n\n\nThe \npander::pandoc.table\n function\n\u00b6\n\n\nExample 1\n\u00b6\n\n\nlibrary(pander)\n\n# given the data in the first row\npandoc.table(output, \n             emphasize.rows = 1, \n             emphasize.strong.cols = 2)\n\n\n\n\nThe \nhtmlTable\n package\n\u00b6\n\n\nhtmlTable\n on GitHub.\n\n\nExample 1\n\u00b6\n\n\noutput <- \n  matrix(paste('Content', LETTERS[1:16]), \n         ncol = 4, byrow = TRUE)\n\nlibrary(htmlTable)\n\nhtmlTable(output,\n          header = paste(c('1st', '2nd', '3rd', '4th'), 'header'),\n          rnames = paste(c('1st', '2nd', '3rd', '4th'), 'row'),\n          rgroup = c('Group A', 'Group B'),\n          n.rgroup = c(2,2),\n          cgroup = c('Cgroup 1', 'Cgroup 2&dagger;'),\n          n.cgroup = c(2,2), \n          caption = 'Basic table with both column spanners (groups) and row groups',\n          tfoot = '&dagger; A table footer commment')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic table with both column spanners (groups) and row groups\n\n\n\n\n\n\n\n\n\n\n\n\nCgroup 1\n\n\n\n\n\u00a0\n\n\n\n\nCgroup 2\u2020\n\n\n\n\n\n\n\n\n\n\n\n\n1st header\n\n\n\n\n2nd header\n\n\n\n\n\u00a0\n\n\n\n\n3rd header\n\n\n\n\n4th header\n\n\n\n\n\n\n\n\n\n\n\n\nGroup A\n\n\n\n\n\n\n\n\n\u00a0\u00a01st row\n\n\n\n\nContent A\n\n\n\n\nContent B\n\n\n\n\n\u00a0\n\n\n\n\nContent C\n\n\n\n\nContent D\n\n\n\n\n\n\n\n\n\u00a0\u00a02nd row\n\n\n\n\nContent E\n\n\n\n\nContent F\n\n\n\n\n\u00a0\n\n\n\n\nContent G\n\n\n\n\nContent H\n\n\n\n\n\n\n\n\nGroup B\n\n\n\n\n\n\n\n\n\u00a0\u00a03rd row\n\n\n\n\nContent I\n\n\n\n\nContent J\n\n\n\n\n\u00a0\n\n\n\n\nContent K\n\n\n\n\nContent L\n\n\n\n\n\n\n\n\n\u00a0\u00a04th row\n\n\n\n\nContent M\n\n\n\n\nContent N\n\n\n\n\n\u00a0\n\n\n\n\nContent O\n\n\n\n\nContent P\n\n\n\n\n\n\n\n\n\n\n\n\n\u2020 A table footer commment\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\u00b6\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst period\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\nSecond period\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\nThird period\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3\n\u00b6\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst period\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\nSecond period\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\nThird period\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 4\n\u00b6\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          col.columns = c(rep('#E6E6F0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n                    tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                          '&Delta;<sub>std</sub> corresponds to the change compared to national average'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst period\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\nSecond period\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\nThird period\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5\n\u00b6\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1),\n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          # I use the &nbsp; - the no breaking space as I don't want to have a\n          # row break in the row group. This adds a little space in the table\n          # when used together with the cspan.rgroup=1.\n          rgroup = c('1st&nbsp;period', '2nd&nbsp;period', '3rd&nbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'),\n          cspan.rgroup = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\n1st\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n2nd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n3rd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6\n\u00b6\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(out_mx,\n          caption = 'Average age in Sweden counties over a period of\n                     15 years. The Norbotten county is typically known\n                     for having a negative migration pattern compared to\n                     Stockholm, while Uppsala has a proportionally large \n                     population of students.',\n          pos.rowlabel = 'bottom',\n          rowlabel='Year', \n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('1st&nbsp;period', '2nd&nbsp;period', '3rd&nbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'),\n          cspan.rgroup = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\nYear\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\n1st\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n\n0.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n\n0.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n\n1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n2nd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n\n1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n\n-1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n\n1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n\n1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n3rd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n\n2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n\n2.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n\n2.5\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n\n-1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7\n\u00b6\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(out_mx,\n          caption = 'Average age in Sweden counties over a period of\n                     15 years. The Norbotten county is typically known\n                     for having a negative migration pattern compared to\n                     Stockholm, while Uppsala has a proportionally large \n                     population of students.',\n          pos.rowlabel = 'bottom',\n          rowlabel = 'Year', \n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4), rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('1st&nbsp;period', '2nd&nbsp;period', '3rd&nbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'),\n          cspan.rgroup = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\nYear\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\n1st\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n\n0.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n\n0.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n\n1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n2nd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n\n1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n\n-1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n\n1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n\n1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n3rd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n\n2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n\n2.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n\n2.5\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n\n-1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \nztable\n package\n\u00b6\n\n\nThe package can also export to \nL\na\nT\ne\nX\n.\n\n\nExample 1\n\u00b6\n\n\nlibrary(ztable)\n\noptions(ztable.type='html')\n\n# given the data in the first row\nzt <- ztable(out_mx, \n             caption = 'Average age in Sweden counties over a period of\n             15 years. The Norbotten county is typically known\n             for having a negative migration pattern compared to\n             Stockholm, while Uppsala has a proportionally large \n             population of students.',\n             zebra.type = 1,\n             zebra = 'peach',\n             align=paste(rep('r', ncol(out_mx) + 1), collapse = ''))\n# zt <- addcgroup(zt,\n#                 cgroup = cgroup,\n#                 n.cgroup = n.cgroup)\n# Causes an error:\n# Error in if (result <= length(vlines)) { : \nzt <- addrgroup(zt, \n                rgroup = c('1st&nbsp;period', '2nd&nbsp;period', '3rd&nbsp;period'),\n                n.rgroup = rep(5, 3))\n\nprint(zt)",
            "title": "Tables"
        },
        {
            "location": "/Tables/#example-1",
            "text": "| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\n: Demonstration of pipe table syntax.   Demonstration of pipe table syntax.    Right  Left  Default  Center      12  12  12  12    123  123  123  123    1  1  1  1",
            "title": "Example 1"
        },
        {
            "location": "/Tables/#example-2",
            "text": ": Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+   Sample grid table.         Fruit  Price  Advantages      Bananas  $1.34   built-in wrapper  bright color     Oranges  $2.10   cures scurvy  tasty",
            "title": "Example 2"
        },
        {
            "location": "/Tables/#the-xtable-package",
            "text": "",
            "title": "The xtable package"
        },
        {
            "location": "/Tables/#example-1_1",
            "text": "library(xtable)\n\n# given the data in the first row\nprint(xtable(output,\n             caption = 'A test table', \n             align = c('l', 'c', 'r')),\n      type = 'html')",
            "title": "Example 1"
        },
        {
            "location": "/Tables/#the-knitrkable-function",
            "text": "",
            "title": "The knitr::kable function"
        },
        {
            "location": "/Tables/#example-1_2",
            "text": "library(knitr)\n\n# given the data in the first row\nkable(output, \n      caption = 'A test table', \n      align = c('c', 'r'))   A test table     1st header  2nd header      1st row  Content A  Content B    2nd row  Content C  Content D",
            "title": "Example 1"
        },
        {
            "location": "/Tables/#the-panderpandoctable-function",
            "text": "",
            "title": "The pander::pandoc.table function"
        },
        {
            "location": "/Tables/#example-1_3",
            "text": "library(pander)\n\n# given the data in the first row\npandoc.table(output, \n             emphasize.rows = 1, \n             emphasize.strong.cols = 2)",
            "title": "Example 1"
        },
        {
            "location": "/Tables/#the-htmltable-package",
            "text": "htmlTable  on GitHub.",
            "title": "The htmlTable package"
        },
        {
            "location": "/Tables/#example-1_4",
            "text": "output <- \n  matrix(paste('Content', LETTERS[1:16]), \n         ncol = 4, byrow = TRUE)\n\nlibrary(htmlTable)\n\nhtmlTable(output,\n          header = paste(c('1st', '2nd', '3rd', '4th'), 'header'),\n          rnames = paste(c('1st', '2nd', '3rd', '4th'), 'row'),\n          rgroup = c('Group A', 'Group B'),\n          n.rgroup = c(2,2),\n          cgroup = c('Cgroup 1', 'Cgroup 2&dagger;'),\n          n.cgroup = c(2,2), \n          caption = 'Basic table with both column spanners (groups) and row groups',\n          tfoot = '&dagger; A table footer commment')      \nBasic table with both column spanners (groups) and row groups      \nCgroup 1  \n\u00a0  \nCgroup 2\u2020      \n1st header  \n2nd header  \n\u00a0  \n3rd header  \n4th header      \nGroup A    \n\u00a0\u00a01st row  \nContent A  \nContent B  \n\u00a0  \nContent C  \nContent D    \n\u00a0\u00a02nd row  \nContent E  \nContent F  \n\u00a0  \nContent G  \nContent H    \nGroup B    \n\u00a0\u00a03rd row  \nContent I  \nContent J  \n\u00a0  \nContent K  \nContent L    \n\u00a0\u00a04th row  \nContent M  \nContent N  \n\u00a0  \nContent O  \nContent P      \n\u2020 A table footer commment",
            "title": "Example 1"
        },
        {
            "location": "/Tables/#example-2_1",
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'))        \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \nFirst period    \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \nSecond period    \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \nThird period    \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 2"
        },
        {
            "location": "/Tables/#example-3",
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'))        \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \nFirst period    \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \nSecond period    \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \nThird period    \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 3"
        },
        {
            "location": "/Tables/#example-4",
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          col.columns = c(rep('#E6E6F0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n                    tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                          '&Delta;<sub>std</sub> corresponds to the change compared to national average'))        \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \nFirst period    \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \nSecond period    \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \nThird period    \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 4"
        },
        {
            "location": "/Tables/#example-5",
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1),\n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          # I use the &nbsp; - the no breaking space as I don't want to have a\n          # row break in the row group. This adds a little space in the table\n          # when used together with the cspan.rgroup=1.\n          rgroup = c('1st&nbsp;period', '2nd&nbsp;period', '3rd&nbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'),\n          cspan.rgroup = 1)        \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \n1st\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \n2nd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n3rd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 5"
        },
        {
            "location": "/Tables/#example-6",
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(out_mx,\n          caption = 'Average age in Sweden counties over a period of\n                     15 years. The Norbotten county is typically known\n                     for having a negative migration pattern compared to\n                     Stockholm, while Uppsala has a proportionally large \n                     population of students.',\n          pos.rowlabel = 'bottom',\n          rowlabel='Year', \n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('1st&nbsp;period', '2nd&nbsp;period', '3rd&nbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'),\n          cspan.rgroup = 1)      \nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.      \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen    \nYear  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \n1st\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0   0.8   \n\u00a0  \n41.9  \n0.0   0.4   \n\u00a0  \n37.3  \n0.0   -1.6   \n\u00a0  \n40.1  \n0.0   -1.4   \n\u00a0  \n37.2  \n0.0   -1.7   \n\u00a0  \n39.3  \n0.0   -2.2     \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3   1.0   \n\u00a0  \n42.2  \n0.3   0.6   \n\u00a0  \n37.4  \n0.1   -1.6   \n\u00a0  \n40.1  \n0.0   -1.5   \n\u00a0  \n37.5  \n0.3   -1.5   \n\u00a0  \n39.4  \n0.1   -2.2     \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5   1.0   \n\u00a0  \n42.5  \n0.6   0.8   \n\u00a0  \n37.5  \n0.2   -1.7   \n\u00a0  \n40.1  \n0.0   -1.6   \n\u00a0  \n37.6  \n0.4   -1.6   \n\u00a0  \n39.6  \n0.3   -2.1     \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8   1.2   \n\u00a0  \n42.8  \n0.9   1.0   \n\u00a0  \n37.6  \n0.3   -1.7   \n\u00a0  \n40.2  \n0.1   -1.6   \n\u00a0  \n37.8  \n0.6   -1.5   \n\u00a0  \n39.7  \n0.4   -2.1     \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0   1.3   \n\u00a0  \n43.0  \n1.1   1.1   \n\u00a0  \n37.7  \n0.4   -1.7   \n\u00a0  \n40.2  \n0.1   -1.7   \n\u00a0  \n38.0  \n0.8   -1.4   \n\u00a0  \n39.8  \n0.5   -2.1     \n2nd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2   1.3   \n\u00a0  \n43.1  \n1.2   1.1   \n\u00a0  \n37.8  \n0.5   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.1  \n0.9   -1.5   \n\u00a0  \n40.0  \n0.7   -2.0     \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4   1.4   \n\u00a0  \n43.4  \n1.5   1.4   \n\u00a0  \n37.9  \n0.6   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.3  \n1.1   -1.4   \n\u00a0  \n40.1  \n0.8   -1.9     \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6   1.5   \n\u00a0  \n43.5  \n1.6   1.4   \n\u00a0  \n37.9  \n0.6   -1.9   \n\u00a0  \n40.2  \n0.1   -1.9   \n\u00a0  \n38.5  \n1.3   -1.3   \n\u00a0  \n40.4  \n1.1   -1.7     \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8   1.7   \n\u00a0  \n43.8  \n1.9   1.7   \n\u00a0  \n37.8  \n0.5   -2.0   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.6  \n1.4   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0   1.8   \n\u00a0  \n44.0  \n2.1   1.9   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.7  \n1.5   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n3rd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2   2.0   \n\u00a0  \n44.2  \n2.3   2.1   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.8  \n1.6   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4   2.1   \n\u00a0  \n44.4  \n2.5   2.3   \n\u00a0  \n37.8  \n0.5   -2.2   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.9  \n1.7   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6   2.2   \n\u00a0  \n44.5  \n2.6   2.3   \n\u00a0  \n37.9  \n0.6   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.0  \n1.8   -1.1   \n\u00a0  \n40.7  \n1.4   -1.5     \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.6  \n2.7   2.4   \n\u00a0  \n37.9  \n0.6   -2.3   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.1  \n1.9   -1.1   \n\u00a0  \n40.8  \n1.5   -1.4     \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.7  \n2.8   2.5   \n\u00a0  \n38.0  \n0.7   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.2  \n2.0   -1.0   \n\u00a0  \n40.9  \n1.6   -1.3       \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 6"
        },
        {
            "location": "/Tables/#example-7",
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(out_mx,\n          caption = 'Average age in Sweden counties over a period of\n                     15 years. The Norbotten county is typically known\n                     for having a negative migration pattern compared to\n                     Stockholm, while Uppsala has a proportionally large \n                     population of students.',\n          pos.rowlabel = 'bottom',\n          rowlabel = 'Year', \n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4), rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('1st&nbsp;period', '2nd&nbsp;period', '3rd&nbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('&Delta;<sub>int</sub> correspnds to the change since start',\n                                '&Delta;<sub>std</sub> corresponds to the change compared to national average'),\n          cspan.rgroup = 1)      \nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.      \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen    \nYear  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \n1st\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0   0.8   \n\u00a0  \n41.9  \n0.0   0.4   \n\u00a0  \n37.3  \n0.0   -1.6   \n\u00a0  \n40.1  \n0.0   -1.4   \n\u00a0  \n37.2  \n0.0   -1.7   \n\u00a0  \n39.3  \n0.0   -2.2     \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3   1.0   \n\u00a0  \n42.2  \n0.3   0.6   \n\u00a0  \n37.4  \n0.1   -1.6   \n\u00a0  \n40.1  \n0.0   -1.5   \n\u00a0  \n37.5  \n0.3   -1.5   \n\u00a0  \n39.4  \n0.1   -2.2     \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5   1.0   \n\u00a0  \n42.5  \n0.6   0.8   \n\u00a0  \n37.5  \n0.2   -1.7   \n\u00a0  \n40.1  \n0.0   -1.6   \n\u00a0  \n37.6  \n0.4   -1.6   \n\u00a0  \n39.6  \n0.3   -2.1     \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8   1.2   \n\u00a0  \n42.8  \n0.9   1.0   \n\u00a0  \n37.6  \n0.3   -1.7   \n\u00a0  \n40.2  \n0.1   -1.6   \n\u00a0  \n37.8  \n0.6   -1.5   \n\u00a0  \n39.7  \n0.4   -2.1     \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0   1.3   \n\u00a0  \n43.0  \n1.1   1.1   \n\u00a0  \n37.7  \n0.4   -1.7   \n\u00a0  \n40.2  \n0.1   -1.7   \n\u00a0  \n38.0  \n0.8   -1.4   \n\u00a0  \n39.8  \n0.5   -2.1     \n2nd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2   1.3   \n\u00a0  \n43.1  \n1.2   1.1   \n\u00a0  \n37.8  \n0.5   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.1  \n0.9   -1.5   \n\u00a0  \n40.0  \n0.7   -2.0     \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4   1.4   \n\u00a0  \n43.4  \n1.5   1.4   \n\u00a0  \n37.9  \n0.6   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.3  \n1.1   -1.4   \n\u00a0  \n40.1  \n0.8   -1.9     \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6   1.5   \n\u00a0  \n43.5  \n1.6   1.4   \n\u00a0  \n37.9  \n0.6   -1.9   \n\u00a0  \n40.2  \n0.1   -1.9   \n\u00a0  \n38.5  \n1.3   -1.3   \n\u00a0  \n40.4  \n1.1   -1.7     \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8   1.7   \n\u00a0  \n43.8  \n1.9   1.7   \n\u00a0  \n37.8  \n0.5   -2.0   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.6  \n1.4   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0   1.8   \n\u00a0  \n44.0  \n2.1   1.9   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.7  \n1.5   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n3rd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2   2.0   \n\u00a0  \n44.2  \n2.3   2.1   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.8  \n1.6   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4   2.1   \n\u00a0  \n44.4  \n2.5   2.3   \n\u00a0  \n37.8  \n0.5   -2.2   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.9  \n1.7   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6   2.2   \n\u00a0  \n44.5  \n2.6   2.3   \n\u00a0  \n37.9  \n0.6   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.0  \n1.8   -1.1   \n\u00a0  \n40.7  \n1.4   -1.5     \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.6  \n2.7   2.4   \n\u00a0  \n37.9  \n0.6   -2.3   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.1  \n1.9   -1.1   \n\u00a0  \n40.8  \n1.5   -1.4     \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.7  \n2.8   2.5   \n\u00a0  \n38.0  \n0.7   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.2  \n2.0   -1.0   \n\u00a0  \n40.9  \n1.6   -1.3       \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 7"
        },
        {
            "location": "/Tables/#the-ztable-package",
            "text": "The package can also export to  L a T e X .",
            "title": "The ztable package"
        },
        {
            "location": "/Tables/#example-1_5",
            "text": "library(ztable)\n\noptions(ztable.type='html')\n\n# given the data in the first row\nzt <- ztable(out_mx, \n             caption = 'Average age in Sweden counties over a period of\n             15 years. The Norbotten county is typically known\n             for having a negative migration pattern compared to\n             Stockholm, while Uppsala has a proportionally large \n             population of students.',\n             zebra.type = 1,\n             zebra = 'peach',\n             align=paste(rep('r', ncol(out_mx) + 1), collapse = ''))\n# zt <- addcgroup(zt,\n#                 cgroup = cgroup,\n#                 n.cgroup = n.cgroup)\n# Causes an error:\n# Error in if (result <= length(vlines)) { : \nzt <- addrgroup(zt, \n                rgroup = c('1st&nbsp;period', '2nd&nbsp;period', '3rd&nbsp;period'),\n                n.rgroup = rep(5, 3))\n\nprint(zt)",
            "title": "Example 1"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/",
            "text": "1, Variables\n\n\n2, Histograms and Distributions\n\n\n3, Scales of Measurement\n\n\n4, Measures of Central Tendency\n\n\n5, Measures of Variability\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, Variables\n\u00b6\n\n\nNominal variables in R\n\n\n# Create a numeric vector with the identifiers of the participants of your survey\nparticipants_1 <- c(2, 3, 5, 7, 11, 13, 17)\n\n# Check what type of values R thinks the vector consists of\nclass(participants_1)\n\n\n\n\n## [1] \"numeric\"\n\n\n\n# Transform the numeric vector to a factor vector\nparticipants_2 <- factor(participants_1)\n\n# Check what type of values R thinks the vector consists of now\nclass(participants_2)\n\n\n\n\n## [1] \"factor\"\n\n\n\nOrdinal variables in R\n\n\n# Create a vector of temperature observations\ntemperature_vector <- c('High', 'Low', 'High', 'Low', 'Medium')\ntemperature_vector\n\n\n\n\n## [1] \"High\"   \"Low\"    \"High\"   \"Low\"    \"Medium\"\n\n\n\n# Specify that they are ordinal variables with the given levels\nfactor_temperature_vector <- factor(temperature_vector, order = TRUE, levels = c('Low', 'Medium', 'High'))\nfactor_temperature_vector\n\n\n\n\n## [1] High   Low    High   Low    Medium\n## Levels: Low < Medium < High\n\n\n\nInterval and Ratio variables in R\n\n\n# Assign to the variable 'longitudes' a vector with the longitudes\n# This is an interval variable.\nlongitudes <- c(10, 20, 30, 40)\n\n# Assign the times it takes for an athlete to run 100 meters to the variable 'chronos'\n# This is a ratio variable.\nchronos <- c(10.60, 10.12, 9.58, 11.1)\n\n\n\n\n2, Histograms and Distributions\n\u00b6\n\n\nCreating histograms in R\n\n\n# Print the data set in the console\nhead(impact)\n\n\n\n\n##   subject condition verbal_memory_baseline visual_memory_baseline\n## 1       1   control                     95                     88\n## 2       2   control                     90                     82\n## 3       3   control                     87                     77\n## 4       4   control                     84                     72\n## 5       5   control                     92                     77\n## 6       6   control                     89                     79\n##   visual.motor_speed_baseline reaction_time_baseline\n## 1                       35.29                   0.42\n## 2                       31.47                   0.63\n## 3                       30.87                   0.56\n## 4                       41.87                   0.66\n## 5                       33.28                   0.56\n## 6                       40.73                   0.81\n##   impulse_control_baseline total_symptom_baseline verbal_memory_retest\n## 1                       11                      0                   97\n## 2                        7                      0                   86\n## 3                        8                      0                   90\n## 4                        7                      0                   85\n## 5                        7                      1                   87\n## 6                        6                      0                   91\n##   visual_memory_retest visual.motor_speed_retest reaction_time_retest\n## 1                   86                     35.61                 0.65\n## 2                   80                     37.01                 0.49\n## 3                   79                     20.15                 0.75\n## 4                   70                     33.26                 0.19\n## 5                   77                     28.34                 0.59\n## 6                   85                     33.47                 0.48\n##   impulse_control_retest total_symptom_retest\n## 1                     10                    0\n## 2                      7                    0\n## 3                      9                    0\n## 4                      8                    0\n## 5                      8                    1\n## 6                      5                    0\n\n\n\n# Use the describe() function to see some summary information per variable\n#describe(impact)\nsummary(impact)\n\n\n\n\n##     subject       condition         verbal_memory_baseline\n##  Min.   : 1.00   Length:40          Min.   :75.00         \n##  1st Qu.:10.75   Class :character   1st Qu.:85.00         \n##  Median :20.50   Mode  :character   Median :91.00         \n##  Mean   :20.50                      Mean   :89.75         \n##  3rd Qu.:30.25                      3rd Qu.:95.00         \n##  Max.   :40.00                      Max.   :98.00         \n##  visual_memory_baseline visual.motor_speed_baseline reaction_time_baseline\n##  Min.   :59.00          Min.   :26.29               Min.   :0.4200        \n##  1st Qu.:68.75          1st Qu.:31.59               1st Qu.:0.5675        \n##  Median :75.00          Median :33.50               Median :0.6500        \n##  Mean   :74.88          Mean   :34.03               Mean   :0.6670        \n##  3rd Qu.:81.25          3rd Qu.:36.44               3rd Qu.:0.7325        \n##  Max.   :91.00          Max.   :41.87               Max.   :1.2000        \n##  impulse_control_baseline total_symptom_baseline verbal_memory_retest\n##  Min.   : 2.000           Min.   :0.00           Min.   :59          \n##  1st Qu.: 7.000           1st Qu.:0.00           1st Qu.:74          \n##  Median : 8.500           Median :0.00           Median :85          \n##  Mean   : 8.275           Mean   :0.05           Mean   :82          \n##  3rd Qu.:10.000           3rd Qu.:0.00           3rd Qu.:91          \n##  Max.   :12.000           Max.   :1.00           Max.   :97          \n##  visual_memory_retest visual.motor_speed_retest reaction_time_retest\n##  Min.   :54.00        Min.   :20.15             Min.   :0.1900      \n##  1st Qu.:66.75        1st Qu.:30.33             1st Qu.:0.5575      \n##  Median :72.00        Median :35.15             Median :0.6500      \n##  Mean   :71.90        Mean   :35.83             Mean   :0.6730      \n##  3rd Qu.:79.00        3rd Qu.:39.41             3rd Qu.:0.7325      \n##  Max.   :86.00        Max.   :60.77             Max.   :1.3000      \n##  impulse_control_retest total_symptom_retest\n##  Min.   : 1.00          Min.   : 0.00       \n##  1st Qu.: 5.00          1st Qu.: 0.00       \n##  Median : 7.00          Median : 7.00       \n##  Mean   : 6.75          Mean   :13.88       \n##  3rd Qu.: 9.00          3rd Qu.:27.00       \n##  Max.   :12.00          Max.   :43.00\n\n\n\n# Select the variable 'verbal_memory_baseline' from the 'impact' data.frame and assign it to the variable 'verbal_baseline'\nverbal_baseline <- impact$verbal_memory_baseline\nverbal_baseline\n\n\n\n\n##  [1] 95 90 87 84 92 89 78 97 93 90 89 97 79 86 85 85 98 95 96 92 79 85 97\n## [24] 89 75 75 84 93 88 97 93 96 84 89 95 95 97 95 92 95\n\n\n\n# Plot a histogram of the verbal_baseline variable that you have just created\nhist(verbal_baseline, main = 'Distribution of verbal memory baseline scores', xlab = 'score', ylab = 'frequency')\n\n\n\n\n\n\nLet us go wine tasting (red wine)\n\n\n# Read in the data set and assign to the object\nred_wine_data <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'red_wine_data', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(red_wine_data)\n\n\n\n\n##   subject condition Ratings\n## 1       1 Australia      77\n## 2       2 Australia      82\n## 3       3 Australia      75\n## 4       4 Australia      92\n## 5       5 Australia      83\n## 6       6 Australia      75\n\n\n\n# Print basic statistical properties of the red_wine_data data.frame. Use the describe() function\n#describe(red_wine_data)\nsummary(red_wine_data)\n\n\n\n\n##     subject       condition            Ratings     \n##  Min.   :  1.0   Length:400         Min.   :39.00  \n##  1st Qu.:100.8   Class :character   1st Qu.:67.00  \n##  Median :200.5   Mode  :character   Median :74.00  \n##  Mean   :200.5                      Mean   :73.94  \n##  3rd Qu.:300.2                      3rd Qu.:81.00  \n##  Max.   :400.0                      Max.   :98.00\n\n\n\n# Split the data.frame in subsets for each country and assign these subsets to the variables below\nred_usa <- subset(red_wine_data, red_wine_data$condition == 'USA')\nred_france <- subset(red_wine_data, red_wine_data$condition == 'France')\nred_australia <- subset(red_wine_data, red_wine_data$condition == 'Australia')\nred_argentina <- subset(red_wine_data, red_wine_data$condition == 'Argentina')\n\n# Select only the Ratings variable for each of these subsets and assign them to the variables below\nred_ratings_usa <- red_usa$Ratings\nred_ratings_france <- red_france$Ratings\nred_ratings_australia <- red_australia$Ratings\nred_ratings_argentina <- red_argentina$Ratings\n\n## Create a 2 by 2 matrix of histograms\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\npar(mfrow = c(2,2))\n\n# Plot four histograms, one for each subject\nhist(red_ratings_usa)\nhist(red_ratings_france)\nhist(red_ratings_australia)\nhist(red_ratings_argentina)\n\n\n\n\n\n\nLet us go wine tasting (white wine)\n\n\n# Read in the data set and assign to the object\nwhite_wine_data <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'white_wine_data', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(white_wine_data)\n\n\n\n\n##   condition Ratings\n## 1 Australia    85.6\n## 2 Australia    85.6\n## 3 Australia    85.6\n## 4 Australia    85.6\n## 5 Australia    85.6\n## 6 Australia    85.6\n\n\n\n# Assign the scores for each country to a variable\nwhite_ratings_france <- subset(white_wine_data, white_wine_data$condition == 'France')$Ratings\nwhite_ratings_argentina <- subset(white_wine_data, white_wine_data$condition == 'Argentina')$Ratings\nwhite_ratings_australia <- subset(white_wine_data, white_wine_data$condition == 'Australia')$Ratings\nwhite_ratings_usa <- subset(white_wine_data, white_wine_data$condition == 'USA')$Ratings\n\n# Plot a histogram for each of the countries\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\npar(mfrow = c(2,2))\n\nhist(white_ratings_usa, main = 'USA white ratings', xlab = 'score')\nhist(white_ratings_australia, main = 'Australia white ratings', xlab = 'score')\nhist(white_ratings_argentina, main = 'Argentina white ratings', xlab = 'score')\nhist(white_ratings_france, main = 'France white ratings', xlab = 'score')\n\n\n\n\n\n\n3, Scales of Measurement\n\u00b6\n\n\nConverting a distribution to Z-scale\n\n\n# Read in the data set and assign to the object\nratings_australia <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'ratings_australia', header = TRUE, startCol = 1, startRow = 1)\n\nratings_australia <- as.vector(ratings_australia$ratings_australia)\n\n\n\n\n# Print the ratings for the Australian red wine\nratings_australia\n\n\n\n\n##   [1] 77 82 75 92 83 75 84 86 85 79 92 84 77 65 89 81 81 88 87 85 87 86 82\n##  [24] 67 85 81 80 71 78 84 91 80 84 81 71 78 78 81 89 86 80 79 86 85 76 76\n##  [47] 84 86 80 87 84 77 83 73 91 95 78 74 85 80 98 81 86 81 76 82 68 91 82\n##  [70] 96 84 76 85 74 72 83 78 81 82 77 77 80 89 70 85 83 88 79 84 83 77 89\n##  [93] 89 86 92 85 72 77 72 78\n\n\n\n# Convert these ratings to Z-scores. Use the `scale()` function\nz_scores_australia <- scale(ratings_australia)\n\n# Plot both the original data and the scaled data in histograms next to each other\npar(mfrow = c(1,2))\n\n# Plot the histogram for the original scores\nhist(ratings_australia)\n\n# Plot the histogram for the Z-scores\nhist(z_scores_australia)\n\n\n\n\n\n\n4, Measures of Central Tendency\n\u00b6\n\n\nThe mean of a Fibonacci sequence\n\n\n# create a vector that contains the Fibonacci elements\nfibonacci <- c(0, 1, 1, 2, 3, 5, 8, 13) \n\n# calculate the mean manually. Use the sum() and the length() functions\nmean <- sum(fibonacci)/length(fibonacci)\nmean\n\n\n\n\n## [1] 4.125\n\n\n\n# calculate the mean the easy way\nmean_check <- mean(fibonacci)\nmean_check\n\n\n\n\n## [1] 4.125\n\n\n\nSetting up histograms\n\n\n# Read in the data set and assign to the object\nwine_data <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wine_data', header = TRUE, startCol = 1, startRow = 1)\n\nhead(wine_data)\n\n\n\n\n##   condition Ratings\n## 1       Red      77\n## 2       Red      82\n## 3       Red      75\n## 4       Red      92\n## 5       Red      83\n## 6       Red      75\n\n\n\n# create the two subsets\nred_wine <- subset(wine_data, wine_data$condition == 'Red')\nwhite_wine <- subset(wine_data, wine_data$condition == 'White')\n\n# Plot the histograms of the ratings of both subsets\npar(mfrow = c(1,2))\nhist(red_wine$Ratings, main = 'Shiraz', xlab = 'Ratings')\nhist(white_wine$Ratings, main = 'Pinot Grigio', xlab = 'Ratings')\n\n\n\n\n\n\nRobustness to outliers\n\n\n# create the outlier and add it to the dataset\noutlier <- data.frame(condition = 'Red', Ratings = 0)\n\nred_wine_extreme <- rbind(red_wine, outlier)\n\n# calculate the difference in means and display it afterwards\ndiff_means <- mean(red_wine$Ratings) - mean(red_wine_extreme$Ratings)\n\ndiff_means\n\n\n\n\n## [1] 0.8093069\n\n\n\n# calculate the difference in medians and display it afterwards\ndiff_medians <- median(red_wine$Ratings) - median(red_wine_extreme$Ratings)\n\ndiff_medians\n\n\n\n\n## [1] 0\n\n\n\n5, Measures of Variability\n\u00b6\n\n\nMichael Jordan\u2019s first NBA season - Global overview\n\n\n# Read in the data set and assign to the object\ndata_jordan <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'data_jordan', header = TRUE, startCol = 1, startRow = 1)\n\nhead(data_jordan)\n\n\n\n\n##   game points\n## 1    1     16\n## 2    2     21\n## 3    3     37\n## 4    4     25\n## 5    5     17\n## 6    6     25\n\n\n\n# Make a scatterplot of the data on which a horizontal line with height equal to the mean is drawn.\nmean_jordan <- mean(data_jordan$points)\nplot(data_jordan$game, data_jordan$points,main = '1st NBA season of Michael Jordan')\n\nabline(h = mean_jordan)\n\n\n\n\n\n\nMichael Jordan\u2019s first NBA season - Calculate the variance manually\n\n\n# Calculate the differences with respect to the mean \ndiff <- data_jordan$points - mean(data_jordan$points)\n\n# Calculate the squared differences\nsquared_diff <- diff^2\n\n# Combine all pieces of the puzzle in order to acquire the variance\nvariance <- sum(squared_diff)/(length(data_jordan$points) - 1)\nvariance\n\n\n\n\n## [1] 66.73427\n\n\n\n# Compare your result to the correct solution. You can find the correct solution by calculating it with the `var()` function.\nvar(data_jordan$points)\n\n\n\n\n## [1] 66.73427",
            "title": "Statistics with R, Course One, Introduction"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#2-histograms-and-distributions",
            "text": "Creating histograms in R  # Print the data set in the console\nhead(impact)  ##   subject condition verbal_memory_baseline visual_memory_baseline\n## 1       1   control                     95                     88\n## 2       2   control                     90                     82\n## 3       3   control                     87                     77\n## 4       4   control                     84                     72\n## 5       5   control                     92                     77\n## 6       6   control                     89                     79\n##   visual.motor_speed_baseline reaction_time_baseline\n## 1                       35.29                   0.42\n## 2                       31.47                   0.63\n## 3                       30.87                   0.56\n## 4                       41.87                   0.66\n## 5                       33.28                   0.56\n## 6                       40.73                   0.81\n##   impulse_control_baseline total_symptom_baseline verbal_memory_retest\n## 1                       11                      0                   97\n## 2                        7                      0                   86\n## 3                        8                      0                   90\n## 4                        7                      0                   85\n## 5                        7                      1                   87\n## 6                        6                      0                   91\n##   visual_memory_retest visual.motor_speed_retest reaction_time_retest\n## 1                   86                     35.61                 0.65\n## 2                   80                     37.01                 0.49\n## 3                   79                     20.15                 0.75\n## 4                   70                     33.26                 0.19\n## 5                   77                     28.34                 0.59\n## 6                   85                     33.47                 0.48\n##   impulse_control_retest total_symptom_retest\n## 1                     10                    0\n## 2                      7                    0\n## 3                      9                    0\n## 4                      8                    0\n## 5                      8                    1\n## 6                      5                    0  # Use the describe() function to see some summary information per variable\n#describe(impact)\nsummary(impact)  ##     subject       condition         verbal_memory_baseline\n##  Min.   : 1.00   Length:40          Min.   :75.00         \n##  1st Qu.:10.75   Class :character   1st Qu.:85.00         \n##  Median :20.50   Mode  :character   Median :91.00         \n##  Mean   :20.50                      Mean   :89.75         \n##  3rd Qu.:30.25                      3rd Qu.:95.00         \n##  Max.   :40.00                      Max.   :98.00         \n##  visual_memory_baseline visual.motor_speed_baseline reaction_time_baseline\n##  Min.   :59.00          Min.   :26.29               Min.   :0.4200        \n##  1st Qu.:68.75          1st Qu.:31.59               1st Qu.:0.5675        \n##  Median :75.00          Median :33.50               Median :0.6500        \n##  Mean   :74.88          Mean   :34.03               Mean   :0.6670        \n##  3rd Qu.:81.25          3rd Qu.:36.44               3rd Qu.:0.7325        \n##  Max.   :91.00          Max.   :41.87               Max.   :1.2000        \n##  impulse_control_baseline total_symptom_baseline verbal_memory_retest\n##  Min.   : 2.000           Min.   :0.00           Min.   :59          \n##  1st Qu.: 7.000           1st Qu.:0.00           1st Qu.:74          \n##  Median : 8.500           Median :0.00           Median :85          \n##  Mean   : 8.275           Mean   :0.05           Mean   :82          \n##  3rd Qu.:10.000           3rd Qu.:0.00           3rd Qu.:91          \n##  Max.   :12.000           Max.   :1.00           Max.   :97          \n##  visual_memory_retest visual.motor_speed_retest reaction_time_retest\n##  Min.   :54.00        Min.   :20.15             Min.   :0.1900      \n##  1st Qu.:66.75        1st Qu.:30.33             1st Qu.:0.5575      \n##  Median :72.00        Median :35.15             Median :0.6500      \n##  Mean   :71.90        Mean   :35.83             Mean   :0.6730      \n##  3rd Qu.:79.00        3rd Qu.:39.41             3rd Qu.:0.7325      \n##  Max.   :86.00        Max.   :60.77             Max.   :1.3000      \n##  impulse_control_retest total_symptom_retest\n##  Min.   : 1.00          Min.   : 0.00       \n##  1st Qu.: 5.00          1st Qu.: 0.00       \n##  Median : 7.00          Median : 7.00       \n##  Mean   : 6.75          Mean   :13.88       \n##  3rd Qu.: 9.00          3rd Qu.:27.00       \n##  Max.   :12.00          Max.   :43.00  # Select the variable 'verbal_memory_baseline' from the 'impact' data.frame and assign it to the variable 'verbal_baseline'\nverbal_baseline <- impact$verbal_memory_baseline\nverbal_baseline  ##  [1] 95 90 87 84 92 89 78 97 93 90 89 97 79 86 85 85 98 95 96 92 79 85 97\n## [24] 89 75 75 84 93 88 97 93 96 84 89 95 95 97 95 92 95  # Plot a histogram of the verbal_baseline variable that you have just created\nhist(verbal_baseline, main = 'Distribution of verbal memory baseline scores', xlab = 'score', ylab = 'frequency')   Let us go wine tasting (red wine)  # Read in the data set and assign to the object\nred_wine_data <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'red_wine_data', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(red_wine_data)  ##   subject condition Ratings\n## 1       1 Australia      77\n## 2       2 Australia      82\n## 3       3 Australia      75\n## 4       4 Australia      92\n## 5       5 Australia      83\n## 6       6 Australia      75  # Print basic statistical properties of the red_wine_data data.frame. Use the describe() function\n#describe(red_wine_data)\nsummary(red_wine_data)  ##     subject       condition            Ratings     \n##  Min.   :  1.0   Length:400         Min.   :39.00  \n##  1st Qu.:100.8   Class :character   1st Qu.:67.00  \n##  Median :200.5   Mode  :character   Median :74.00  \n##  Mean   :200.5                      Mean   :73.94  \n##  3rd Qu.:300.2                      3rd Qu.:81.00  \n##  Max.   :400.0                      Max.   :98.00  # Split the data.frame in subsets for each country and assign these subsets to the variables below\nred_usa <- subset(red_wine_data, red_wine_data$condition == 'USA')\nred_france <- subset(red_wine_data, red_wine_data$condition == 'France')\nred_australia <- subset(red_wine_data, red_wine_data$condition == 'Australia')\nred_argentina <- subset(red_wine_data, red_wine_data$condition == 'Argentina')\n\n# Select only the Ratings variable for each of these subsets and assign them to the variables below\nred_ratings_usa <- red_usa$Ratings\nred_ratings_france <- red_france$Ratings\nred_ratings_australia <- red_australia$Ratings\nred_ratings_argentina <- red_argentina$Ratings\n\n## Create a 2 by 2 matrix of histograms\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\npar(mfrow = c(2,2))\n\n# Plot four histograms, one for each subject\nhist(red_ratings_usa)\nhist(red_ratings_france)\nhist(red_ratings_australia)\nhist(red_ratings_argentina)   Let us go wine tasting (white wine)  # Read in the data set and assign to the object\nwhite_wine_data <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'white_wine_data', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(white_wine_data)  ##   condition Ratings\n## 1 Australia    85.6\n## 2 Australia    85.6\n## 3 Australia    85.6\n## 4 Australia    85.6\n## 5 Australia    85.6\n## 6 Australia    85.6  # Assign the scores for each country to a variable\nwhite_ratings_france <- subset(white_wine_data, white_wine_data$condition == 'France')$Ratings\nwhite_ratings_argentina <- subset(white_wine_data, white_wine_data$condition == 'Argentina')$Ratings\nwhite_ratings_australia <- subset(white_wine_data, white_wine_data$condition == 'Australia')$Ratings\nwhite_ratings_usa <- subset(white_wine_data, white_wine_data$condition == 'USA')$Ratings\n\n# Plot a histogram for each of the countries\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\npar(mfrow = c(2,2))\n\nhist(white_ratings_usa, main = 'USA white ratings', xlab = 'score')\nhist(white_ratings_australia, main = 'Australia white ratings', xlab = 'score')\nhist(white_ratings_argentina, main = 'Argentina white ratings', xlab = 'score')\nhist(white_ratings_france, main = 'France white ratings', xlab = 'score')",
            "title": "2, Histograms and Distributions"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#3-scales-of-measurement",
            "text": "Converting a distribution to Z-scale  # Read in the data set and assign to the object\nratings_australia <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'ratings_australia', header = TRUE, startCol = 1, startRow = 1)\n\nratings_australia <- as.vector(ratings_australia$ratings_australia)  # Print the ratings for the Australian red wine\nratings_australia  ##   [1] 77 82 75 92 83 75 84 86 85 79 92 84 77 65 89 81 81 88 87 85 87 86 82\n##  [24] 67 85 81 80 71 78 84 91 80 84 81 71 78 78 81 89 86 80 79 86 85 76 76\n##  [47] 84 86 80 87 84 77 83 73 91 95 78 74 85 80 98 81 86 81 76 82 68 91 82\n##  [70] 96 84 76 85 74 72 83 78 81 82 77 77 80 89 70 85 83 88 79 84 83 77 89\n##  [93] 89 86 92 85 72 77 72 78  # Convert these ratings to Z-scores. Use the `scale()` function\nz_scores_australia <- scale(ratings_australia)\n\n# Plot both the original data and the scaled data in histograms next to each other\npar(mfrow = c(1,2))\n\n# Plot the histogram for the original scores\nhist(ratings_australia)\n\n# Plot the histogram for the Z-scores\nhist(z_scores_australia)",
            "title": "3, Scales of Measurement"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#4-measures-of-central-tendency",
            "text": "The mean of a Fibonacci sequence  # create a vector that contains the Fibonacci elements\nfibonacci <- c(0, 1, 1, 2, 3, 5, 8, 13) \n\n# calculate the mean manually. Use the sum() and the length() functions\nmean <- sum(fibonacci)/length(fibonacci)\nmean  ## [1] 4.125  # calculate the mean the easy way\nmean_check <- mean(fibonacci)\nmean_check  ## [1] 4.125  Setting up histograms  # Read in the data set and assign to the object\nwine_data <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wine_data', header = TRUE, startCol = 1, startRow = 1)\n\nhead(wine_data)  ##   condition Ratings\n## 1       Red      77\n## 2       Red      82\n## 3       Red      75\n## 4       Red      92\n## 5       Red      83\n## 6       Red      75  # create the two subsets\nred_wine <- subset(wine_data, wine_data$condition == 'Red')\nwhite_wine <- subset(wine_data, wine_data$condition == 'White')\n\n# Plot the histograms of the ratings of both subsets\npar(mfrow = c(1,2))\nhist(red_wine$Ratings, main = 'Shiraz', xlab = 'Ratings')\nhist(white_wine$Ratings, main = 'Pinot Grigio', xlab = 'Ratings')   Robustness to outliers  # create the outlier and add it to the dataset\noutlier <- data.frame(condition = 'Red', Ratings = 0)\n\nred_wine_extreme <- rbind(red_wine, outlier)\n\n# calculate the difference in means and display it afterwards\ndiff_means <- mean(red_wine$Ratings) - mean(red_wine_extreme$Ratings)\n\ndiff_means  ## [1] 0.8093069  # calculate the difference in medians and display it afterwards\ndiff_medians <- median(red_wine$Ratings) - median(red_wine_extreme$Ratings)\n\ndiff_medians  ## [1] 0",
            "title": "4, Measures of Central Tendency"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#5-measures-of-variability",
            "text": "Michael Jordan\u2019s first NBA season - Global overview  # Read in the data set and assign to the object\ndata_jordan <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'data_jordan', header = TRUE, startCol = 1, startRow = 1)\n\nhead(data_jordan)  ##   game points\n## 1    1     16\n## 2    2     21\n## 3    3     37\n## 4    4     25\n## 5    5     17\n## 6    6     25  # Make a scatterplot of the data on which a horizontal line with height equal to the mean is drawn.\nmean_jordan <- mean(data_jordan$points)\nplot(data_jordan$game, data_jordan$points,main = '1st NBA season of Michael Jordan')\n\nabline(h = mean_jordan)   Michael Jordan\u2019s first NBA season - Calculate the variance manually  # Calculate the differences with respect to the mean \ndiff <- data_jordan$points - mean(data_jordan$points)\n\n# Calculate the squared differences\nsquared_diff <- diff^2\n\n# Combine all pieces of the puzzle in order to acquire the variance\nvariance <- sum(squared_diff)/(length(data_jordan$points) - 1)\nvariance  ## [1] 66.73427  # Compare your result to the correct solution. You can find the correct solution by calculating it with the `var()` function.\nvar(data_jordan$points)  ## [1] 66.73427",
            "title": "5, Measures of Variability"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Two,_Student_s_T-test/",
            "text": "1, Introduction to t-tests\n\n\n2, Independent t-tests\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, Introduction to t-tests\n\u00b6\n\n\n\n\nTest p-values for significance with z-tests, t-tests.\n\n\nSingle sample t-test: group of people from a particular geographic region perform on a well-known test of intelligence. In particular, you are interested in finding out whether or not this group scores significantly higher than the overall population on an IQ test. This is a form of Null Hypothesis Significance Testing (NHST), where the null hypothesis is that there\u2019s no difference between this group and the overall population.\n\n\nDependent t-test: single group of voters to rate their likelihood of voting for the candidate before the speech and again after the speech; understand if voters from a particular neighborhood are likely to vote differently when compared to the overall\n\n    voting population.\n\n\nIndependent t-test: significant difference in preferences between these two groups; compare liberals and convervatives.\n\n\nt-distribution, observed value, expected value, standard error.\n\n\n\n\nGenerate density plots of different t-distributions\n\n\n# Generate a vector of 100 values between -4 and 4\nx <- seq(-4, 4, length = 100)\n\n# Simulate the t-distribution\ny_1 <- dt(x, 4)\ny_2 <- dt(x, 6)\ny_3 <- dt(x, 8)\ny_4 <- dt(x, 10)\ny_5 <- dt(x, 12)\n\n# Plot the t-distributions\nplot(x, y_1, type = 'l', lwd = 2, xlab = 'T value', ylab = 'Density', main = 'Comparison of t-distributions')\n\nlines(x, y_2, col = 'red')\n#lines(x, y_3, col = 'orange')\n#lines(x, y_4, col = 'green')\n#lines(x, y_5, col = 'blue')\n\n# Add a legend\nlegend('topright', c('df = 4', 'df = 6', 'df = 8', 'df = 10', 'df = 12'), title = 'T distributions', col = c('black', 'red', 'orange', 'green', 'blue'), lty = 1)\n\n\n\n\n\n\nThe working memory dataset\n\n\nConduct a dependent (or paired) t-test on the \u201cworking memory\u201d dataset. This dataset consists of the intelligence scores for subjects before and after training, as well as for a control group. Our goal is to assess whether intelligence training results in significantly different intelligence scores for the individuals.\n\n\nThe observations of individuals before and after training are two samples from the same group at different points in time, which calls for a dependent t-test. This will test whether or not the difference in mean intelligence scores before and after training are significant.\n\n\n# Print the data set in the console\nhead(wm)\n\n\n\n\n##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1\n\n\n\nlibrary(Hmisc)\n\n# Create a subset for the data that contains information on those subject who trained\nwm_t <- subset(wm, wm$train == 1)\n\n # Summary statistics \ndescribe(wm_t)\n\n\n\n\n## wm_t \n## \n##  5  Variables      80  Observations\n## ---------------------------------------------------------------------------\n## cond \n##        n  missing distinct \n##       80        0        4 \n##                               \n## Value       t08  t12  t17  t19\n## Frequency    20   20   20   20\n## Proportion 0.25 0.25 0.25 0.25\n## ---------------------------------------------------------------------------\n## pre \n##        n  missing distinct     Info     Mean      Gmd \n##       80        0        5    0.955    10.03    1.551 \n##                                         \n## Value          8     9    10    11    12\n## Frequency     12    20    19    12    17\n## Proportion 0.150 0.250 0.238 0.150 0.212\n## ---------------------------------------------------------------------------\n## post \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       80        0       13    0.984    13.51     2.87     9.95    10.00 \n##      .25      .50      .75      .90      .95 \n##    12.00    14.00    15.00    17.00    18.00 \n##                                                                       \n## Value          7     8     9    10    11    12    13    14    15    16\n## Frequency      1     1     2     5     8    11    11    15     8     9\n## Proportion 0.012 0.012 0.025 0.062 0.100 0.138 0.138 0.188 0.100 0.112\n##                             \n## Value         17    18    19\n## Frequency      4     2     3\n## Proportion 0.050 0.025 0.038\n## ---------------------------------------------------------------------------\n## gain \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       80        0       10    0.975    3.487    2.415      0.0      1.0 \n##      .25      .50      .75      .90      .95 \n##      2.0      3.0      5.0      6.1      7.0 \n##                                                                       \n## Value         -1     0     1     2     3     4     5     6     7     9\n## Frequency      2     3     7    18    12    16     6     8     6     2\n## Proportion 0.025 0.038 0.088 0.225 0.150 0.200 0.075 0.100 0.075 0.025\n## ---------------------------------------------------------------------------\n## train \n##        n  missing distinct     Info     Mean      Gmd \n##       80        0        1        0        1        0 \n##              \n## Value       1\n## Frequency  80\n## Proportion  1\n## ---------------------------------------------------------------------------\n\n\n\n# Create a boxplot with pre- and post-training groups \nboxplot(wm_t$pre, wm_t$post, main = \"Boxplot\", xlab = \"Pre and Post Training\", ylab = \"Intelligence Score\", col = c(\"red\", \"green\"))\n\n\n\n\n\n\nPerforming dependent t-tests manually in R (1)\n\n\nConducting a dependent t-test, also known as a paired t-test, requires the following steps:\n\n\n\n\nDefine null and alternative hypotheses\n\n\nDecide significance level \n\u03b1\n\n\nCompute observed t-value\n\n\nFind critical value\n\n\nCompare observed value to critical value\n\n\n\n\nWe\u2019re performing a Null Hypothesis Significance Test (NHST), so our null hypothesis is that there\u2019s no effect (i.e. training has no impact on intelligence scores). The alternative hypothesis is that training results in signficantly different  intelligence scores. We\u2019ll use a significance level of 0.05, which is very common in statistics.\n\n\nCompute the observed t-value.\n\n\n# Define the sample size\nn <- dim(wm_t)[1]\n\n# Calculate the degrees of freedom\ndf <- n - 1\n\n # Find the critical t-value\nt_crit <- abs(qt(0.025, df))\n\n# Calculate the mean of the difference in scores. The differences are already in the dataset under the column 'gain'.\nmean_diff <- sum(wm_t$gain)/n\n\n# Calculate the standard deviation\nxdt <- sum(wm_t$gain^2)\nxdt2 <- xdt/n\nsd_diff2 <- sqrt((xdt - xdt2)/(n - 1))\nsd_diff <- sqrt((sum(wm_t$gain^2) - ((sum(wm_t$gain))^2/n))/(n - 1))\nsd_diff2\n\n\n\n\n## [1] 4.091149\n\n\n\nsd_diff\n\n\n\n\n## [1] 2.152383\n\n\n\nPerforming dependent t-tests manually in R (2)\n\n\nNow that we\u2019ve determined our null and alternative hypotheses, decided on a significance level, and computed our observed t-value, all that remains is to calculate the critical value for this test and compare it to our observed t-value. This will tell us whether we have sufficient evidence to reject our null hypothesis. We\u2019ll even go one step further and compute an effect size with Cohen\u2019s d!\n\n\nThe critical value is the point on the relevant t-distribution that determines whether the value we observed is extreme enough to warrant rejecting the null hypothesis. Recall that a t-distribution is defined by its degrees of freedom, which in turn is equal to the sample size minus 1. In this example, we have 80 subjects so the relevant t-distribution has 79 degrees of freedom.\n\n\nWe\u2019re performing a two-tailed t-test in this situation since we care about detecting a significant effect in either the positive or negative direction. In other words, we want to know if training significantly increases or decreases intelligence, however, given that our observed t-value is positive (14.49) the right-hand is the only relevant value here.\n\n\nFurthermore, since our desired significance level (i.e. alpha) is 0.05, our critical value is the point on our t-distribution at which 0.025 (0.05 / 2) of its total area of 1 is to the right and thus 0.975 (1 - 0.025) of its total area is to the left.\n\n\nThis point is called the 0.975 quantile and is computed for a\n\nt-distrbution.\n\n\n# The variables from the previous exercise are still preloaded, type ls() in the console to see them\nn <- dim(wm_t)[1]\ndf <- n - 1\nt_crit <- abs(qt(0.025, df))\nmean_diff <- sum(wm_t$gain)/n\nsd_diff <- sqrt((sum(wm_t$gain^2) - ((sum(wm_t$gain))^2/n))/(n - 1))\n\n# Calculate the t-value for this test\nt_value <- mean_diff/(sd_diff/sqrt(n))\n\n# Check whether or not the mean difference is statistically significant\nt_value\n\n\n\n\n## [1] 14.49238\n\n\n\nt_crit\n\n\n\n\n## [1] 1.99045\n\n\n\n# Calculate the confidence interval\nconf_upper <- mean_diff + t_crit * (sd_diff/sqrt(n))\nconf_lower <- mean_diff - t_crit * (sd_diff/sqrt(n))\nconf_upper\n\n\n\n\n## [1] 3.966489\n\n\n\nconf_lower\n\n\n\n\n## [1] 3.008511\n\n\n\n# Calculate Cohen's d\ncohens_d <- mean_diff/sd_diff\ncohens_d\n\n\n\n\n## [1] 1.620297\n\n\n\nLetting R do all the dirty work: Dependent t-tests\n\n\nThe \nCohensD\n function (not showns).\n\n\nCohen\u2019s d Determine that the difference pre- and post-training is statistically significant; and the effect size, meaning the effect of training on intelligence gains particularly strong or not.\n\n\nCohen\u2019s d is unbiased by sample size. Cohen\u2019s d provides a standardized difference between two means. Cohen\u2019s d is calculated by subtracting one group mean from the other, then dividing by the pooled standard deviation.\n\n\n# Conduct a paired t-test using the t.test function\nt.test(wm_t$post, wm_t$pre, paired = TRUE)\n\n\n\n\n## \n##  Paired t-test\n## \n## data:  wm_t$post and wm_t$pre\n## t = 14.492, df = 79, p-value < 2.2e-16\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  3.008511 3.966489\n## sample estimates:\n## mean of the differences \n##                  3.4875\n\n\n\n# Calculate Cohen's d\nlibrary(lsr)\n\ncohensD(wm_t$post, wm_t$pre, method = 'paired')\n\n\n\n\n## [1] 1.620297\n\n\n\n2, Independent t-tests\n\u00b6\n\n\nAn independent t-test is appropriate when you want to compare the the means for two independent groups.\n\n\nPreliminary statistics\n\n\nFor independent t-tests you will revisit the working memory dataset from the previous chapter. In this dataset, subjects were randomly assigned to four different training groups that trained for 8, 12, 17 and 19 days.\n\n\n# Read in the data set and assign to the object\nwm_t <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm_t', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(wm_t)\n\n\n\n\n##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1\n\n\n\nAdd statistical functions.\n\n\n# Load the psych(ology) package\nlibrary(psych)\n\n\n\n\nLevene\u2019s test for homogeneity of variance.\n\n\n# Create subsets for each training time\nwm_t08 <- subset(wm_t, cond == 't08')\nwm_t12 <- subset(wm_t, cond == 't12')\nwm_t17 <- subset(wm_t, cond == 't17')\nwm_t19 <- subset(wm_t, cond == 't19')\n\n# Summary statistics of the change in training scores before and after exercise\ndescribe(wm_t08)\n\n\n\n\n##       vars  n  mean   sd median trimmed  mad min  max range  skew kurtosis\n## cond*    1 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf    NA       NA\n## pre      2 20 10.05 1.50   10.0   10.06 1.48   8   12     4  0.01    -1.53\n## post     3 20 11.40 2.14   11.5   11.50 2.22   7   15     8 -0.25    -0.84\n## gain     4 20  1.35 1.23    1.0    1.44 1.48  -1    3     4 -0.32    -0.82\n## train    5 20  1.00 0.00    1.0    1.00 0.00   1    1     0   NaN      NaN\n##         se\n## cond*   NA\n## pre   0.34\n## post  0.48\n## gain  0.27\n## train 0.00\n\n\n\ndescribe(wm_t12)\n\n\n\n\n##       vars  n mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20  NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20  9.9 1.45     10    9.88 1.48   8   12     4 0.16    -1.43\n## post     3 20 12.5 1.88     12   12.38 2.22  10   17     7 0.48    -0.54\n## gain     4 20  2.6 1.27      2    2.50 0.00   0    5     5 0.44    -0.54\n## train    5 20  1.0 0.00      1    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.32\n## post  0.42\n## gain  0.28\n## train 0.00\n\n\n\ndescribe(wm_t17)\n\n\n\n\n##       vars  n mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20  NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20 10.0 1.34     10   10.00 1.48   8   12     4 0.25    -1.34\n## post     3 20 14.4 1.85     14   14.25 1.48  12   19     7 0.63    -0.27\n## gain     4 20  4.4 1.39      4    4.25 1.48   3    7     4 0.64    -1.12\n## train    5 20  1.0 0.00      1    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.30\n## post  0.41\n## gain  0.31\n## train 0.00\n\n\n\ndescribe(wm_t19)\n\n\n\n\n##       vars  n  mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20 10.15 1.27   10.0   10.19 1.48   8   12     4 0.03    -1.10\n## post     3 20 15.75 1.86   16.0   15.69 1.48  13   19     6 0.16    -1.03\n## gain     4 20  5.60 1.73    5.5    5.50 2.22   3    9     6 0.36    -0.76\n## train    5 20  1.00 0.00    1.0    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.28\n## post  0.42\n## gain  0.39\n## train 0.00\n\n\n\n# Create a boxplot of the different training times\nggplot(wm_t, aes(x = cond, y = gain, fill = cond)) + geom_boxplot()\n\n\n\n\n\n\n# Levene's test\nlibrary(car)\n\nleveneTest(wm_t$gain ~ wm_t$cond)\n\n\n\n\n## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  3  1.3134 0.2763\n##       76\n\n\n\nConducting an independent t-test manually (1)\n\n\nPerform an independent t-test the same way we did for the dependent t-test in the previous chapter. Continuing with the working memory example, our null hypothesis is that the difference in intelligence score gain between the group that trained for 8 days and the group that trained for 19 days is equal to zero. If our observed t-value is sufficiently large, we can reject the null in favor of the alternative hypothesis, which would imply a significant difference in intelligence gain between the two training groups.\n\n\nCalculation of the observed t-value for an independent t-test is similar to the dependent t-test, but involves slightly  different formulas.\n\n\n# Calculate mean difference by subtracting the gain for t08 by the gain for t19\nmean_t08 <- mean(wm_t08$gain)\nmean_t19 <- mean(wm_t19$gain)\nmean_diff <- (mean_t19 - mean_t08)\n\n# Calculate degrees of freedom\nn_t08 <- dim(wm_t08)[1]\nn_t19 <- dim(wm_t19)[1]\ndf <- n_t08 + n_t19 - 2\n\n# Calculate the pooled standard error\nvar_t08 <- (sum((wm_t08$gain - mean_t08)^2))/(n_t08 - 1)\nvar_t19 <- (sum((wm_t19$gain - mean_t19)^2))/(n_t19 - 1)\nse_pooled <- sqrt((var_t08/n_t08) + (var_t19/n_t19))\n\n\n\n\nConducting an independent t-test manually (2)\n\n\nCompute the observed t-value. Then we will determine the p-value using the relevant t-distribution. If you recall, in the last chapter we calculated the critical value. The p-value is simply an alternative approach to hypothesis testing and determining the significance of your results. It\u2019s good to practice both! Finally, we will finish by calculating effect size via Cohen\u2019s d.\n\n\n# All variables from the previous exercises are preloaded in your workspace\n# Type ls() to see them\n# Calculate the t-value\nt_value <- mean_diff/se_pooled\n\n# Calculate p-value\n#two-tail test, 0.05/2 = 0.025\np_value <- 2*(1-pt(t_value,df = df))\n\n# Calculate Cohen's d\nsd_t08 <- sd(wm_t08$gain)\nsd_t19 <- sd(wm_t19$gain)\npooled_sd <- (sd_t08 + sd_t19) / 2\ncohens_d <- mean_diff/pooled_sd\n\n\n\n\nLetting R do all the dirty work: Independent t-tests\n\n\n# Conduct an independent t-test \nt.test(wm_t19$gain, wm_t08$gain,var.equal = TRUE)\n\n\n\n\n## \n##  Two Sample t-test\n## \n## data:  wm_t19$gain and wm_t08$gain\n## t = 8.9677, df = 38, p-value = 6.443e-11\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  3.290588 5.209412\n## sample estimates:\n## mean of x mean of y \n##      5.60      1.35\n\n\n\n# Calculate Cohen's d\ncohensD(wm_t19$gain, wm_t08$gain, method = 'pooled')\n\n\n\n\n## [1] 2.835822",
            "title": "Statistics with R, Course Two, Student's t-test"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Two,_Student_s_T-test/#2-independent-t-tests",
            "text": "An independent t-test is appropriate when you want to compare the the means for two independent groups.  Preliminary statistics  For independent t-tests you will revisit the working memory dataset from the previous chapter. In this dataset, subjects were randomly assigned to four different training groups that trained for 8, 12, 17 and 19 days.  # Read in the data set and assign to the object\nwm_t <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm_t', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(wm_t)  ##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1  Add statistical functions.  # Load the psych(ology) package\nlibrary(psych)  Levene\u2019s test for homogeneity of variance.  # Create subsets for each training time\nwm_t08 <- subset(wm_t, cond == 't08')\nwm_t12 <- subset(wm_t, cond == 't12')\nwm_t17 <- subset(wm_t, cond == 't17')\nwm_t19 <- subset(wm_t, cond == 't19')\n\n# Summary statistics of the change in training scores before and after exercise\ndescribe(wm_t08)  ##       vars  n  mean   sd median trimmed  mad min  max range  skew kurtosis\n## cond*    1 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf    NA       NA\n## pre      2 20 10.05 1.50   10.0   10.06 1.48   8   12     4  0.01    -1.53\n## post     3 20 11.40 2.14   11.5   11.50 2.22   7   15     8 -0.25    -0.84\n## gain     4 20  1.35 1.23    1.0    1.44 1.48  -1    3     4 -0.32    -0.82\n## train    5 20  1.00 0.00    1.0    1.00 0.00   1    1     0   NaN      NaN\n##         se\n## cond*   NA\n## pre   0.34\n## post  0.48\n## gain  0.27\n## train 0.00  describe(wm_t12)  ##       vars  n mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20  NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20  9.9 1.45     10    9.88 1.48   8   12     4 0.16    -1.43\n## post     3 20 12.5 1.88     12   12.38 2.22  10   17     7 0.48    -0.54\n## gain     4 20  2.6 1.27      2    2.50 0.00   0    5     5 0.44    -0.54\n## train    5 20  1.0 0.00      1    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.32\n## post  0.42\n## gain  0.28\n## train 0.00  describe(wm_t17)  ##       vars  n mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20  NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20 10.0 1.34     10   10.00 1.48   8   12     4 0.25    -1.34\n## post     3 20 14.4 1.85     14   14.25 1.48  12   19     7 0.63    -0.27\n## gain     4 20  4.4 1.39      4    4.25 1.48   3    7     4 0.64    -1.12\n## train    5 20  1.0 0.00      1    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.30\n## post  0.41\n## gain  0.31\n## train 0.00  describe(wm_t19)  ##       vars  n  mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20 10.15 1.27   10.0   10.19 1.48   8   12     4 0.03    -1.10\n## post     3 20 15.75 1.86   16.0   15.69 1.48  13   19     6 0.16    -1.03\n## gain     4 20  5.60 1.73    5.5    5.50 2.22   3    9     6 0.36    -0.76\n## train    5 20  1.00 0.00    1.0    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.28\n## post  0.42\n## gain  0.39\n## train 0.00  # Create a boxplot of the different training times\nggplot(wm_t, aes(x = cond, y = gain, fill = cond)) + geom_boxplot()   # Levene's test\nlibrary(car)\n\nleveneTest(wm_t$gain ~ wm_t$cond)  ## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  3  1.3134 0.2763\n##       76  Conducting an independent t-test manually (1)  Perform an independent t-test the same way we did for the dependent t-test in the previous chapter. Continuing with the working memory example, our null hypothesis is that the difference in intelligence score gain between the group that trained for 8 days and the group that trained for 19 days is equal to zero. If our observed t-value is sufficiently large, we can reject the null in favor of the alternative hypothesis, which would imply a significant difference in intelligence gain between the two training groups.  Calculation of the observed t-value for an independent t-test is similar to the dependent t-test, but involves slightly  different formulas.  # Calculate mean difference by subtracting the gain for t08 by the gain for t19\nmean_t08 <- mean(wm_t08$gain)\nmean_t19 <- mean(wm_t19$gain)\nmean_diff <- (mean_t19 - mean_t08)\n\n# Calculate degrees of freedom\nn_t08 <- dim(wm_t08)[1]\nn_t19 <- dim(wm_t19)[1]\ndf <- n_t08 + n_t19 - 2\n\n# Calculate the pooled standard error\nvar_t08 <- (sum((wm_t08$gain - mean_t08)^2))/(n_t08 - 1)\nvar_t19 <- (sum((wm_t19$gain - mean_t19)^2))/(n_t19 - 1)\nse_pooled <- sqrt((var_t08/n_t08) + (var_t19/n_t19))  Conducting an independent t-test manually (2)  Compute the observed t-value. Then we will determine the p-value using the relevant t-distribution. If you recall, in the last chapter we calculated the critical value. The p-value is simply an alternative approach to hypothesis testing and determining the significance of your results. It\u2019s good to practice both! Finally, we will finish by calculating effect size via Cohen\u2019s d.  # All variables from the previous exercises are preloaded in your workspace\n# Type ls() to see them\n# Calculate the t-value\nt_value <- mean_diff/se_pooled\n\n# Calculate p-value\n#two-tail test, 0.05/2 = 0.025\np_value <- 2*(1-pt(t_value,df = df))\n\n# Calculate Cohen's d\nsd_t08 <- sd(wm_t08$gain)\nsd_t19 <- sd(wm_t19$gain)\npooled_sd <- (sd_t08 + sd_t19) / 2\ncohens_d <- mean_diff/pooled_sd  Letting R do all the dirty work: Independent t-tests  # Conduct an independent t-test \nt.test(wm_t19$gain, wm_t08$gain,var.equal = TRUE)  ## \n##  Two Sample t-test\n## \n## data:  wm_t19$gain and wm_t08$gain\n## t = 8.9677, df = 38, p-value = 6.443e-11\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  3.290588 5.209412\n## sample estimates:\n## mean of x mean of y \n##      5.60      1.35  # Calculate Cohen's d\ncohensD(wm_t19$gain, wm_t08$gain, method = 'pooled')  ## [1] 2.835822",
            "title": "2, Independent t-tests"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/",
            "text": "1, An introduction to ANOVA\n\n\n2, Post-hoc analysis\n\n\n3, Between groups factorial ANOVA\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, An introduction to ANOVA\n\u00b6\n\n\nWorking memory experiment\n\n\nWe\u2019ll use data from the working memory experiment, which investigates the relationship between the number of training days and a change in IQ. There are four independent groups, each of which trained for a different period of time: 8, 12, 17, or 19 days. The independent variable is the number of training days and the dependent variable is the IQ gain.\n\n\n# Print the data set in the console\nhead(wm)\n\n\n\n\n##   subject condition   iq\n## 1       1    8 days 12.4\n## 2       2    8 days 11.8\n## 3       3    8 days 14.6\n## 4       4    8 days  7.7\n## 5       5    8 days 15.7\n## 6       6    8 days 11.6\n\n\n\nlibrary(psych)\n\n# Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions)\ndescribeBy(wm, wm$condition)\n\n\n\n\n## \n##  Descriptive statistics by group \n## group: 12 days\n##            vars  n mean   sd median trimmed  mad  min  max range skew\n## subject       1 20 30.5 5.92  30.50   30.50 7.41 21.0 40.0  19.0 0.00\n## condition*    2 20  NaN   NA     NA     NaN   NA  Inf -Inf  -Inf   NA\n## iq            3 20 11.7 2.58  11.65   11.69 2.89  6.9 16.1   9.2 0.05\n##            kurtosis   se\n## subject       -1.38 1.32\n## condition*       NA   NA\n## iq            -1.06 0.58\n## -------------------------------------------------------- \n## group: 17 days\n##            vars  n mean   sd median trimmed  mad  min  max range skew\n## subject       1 20 50.5 5.92   50.5    50.5 7.41 41.0 60.0  19.0 0.00\n## condition*    2 20  NaN   NA     NA     NaN   NA  Inf -Inf  -Inf   NA\n## iq            3 20 13.9 2.26   13.6    13.9 2.00  9.8 18.1   8.3 0.11\n##            kurtosis   se\n## subject       -1.38 1.32\n## condition*       NA   NA\n## iq            -0.85 0.50\n## -------------------------------------------------------- \n## group: 19 days\n##            vars  n  mean   sd median trimmed  mad  min  max range  skew\n## subject       1 20 70.50 5.92   70.5   70.50 7.41 61.0 80.0  19.0  0.00\n## condition*    2 20   NaN   NA     NA     NaN   NA  Inf -Inf  -Inf    NA\n## iq            3 20 14.75 2.50   15.3   14.71 2.15 10.4 19.2   8.8 -0.09\n##            kurtosis   se\n## subject       -1.38 1.32\n## condition*       NA   NA\n## iq            -0.99 0.56\n## -------------------------------------------------------- \n## group: 8 days\n##            vars  n  mean   sd median trimmed  mad min  max range  skew\n## subject       1 20 10.50 5.92   10.5   10.50 7.41 1.0 20.0  19.0  0.00\n## condition*    2 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf    NA\n## iq            3 20 10.91 2.63   11.3   10.97 2.67 5.4 15.7  10.3 -0.21\n##            kurtosis   se\n## subject       -1.38 1.32\n## condition*       NA   NA\n## iq            -0.70 0.59\n\n\n\n# Boxplot IQ versus cond\nboxplot(wm$iq ~ wm$condition, main=\"Boxplot\", xlab=\"Group (cond)\", ylab=\"IQ\")\n\n\n\n\n\n\nNotice that the IQ increases as the amount of training sessions increases.\n\n\nt-test vs ANOVA\n\n\nANOVA is used when more than two group means are compared, whereas a t-test can only compare two group means.\n\n\nGenerate density plot of the F-distribution\n\n\nThe test statistic associated with ANOVA is the F-test (or F-ratio). Recall that when carrying out a t-test, you computed an observed t-value, then compared that with a critical value derived from the relevant t-distribution. That t-distribution came from a family of t-distributions, each of which was defined entirely by its degrees of freedom.\n\n\nANOVA uses the same principle, but instead an observed F-value is computed and compared to the relevant F-distribution. That F-distribution comes from a family of F-distributions, each of which is defined by two numbers (i.e. degrees of freedom).\n\n\nF-distribution has a different shape than the t-distribution.\n\n\n# Create the vector x\nx <- seq(from = 0, to = 2, length = 100)\n\n# Simulate the F-distributions\ny_1 <- df(x, 1, 1)\ny_2 <- df(x, 3, 1)\ny_3 <- df(x, 6, 1)\ny_4 <- df(x, 3, 3)\ny_5 <- df(x, 6, 3)\ny_6 <- df(x, 3, 6)\ny_7 <- df(x, 6, 6)\n\n# Plot the F-distributions\nplot(x, y_1, col = 1, 'l')\nlines(x,y_2, col = 2, 'l')\nlines(x,y_3, col = 3, 'l')\nlines(x,y_4, col = 4, 'l')\nlines(x,y_5, col = 5, 'l')\nlines(x,y_6, col = 6, 'l')\nlines(x,y_7, col = 7, 'l')\n\n# Add the legend in the top right corner and with the title 'F distributions'\nlegend('topright', title = 'F distributions', c('df = (1,1)', 'df = (3,1)', 'df = (6,1)', 'df = (3,3)', 'df = (6,3)', 'df = (3,6)', 'df = (6,6)'), col = c(1, 2, 3, 4, 5, 6, 7), lty = 1)\n\n\n\n\n\n\nThe F-distribution cannot take negative values, because it is a ratio of variances and variances are always non-negative numbers. The distribution represents the ratio between the variance between groups and the variance within groups.\n\n\nBetween group sum of squares\n\n\nTo calculate the F-value, you need to calculate the ratio between the variance between groups and the variance within groups. Furthermore, to calculate the variance (i.e. mean of squares), you first have to calculate the sum of squares.\n\n\nNow, remember that the working memory experiment investigates the relationship between the change in IQ and the number of training sessions. Calculate the between group sum of squares for the data from this experiment.\n\n\n# Define number of subjects in each group\nn <- 20\n\n# Calculate group means\nY_j <- as.numeric(tapply(wm$iq, wm$condition, mean))\nY_j\n\n\n\n\n## [1] 11.700 13.905 14.750 10.910\n\n\n\n# Calculate the grand mean\nY_T <- mean(wm$iq)\nY_T\n\n\n\n\n## [1] 12.81625\n\n\n\n# Calculate the sum of squares\nSS_A <- sum((Y_j - Y_T)^2) * n\nSS_A\n\n\n\n\n## [1] 196.0914\n\n\n\nWithin groups sum of squares\n\n\nTo calculate the F-value, you also need the variance within groups. Similar to the last exercise, we\u2019ll start by computing the within groups sum of squares.\n\n\n# Create four subsets of the four groups, containing the IQ results\n\n# Make the subset for the group cond = '8 days'\nY_i1 <- subset(wm$iq, wm$condition == '8 days')\n\n# Make the subset for the group cond = '12 days'\nY_i2 <- subset(wm$iq, wm$condition == '12 days')\n\n# Make the subset for the group cond = '17 days'\nY_i3 <- subset(wm$iq, wm$condition == '17 days')\n\n# Make the subset for the group cond = '19 days'\nY_i4 <- subset(wm$iq, wm$condition == '19 days')\n\n# subtract the individual values by their group means\n# You have already calculated the group means in the previous exercise so use this result, the vector that contains these group means was called Y_j\nS_1 <- Y_i1 - Y_j[1]\nS_2 <- Y_i2 - Y_j[2]\n\n# Do it without the vector Y_j, so calculate the group means again.\nS_3 <- Y_i3 - mean(Y_i3)\nS_4 <- Y_i4 - mean(Y_i4)\n\n#Put everything back in one vector\nS_T <- c(S_1,S_2,S_3,S_4)\n\n#Calculate the sum of squares by using the vector S_T\nSS_SA <- sum(S_T^2)\n\n\n\n\nCalculating the F-ratio\n\n\nCalculate the F-ratio.\n\n\n# Number of groups\na <- 4\n\n# Number of subject in each group\nn <- 20\n\n# Define the degrees of freedom\ndf_A <- a - 1\ndf_SA <- a*(n - 1)\n\n# Calculate the mean squares (variances) by using the sum of squares SS_A and SS_SA\nMS_A <- SS_A/df_A\nMS_SA <- SS_SA/df_SA\n\n# Calculate the F-ratio\nF <- MS_A/MS_SA\n\n\n\n\nA faster way: ANOVA in R\n\n\nNormally, we do not have to do all calculations.\n\n\n# Apply the aov function\nanova.wm <- aov(wm$iq ~ wm$condition)\nanova.wm\n\n\n\n\n## Call:\n##    aov(formula = wm$iq ~ wm$condition)\n## \n## Terms:\n##                 wm$condition Residuals\n## Sum of Squares      196.0914  473.4175\n## Deg. of Freedom            3        76\n## \n## Residual standard error: 2.495832\n## Estimated effects may be unbalanced\n\n\n\n# Look at the summary table of the result\nsummary(anova.wm)\n\n\n\n\n##              Df Sum Sq Mean Sq F value   Pr(>F)    \n## wm$condition  3  196.1   65.36   10.49 7.47e-06 ***\n## Residuals    76  473.4    6.23                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nF-value is significant.\n\n\nLevene\u2019s test\n\n\nThe assumptions of ANOVA are relatively simple. Similar to an independent t-test, we have a continuous dependent variable, which we assume to be normally distributed. Furthermore, we assume homogeneity of variance, which can be tested with Levene\u2019s test.\n\n\nlibrary(car)\n\n# Levene's test\nleveneTest(wm$iq ~ wm$condition)\n\n\n\n\n## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  3  0.1405 0.9355\n##       76\n\n\n\n# Levene's test with the change for the default, namely center = mean\nleveneTest(wm$iq ~ wm$condition, center = mean)\n\n\n\n\n## Levene's Test for Homogeneity of Variance (center = mean)\n##       Df F value Pr(>F)\n## group  3  0.1598  0.923\n##       76\n\n\n\nThe assumption of homogeneity of variance hold: the within group variance equivalent for all groups.\n\n\n2, Post-hoc analysis\n\u00b6\n\n\nPost-hoc tests help finding out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.\n\n\nWhat does it mean to inflate the type I error?\n\n\nSuppose the post-hoc test involves performing three pairwise comparisons, each with the probability of a type I error set at 5%. The probability of making at least one type I error: if you assume independence of these three events, the maximum familywise error rate is then equal to: 1 - (0.95 x 0.95 x 0.95) = 14.26 %.\n\n\nIn other words, the probability of having at least one false alarm (i.e. type I error) is 14.26%.\n\n\nWhat is the maximum familywise error rate for the working memory experiment, assuming that you do all possible pairwise comparisons with a type I error of 5%? 26.49%.\n\n\nNull Hypothesis Significance Testing (NHST) is a statistical method used to test whether or not you are able to reject or retain the null hypothesis. This type of test can confront you with a type I error. This happens when the test rejects the null hypothesis, while it is actually true in reality. Furthermore, the test can also deliver a type II error. This is the failure to reject a null hypothesis when it is false. All hypothesis tests have a probability of making type I and II errors.\n\n\nSensitivity and specificity are two concepts that statisticians use to measure the performance of a statistical test. The sensitivity of a test is its true positive rate:\n\n\n\n\nsensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives}\n\n\n\n\nThe specificity of a test is its true negative rate:\n\n\n\n\nspecificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives}\n\n\n\n\nCalculate both the sensitivity and specificity of the test based on numbers displayed in the NHST table?\n\n\n\n\nThe sensitivity is 0.89 and the specificity is 0.85.\n\n\nCalculate and interpret the results of Tukey\n\n\nIn a situation were you do multiple pairwise comparisons, the probability of type I errors in the process inflates substantially. Therefore, it is better to build in adjustments to take this into account. This is what Tukey tests and other post-hoc procedures do. They adjust the p-value to prevent inflation of the type I error rate. Use Tukey\u2019s procedure.\n\n\n# Read in the data set and assign to the object\nwm <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(wm)\n\n\n\n\n##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1\n\n\n\n# Revision: Analysis of variance\nanova_wm <- aov(wm$gain ~ wm$cond)\n\n# Summary Analysis of Variance\nsummary(anova_wm)\n\n\n\n\n##              Df Sum Sq Mean Sq F value Pr(>F)    \n## wm$cond       4  274.0   68.51   34.57 <2e-16 ***\n## Residuals   115  227.9    1.98                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Post-hoc (Tukey)\nTukeyHSD(anova_wm)\n\n\n\n\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = wm$gain ~ wm$cond)\n## \n## $`wm$cond`\n##               diff         lwr       upr     p adj\n## t08-control -0.625 -1.69355785 0.4435578 0.4871216\n## t12-control  0.625 -0.44355785 1.6935578 0.4871216\n## t17-control  2.425  1.35644215 3.4935578 0.0000001\n## t19-control  3.625  2.55644215 4.6935578 0.0000000\n## t12-t08      1.250  0.01613568 2.4838643 0.0454650\n## t17-t08      3.050  1.81613568 4.2838643 0.0000000\n## t19-t08      4.250  3.01613568 5.4838643 0.0000000\n## t17-t12      1.800  0.56613568 3.0338643 0.0008953\n## t19-t12      3.000  1.76613568 4.2338643 0.0000000\n## t19-t17      1.200 -0.03386432 2.4338643 0.0607853\n\n\n\n# Plot confidence intervals\n#plot(c(TukeyHSD(anova_wm)$lwr,TukeyHSD(anova_wm)$upr))\nplot(TukeyHSD(anova_wm))\n\n\n\n\n\n\nBonferroni adjusted p-values\n\n\nJust like Tukey\u2019s procedure, the Bonferroni correction is a method that is used to counteract the problem of inflated type I errors while engaging in multiple pairwise comparisons between subgroups. Bonferroni is generally known as the most conservative method to control the familywise error rate.\n\n\nBonferroni is based on the idea that if you test \nN\n dependent or independent hypotheses, one way of maintaining the familywise error rate is to test each individual hypothesis at a statistical significance level that is deflated by a factor of \n\\frac{1}{n}\n. So, for a significance level for the whole family of tests of \n\u03b1\n, the Bonferroni correction would be to test each of the individual tests at a significance level of \n\\frac{\\alpha}{n}\n.\n\n\nThe Bonferroni correction is controversial. It is a strict measure to limit false positives and generates conservative p-value. Alternative: increase the sample size, compute the false discovery rate (the expected percent of false predictions in the set of predictions. For example if the algorithm returns 100 results with a false discovery rate of .3 then we should expect 70 of them to be correct.), and the Holm-Bonferroni method.\n\n\n# Use `p.adjust` \nbonferroni_ex <- p.adjust(.005, method='bonferroni', n = 8)\n\nbonferroni_ex\n\n\n\n\n## [1] 0.04\n\n\n\n# Pairwise T-test\npairwise.t.test(wm$gain,wm$cond, p.adjust = 'bonferroni')\n\n\n\n\n## \n##  Pairwise comparisons using t tests with pooled SD \n## \n## data:  wm$gain and wm$cond \n## \n##     control t08     t12     t17    \n## t08 1.00000 -       -       -      \n## t12 1.00000 0.05862 -       -      \n## t17 5.9e-08 3.8e-09 0.00096 -      \n## t19 6.4e-15 2.9e-15 6.7e-09 0.08084\n## \n## P value adjustment method: bonferroni\n\n\n\n3, Between groups factorial ANOVA\n\u00b6\n\n\nData exploration with a barplot\n\n\nWe\u2019ll use in this chapter is a randomized controlled experiment investigating the effects of talking on a cell phone while driving. The dependent variable in the experiment is the number of driving errors that subjects made in a driving simulator. There are two independent variables:\n\n\n\n\nConversation difficulty: None, Low, High\n\n\nDriving difficulty: Easy, Difficult\n\n\n\n\n\n\n\n# Read in the data set and assign to the object\nab <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'ab', header = TRUE, startCol = 1, startRow = 1)\n\nab$subject <- as.integer(ab$subject)\nab$conversation <- as.factor(ab$conversation)\nab$driving <- as.factor(ab$driving)\nab$error <- as.integer(ab$error)\n\n# This will print the data set in the console\nhead(ab)\n\n\n\n\n##   subject conversation driving errors error\n## 1       1         None    Easy     20    20\n## 2       2         None    Easy     19    19\n## 3       3         None    Easy     31    31\n## 4       4         None    Easy     27    27\n## 5       5         None    Easy     31    31\n## 6       6         None    Easy     17    17\n\n\n\nEach of the subjects was randomly assigned to one of six conditions formed by combining different values of the independent variables.\n\n\n\n\nsubject\n: unique identifier for each subject conversation: level of conversation difficulty.\n\n\ndriving\n : level of driving difficulty in the simulator.\n\n\nerrors\n: number of driving errors made.\n\n\n\n\n\n\n\n# Use the tapply function to create your groups\nab_groups <- tapply(ab$errors, list(ab$driving, ab$conversation), sum)\n\n# Make the required barplot\nbarplot(ab_groups, beside = TRUE, col = c('orange','blue'), main = 'Driving Errors', xlab = 'Conversation Demands', ylab = 'Errors')\n\n# Add the legend\nlegend('topright', c('Difficult','Easy'), title = 'Driving', fill = c('orange','blue'))\n\n\n\n\n\n\nThe driving errors made during different driving conditions are influenced by the level of conversation demand. In other words, the driving conditions have a different effect on the number of errors made, depending on the level of conversation demand.\n\n\nThe homogeneity of variance assumption\n\n\nBefore we do factorial ANOVA, we need to test the homogeneity of the variance assumption. When studying one-way ANOVA, we tested this assumption with the \nleveneTest\n function.\n\n\nWe now have two independent variables instead of just one.\n\n\n# Test the homogeneity of variance assumption\nleveneTest(ab$errors ~ ab$conversation * ab$driving)\n\n\n\n\n## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value Pr(>F)\n## group   5  0.5206 0.7602\n##       114\n\n\n\nThe homogeneity of variance assumption holds.\n\n\nBy performing a \nleveneTest\n, we can check whether or not the homogeneity of variance assumption holds for a given dataset. The assumption must hold for the results of an ANOVA analysis to be valid.\n\n\nRecall from the first chapter that ANOVA makes use of F-statistics, or F-ratios, in which two types of degrees of freedom are involved.\n\n\ndim(ab)\n\n\n\n\n## [1] 120   5\n\n\n\nstr(ab$conversation)\n\n\n\n\n##  Factor w/ 3 levels \"High demand\",..: 3 3 3 3 3 3 3 3 3 3 ...\n\n\n\nstr(ab$driving)\n\n\n\n\n##  Factor w/ 2 levels \"Difficult\",\"Easy\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\nThere are 120 subjets and 2 (Easy, Difficult) * 3 (High Demand, Low Demand, None) = 6 groups.\n\n\nThe factorial ANOVA\n\n\n# Factorial ANOVA\nab_model <- aov(ab$errors ~ ab$conversation * ab$driving)\n\n# Get the summary table\nsummary(ab_model)\n\n\n\n\n##                             Df Sum Sq Mean Sq F value   Pr(>F)    \n## ab$conversation              2   4416    2208   36.14 6.98e-13 ***\n## ab$driving                   1   5782    5782   94.64  < 2e-16 ***\n## ab$conversation:ab$driving   2   1639     820   13.41 5.86e-06 ***\n## Residuals                  114   6965      61                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nBased on the summary table of the factorial ANOVA, the main effect for driving difficulty, the main effect for conversation difficulty and the interaction effect are all significant.\n\n\nThe interaction effect\n\n\nNow it\u2019s time to explore the interaction effect. You will do this with the help of a simple effects analysis.\n\n\nWhy a simple effects analysis? Well, remember what we had to do when you had a significant main effect in a one-way ANOVA? There, you just had to perform some post-hoc tests to see from which level of the categorical variable the main effect was coming. With an interaction effect, it is quite similar. Conduct a simple effects analysis of the variable \nconversation\n on the outcome variable \nerrors\n at each level of \ndriving\n.\n\n\n# Create the two subsets\nab_1 <- subset(ab, ab$driving == 'Easy')  \nab_2 <- subset(ab, ab$driving == 'Difficult')\n\n# Perform the one-way ANOVA for both subsets\naov_ab_1 <- aov(ab_1$errors ~ ab_1$conversation)\naov_ab_2 <- aov(ab_2$errors ~ ab_2$conversation)\n\n# Get the summary tables for both aov_ab_1 and aov_ab_2\nsummary(aov_ab_1)\n\n\n\n\n##                   Df Sum Sq Mean Sq F value Pr(>F)  \n## ab_1$conversation  2  504.7   252.3   4.928 0.0106 *\n## Residuals         57 2918.5    51.2                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(aov_ab_2)\n\n\n\n\n##                   Df Sum Sq Mean Sq F value   Pr(>F)    \n## ab_2$conversation  2   5551    2776   39.09 2.05e-11 ***\n## Residuals         57   4047      71                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThere a significant simple effect for the easy driving condition based on the summary table of \naov_ab_1\n.\n\n\nThere a significant simple effect for the difficult driving condition based on the summary table of \naov_ab_2\n.\n\n\nThe effect sizes\n\n\nThe definition of an interaction effect states that the effect of one variable changes across levels of the other variable. For example, we might expect the effect of conversation to be greater when driving conditions are difficult than when they are relatively easy.\n\n\nUnfortunately, it is not quite that simple. In order to really understand the different effect sizes, you should make use of the \netaSquared\n function.\n\n\nlibrary(lsr)\n\n# Calculate the etaSquared for the easy driving case\n#easy is ab_1\n#difficult is ab_2\netaSquared(aov_ab_1, anova = TRUE)\n\n\n\n\n##                     eta.sq eta.sq.part      SS df        MS        F\n## ab_1$conversation 0.147433    0.147433  504.70  2 252.35000 4.928458\n## Residuals         0.852567          NA 2918.55 57  51.20263       NA\n##                            p\n## ab_1$conversation 0.01061116\n## Residuals                 NA\n\n\n\n# Calculate the etaSquared for the difficult driving case\netaSquared(aov_ab_2, anova = TRUE)\n\n\n\n\n##                      eta.sq eta.sq.part       SS df         MS        F\n## ab_2$conversation 0.5783571   0.5783571 5551.033  2 2775.51667 39.09275\n## Residuals         0.4216429          NA 4046.900 57   70.99825       NA\n##                              p\n## ab_2$conversation 2.046097e-11\n## Residuals                   NA\n\n\n\nBased on the output of the \netaSquared\n function for the easy driving condition, the percentage of variance explained by the conversation variable is 14.7%; the percentage of variance explained by the conversation variable is 57.8%.\n\n\nPairwise comparisons\n\n\nFinally, let us look at pairwise comparisons for the simple effects. You can do this with the Tukey post-hoc test.\n\n\n# Tukey for easy driving\nTukeyHSD(aov_ab_1)\n\n\n\n\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = ab_1$errors ~ ab_1$conversation)\n## \n## $`ab_1$conversation`\n##                         diff        lwr        upr     p adj\n## Low demand-High demand -6.05 -11.495243 -0.6047574 0.0260458\n## None-High demand       -6.25 -11.695243 -0.8047574 0.0207614\n## None-Low demand        -0.20  -5.645243  5.2452426 0.9957026\n\n\n\n# Tukey for difficult driving\nTukeyHSD(aov_ab_2)\n\n\n\n\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = ab_2$errors ~ ab_2$conversation)\n## \n## $`ab_2$conversation`\n##                          diff       lwr        upr     p adj\n## Low demand-High demand  -9.75 -16.16202  -3.337979 0.0015849\n## None-High demand       -23.45 -29.86202 -17.037979 0.0000000\n## None-Low demand        -13.70 -20.11202  -7.287979 0.0000103\n\n\n\nFor \u2018Easy Driving\u2019, two mean differences in terms of number of errors are significant (below 0.05).\n\n\nFor \u2018Difficult Driving\u2019, three mean differences in terms of number of errors are significant (below 0.05).",
            "title": "Statistics with R, Course Three, Analysis of Variance"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/#2-post-hoc-analysis",
            "text": "Post-hoc tests help finding out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.  What does it mean to inflate the type I error?  Suppose the post-hoc test involves performing three pairwise comparisons, each with the probability of a type I error set at 5%. The probability of making at least one type I error: if you assume independence of these three events, the maximum familywise error rate is then equal to: 1 - (0.95 x 0.95 x 0.95) = 14.26 %.  In other words, the probability of having at least one false alarm (i.e. type I error) is 14.26%.  What is the maximum familywise error rate for the working memory experiment, assuming that you do all possible pairwise comparisons with a type I error of 5%? 26.49%.  Null Hypothesis Significance Testing (NHST) is a statistical method used to test whether or not you are able to reject or retain the null hypothesis. This type of test can confront you with a type I error. This happens when the test rejects the null hypothesis, while it is actually true in reality. Furthermore, the test can also deliver a type II error. This is the failure to reject a null hypothesis when it is false. All hypothesis tests have a probability of making type I and II errors.  Sensitivity and specificity are two concepts that statisticians use to measure the performance of a statistical test. The sensitivity of a test is its true positive rate:   sensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives}   The specificity of a test is its true negative rate:   specificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives}   Calculate both the sensitivity and specificity of the test based on numbers displayed in the NHST table?   The sensitivity is 0.89 and the specificity is 0.85.  Calculate and interpret the results of Tukey  In a situation were you do multiple pairwise comparisons, the probability of type I errors in the process inflates substantially. Therefore, it is better to build in adjustments to take this into account. This is what Tukey tests and other post-hoc procedures do. They adjust the p-value to prevent inflation of the type I error rate. Use Tukey\u2019s procedure.  # Read in the data set and assign to the object\nwm <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(wm)  ##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1  # Revision: Analysis of variance\nanova_wm <- aov(wm$gain ~ wm$cond)\n\n# Summary Analysis of Variance\nsummary(anova_wm)  ##              Df Sum Sq Mean Sq F value Pr(>F)    \n## wm$cond       4  274.0   68.51   34.57 <2e-16 ***\n## Residuals   115  227.9    1.98                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # Post-hoc (Tukey)\nTukeyHSD(anova_wm)  ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = wm$gain ~ wm$cond)\n## \n## $`wm$cond`\n##               diff         lwr       upr     p adj\n## t08-control -0.625 -1.69355785 0.4435578 0.4871216\n## t12-control  0.625 -0.44355785 1.6935578 0.4871216\n## t17-control  2.425  1.35644215 3.4935578 0.0000001\n## t19-control  3.625  2.55644215 4.6935578 0.0000000\n## t12-t08      1.250  0.01613568 2.4838643 0.0454650\n## t17-t08      3.050  1.81613568 4.2838643 0.0000000\n## t19-t08      4.250  3.01613568 5.4838643 0.0000000\n## t17-t12      1.800  0.56613568 3.0338643 0.0008953\n## t19-t12      3.000  1.76613568 4.2338643 0.0000000\n## t19-t17      1.200 -0.03386432 2.4338643 0.0607853  # Plot confidence intervals\n#plot(c(TukeyHSD(anova_wm)$lwr,TukeyHSD(anova_wm)$upr))\nplot(TukeyHSD(anova_wm))   Bonferroni adjusted p-values  Just like Tukey\u2019s procedure, the Bonferroni correction is a method that is used to counteract the problem of inflated type I errors while engaging in multiple pairwise comparisons between subgroups. Bonferroni is generally known as the most conservative method to control the familywise error rate.  Bonferroni is based on the idea that if you test  N  dependent or independent hypotheses, one way of maintaining the familywise error rate is to test each individual hypothesis at a statistical significance level that is deflated by a factor of  \\frac{1}{n} . So, for a significance level for the whole family of tests of  \u03b1 , the Bonferroni correction would be to test each of the individual tests at a significance level of  \\frac{\\alpha}{n} .  The Bonferroni correction is controversial. It is a strict measure to limit false positives and generates conservative p-value. Alternative: increase the sample size, compute the false discovery rate (the expected percent of false predictions in the set of predictions. For example if the algorithm returns 100 results with a false discovery rate of .3 then we should expect 70 of them to be correct.), and the Holm-Bonferroni method.  # Use `p.adjust` \nbonferroni_ex <- p.adjust(.005, method='bonferroni', n = 8)\n\nbonferroni_ex  ## [1] 0.04  # Pairwise T-test\npairwise.t.test(wm$gain,wm$cond, p.adjust = 'bonferroni')  ## \n##  Pairwise comparisons using t tests with pooled SD \n## \n## data:  wm$gain and wm$cond \n## \n##     control t08     t12     t17    \n## t08 1.00000 -       -       -      \n## t12 1.00000 0.05862 -       -      \n## t17 5.9e-08 3.8e-09 0.00096 -      \n## t19 6.4e-15 2.9e-15 6.7e-09 0.08084\n## \n## P value adjustment method: bonferroni",
            "title": "2, Post-hoc analysis"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/#3-between-groups-factorial-anova",
            "text": "Data exploration with a barplot  We\u2019ll use in this chapter is a randomized controlled experiment investigating the effects of talking on a cell phone while driving. The dependent variable in the experiment is the number of driving errors that subjects made in a driving simulator. There are two independent variables:   Conversation difficulty: None, Low, High  Driving difficulty: Easy, Difficult    # Read in the data set and assign to the object\nab <- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'ab', header = TRUE, startCol = 1, startRow = 1)\n\nab$subject <- as.integer(ab$subject)\nab$conversation <- as.factor(ab$conversation)\nab$driving <- as.factor(ab$driving)\nab$error <- as.integer(ab$error)\n\n# This will print the data set in the console\nhead(ab)  ##   subject conversation driving errors error\n## 1       1         None    Easy     20    20\n## 2       2         None    Easy     19    19\n## 3       3         None    Easy     31    31\n## 4       4         None    Easy     27    27\n## 5       5         None    Easy     31    31\n## 6       6         None    Easy     17    17  Each of the subjects was randomly assigned to one of six conditions formed by combining different values of the independent variables.   subject : unique identifier for each subject conversation: level of conversation difficulty.  driving  : level of driving difficulty in the simulator.  errors : number of driving errors made.    # Use the tapply function to create your groups\nab_groups <- tapply(ab$errors, list(ab$driving, ab$conversation), sum)\n\n# Make the required barplot\nbarplot(ab_groups, beside = TRUE, col = c('orange','blue'), main = 'Driving Errors', xlab = 'Conversation Demands', ylab = 'Errors')\n\n# Add the legend\nlegend('topright', c('Difficult','Easy'), title = 'Driving', fill = c('orange','blue'))   The driving errors made during different driving conditions are influenced by the level of conversation demand. In other words, the driving conditions have a different effect on the number of errors made, depending on the level of conversation demand.  The homogeneity of variance assumption  Before we do factorial ANOVA, we need to test the homogeneity of the variance assumption. When studying one-way ANOVA, we tested this assumption with the  leveneTest  function.  We now have two independent variables instead of just one.  # Test the homogeneity of variance assumption\nleveneTest(ab$errors ~ ab$conversation * ab$driving)  ## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value Pr(>F)\n## group   5  0.5206 0.7602\n##       114  The homogeneity of variance assumption holds.  By performing a  leveneTest , we can check whether or not the homogeneity of variance assumption holds for a given dataset. The assumption must hold for the results of an ANOVA analysis to be valid.  Recall from the first chapter that ANOVA makes use of F-statistics, or F-ratios, in which two types of degrees of freedom are involved.  dim(ab)  ## [1] 120   5  str(ab$conversation)  ##  Factor w/ 3 levels \"High demand\",..: 3 3 3 3 3 3 3 3 3 3 ...  str(ab$driving)  ##  Factor w/ 2 levels \"Difficult\",\"Easy\": 2 2 2 2 2 2 2 2 2 2 ...  There are 120 subjets and 2 (Easy, Difficult) * 3 (High Demand, Low Demand, None) = 6 groups.  The factorial ANOVA  # Factorial ANOVA\nab_model <- aov(ab$errors ~ ab$conversation * ab$driving)\n\n# Get the summary table\nsummary(ab_model)  ##                             Df Sum Sq Mean Sq F value   Pr(>F)    \n## ab$conversation              2   4416    2208   36.14 6.98e-13 ***\n## ab$driving                   1   5782    5782   94.64  < 2e-16 ***\n## ab$conversation:ab$driving   2   1639     820   13.41 5.86e-06 ***\n## Residuals                  114   6965      61                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  Based on the summary table of the factorial ANOVA, the main effect for driving difficulty, the main effect for conversation difficulty and the interaction effect are all significant.  The interaction effect  Now it\u2019s time to explore the interaction effect. You will do this with the help of a simple effects analysis.  Why a simple effects analysis? Well, remember what we had to do when you had a significant main effect in a one-way ANOVA? There, you just had to perform some post-hoc tests to see from which level of the categorical variable the main effect was coming. With an interaction effect, it is quite similar. Conduct a simple effects analysis of the variable  conversation  on the outcome variable  errors  at each level of  driving .  # Create the two subsets\nab_1 <- subset(ab, ab$driving == 'Easy')  \nab_2 <- subset(ab, ab$driving == 'Difficult')\n\n# Perform the one-way ANOVA for both subsets\naov_ab_1 <- aov(ab_1$errors ~ ab_1$conversation)\naov_ab_2 <- aov(ab_2$errors ~ ab_2$conversation)\n\n# Get the summary tables for both aov_ab_1 and aov_ab_2\nsummary(aov_ab_1)  ##                   Df Sum Sq Mean Sq F value Pr(>F)  \n## ab_1$conversation  2  504.7   252.3   4.928 0.0106 *\n## Residuals         57 2918.5    51.2                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  summary(aov_ab_2)  ##                   Df Sum Sq Mean Sq F value   Pr(>F)    \n## ab_2$conversation  2   5551    2776   39.09 2.05e-11 ***\n## Residuals         57   4047      71                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  There a significant simple effect for the easy driving condition based on the summary table of  aov_ab_1 .  There a significant simple effect for the difficult driving condition based on the summary table of  aov_ab_2 .  The effect sizes  The definition of an interaction effect states that the effect of one variable changes across levels of the other variable. For example, we might expect the effect of conversation to be greater when driving conditions are difficult than when they are relatively easy.  Unfortunately, it is not quite that simple. In order to really understand the different effect sizes, you should make use of the  etaSquared  function.  library(lsr)\n\n# Calculate the etaSquared for the easy driving case\n#easy is ab_1\n#difficult is ab_2\netaSquared(aov_ab_1, anova = TRUE)  ##                     eta.sq eta.sq.part      SS df        MS        F\n## ab_1$conversation 0.147433    0.147433  504.70  2 252.35000 4.928458\n## Residuals         0.852567          NA 2918.55 57  51.20263       NA\n##                            p\n## ab_1$conversation 0.01061116\n## Residuals                 NA  # Calculate the etaSquared for the difficult driving case\netaSquared(aov_ab_2, anova = TRUE)  ##                      eta.sq eta.sq.part       SS df         MS        F\n## ab_2$conversation 0.5783571   0.5783571 5551.033  2 2775.51667 39.09275\n## Residuals         0.4216429          NA 4046.900 57   70.99825       NA\n##                              p\n## ab_2$conversation 2.046097e-11\n## Residuals                   NA  Based on the output of the  etaSquared  function for the easy driving condition, the percentage of variance explained by the conversation variable is 14.7%; the percentage of variance explained by the conversation variable is 57.8%.  Pairwise comparisons  Finally, let us look at pairwise comparisons for the simple effects. You can do this with the Tukey post-hoc test.  # Tukey for easy driving\nTukeyHSD(aov_ab_1)  ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = ab_1$errors ~ ab_1$conversation)\n## \n## $`ab_1$conversation`\n##                         diff        lwr        upr     p adj\n## Low demand-High demand -6.05 -11.495243 -0.6047574 0.0260458\n## None-High demand       -6.25 -11.695243 -0.8047574 0.0207614\n## None-Low demand        -0.20  -5.645243  5.2452426 0.9957026  # Tukey for difficult driving\nTukeyHSD(aov_ab_2)  ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = ab_2$errors ~ ab_2$conversation)\n## \n## $`ab_2$conversation`\n##                          diff       lwr        upr     p adj\n## Low demand-High demand  -9.75 -16.16202  -3.337979 0.0015849\n## None-High demand       -23.45 -29.86202 -17.037979 0.0000000\n## None-Low demand        -13.70 -20.11202  -7.287979 0.0000103  For \u2018Easy Driving\u2019, two mean differences in terms of number of errors are significant (below 0.05).  For \u2018Difficult Driving\u2019, three mean differences in terms of number of errors are significant (below 0.05).",
            "title": "3, Between groups factorial ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Four,_Repeated_Measures_Anova/",
            "text": "1, An introduction to repeated measures\n\n\n2, Repeated measures ANOVA\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, An introduction to repeated measures\n\u00b6\n\n\nThe independent t-test is analogous to between-groups ANOVA and the paired-sample t-test is analogous to repeated measures ANOVA.\n\n\nIn a between-groups design, each subject is exposed to two or more treatments or conditions over time. In a within-subjects design, each subject is allocated to exactly one treatment or condition.\n\n\nBetween:\n\n\n\n\nExperiment 1: You want to test the effect of alcohol on test scores of students. There are three conditions: the student consumed no alcohol, two glasses of beer, or five glasses of beer. Alcohol tolerance and time spent studying should also be considered somehow.\n\n\nExperiment 2: You want to investigate the effects of certain fertilizers on plant growth. Assume you have two different\n\n    fertilizers, A and B. Consider three conditions: you give the plant no fertilizer, fertilizer A, or fertilizer B. You measure the height of the plant after a specific period of time to see whether the fertilizers had an effect.\n\n\n\n\nUse a within-subjects design for for Experiment 1 and between-groups design for Experiment 2.\n\n\nIs it always either manipulation between-groups or manipulation within-groups, or are there experiments where you could use either approach? In some cases, either approach is possible.\n\n\nExplore the working memory data\n\n\n# Print the data set in the console\nstr(wm)\n\n\n\n\n## 'data.frame':    80 obs. of  3 variables:\n##  $ subject  : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ condition: Factor w/ 4 levels \"12 days\",\"17 days\",..: 4 4 4 4 4 4 4 4 4 4 ...\n##  $ iq       : num  12.4 11.8 14.6 7.7 15.7 11.6 7 8.4 10.7 10.6 ...\n\n\n\nlibrary(psych)\nlibrary(ggplot2)\n\n# Define the variable subject as a categorical variable\nwm$subject <- factor(wm$subject)\n\n# Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions)\ndescribeBy(wm, wm$condition)\n\n\n\n\n## \n##  Descriptive statistics by group \n## group: 12 days\n##            vars  n mean   sd median trimmed  mad min  max range skew\n## subject*      1 20 10.5 5.92  10.50   10.50 7.41 1.0 20.0  19.0 0.00\n## condition*    2 20  1.0 0.00   1.00    1.00 0.00 1.0  1.0   0.0  NaN\n## iq            3 20 11.7 2.58  11.65   11.69 2.89 6.9 16.1   9.2 0.05\n##            kurtosis   se\n## subject*      -1.38 1.32\n## condition*      NaN 0.00\n## iq            -1.06 0.58\n## -------------------------------------------------------- \n## group: 17 days\n##            vars  n mean   sd median trimmed  mad min  max range skew\n## subject*      1 20 10.5 5.92   10.5    10.5 7.41 1.0 20.0  19.0 0.00\n## condition*    2 20  2.0 0.00    2.0     2.0 0.00 2.0  2.0   0.0  NaN\n## iq            3 20 13.9 2.26   13.6    13.9 2.00 9.8 18.1   8.3 0.11\n##            kurtosis   se\n## subject*      -1.38 1.32\n## condition*      NaN 0.00\n## iq            -0.85 0.50\n## -------------------------------------------------------- \n## group: 19 days\n##            vars  n  mean   sd median trimmed  mad  min  max range  skew\n## subject*      1 20 10.50 5.92   10.5   10.50 7.41  1.0 20.0  19.0  0.00\n## condition*    2 20  3.00 0.00    3.0    3.00 0.00  3.0  3.0   0.0   NaN\n## iq            3 20 14.75 2.50   15.3   14.71 2.15 10.4 19.2   8.8 -0.09\n##            kurtosis   se\n## subject*      -1.38 1.32\n## condition*      NaN 0.00\n## iq            -0.99 0.56\n## -------------------------------------------------------- \n## group: 8 days\n##            vars  n  mean   sd median trimmed  mad min  max range  skew\n## subject*      1 20 10.50 5.92   10.5   10.50 7.41 1.0 20.0  19.0  0.00\n## condition*    2 20  4.00 0.00    4.0    4.00 0.00 4.0  4.0   0.0   NaN\n## iq            3 20 10.91 2.63   11.3   10.97 2.67 5.4 15.7  10.3 -0.21\n##            kurtosis   se\n## subject*      -1.38 1.32\n## condition*      NaN 0.00\n## iq            -0.70 0.59\n\n\n\n# Boxplot IQ versus condition\nboxplot(wm$iq ~ wm$condition, main = 'Boxplot', xlab = 'Training sessions', ylab = 'IQ')\n\n\n\n\n\n\n# Illustration data, each line represents the development of each subject by number of trainings\nggplot(data = wm, aes(x = wm$condition, y = wm$iq, group = wm$subject, colour = wm$subject)) + geom_line() + geom_point()\n\n\n\n\n\n\nReduced cost\n\n\nThe cost advantage of using manipulation within groups verses manipulation between groups for the working memory experiment is you need 60 subjects fewer.\n\n\nStatistically more powerful\n\n\nRepeated measures analysis accounts for individual differences across the experiment. This reduces the error term, which  increases statistical power.\n\n\nCounterbalancing\n\n\nSuppose you have three levels of an independent variable A (i.e. A1, A2, A3) and a blocked design. You want to use full counterbalancing to take into account order effects.\n\n\nWhat are all the possible orders that you need to use? In other words, what are the order conditions?\n\n\n(A1, A2, A3), (A1, A3, A2) , (A2, A1, A3), (A2, A3, A1) , (A3, A2, A1),\n\n(A3, A1, A2)\n\n\nNumber of order conditions?\n\n\nAssume the number of levels of the independent variable goes up and you want to completely counterbalance. What will happen to the number of order conditions you\u2019ll need?\n\n\nThe number becomes really large. An independent variable with n levels will have \nn!=n*(n???1)*(n???2)*...\n order conditions.\n\n\nLatin Squares design\n\n\nAs you hopefully realized in the previous exercise, completely counterbalancing is not always a practical solution for taking into account order effects. This is because the number of different orders required gets really large as the number of possible conditions increases.\n\n\nThe most common workaround to this problem is the Latin Squares design, in which you do not completely counterbalance, but instead put each condition at every position (at least) once.\n\n\nWhich of the following examples has been constructed according to the Latin Squares design?\n\n\n(A1, A2, A3), (A2, A3, A1), (A3, A1, A2)\n\n\nMore on Latin Squares\n\n\nThe number of order conditions is always equal to the number of levels of your independent variable.\n\n\nWhy is missing data a problem?\n\n\nIn a between-groups design, it is okay to have a slightly different number of subjects in each group, so if one subject drops out in one of the conditions then that group has just one less subject. Now you want to look at how subjects change over time and the different scores between two or more conditions for each subject.\n\n\nUnderstanding sphericity\n\n\nThe variances of the differences between all possible pairs of groups (i.e. levels of the independent variable) are equal.\n\n\nMauchly\u2019s test\n\n\n# Define iq as a data frame where each column represents a condition\niq <- cbind(wm$iq[wm$condition == '8 days'], wm$iq[wm$condition == '12 days'], wm$iq[wm$condition == '17 days'], wm$iq[wm$condition == '19 days'])\n\n# Make an mlm object\nmlm <- lm(iq ~ 1)\n\n# Mauchly's test\nmauchly.test(mlm, x = ~ 1)\n\n\n\n\n## \n##  Mauchly's test of sphericity\n## \n## data:  SSD matrix from lm(formula = iq ~ 1)\n## W = 0.81725, p-value = 0.9407\n\n\n\nBased on the results, the sphericity assumption holds.\n\n\nPros of repeated measures\n\n\nLess cost and statistically more powerful\n\n\nCons of repeated measures\n\n\nOrder effects, counterbalancing, missing data, and an extra assumption\n\n\n2, Repeated measures ANOVA\n\u00b6\n\n\nThe systematic between groups variance\n\n\nTo understand everything a bit better, we will calculate the F-ratio for a repeated measures design by ourself in the next exercises.\n\n\nFirst, we will need the systematic between-groups variance. This is the same as in the between-groups design\u2013the variance due to grouping by condition.\n\n\n# Define number of subjects for each condition\nn <- 20\n\n# Calculate group means\ny_j <- tapply(wm$iq, wm$condition, mean)\n\n# Calculate the grand mean\ny_t <- mean(y_j)\n\n# Calculate the sum of squares\nss_cond <- sum((y_j - y_t)^2)*n\n\n# Define the degrees of freedom for conditions\ndf <- (4 - 1)\n\n# Calculate the mean squares (variance)\nms_cond <- ss_cond/df\n\n\n\n\nThe subject variance\n\n\nWe will also need the error term of the repeated measures design. This can be calculated in a few steps.\n\n\n\n\nFirst calculate the systematic variance due to subjects.\n\n\nBelow, we will calculate the unsystematic variance, like we did with the between-groups design.\n\n\nIf we subtract these two results, we will get the error term of the repeated measures design.\n\n\n\n\nThe systematic (stable) subject variance will be taken out of the error term, so the error term is reduced in comparison with the between-groups design.\n\n\n# Define number of conditions for each subject\nn <- 4\n\n# Calculate subject means\ny_j <- tapply(wm$iq, wm$subject, mean)\n\n# Calculate the grand mean\ny_t <- mean(y_j)\n\n# Calculate the sum of squares\nss_subjects <- sum((y_j - y_t)^2)*n\n\n# Define the degrees of freedom for subjects\ndf <- (20 - 1)\n\n# Calculate the mean squares (variance)\nms_subjects <- ss_subjects/df\n\n\n\n\nThe unsystematic within groups variance\n\n\nTo calculate the error term of the repeated measures design, we need the unsystematic within-groups variance: the unsystematic variance or the error term of the between-groups design.\n\n\n# Create four subsets of the four groups, containing the IQ results\n    # Make the subset for the group condition = \"8 days\"\ny_i1 <- subset(wm$iq, wm$condition == \"8 days\")\n    # Make the subset for the group condition = \"12 days\"\ny_i2 <- subset(wm$iq, wm$condition == \"12 days\")\n    # Make the subset for the group condition = \"17 days\"\ny_i3 <- subset(wm$iq, wm$condition == \"17 days\")\n    # Make the subset for the group condition = \"19 days\"\ny_i4 <- subset(wm$iq, wm$condition == \"19 days\")\n\n# Subtract the individual values by their group means\ns_1 <- y_i1 - mean(y_i1)\ns_2 <- y_i2 - mean(y_i2)\ns_3 <- y_i3 - mean(y_i3)\ns_4 <- y_i4 - mean(y_i4)\n\n# Put everything back into one vector\ns_t <- c(s_1, s_2, s_3, s_4)\n\n# Calculate the within sum of squares by using the vector s_t\nss_sa <- sum(s_t^2)\n\n# Define the degrees of freedom\ndf <- 4*(20-1)\n\n# Calculate the mean squares (variances)\nms_sa <- ss_sa/df\n\n\n\n\nThe unsystematic variance for the repeated measures design\n\n\nNow we can easily calculate the unsystematic variance for the repeated measures design, also called the error term.\n\n\n# ss_sa = ss_subjects + ss_rm\nss_rm <- ss_sa - ss_subjects\n\n# Define the degrees of freedom\ndf <- (20 - 1)*(4 - 1)\n\n# Calculate the mean squares (variances)\nms_rm <- ss_rm/df\n\n\n\n\nNow we\u2019ve calculated the error term of the repeated measures design, which is clearly smaller than the error term of the between-groups design because you reduced this one. To complete you can now calculate the F-ratio and the corresponding p-value.\n\n\nF-ratio and p-value\n\n\nTo do the ANOVA analysis we actually need the F-ratio and the corresponding p-value.\n\n\n# Calculate the F-ratio\nf_rat <- ms_cond/ms_rm\n\n# Define the degrees of freedom of the F-distribution\n# df1 for freedom of conditions (ss_cond)\n# df2 for (freedom of) conditions and subjects (sa_sa)\ndf1 <- (4 - 1)\ndf2 <- (4 - 1)*(20 - 1)\n\n# Calculate the p-value\np <- 1 - pf(f_rat, df1, df2)\n\n\n\n\nError term in a repeated measures design?\n\n\nThe inconsistent individual differences across conditions, the effect of subjects that differ across conditions. So it is an interaction between subjects and condition.\n\n\nAnova in R\n\n\nANOVA in R is usually done with the \naov\n function.\n\n\n# anova model\nmodel <- aov(wm$iq ~ wm$condition + Error(wm$subject / wm$condition))\n\n# summary model\nsummary(model)\n\n\n\n\n## \n## Error: wm$subject\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## Residuals 19  175.6   9.242               \n## \n## Error: wm$subject:wm$condition\n##              Df Sum Sq Mean Sq F value   Pr(>F)    \n## wm$condition  3  196.1   65.36   12.51 2.16e-06 ***\n## Residuals    57  297.8    5.22                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe F-ratio is significant. Therefore, the number of training days does affect the IQ scores.\n\n\nEffect size\n\n\nCalculate eta-squared, which helps you estimate effect size.\n\n\n# Define the total sum of squares\n\n# ss_cond (syst. between groups or of the effect), \n# ss_sa (unsyst. within groups) =\n# ss_rm (unsyst. for repeated measures design) + # ss_subjects (ok)\nss_total <- ss_cond + ss_rm\n\n# Calculate the effect size\neta_sq <- ss_cond / ss_total\n\n\n\n\nPost-hoc test one\n\n\nWe will now look at a few different procedures for the post-hoc test. Let\u2019s start with the Holm procedure.\n\n\n# Post-hoc test: default procedure\nwith(wm, pairwise.t.test(iq, condition, paired = T))\n\n\n\n\n## \n##  Pairwise comparisons using paired t tests \n## \n## data:  iq and condition \n## \n##         12 days 17 days 19 days\n## 17 days 0.01955 -       -      \n## 19 days 0.00270 0.40038 -      \n## 8 days  0.40038 0.00244 0.00054\n## \n## P value adjustment method: holm\n\n\n\nWe get a table with some values as the output for the post-hoc test and a line saying that we have used the Holm procedure. What are the values in the table of the output? p-values.\n\n\nRecall that the hypotheses are tested at a 5% significance level.\n\n\nWe can conclude that all pairwise comparisons are significant, except for the comparison between 19 and 17 and the comparison between 12 and 8.\n\n\nPost-hoc test: Bonferroni\n\n\nNow we\u2019ll take a look at the most conservative procedure, Bonferroni. This procedure will apply the most extreme adjustments to the p-values.\n\n\n# Post-hoc test: Bonferroni procedure\nwith(wm, pairwise.t.test(iq, condition, paired = TRUE, p.adjust.method = 'bonferroni'))\n\n\n\n\n## \n##  Pairwise comparisons using paired t tests \n## \n## data:  iq and condition \n## \n##         12 days 17 days 19 days\n## 17 days 0.03910 -       -      \n## 19 days 0.00405 1.00000 -      \n## 8 days  1.00000 0.00293 0.00054\n## \n## P value adjustment method: bonferroni\n\n\n\nNotice the change in p-values in comparison with the previous procedure.\n\n\nPaired t-test\n\n\nAssume that you do not know how to perform an analysis of variance (ANOVA), we may do a number of paired t-tests instead.\n\n\nHave a look at just one paired t-test and take, for example, the comparison between 12 days and 17 days.\n\n\n# Define two subsets containing the IQ scores for the condition group '12 days' and '17 days'\ncond_12days <- subset(wm, condition == '12 days')$iq\ncond_17days <- subset(wm, condition == '17 days')$iq\ncond_12days\n\n\n\n\n##  [1] 12.5 11.6  8.9  8.3 10.9 13.4 12.3  8.7  9.7  9.7  6.9 10.5 11.6 13.8\n## [15] 15.6 11.7 16.1 11.7 15.0 15.1\n\n\n\n# t-test\nt.test(cond_12days, cond_17days, paired = TRUE)\n\n\n\n\n## \n##  Paired t-test\n## \n## data:  cond_12days and cond_17days\n## t = -3.0549, df = 19, p-value = 0.006517\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.7157116 -0.6942884\n## sample estimates:\n## mean of the differences \n##                  -2.205\n\n\n\nIt is clear that we can apply different procedures for post-hoc tests. These procedures differ with respect to how they handle inflation of the possibility of a type I error and will therefore give us different p-values. However, these p-values will always be in a certain range.\n\n\nWhat is the (smallest) range of p-values for the comparison between 12 days and 17 days? Look at the results from applying the Bonferroni procedure as well as the paired t-test.\n\n\np-values:\n\n\n\n\nBonferroni 12 days-17 days: 0.03910\n\n\nPaired t-test (cond_12days, cond_17days): 0.006517\n\n\n\n\nTherefore: \n0.0065, 0.0391\n.",
            "title": "Statistics with R, Course Four, Repeated Measures ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Four,_Repeated_Measures_Anova/#2-repeated-measures-anova",
            "text": "The systematic between groups variance  To understand everything a bit better, we will calculate the F-ratio for a repeated measures design by ourself in the next exercises.  First, we will need the systematic between-groups variance. This is the same as in the between-groups design\u2013the variance due to grouping by condition.  # Define number of subjects for each condition\nn <- 20\n\n# Calculate group means\ny_j <- tapply(wm$iq, wm$condition, mean)\n\n# Calculate the grand mean\ny_t <- mean(y_j)\n\n# Calculate the sum of squares\nss_cond <- sum((y_j - y_t)^2)*n\n\n# Define the degrees of freedom for conditions\ndf <- (4 - 1)\n\n# Calculate the mean squares (variance)\nms_cond <- ss_cond/df  The subject variance  We will also need the error term of the repeated measures design. This can be calculated in a few steps.   First calculate the systematic variance due to subjects.  Below, we will calculate the unsystematic variance, like we did with the between-groups design.  If we subtract these two results, we will get the error term of the repeated measures design.   The systematic (stable) subject variance will be taken out of the error term, so the error term is reduced in comparison with the between-groups design.  # Define number of conditions for each subject\nn <- 4\n\n# Calculate subject means\ny_j <- tapply(wm$iq, wm$subject, mean)\n\n# Calculate the grand mean\ny_t <- mean(y_j)\n\n# Calculate the sum of squares\nss_subjects <- sum((y_j - y_t)^2)*n\n\n# Define the degrees of freedom for subjects\ndf <- (20 - 1)\n\n# Calculate the mean squares (variance)\nms_subjects <- ss_subjects/df  The unsystematic within groups variance  To calculate the error term of the repeated measures design, we need the unsystematic within-groups variance: the unsystematic variance or the error term of the between-groups design.  # Create four subsets of the four groups, containing the IQ results\n    # Make the subset for the group condition = \"8 days\"\ny_i1 <- subset(wm$iq, wm$condition == \"8 days\")\n    # Make the subset for the group condition = \"12 days\"\ny_i2 <- subset(wm$iq, wm$condition == \"12 days\")\n    # Make the subset for the group condition = \"17 days\"\ny_i3 <- subset(wm$iq, wm$condition == \"17 days\")\n    # Make the subset for the group condition = \"19 days\"\ny_i4 <- subset(wm$iq, wm$condition == \"19 days\")\n\n# Subtract the individual values by their group means\ns_1 <- y_i1 - mean(y_i1)\ns_2 <- y_i2 - mean(y_i2)\ns_3 <- y_i3 - mean(y_i3)\ns_4 <- y_i4 - mean(y_i4)\n\n# Put everything back into one vector\ns_t <- c(s_1, s_2, s_3, s_4)\n\n# Calculate the within sum of squares by using the vector s_t\nss_sa <- sum(s_t^2)\n\n# Define the degrees of freedom\ndf <- 4*(20-1)\n\n# Calculate the mean squares (variances)\nms_sa <- ss_sa/df  The unsystematic variance for the repeated measures design  Now we can easily calculate the unsystematic variance for the repeated measures design, also called the error term.  # ss_sa = ss_subjects + ss_rm\nss_rm <- ss_sa - ss_subjects\n\n# Define the degrees of freedom\ndf <- (20 - 1)*(4 - 1)\n\n# Calculate the mean squares (variances)\nms_rm <- ss_rm/df  Now we\u2019ve calculated the error term of the repeated measures design, which is clearly smaller than the error term of the between-groups design because you reduced this one. To complete you can now calculate the F-ratio and the corresponding p-value.  F-ratio and p-value  To do the ANOVA analysis we actually need the F-ratio and the corresponding p-value.  # Calculate the F-ratio\nf_rat <- ms_cond/ms_rm\n\n# Define the degrees of freedom of the F-distribution\n# df1 for freedom of conditions (ss_cond)\n# df2 for (freedom of) conditions and subjects (sa_sa)\ndf1 <- (4 - 1)\ndf2 <- (4 - 1)*(20 - 1)\n\n# Calculate the p-value\np <- 1 - pf(f_rat, df1, df2)  Error term in a repeated measures design?  The inconsistent individual differences across conditions, the effect of subjects that differ across conditions. So it is an interaction between subjects and condition.  Anova in R  ANOVA in R is usually done with the  aov  function.  # anova model\nmodel <- aov(wm$iq ~ wm$condition + Error(wm$subject / wm$condition))\n\n# summary model\nsummary(model)  ## \n## Error: wm$subject\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## Residuals 19  175.6   9.242               \n## \n## Error: wm$subject:wm$condition\n##              Df Sum Sq Mean Sq F value   Pr(>F)    \n## wm$condition  3  196.1   65.36   12.51 2.16e-06 ***\n## Residuals    57  297.8    5.22                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  The F-ratio is significant. Therefore, the number of training days does affect the IQ scores.  Effect size  Calculate eta-squared, which helps you estimate effect size.  # Define the total sum of squares\n\n# ss_cond (syst. between groups or of the effect), \n# ss_sa (unsyst. within groups) =\n# ss_rm (unsyst. for repeated measures design) + # ss_subjects (ok)\nss_total <- ss_cond + ss_rm\n\n# Calculate the effect size\neta_sq <- ss_cond / ss_total  Post-hoc test one  We will now look at a few different procedures for the post-hoc test. Let\u2019s start with the Holm procedure.  # Post-hoc test: default procedure\nwith(wm, pairwise.t.test(iq, condition, paired = T))  ## \n##  Pairwise comparisons using paired t tests \n## \n## data:  iq and condition \n## \n##         12 days 17 days 19 days\n## 17 days 0.01955 -       -      \n## 19 days 0.00270 0.40038 -      \n## 8 days  0.40038 0.00244 0.00054\n## \n## P value adjustment method: holm  We get a table with some values as the output for the post-hoc test and a line saying that we have used the Holm procedure. What are the values in the table of the output? p-values.  Recall that the hypotheses are tested at a 5% significance level.  We can conclude that all pairwise comparisons are significant, except for the comparison between 19 and 17 and the comparison between 12 and 8.  Post-hoc test: Bonferroni  Now we\u2019ll take a look at the most conservative procedure, Bonferroni. This procedure will apply the most extreme adjustments to the p-values.  # Post-hoc test: Bonferroni procedure\nwith(wm, pairwise.t.test(iq, condition, paired = TRUE, p.adjust.method = 'bonferroni'))  ## \n##  Pairwise comparisons using paired t tests \n## \n## data:  iq and condition \n## \n##         12 days 17 days 19 days\n## 17 days 0.03910 -       -      \n## 19 days 0.00405 1.00000 -      \n## 8 days  1.00000 0.00293 0.00054\n## \n## P value adjustment method: bonferroni  Notice the change in p-values in comparison with the previous procedure.  Paired t-test  Assume that you do not know how to perform an analysis of variance (ANOVA), we may do a number of paired t-tests instead.  Have a look at just one paired t-test and take, for example, the comparison between 12 days and 17 days.  # Define two subsets containing the IQ scores for the condition group '12 days' and '17 days'\ncond_12days <- subset(wm, condition == '12 days')$iq\ncond_17days <- subset(wm, condition == '17 days')$iq\ncond_12days  ##  [1] 12.5 11.6  8.9  8.3 10.9 13.4 12.3  8.7  9.7  9.7  6.9 10.5 11.6 13.8\n## [15] 15.6 11.7 16.1 11.7 15.0 15.1  # t-test\nt.test(cond_12days, cond_17days, paired = TRUE)  ## \n##  Paired t-test\n## \n## data:  cond_12days and cond_17days\n## t = -3.0549, df = 19, p-value = 0.006517\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.7157116 -0.6942884\n## sample estimates:\n## mean of the differences \n##                  -2.205  It is clear that we can apply different procedures for post-hoc tests. These procedures differ with respect to how they handle inflation of the possibility of a type I error and will therefore give us different p-values. However, these p-values will always be in a certain range.  What is the (smallest) range of p-values for the comparison between 12 days and 17 days? Look at the results from applying the Bonferroni procedure as well as the paired t-test.  p-values:   Bonferroni 12 days-17 days: 0.03910  Paired t-test (cond_12days, cond_17days): 0.006517   Therefore:  0.0065, 0.0391 .",
            "title": "2, Repeated measures ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Five,_Correlation_and_Regression/",
            "text": "1, An introduction to Correlation\n\n\n2, An introduction to Linear Regression Models\n\n\n3, Linear Regression Models continued\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, An introduction to Correlation\n\u00b6\n\n\nManual computation of correlation coefficients - Part 1\n\n\n# Print the data set in the console\nstr(PE)\n\n\n\n\n## 'data.frame':    200 obs. of  4 variables:\n##  $ pid        : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ age        : num  60 40 29 47 48 42 55 43 39 51 ...\n##  $ activeyears: num  10 9 2 10 9 6 8 19 9 14 ...\n##  $ endurance  : num  18 36 51 18 23 30 8 40 28 15 ...\n\n\n\n# Take a quick peek at both vectors\nA <- PE$activeyears\nB <- PE$endurance\n\n# Save the differences of each vector element with the mean in a new variable\ndiff_A <- A - mean(A)\ndiff_B <- B - mean(B)\n\n# Do the summation of the elements of the vectors and divide by N-1 in order to acquire the covariance between the two vectors\ncov <- sum(diff_A*diff_B) / (length(A) - 1)\ncov\n\n\n\n\n## [1] 16.59045\n\n\n\nManual computation of correlation coefficients - Part 2\n\n\n# Square the differences that were found in the previous step\nsq_diff_A <- diff_A^2\nsq_diff_B <- diff_B^2\n\n# Take the sum of the elements, divide them by N-1 and consequently take the square root to acquire the sample standard deviations\nsd_A <- sqrt(sum(sq_diff_A)/(length(A) - 1))\nsd_B <- sqrt(sum(sq_diff_B)/(length(B) - 1))\nsd_A\n\n\n\n\n## [1] 4.687134\n\n\n\nsd_B\n\n\n\n\n## [1] 10.83963\n\n\n\nManual computation of correlation coefficients - part 3\n\n\n# Combine all the pieces of the puzzle\ncorrelation <- cov/(sd_A*sd_B)\ncorrelation\n\n\n\n\n## [1] 0.3265402\n\n\n\n# Check the validity of your result with the cor() command\ncor(A,B)\n\n\n\n\n## [1] 0.3265402\n\n\n\nCreating scatterplots\n\n\nlibrary(psych)\n\n# Summary statistics\ndescribe(PE)\n\n\n\n\n##             vars   n   mean    sd median trimmed   mad min max range skew\n## pid            1 200 101.81 58.85  101.5  101.71 74.87   1 204   203 0.01\n## age            2 200  49.41 10.48   48.0   49.46 10.38  20  82    62 0.06\n## activeyears    3 200  10.68  4.69   11.0   10.57  4.45   0  26    26 0.30\n## endurance      4 200  26.50 10.84   27.0   26.22 10.38   3  55    52 0.22\n##             kurtosis   se\n## pid            -1.21 4.16\n## age            -0.14 0.74\n## activeyears     0.46 0.33\n## endurance      -0.44 0.77\n\n\n\n# Scatter plots\npar(mfrow = c(1, 3))\n\nplot(PE$age ~ PE$activeyears)\nplot(PE$endurance ~ PE$activeyears)\nplot(PE$endurance ~ PE$age)\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nCorrelation matrix\n\n\n# Correlation Analysis \nround(cor(PE[2:4], use = 'pairwise.complete.obs', method = 'pearson'), 2)  \n\n\n\n\n##               age activeyears endurance\n## age          1.00        0.33     -0.08\n## activeyears  0.33        1.00      0.33\n## endurance   -0.08        0.33      1.00\n\n\n\n# Do some correlation tests. If the null hypothesis of no correlation can be rejected on a significance level of 5%, then the relationship between variables is  significantly different from zero at the 95% confidence level\n\ncor.test(PE$age, PE$activeyears)\n\n\n\n\n## \n##  Pearson's product-moment correlation\n## \n## data:  PE$age and PE$activeyears\n## t = 4.9022, df = 198, p-value = 1.969e-06\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.1993491 0.4473145\n## sample estimates:\n##       cor \n## 0.3289909\n\n\n\ncor.test(PE$age, PE$endurance)\n\n\n\n\n## \n##  Pearson's product-moment correlation\n## \n## data:  PE$age and PE$endurance\n## t = -1.1981, df = 198, p-value = 0.2323\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.22097811  0.05454491\n## sample estimates:\n##         cor \n## -0.08483813\n\n\n\ncor.test(PE$endurance, PE$activeyears)\n\n\n\n\n## \n##  Pearson's product-moment correlation\n## \n## data:  PE$endurance and PE$activeyears\n## t = 4.8613, df = 198, p-value = 2.37e-06\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.1967110 0.4451154\n## sample estimates:\n##       cor \n## 0.3265402\n\n\n\nCAUTION:\n\n\n\n\nThe magnitude of a correlation depends upon many factors, including sampling.\n\n\nThe magnitude of a correlation is also influenced by measurement of X & Y.\n\n\nThe correlation coefficient is a sample statistic, just like the mean.\n\n\n\n\n2, An introduction to Linear Regression Models\n\u00b6\n\n\nImpact experiment\n\n\n# Create a correlation matrix for the dataset (9-14 are the '2' variables only)\ncorrelations <- cor(PE[9:14,])\n\n# Create the scatterplot matrix for the dataset\nlibrary(corrplot)\n\ncorrplot(correlations)\n\n\n\n\n\n\nManual computation of a simple linear regression\n\n\n# Calculate the required means, standard deviations and correlation coefficient\nmean_ay <- mean(PE$activeyears)\nmean_e <- mean(PE$endurance)\nsd_ay <- sd(PE$activeyears)\nsd_e <- sd(PE$endurance)\nr <- cor(PE$activeyears, PE$endurance)\n\n# Calculate the slope\nB_1 <- r * (sd_e)/(sd_ay)\n\n# Calculate the intercept\nB_0 <- mean_e - B_1 * mean_ay\n\n# Plot of ic2 against sym2\nplot(PE$activeyear, PE$endurance, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\n\n# Add the regression line\nabline(B_0, B_1, col = 'red')\n\n\n\n\n\n\nExecuting a simple linear regression using R\n\n\n# Construct the regression model\nmodel_1 <- lm(PE$endurance ~ PE$activeyear)\n\n# Look at the results of the regression by using the summary function\nsummary(model_1)\n\n\n\n\n## \n## Call:\n## lm(formula = PE$endurance ~ PE$activeyear)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -20.5006  -7.8066   0.5304   5.7649  31.0511 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    18.4386     1.8104  10.185  < 2e-16 ***\n## PE$activeyear   0.7552     0.1553   4.861 2.37e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.27 on 198 degrees of freedom\n## Multiple R-squared:  0.1066, Adjusted R-squared:  0.1021 \n## F-statistic: 23.63 on 1 and 198 DF,  p-value: 2.37e-06\n\n\n\n# Extract the predicted values\npredicted <- fitted(model_1)\n\n# Create a scatter plot of Impulse Control against Symptom Score\nplot(PE$endurance ~ PE$activeyear, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\n\n# Add a regression line\nabline(model_1, col = 'red')\nabline(lm(predicted ~ PE$activeyears), col = 'green', lty = 2)\n\n\n\n\n\n\nExecuting a multiple regression in R\n\n\n# Multiple Regression\nmodel_2 <- lm(PE$endurance ~ PE$activeyear + PE$age)\n\n# Examine the results of the regression\nsummary(model_2)\n\n\n\n\n## \n## Call:\n## lm(formula = PE$endurance ~ PE$activeyear + PE$age)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -21.4598  -7.7398   0.6984   5.5364  27.9230 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    27.7035     3.4779   7.966 1.29e-13 ***\n## PE$activeyear   0.9192     0.1610   5.708 4.16e-08 ***\n## PE$age         -0.2229     0.0720  -3.096  0.00225 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.06 on 197 degrees of freedom\n## Multiple R-squared:  0.1481, Adjusted R-squared:  0.1394 \n## F-statistic: 17.12 on 2 and 197 DF,  p-value: 1.394e-07\n\n\n\n# Extract the predicted values\npredicted <- fitted(model_2)\n\n# Plotting predicted scores against observed scores\nplot(predicted ~ PE$activeyears, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\nabline(lm(predicted ~ PE$activeyears), col = 'green')\n\n\n\n\n\n\n3, Linear Regression Models continued\n\u00b6\n\n\nCalculating the sum of squared residuals\n\n\n# Create a linear regression with `ic2` and `vismem2` as regressors\nmodel_1 <- lm(PE$endurance ~ PE$activeyear + PE$age)\n\n# Extract the predicted values\npredicted_1 <- fitted(model_1)\n\n# Calculate the squared deviation of the predicted values from the observed values \ndeviation_1 <- (predicted_1 - PE$endurance)^2\n\n# Sum the squared deviations\nSSR_1 <- sum(deviation_1)\nSSR_1\n\n\n\n\n## [1] 19919.55\n\n\n\nStandardized linear regression\n\n\n# Create a standardized simple linear regression\nmodel_1_z <- lm(scale(PE$endurance) ~ scale(PE$activeyear))\n\n# Look at the output of this regression model\nsummary(model_1_z)\n\n\n\n\n## \n## Call:\n## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.89126 -0.72019  0.04893  0.53184  2.86459 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)          4.871e-17  6.700e-02   0.000        1    \n## scale(PE$activeyear) 3.265e-01  6.717e-02   4.861 2.37e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9476 on 198 degrees of freedom\n## Multiple R-squared:  0.1066, Adjusted R-squared:  0.1021 \n## F-statistic: 23.63 on 1 and 198 DF,  p-value: 2.37e-06\n\n\n\n# Extract the R-Squared value for this regression\nr_square_1 <- summary(model_1_z)$r.squared\nr_square_1\n\n\n\n\n## [1] 0.1066285\n\n\n\n# Calculate the correlation coefficient\ncorr_coef_1 <- sqrt(r_square_1)\ncorr_coef_1\n\n\n\n\n## [1] 0.3265402\n\n\n\n# Create a standardized multiple linear regression\nmodel_2_z <- lm(scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age))\n\n# Look at the output of this regression model\nsummary(model_2_z)\n\n\n\n\n## \n## Call:\n## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.97975 -0.71403  0.06443  0.51076  2.57601 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)           5.590e-17  6.560e-02   0.000  1.00000    \n## scale(PE$activeyear)  3.975e-01  6.964e-02   5.708 4.16e-08 ***\n## scale(PE$age)        -2.156e-01  6.964e-02  -3.096  0.00225 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9277 on 197 degrees of freedom\n## Multiple R-squared:  0.1481, Adjusted R-squared:  0.1394 \n## F-statistic: 17.12 on 2 and 197 DF,  p-value: 1.394e-07\n\n\n\n# Extract the R-Squared value for this regression\nr_square_2 <- summary(model_2_z)$r.squared\nr_square_2\n\n\n\n\n## [1] 0.1480817\n\n\n\n# Calculate the correlation coefficient\ncorr_coef_2 <- sqrt(r_square_2)\ncorr_coef_2\n\n\n\n\n## [1] 0.3848139\n\n\n\nAssumptions of linear regression:\n\n\n\n\nNormal distribution for Y.\n\n\nLinear relationship between X and Y.\n\n\nHomoscedasticity.\n\n\nReliability of X and Y.\n\n\nValidity of X and Y.\n\n\nRandom and representative sampling.\n\n\n\n\nCheck it out with Anscombe\u2019s quartet plots.\n\n\nPlotting residuals\n\n\n# Extract the residuals from the model\nresidual <- resid(model_2)\n\n# Draw a histogram of the residuals\nhist(residual)\n\n\n\n\n\n\n# Extract the predicted symptom scores from the model\npredicted <- fitted(model_2)\n\n# Plot the residuals against the predicted symptom scores\nplot(residual ~ predicted, main = 'Scatterplot', ylab = 'Model 2 Residuals', xlab = 'Model 2 Predicted Scores')\nabline(lm(residual ~ predicted), col = 'red')",
            "title": "Statistics with R, Course Five, Correlation and Regression"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Five,_Correlation_and_Regression/#2-an-introduction-to-linear-regression-models",
            "text": "Impact experiment  # Create a correlation matrix for the dataset (9-14 are the '2' variables only)\ncorrelations <- cor(PE[9:14,])\n\n# Create the scatterplot matrix for the dataset\nlibrary(corrplot)\n\ncorrplot(correlations)   Manual computation of a simple linear regression  # Calculate the required means, standard deviations and correlation coefficient\nmean_ay <- mean(PE$activeyears)\nmean_e <- mean(PE$endurance)\nsd_ay <- sd(PE$activeyears)\nsd_e <- sd(PE$endurance)\nr <- cor(PE$activeyears, PE$endurance)\n\n# Calculate the slope\nB_1 <- r * (sd_e)/(sd_ay)\n\n# Calculate the intercept\nB_0 <- mean_e - B_1 * mean_ay\n\n# Plot of ic2 against sym2\nplot(PE$activeyear, PE$endurance, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\n\n# Add the regression line\nabline(B_0, B_1, col = 'red')   Executing a simple linear regression using R  # Construct the regression model\nmodel_1 <- lm(PE$endurance ~ PE$activeyear)\n\n# Look at the results of the regression by using the summary function\nsummary(model_1)  ## \n## Call:\n## lm(formula = PE$endurance ~ PE$activeyear)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -20.5006  -7.8066   0.5304   5.7649  31.0511 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    18.4386     1.8104  10.185  < 2e-16 ***\n## PE$activeyear   0.7552     0.1553   4.861 2.37e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.27 on 198 degrees of freedom\n## Multiple R-squared:  0.1066, Adjusted R-squared:  0.1021 \n## F-statistic: 23.63 on 1 and 198 DF,  p-value: 2.37e-06  # Extract the predicted values\npredicted <- fitted(model_1)\n\n# Create a scatter plot of Impulse Control against Symptom Score\nplot(PE$endurance ~ PE$activeyear, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\n\n# Add a regression line\nabline(model_1, col = 'red')\nabline(lm(predicted ~ PE$activeyears), col = 'green', lty = 2)   Executing a multiple regression in R  # Multiple Regression\nmodel_2 <- lm(PE$endurance ~ PE$activeyear + PE$age)\n\n# Examine the results of the regression\nsummary(model_2)  ## \n## Call:\n## lm(formula = PE$endurance ~ PE$activeyear + PE$age)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -21.4598  -7.7398   0.6984   5.5364  27.9230 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    27.7035     3.4779   7.966 1.29e-13 ***\n## PE$activeyear   0.9192     0.1610   5.708 4.16e-08 ***\n## PE$age         -0.2229     0.0720  -3.096  0.00225 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.06 on 197 degrees of freedom\n## Multiple R-squared:  0.1481, Adjusted R-squared:  0.1394 \n## F-statistic: 17.12 on 2 and 197 DF,  p-value: 1.394e-07  # Extract the predicted values\npredicted <- fitted(model_2)\n\n# Plotting predicted scores against observed scores\nplot(predicted ~ PE$activeyears, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\nabline(lm(predicted ~ PE$activeyears), col = 'green')",
            "title": "2, An introduction to Linear Regression Models"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Five,_Correlation_and_Regression/#3-linear-regression-models-continued",
            "text": "Calculating the sum of squared residuals  # Create a linear regression with `ic2` and `vismem2` as regressors\nmodel_1 <- lm(PE$endurance ~ PE$activeyear + PE$age)\n\n# Extract the predicted values\npredicted_1 <- fitted(model_1)\n\n# Calculate the squared deviation of the predicted values from the observed values \ndeviation_1 <- (predicted_1 - PE$endurance)^2\n\n# Sum the squared deviations\nSSR_1 <- sum(deviation_1)\nSSR_1  ## [1] 19919.55  Standardized linear regression  # Create a standardized simple linear regression\nmodel_1_z <- lm(scale(PE$endurance) ~ scale(PE$activeyear))\n\n# Look at the output of this regression model\nsummary(model_1_z)  ## \n## Call:\n## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.89126 -0.72019  0.04893  0.53184  2.86459 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)          4.871e-17  6.700e-02   0.000        1    \n## scale(PE$activeyear) 3.265e-01  6.717e-02   4.861 2.37e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9476 on 198 degrees of freedom\n## Multiple R-squared:  0.1066, Adjusted R-squared:  0.1021 \n## F-statistic: 23.63 on 1 and 198 DF,  p-value: 2.37e-06  # Extract the R-Squared value for this regression\nr_square_1 <- summary(model_1_z)$r.squared\nr_square_1  ## [1] 0.1066285  # Calculate the correlation coefficient\ncorr_coef_1 <- sqrt(r_square_1)\ncorr_coef_1  ## [1] 0.3265402  # Create a standardized multiple linear regression\nmodel_2_z <- lm(scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age))\n\n# Look at the output of this regression model\nsummary(model_2_z)  ## \n## Call:\n## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.97975 -0.71403  0.06443  0.51076  2.57601 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)           5.590e-17  6.560e-02   0.000  1.00000    \n## scale(PE$activeyear)  3.975e-01  6.964e-02   5.708 4.16e-08 ***\n## scale(PE$age)        -2.156e-01  6.964e-02  -3.096  0.00225 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9277 on 197 degrees of freedom\n## Multiple R-squared:  0.1481, Adjusted R-squared:  0.1394 \n## F-statistic: 17.12 on 2 and 197 DF,  p-value: 1.394e-07  # Extract the R-Squared value for this regression\nr_square_2 <- summary(model_2_z)$r.squared\nr_square_2  ## [1] 0.1480817  # Calculate the correlation coefficient\ncorr_coef_2 <- sqrt(r_square_2)\ncorr_coef_2  ## [1] 0.3848139  Assumptions of linear regression:   Normal distribution for Y.  Linear relationship between X and Y.  Homoscedasticity.  Reliability of X and Y.  Validity of X and Y.  Random and representative sampling.   Check it out with Anscombe\u2019s quartet plots.  Plotting residuals  # Extract the residuals from the model\nresidual <- resid(model_2)\n\n# Draw a histogram of the residuals\nhist(residual)   # Extract the predicted symptom scores from the model\npredicted <- fitted(model_2)\n\n# Plot the residuals against the predicted symptom scores\nplot(residual ~ predicted, main = 'Scatterplot', ylab = 'Model 2 Residuals', xlab = 'Model 2 Predicted Scores')\nabline(lm(residual ~ predicted), col = 'red')",
            "title": "3, Linear Regression Models continued"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Six,_Multiple_Regression/",
            "text": "1, A gentle introduction to the principles of multiple regression\n\n\n2, Intuition behind estimation of multiple regression  coefficients\n\n\n3, Dummy coding\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets only.\n\n\n\n\n\n\n1, A gentle introduction to the principles of multiple regression\n\u00b6\n\n\nMultiple regression: visualization of the relationships\n\n\n# Perform the two single regressions and save them in a variable\n#model_years <- lm(salary ~ years, data = fs)\n#model_pubs <-  lm(salary ~ pubs, data = fs)\nmodel_years <- lm(fs$salary ~ fs$years)\nmodel_pubs <- lm(fs$salary ~ fs$pubs)\n\n# Plot both enhanced scatter plots in one plot matrix of 1 by 2\npar(mfrow = c(1, 2))\n\n#plot(fs$years, fs$salary, main = 'plot_years', xlab = 'years', ylab = 'salary')\nplot(fs$salary ~ fs$years, main = 'plot_years', xlab = 'years', ylab = 'salary')\nabline(model_years)\n\n#plot(fs$pubs, fs$salary, main = 'plot_pubs', xlab = 'pubs', ylab = 'salary')\nplot(fs$salary ~ fs$pubs, main = 'plot_pubs', xlab = 'pubs', ylab = 'salary')\nabline(model_pubs)\n\n\n\n\nMultiple regression: model selection\n\n\n# Do a single regression of salary onto years of experience and check the output\nmodel_1 <- lm(fs$salary ~ fs$years)\nsummary(model_1)\n\n# Do a multiple regression of salary onto years of experience and numbers of publications and check the output\nmodel_2 <- lm(fs$salary ~ fs$years + fs$pubs)\nsummary(model_2)\n\n# Save the R squared of both models in preliminary variables\npreliminary_model_1 <- summary(model_1)$r.squared\npreliminary_model_2 <- summary(model_2)$r.squared\n\n# Round them off while you save them in new variables\nr_squared <- c()\nr_squared[1] <- round(preliminary_model_1, 3)\nr_squared[2] <- round(preliminary_model_2, 3)\n\n# Print out the vector to see both R squared coefficients\nr_squared\n\n\n\n\nMultiple regression: beware of redundancy\n\n\n# Do multiple regression and check the regression output\nmodel_3 <- lm(fs$salary ~ fs$years + fs$pubs + fs$age)\nsummary(model_3)\n\n# Round off the R squared coefficients and save the result in the vector (in one step!)\nr_squared[3] <- round(summary(model_3)$r.squared,3)\n\n# Print out the vector in order to display all R squared coefficients simultaneously\nr_squared\n\n\n\n\n2, Intuition behind estimation of multiple regression coefficients\n\u00b6\n\n\nDefinition of matrices\n\n\n# Construction of 3 by 8 matrix r that contains the numbers 1 up to 24\nr <- matrix(seq(1,24), 3)\n\n# Construction of 3 by 8 matrix s that contains the numbers 21 up to 44\ns <- matrix(seq(21,44), 3)\n\n# Take the transpose t of matrix r\nt <- t(r)\n\n\n\n\nAddition, subtraction and multiplication of matrices\n\n\n# Compute the sum of matrices r and s\noperation_1 <- r + s\n\n# Compute the difference between matrices r and s\noperation_2 <- r - s\n\n# Multiply matrices t and s\noperation_3 <- t %*% s\n\n\n\n\nRow vector of sums\n\n\n# The raw dataframe `X` is already loaded in.\nX\n\n# Construction of 1 by 10 matrix I of which the elements are all 1\nI <- matrix(rep(1,10), 1, 10)\n\n# Compute the row vector of sums\nt_mat <- I %*% X\n\n\n\n\nRow vector of means and matrix of means\n\n\n# The data matrix `X` and the row vector of sums (`T`) are saved and can be used.\n# Number of observations\nn = 10\n\n# Compute the row vector of means\n# you summed up the row, you divide by the nrow to compute the average\nM <- t_mat / 10\n\n# Construction of 10 by 1 matrix J of which the elements are all 1\nJ <- matrix(rep(1,10), 10, 1)\n\n# Compute the matrix of means \nMM <- J %*% M\n\n\n\n\nMatrix of deviation scores\n\n\n# The previously generated matrices X, M and MM do not need to be constructed again but are saved and can be used.\n\n# Matrix of deviation scores D \nD <- X - MM\n\n\n\n\nSum of squares and sum of cross products matrix\n\n\n# The previously generated matrices X, M, MM and D do not need to be constructed again but are saved and can be used.\n\n# Sum of squares and sum of cross products matrix\nS <- t(D) %*% D\n\n\n\n\nCalculating the correlation matrix\n\n\n# The previously generated matrices X, M, MM, D and S do not need to be constructed again but are saved and can be used.\nn = 10\n\n# Construct the variance-covariance matrix \nC <-  S * 1/n\n\n# Generate the standard deviations matrix \nSD <- diag(x = diag(C)^(1/2), nrow = 3, ncol = 3)\n\n# Compute the correlation matrix\nR <- solve(SD) %*% C %*% solve(SD)\n\n\n\n\n3, Dummy coding\n\u00b6\n\n\nStarting off\n\n\n# Summary statistics\ndescribeBy(fs, fs$dept)\n\n\n\n\nA system to code categorical predictors in a regression analysis\n\n\nSuppose we have a categorical vector of observations. The vector counts 4 distinct groups. Here is how to assign the dummy variables:\n\n\n\n\n\n\n\n\nProfID\n\n\nGroup\n\n\nPubs\n\n\nD1\n\n\nD2\n\n\nD3\n\n\n\n\n\n\n\n\n\n\nNU\n\n\nCognitive\n\n\n83\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\nZH\n\n\nClinical\n\n\n74\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n\n\nMK\n\n\nDevelopmental\n\n\n80\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\nRH\n\n\nSocial\n\n\n68\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n\n\n\n\n\n\nGroup\n\n\nM\n\n\nSD\n\n\nN\n\n\n\n\n\n\n\n\n\n\nCognitive\n\n\n93.31\n\n\n29.48\n\n\n13\n\n\n\n\n\n\nClinical\n\n\n60.67\n\n\n11.12\n\n\n8\n\n\n\n\n\n\nDevelopmental\n\n\n103.5\n\n\n23.64\n\n\n6\n\n\n\n\n\n\nSocial\n\n\n70.13\n\n\n21.82\n\n\n9\n\n\n\n\n\n\n\n\n\nModel:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\n(\nD\n1\n)+\n\u03b2\n2\n(\nD\n2\n)+\n\u03b2\n3\n(\nD\n3\n)+\n\u03f5\n\n\nCoefficient:\n\n\n\n\n\n\n\n\n\n\nB\n\n\nSE\n\n\nB\n\n\nt\n\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n93.31\n\n\n6.5\n\n\n0\n\n\n14.37\n\n\n<.001\n\n\n\n\n\n\nD1 (Clinical)\n\n\n-32.64\n\n\n10.16\n\n\n-0.51\n\n\n-3.21\n\n\n0.003\n\n\n\n\n\n\nD2 (Devel)\n\n\n10.19\n\n\n11.56\n\n\n0.14\n\n\n0.88\n\n\n0.384\n\n\n\n\n\n\nD3 (Social)\n\n\n-23.18\n\n\n10.52\n\n\n-0.35\n\n\n-2.2\n\n\n0.035\n\n\n\n\n\n\n\n\n\nCreating dummy variables (1)\n\n\n# Create the dummy variables\ndept_code <- dummy.code(fs$dept)\ndept_code\n\n# Merge the dataset in an extended dataframe\nextended_fs <- cbind(fs, dept_code)\n\n# Look at the extended dataframe\nextended_fs\n\n# Provide summary statistics\nsummary(extended_fs)\n\n\n\n\nCreating dummy variables (2)\n\n\n# Regress salary against years and publications\nmodel <- lm(fs$salary ~ fs$years + fs$pubs)\n\n# Apply the summary function to get summarized results for model\nsummary(model)\n\n# Compute the confidence intervals for model\nconfint(model) \n\n# Create dummies for the categorical variable fs$dept by using the C() function\ndept_code <- C(fs$dept, treatment)\n\n# Regress salary against years, publications and department \nmodel_dummy <- lm(fs$salary ~ fs$years + fs$pubs + dept_code)\n\n# Apply the summary function to get summarized results for model_dummy\nsummary(model_dummy)\n\n# Compute the confidence intervals for model_dummy\nconfint(model_dummy)\n\n\n\n\nModel selection: ANOVA\n\n\n# Compare model 4 with model3\nanova(model, model_dummy)\n\n\n\n\nDiscrepancy between actual and predicted means\n\n\n# Actual means of fs$salary\ntapply(fs$salary, fs$dept, mean)\n\n\n\n\nUnweighted effects coding\n\n\nConsult the PDF for \u2018Unweighted Effects Coding\u2019.\n\n\n# Number of levels\nfs$dept\n\n# Factorize the categorical variable fs$dept and name the factorized variable dept.f\ndept.f <- factor(fs$dept)\n\n# Assign the 3 levels generated in step 2 to dept.f\ncontrasts(dept.f) <- contr.sum(3)\n\n# Regress salary against dept.f\nmodel_unweighted <- lm(fs$salary ~ dept.f)\n\n# Apply the summary() function\nsummary(model_unweighted)\n\n\n\n\nWeighted effects coding\n\n\nConsult \u2018Weighted Effects Coding\u2019.\n\n\n# Factorize the categorical variable fs$dept and name the factorized variable dept.g\ndept.g <- factor(fs$dept)\n\n# Assign the weights matrix to dept.g \ncontrasts(dept.g) <- weights\n\n# Regress salary against dept.f and apply the summary() function\nmodel_weighted <- lm(fs$salary ~ dept.g)\n\n# Apply the summary() function\nsummary(model_weighted)",
            "title": "Statistics with R, Course Six, Multiple Regression"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Six,_Multiple_Regression/#2-intuition-behind-estimation-of-multiple-regression-coefficients",
            "text": "Definition of matrices  # Construction of 3 by 8 matrix r that contains the numbers 1 up to 24\nr <- matrix(seq(1,24), 3)\n\n# Construction of 3 by 8 matrix s that contains the numbers 21 up to 44\ns <- matrix(seq(21,44), 3)\n\n# Take the transpose t of matrix r\nt <- t(r)  Addition, subtraction and multiplication of matrices  # Compute the sum of matrices r and s\noperation_1 <- r + s\n\n# Compute the difference between matrices r and s\noperation_2 <- r - s\n\n# Multiply matrices t and s\noperation_3 <- t %*% s  Row vector of sums  # The raw dataframe `X` is already loaded in.\nX\n\n# Construction of 1 by 10 matrix I of which the elements are all 1\nI <- matrix(rep(1,10), 1, 10)\n\n# Compute the row vector of sums\nt_mat <- I %*% X  Row vector of means and matrix of means  # The data matrix `X` and the row vector of sums (`T`) are saved and can be used.\n# Number of observations\nn = 10\n\n# Compute the row vector of means\n# you summed up the row, you divide by the nrow to compute the average\nM <- t_mat / 10\n\n# Construction of 10 by 1 matrix J of which the elements are all 1\nJ <- matrix(rep(1,10), 10, 1)\n\n# Compute the matrix of means \nMM <- J %*% M  Matrix of deviation scores  # The previously generated matrices X, M and MM do not need to be constructed again but are saved and can be used.\n\n# Matrix of deviation scores D \nD <- X - MM  Sum of squares and sum of cross products matrix  # The previously generated matrices X, M, MM and D do not need to be constructed again but are saved and can be used.\n\n# Sum of squares and sum of cross products matrix\nS <- t(D) %*% D  Calculating the correlation matrix  # The previously generated matrices X, M, MM, D and S do not need to be constructed again but are saved and can be used.\nn = 10\n\n# Construct the variance-covariance matrix \nC <-  S * 1/n\n\n# Generate the standard deviations matrix \nSD <- diag(x = diag(C)^(1/2), nrow = 3, ncol = 3)\n\n# Compute the correlation matrix\nR <- solve(SD) %*% C %*% solve(SD)",
            "title": "2, Intuition behind estimation of multiple regression coefficients"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Six,_Multiple_Regression/#3-dummy-coding",
            "text": "Starting off  # Summary statistics\ndescribeBy(fs, fs$dept)  A system to code categorical predictors in a regression analysis  Suppose we have a categorical vector of observations. The vector counts 4 distinct groups. Here is how to assign the dummy variables:     ProfID  Group  Pubs  D1  D2  D3      NU  Cognitive  83  0  0  0    ZH  Clinical  74  1  0  0    MK  Developmental  80  0  1  0    RH  Social  68  0  0  1     Summary statistics:     Group  M  SD  N      Cognitive  93.31  29.48  13    Clinical  60.67  11.12  8    Developmental  103.5  23.64  6    Social  70.13  21.82  9     Model:  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 ( D 1 )+ \u03b2 2 ( D 2 )+ \u03b2 3 ( D 3 )+ \u03f5  Coefficient:      B  SE  B  t  p       93.31  6.5  0  14.37  <.001    D1 (Clinical)  -32.64  10.16  -0.51  -3.21  0.003    D2 (Devel)  10.19  11.56  0.14  0.88  0.384    D3 (Social)  -23.18  10.52  -0.35  -2.2  0.035     Creating dummy variables (1)  # Create the dummy variables\ndept_code <- dummy.code(fs$dept)\ndept_code\n\n# Merge the dataset in an extended dataframe\nextended_fs <- cbind(fs, dept_code)\n\n# Look at the extended dataframe\nextended_fs\n\n# Provide summary statistics\nsummary(extended_fs)  Creating dummy variables (2)  # Regress salary against years and publications\nmodel <- lm(fs$salary ~ fs$years + fs$pubs)\n\n# Apply the summary function to get summarized results for model\nsummary(model)\n\n# Compute the confidence intervals for model\nconfint(model) \n\n# Create dummies for the categorical variable fs$dept by using the C() function\ndept_code <- C(fs$dept, treatment)\n\n# Regress salary against years, publications and department \nmodel_dummy <- lm(fs$salary ~ fs$years + fs$pubs + dept_code)\n\n# Apply the summary function to get summarized results for model_dummy\nsummary(model_dummy)\n\n# Compute the confidence intervals for model_dummy\nconfint(model_dummy)  Model selection: ANOVA  # Compare model 4 with model3\nanova(model, model_dummy)  Discrepancy between actual and predicted means  # Actual means of fs$salary\ntapply(fs$salary, fs$dept, mean)  Unweighted effects coding  Consult the PDF for \u2018Unweighted Effects Coding\u2019.  # Number of levels\nfs$dept\n\n# Factorize the categorical variable fs$dept and name the factorized variable dept.f\ndept.f <- factor(fs$dept)\n\n# Assign the 3 levels generated in step 2 to dept.f\ncontrasts(dept.f) <- contr.sum(3)\n\n# Regress salary against dept.f\nmodel_unweighted <- lm(fs$salary ~ dept.f)\n\n# Apply the summary() function\nsummary(model_unweighted)  Weighted effects coding  Consult \u2018Weighted Effects Coding\u2019.  # Factorize the categorical variable fs$dept and name the factorized variable dept.g\ndept.g <- factor(fs$dept)\n\n# Assign the weights matrix to dept.g \ncontrasts(dept.g) <- weights\n\n# Regress salary against dept.f and apply the summary() function\nmodel_weighted <- lm(fs$salary ~ dept.g)\n\n# Apply the summary() function\nsummary(model_weighted)",
            "title": "3, Dummy coding"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Seven,_Moderation_and_Mediation/",
            "text": "1, An introduction to moderation\n\n\n2, An introduction to centering predictors\n\n\n3, An introduction to mediation\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets only.\n\n\n\n\n\n\n1, An introduction to moderation\n\u00b6\n\n\nData exploration\n\n\nlibrary(psych)\n\n# Summary statistics\ndescribeBy(mod, mod$condition)\n\n# Create a boxplot of the data\nboxplot(mod$iq ~ mod$condition, main = 'Boxplot', ylab = 'IQ', xlab = 'Group condition')\n\n\n\n\nCalculate correlations\n\n\n# Create subsets of the three groups\n\n# Make the subset for the group condition = 'control'\nmod_control <- subset(mod, condition == 'control')\n\n# Make the subset for the group condition = 'threat1'\nmod_threat1 <- subset(mod, condition == 'threat1')\n\n# Make the subset for the group condition = 'threat2'\nmod_threat2 <- subset(mod, condition == 'threat2')\n\n# Calculate the correlations\ncor(mod_control$iq, mod_control$wm, method = 'pearson')\n\ncor(mod_threat1$iq, mod_threat1$wm, method = 'pearson')\n\ncor(mod_threat2$iq, mod_threat2$wm, method = 'pearson')\n\n\n\n\nModel with and without moderation\n\n\nA moderator variable (Z) will enhance a regression model if the relationship between X and Y varies as a function of Z.\n\n\nExperimental research\n\n\n\n\nThe manipulation of an X causes change in a Y.\n\n\nA moderator variable (Z) implies that the effect of the X on the Y is NOT consistent across the distribution of Z.\n\n\n\n\nCorrelational research\n\n\n\n\nAssume a correlation between X and Y.\n\n\nA moderator variable (Z) implies that the correlation between X and Y is NOT consistent across the distribution of Z.\n\n\n\n\nIf both X and Z are continuous:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nX\n\u2005+\u2005\n\u03b2\n2\nZ\n\u2005+\u2005\n\u03b2\n3\n(\nX\n\u2005*\u2005\nZ\n)+\n\u03f5\n\n\nIf X is categorical and Z is continuous:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\n(\nD\n1\n)+\n\u03b2\n2\n(\nD\n2\n)+\n\u03b2\n3\nZ\n\u2005+\u2005\n\u03b2\n4\n(\nD\n1\n\u2005*\u2005\nZ\n)+\n\u03b2\n5\n(\nD\n2\n\u2005*\u2005\nZ\n)+\n\u03f5\n\n\nConsult the PDF for performing tests.\n\n\n# Model without moderation (tests for 'first-order effects')\nmodel_1 <- lm(mod$iq ~ mod$wm + mod$d1 + mod$d2)\n\n # Make a summary of model_1\nsummary(model_1)\n\n# Create new predictor variables\nwm_d1 <- mod$wm * mod$d1\nwm_d2 <- mod$wm * mod$d2\n\n# Model with moderation\nmodel_2 <- lm(mod$iq ~ mod$wm + mod$d1 + mod$d2 + wm_d1 + wm_d2)\n\n# Make a summary of model_2\nsummary(model_2)\n\n\n\n\nModel comparison\n\n\n# Compare model_1 and model_2\nanova(model_1, model_2)\n\n\n\n\nScatterplot\n\n\n# Choose colors to represent the points by group\ncolor <- c('red', 'green', 'blue')\n\n# Illustration of the first-order effects of working memory on IQ\nggplot(mod, aes(x = wm, y = iq)) + geom_smooth(method = 'lm', color = 'black') + \n  geom_point(aes(color = condition))\n\n# Illustration of the moderation effect of working memory on IQ\nggplot(mod, aes(x = wm, y = iq)) + geom_smooth(aes(group = condition), method = 'lm', se = T, color = 'black', fullrange = T) + geom_point(aes(color = condition))\n\n\n\n\nCentering data\n\n\n# Define wm_center\nwm_center <- mod$wm - mean(mod$wm)\n\n# Compare with the variable wm.centered\nall.equal(wm_center, mod$wm.centered)\n\n\n\n\n2, An introduction to centering predictors\n\u00b6\n\n\nCentering versus no centering\n\n\nTo center means to put in deviation form: \nX\nC\n\u2004=\u2004\nX\n\u2005\u2212\u2005\nM\n. Convert raw scores to deviation scores.\n\n\nTwo reason:\n\n\n\n\nConceptual: regression constant will be more meaningful.\n\n\nStatistical: avoid multicolinearity.\n\n\n\n\nConceptual.\n\n\n\n\nThe intercept, \n\u03b2\n0\n, is the predicted score on Y (child\u2019s verbal ability) when all predictors (X = mother\u2019s vocabulary, Z = child\u2019s age) are zero.\n\n\nIf X = zero or Z = zero is meaningless, or impossible, then \n\u03b2\n0\n will be difficult to interpret.\n\n\nIn contrast, if X = zero and Z = zero, are the average then \n\u03b2\n0\n is easy to interpret.\n\n\nThe regression coefficient \n\u03b2\n1\n is the slope for X assuming an average score on Z.\n\n\nNo moderation effect implies that \n\u03b2\n1\n is consistent across the entire distribution of Z.\n\n\nIn contrast, a moderation effect implies that \n\u03b2\n1\n is NOT consistent across the entire distribution of Z.\n\n\nWhere in the distribution of Z is \n\u03b2\n1\n most representative of the relationship between X & Y?\n\n\n\n\nStatistical\n\n\n\n\nThe predictors, X and Z, can become highly correlated with the product, (X*Z).\n\n\nMulticolinearity: when two predictor variables in a GLM are so highly correlated that they are essentially redundant and it becomes difficult to estimate \n\u03b2\n values associated with each predictor.\n\n\n\n\n\n\n\n# Model without moderation and with centered data\nmodel_1_centered <- lm(mod$iq ~ mod$wm.centered + mod$d1 + mod$d2)\n\n# Make a summary of model_1_centered\nsummary(model_1_centered)\n\n\n\n\nCentering versus no centering with moderation\n\n\n# Create new predictor variables\nwm_d1_centered <- mod$wm.centered * mod$d1\nwm_d2_centered <- mod$wm.centered * mod$d2\n\n# Define model_2_centered\nmodel_2_centered <- lm(mod$iq ~ mod$wm.centered + mod$d1 + mod$d2 + wm_d1_centered + wm_d2_centered)\n\n# Make a summary of model_2_centered\nsummary(model_2_centered)\n\n\n\n\nModel comparison\n\n\n# Compare model_1_centered and model_2_centered\nanova(model_1_centered, model_2_centered)\n\n# Compare model_1 and model_2\nanova(model_1, model_2)\n\n\n\n\nSome correlations\n\n\n# Calculate the correlations between working memory capacity and the product terms\n cor_wmd1 <- cor(mod$wm, wm_d1)\n cor_wmd2 <- cor(mod$wm, wm_d2)\n cor_wmd1_centered <- cor(mod$wm.centered, wm_d1_centered)\n cor_wmd2_centered <- cor(mod$wm.centered, wm_d2_centered)\n\n# Calculate the correlations between the dummy variables and the product terms\n cor_d1d1<- cor(mod$d1, wm_d1)\n cor_d2d2 <- cor(mod$d2, wm_d2)\n cor_d1d1_centered <- cor(mod$d1, wm_d1_centered)\n cor_d2d2_centered <- cor(mod$d2, wm_d2_centered)\n\n# correlations\n rbind(c(cor_wmd1, cor_wmd2), c(cor_wmd1_centered, cor_wmd2_centered))\n rbind(c(cor_d1d1, cor_d2d2), c(cor_d1d1_centered, cor_d2d2_centered))\n\n\n\n\n3, An introduction to mediation\n\u00b6\n\n\nModel with and without mediation\n\n\n\n\nX: Experimental manipulation (Stereotype threat).\n\n\nY: Behavioral outcome (IQ score).\n\n\nM: Mediator (Mechanism = Working memory capacity).\n\n\n\n\nA mediation analysis is typically conducted to better understand an observed effect of a correlation between X and Y. Why, and how, does stereotype threat influence IQ test performance?\n\n\nIf X and Y are correlated then we can use regression to predict Y from X\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nX\n\u2005+\u2005\n\u03f5\n\n\nIf X and Y are correlated BECAUSE of the mediator M, then (X \u00e0 M \u00e0 Y):\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nM\n\u2005+\u2005\n\u03f5\n\n\nM\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nX\n\u2005+\u2005\n\u03f5\n\n\nIf X and Y are correlated BECAUSE of the mediator M, and:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nM\n\u2005+\u2005\n\u03b2\n2\nX\n\u2005+\u2005\n\u03f5\n\n\nA mediator variable (M) accounts for some (partial) or all of the relationship between X and Y.\n\n\nCAUTION:\n\n\n\n\nCorrelation does not imply causation!\n\n\nIn other words, there is a BIG difference between statistical mediation and true causal mediation.\n\n\n\n\nData exploration\n\n\n# Summary statistics\ndescribeBy(med, med$condition)\n\n# Create a boxplot of the data\nboxplot(med$iq ~ med$condition, main = 'Boxplot', xlab = 'Group condition', ylab = 'IQ')\n\n\n\n\nRun 3 regression models on the data\n\n\n# Run the three regression models\n# outcome ~ predictor\nmodel_yx <- lm(med$iq ~ med$condition)\n\n# mediator ~ predictor\nmodel_mx <- lm(med$wm ~ med$condition)\n\n# outcome ~ predictor + mediator\nmodel_yxm <- lm(med$iq ~ med$condition + med$wm)\n\n# Make a summary of the three models\nsummary(model_yx)\nsummary(model_mx)\nsummary(model_yxm)\n\n\n\n\nSobel test\n\n\n# Compare the previous results to the output of the sobel function\n\n# sobel(pred,med,out)\nmodel_all <- sobel(med$condition, med$wm, med$iq)\nmodel_all\n\n\n\n\nThe Sobel test is a method of testing the significance of a mediation\n\neffect. Consult the PDF.",
            "title": "Statistics with R, Course Seven, Moderation and Mediation"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Seven,_Moderation_and_Mediation/#2-an-introduction-to-centering-predictors",
            "text": "Centering versus no centering  To center means to put in deviation form:  X C \u2004=\u2004 X \u2005\u2212\u2005 M . Convert raw scores to deviation scores.  Two reason:   Conceptual: regression constant will be more meaningful.  Statistical: avoid multicolinearity.   Conceptual.   The intercept,  \u03b2 0 , is the predicted score on Y (child\u2019s verbal ability) when all predictors (X = mother\u2019s vocabulary, Z = child\u2019s age) are zero.  If X = zero or Z = zero is meaningless, or impossible, then  \u03b2 0  will be difficult to interpret.  In contrast, if X = zero and Z = zero, are the average then  \u03b2 0  is easy to interpret.  The regression coefficient  \u03b2 1  is the slope for X assuming an average score on Z.  No moderation effect implies that  \u03b2 1  is consistent across the entire distribution of Z.  In contrast, a moderation effect implies that  \u03b2 1  is NOT consistent across the entire distribution of Z.  Where in the distribution of Z is  \u03b2 1  most representative of the relationship between X & Y?   Statistical   The predictors, X and Z, can become highly correlated with the product, (X*Z).  Multicolinearity: when two predictor variables in a GLM are so highly correlated that they are essentially redundant and it becomes difficult to estimate  \u03b2  values associated with each predictor.    # Model without moderation and with centered data\nmodel_1_centered <- lm(mod$iq ~ mod$wm.centered + mod$d1 + mod$d2)\n\n# Make a summary of model_1_centered\nsummary(model_1_centered)  Centering versus no centering with moderation  # Create new predictor variables\nwm_d1_centered <- mod$wm.centered * mod$d1\nwm_d2_centered <- mod$wm.centered * mod$d2\n\n# Define model_2_centered\nmodel_2_centered <- lm(mod$iq ~ mod$wm.centered + mod$d1 + mod$d2 + wm_d1_centered + wm_d2_centered)\n\n# Make a summary of model_2_centered\nsummary(model_2_centered)  Model comparison  # Compare model_1_centered and model_2_centered\nanova(model_1_centered, model_2_centered)\n\n# Compare model_1 and model_2\nanova(model_1, model_2)  Some correlations  # Calculate the correlations between working memory capacity and the product terms\n cor_wmd1 <- cor(mod$wm, wm_d1)\n cor_wmd2 <- cor(mod$wm, wm_d2)\n cor_wmd1_centered <- cor(mod$wm.centered, wm_d1_centered)\n cor_wmd2_centered <- cor(mod$wm.centered, wm_d2_centered)\n\n# Calculate the correlations between the dummy variables and the product terms\n cor_d1d1<- cor(mod$d1, wm_d1)\n cor_d2d2 <- cor(mod$d2, wm_d2)\n cor_d1d1_centered <- cor(mod$d1, wm_d1_centered)\n cor_d2d2_centered <- cor(mod$d2, wm_d2_centered)\n\n# correlations\n rbind(c(cor_wmd1, cor_wmd2), c(cor_wmd1_centered, cor_wmd2_centered))\n rbind(c(cor_d1d1, cor_d2d2), c(cor_d1d1_centered, cor_d2d2_centered))",
            "title": "2, An introduction to centering predictors"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Seven,_Moderation_and_Mediation/#3-an-introduction-to-mediation",
            "text": "Model with and without mediation   X: Experimental manipulation (Stereotype threat).  Y: Behavioral outcome (IQ score).  M: Mediator (Mechanism = Working memory capacity).   A mediation analysis is typically conducted to better understand an observed effect of a correlation between X and Y. Why, and how, does stereotype threat influence IQ test performance?  If X and Y are correlated then we can use regression to predict Y from X  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5  If X and Y are correlated BECAUSE of the mediator M, then (X \u00e0 M \u00e0 Y):  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03f5  M \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5  If X and Y are correlated BECAUSE of the mediator M, and:  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03b2 2 X \u2005+\u2005 \u03f5  A mediator variable (M) accounts for some (partial) or all of the relationship between X and Y.  CAUTION:   Correlation does not imply causation!  In other words, there is a BIG difference between statistical mediation and true causal mediation.   Data exploration  # Summary statistics\ndescribeBy(med, med$condition)\n\n# Create a boxplot of the data\nboxplot(med$iq ~ med$condition, main = 'Boxplot', xlab = 'Group condition', ylab = 'IQ')  Run 3 regression models on the data  # Run the three regression models\n# outcome ~ predictor\nmodel_yx <- lm(med$iq ~ med$condition)\n\n# mediator ~ predictor\nmodel_mx <- lm(med$wm ~ med$condition)\n\n# outcome ~ predictor + mediator\nmodel_yxm <- lm(med$iq ~ med$condition + med$wm)\n\n# Make a summary of the three models\nsummary(model_yx)\nsummary(model_mx)\nsummary(model_yxm)  Sobel test  # Compare the previous results to the output of the sobel function\n\n# sobel(pred,med,out)\nmodel_all <- sobel(med$condition, med$wm, med$iq)\nmodel_all  The Sobel test is a method of testing the significance of a mediation \neffect. Consult the PDF.",
            "title": "3, An introduction to mediation"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/",
            "text": "1, Descriptive Statistics\n\n\nGroup data\n\n\n\n\n\n\n2, Frequency Tables, CrossTables, and\n\n    Independence\n\n\n2D frequency tables\n\n\n3D frequency tables\n\n\nCrossTable\n\n\nTests of independence\n\n\nMeasures of association\n\n\n\n\n\n\n3, Correlations\n\n\n4, t-tests\n\n\n5, Nonparametric statistics\n\n\nBivariate tests\n\n\nANOVA\n\n\n\n\n\n\n6, Multiple Regressions\n\n\nFitting the Model\n\n\nDiagnostic plots\n\n\nComparing two models with ANOVA\n\n\nCross validation\n\n\nVariable selection \u2013 Heuristic methods\n\n\nVariable selection \u2013 Graphical methods\n\n\nVariable selection \u2013 Relative importance\n\n\nGoing further\n\n\n\n\n\n\n7, Regression diagnostics\n\n\nOutliers\n\n\nInfluential observations\n\n\nNonnormality\n\n\nHeteroscedasticity\n\n\nMulticollinearity\n\n\nNonlinearity\n\n\nAutocorrelation\n\n\nGlobal diagnostic\n\n\n\n\n\n\n8, ANOVA\n\n\nEvaluate model effects\n\n\nCompare nested models directly\n\n\nMultiple comparisons\n\n\nVisualizing results\n\n\nMANOVA\n\n\nGoing further\n\n\n\n\n\n\n9, (M)ANOVA Assumptions\n\n\nOutliers\n\n\nUnivariate normality\n\n\nMultivariate normality\n\n\nHeteroscedasticity\n\n\n\n\n\n\n10, Resampling Statistics\n\n\nIndependent k-sample location tests\n\n\nSymmetry of a response for repeated measurements\n\n\nIndependence of two numeric variables\n\n\nIndependence in contingency tables\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nCodes and snippets.\n\n\n\n\n\n\n1, Descriptive Statistics\n\u00b6\n\n\nExtract basic, exploratory statistics from datasets with \napply\n, \nsummary\n, \nfivenum\n, \ndescribe\n, and \nstat.desc\n.\n\n\n# dataset\nhead(longley, 3)\n\n\n\n\n##      GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234.289      235.6        159.0    107.608 1947   60.323\n## 1948         88.5 259.426      232.5        145.6    108.632 1948   61.122\n## 1949         88.2 258.054      368.2        161.6    109.773 1949   60.171\n\n\n\n# apply a function\n# excluding missing values\nsapply(longley, mean, na.rm = TRUE) \n\n\n\n\n## GNP.deflator          GNP   Unemployed Armed.Forces   Population \n##     101.6813     387.6984     319.3313     260.6687     117.4240 \n##         Year     Employed \n##    1954.5000      65.3170\n\n\n\n# mean, median, 25th and 75th quartiles, min, max\nsummary(longley)\n\n\n\n\n##   GNP.deflator         GNP          Unemployed     Armed.Forces  \n##  Min.   : 83.00   Min.   :234.3   Min.   :187.0   Min.   :145.6  \n##  1st Qu.: 94.53   1st Qu.:317.9   1st Qu.:234.8   1st Qu.:229.8  \n##  Median :100.60   Median :381.4   Median :314.4   Median :271.8  \n##  Mean   :101.68   Mean   :387.7   Mean   :319.3   Mean   :260.7  \n##  3rd Qu.:111.25   3rd Qu.:454.1   3rd Qu.:384.2   3rd Qu.:306.1  \n##  Max.   :116.90   Max.   :554.9   Max.   :480.6   Max.   :359.4  \n##    Population         Year         Employed    \n##  Min.   :107.6   Min.   :1947   Min.   :60.17  \n##  1st Qu.:111.8   1st Qu.:1951   1st Qu.:62.71  \n##  Median :116.8   Median :1954   Median :65.50  \n##  Mean   :117.4   Mean   :1954   Mean   :65.32  \n##  3rd Qu.:122.3   3rd Qu.:1958   3rd Qu.:68.29  \n##  Max.   :130.1   Max.   :1962   Max.   :70.55\n\n\n\n# Tukey min, lower-hinge, median, upper-hinge, max\nfivenum(longley$GNP)\n\n\n\n\n## [1] 234.289 306.787 381.427 463.625 554.894\n\n\n\n# n, nmiss, unique, mean, 5, 10, 25, 50, 75, 90, 95th percentiles\n# 5 lowest and 5 highest scores\nlibrary(Hmisc)\n\ndescribe(longley)\n\n\n\n\n## longley \n## \n##  7  Variables      16  Observations\n## ---------------------------------------------------------------------------\n## GNP.deflator \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    101.7    12.74    86.90    88.35 \n##      .25      .50      .75      .90      .95 \n##    94.53   100.60   111.25   114.95   116.00 \n##                                                                       \n## Value       83.0  88.2  88.5  89.5  96.2  98.1  99.0 100.0 101.2 104.6\n## Frequency      1     1     1     1     1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062\n##                                               \n## Value      108.4 110.8 112.6 114.2 115.7 116.9\n## Frequency      1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062\n## ---------------------------------------------------------------------------\n## GNP \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    387.7    117.8    252.1    258.7 \n##      .25      .50      .75      .90      .95 \n##    317.9    381.4    454.1    510.4    527.4 \n##                                                                           \n## Value      234.289 258.054 259.426 284.599 328.975 346.999 363.112 365.385\n## Frequency        1       1       1       1       1       1       1       1\n## Proportion   0.062   0.062   0.062   0.062   0.062   0.062   0.062   0.062\n##                                                                           \n## Value      397.469 419.180 442.769 444.546 482.704 502.601 518.173 554.894\n## Frequency        1       1       1       1       1       1       1       1\n## Proportion   0.062   0.062   0.062   0.062   0.062   0.062   0.062   0.062\n## ---------------------------------------------------------------------------\n## Unemployed \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    319.3    110.1    191.6    201.6 \n##      .25      .50      .75      .90      .95 \n##    234.8    314.4    384.2    434.4    471.2 \n##                                                                       \n## Value      187.0 193.2 209.9 232.5 235.6 282.2 290.4 293.6 335.1 357.8\n## Frequency      1     1     1     1     1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062\n##                                               \n## Value      368.2 381.3 393.1 400.7 468.1 480.6\n## Frequency      1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062\n## ---------------------------------------------------------------------------\n## Armed.Forces \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    260.7    79.85    155.7    160.3 \n##      .25      .50      .75      .90      .95 \n##    229.8    271.8    306.1    344.9    355.9 \n##                                                                       \n## Value      145.6 159.0 161.6 165.0 251.4 255.2 257.2 263.7 279.8 282.7\n## Frequency      1     1     1     1     1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062\n##                                               \n## Value      285.7 304.8 309.9 335.0 354.7 359.4\n## Frequency      1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062\n## ---------------------------------------------------------------------------\n## Population \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    117.4    8.229    108.4    109.2 \n##      .25      .50      .75      .90      .95 \n##    111.8    116.8    122.3    126.6    128.4 \n##                                                                           \n## Value      107.608 108.632 109.773 110.929 112.075 113.270 115.094 116.219\n## Frequency        1       1       1       1       1       1       1       1\n## Proportion   0.062   0.062   0.062   0.062   0.062   0.062   0.062   0.062\n##                                                                           \n## Value      117.388 118.734 120.445 121.950 123.366 125.368 127.852 130.081\n## Frequency        1       1       1       1       1       1       1       1\n## Proportion   0.062   0.062   0.062   0.062   0.062   0.062   0.062   0.062\n## ---------------------------------------------------------------------------\n## Year \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1     1954    5.667     1948     1948 \n##      .25      .50      .75      .90      .95 \n##     1951     1954     1958     1960     1961 \n##                                                                       \n## Value       1947  1948  1949  1950  1951  1952  1953  1954  1955  1956\n## Frequency      1     1     1     1     1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062\n##                                               \n## Value       1957  1958  1959  1960  1961  1962\n## Frequency      1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062\n## ---------------------------------------------------------------------------\n## Employed \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    65.32    4.153    60.28    60.72 \n##      .25      .50      .75      .90      .95 \n##    62.71    65.50    68.29    69.45    69.81 \n##                                                                          \n## Value      60.171 60.323 61.122 61.187 63.221 63.639 63.761 64.989 66.019\n## Frequency       1      1      1      1      1      1      1      1      1\n## Proportion  0.062  0.062  0.062  0.062  0.062  0.062  0.062  0.062  0.062\n##                                                            \n## Value      66.513 67.857 68.169 68.655 69.331 69.564 70.551\n## Frequency       1      1      1      1      1      1      1\n## Proportion  0.062  0.062  0.062  0.062  0.062  0.062  0.062\n## ---------------------------------------------------------------------------\n\n\n\n# nbr.val, nbr.null, nbr.na, min max, range, sum,\n# median, mean, SE.mean, CI.mean, var, std.dev, coef.var\nlibrary(pastecs)\n\nstat.desc(longley)\n\n\n\n\n##              GNP.deflator          GNP   Unemployed Armed.Forces\n## nbr.val        16.0000000   16.0000000   16.0000000   16.0000000\n## nbr.null        0.0000000    0.0000000    0.0000000    0.0000000\n## nbr.na          0.0000000    0.0000000    0.0000000    0.0000000\n## min            83.0000000  234.2890000  187.0000000  145.6000000\n## max           116.9000000  554.8940000  480.6000000  359.4000000\n## range          33.9000000  320.6050000  293.6000000  213.8000000\n## sum          1626.9000000 6203.1750000 5109.3000000 4170.7000000\n## median        100.6000000  381.4270000  314.3500000  271.7500000\n## mean          101.6812500  387.6984375  319.3312500  260.6687500\n## SE.mean         2.6978884   24.8487344   23.3616062   17.3979901\n## CI.mean.0.95    5.7504129   52.9638237   49.7940849   37.0829381\n## var           116.4576250 9879.3536593 8732.2342917 4843.0409583\n## std.dev        10.7915534   99.3949378   93.4464247   69.5919604\n## coef.var        0.1061312    0.2563718    0.2926316    0.2669747\n##                Population         Year     Employed\n## nbr.val      1.600000e+01 1.600000e+01 1.600000e+01\n## nbr.null     0.000000e+00 0.000000e+00 0.000000e+00\n## nbr.na       0.000000e+00 0.000000e+00 0.000000e+00\n## min          1.076080e+02 1.947000e+03 6.017100e+01\n## max          1.300810e+02 1.962000e+03 7.055100e+01\n## range        2.247300e+01 1.500000e+01 1.038000e+01\n## sum          1.878784e+03 3.127200e+04 1.045072e+03\n## median       1.168035e+02 1.954500e+03 6.550400e+01\n## mean         1.174240e+02 1.954500e+03 6.531700e+01\n## SE.mean      1.739025e+00 1.190238e+00 8.779921e-01\n## CI.mean.0.95 3.706645e+00 2.536932e+00 1.871396e+00\n## var          4.838735e+01 2.266667e+01 1.233392e+01\n## std.dev      6.956102e+00 4.760952e+00 3.511968e+00\n## coef.var     5.923918e-02 2.435893e-03 5.376806e-02\n\n\n\n# item name, item number, nvalid, mean, sd,\n# median, mad, min, max, skew, kurtosis, se\nlibrary(psych)\n\ndescribe(longley)\n\n\n\n\n##              vars  n    mean    sd  median trimmed    mad     min     max\n## GNP.deflator    1 16  101.68 10.79  100.60  101.93  15.79   83.00  116.90\n## GNP             2 16  387.70 99.39  381.43  386.71 118.57  234.29  554.89\n## Unemployed      3 16  319.33 93.45  314.35  317.26 116.75  187.00  480.60\n## Armed.Forces    4 16  260.67 69.59  271.75  261.84  52.78  145.60  359.40\n## Population      5 16  117.42  6.96  116.80  117.22   8.17  107.61  130.08\n## Year            6 16 1954.50  4.76 1954.50 1954.50   5.93 1947.00 1962.00\n## Employed        7 16   65.32  3.51   65.50   65.31   4.31   60.17   70.55\n##               range  skew kurtosis    se\n## GNP.deflator  33.90 -0.13    -1.40  2.70\n## GNP          320.61  0.02    -1.35 24.85\n## Unemployed   293.60  0.14    -1.30 23.36\n## Armed.Forces 213.80 -0.37    -1.20 17.40\n## Population    22.47  0.26    -1.27  1.74\n## Year          15.00  0.00    -1.43  1.19\n## Employed      10.38 -0.09    -1.55  0.88\n\n\n\n# with\nwith(longley, median(GNP))\n\n\n\n\n## [1] 381.427\n\n\n\n# vs.\nmedian(longley$GNP)\n\n\n\n\n## [1] 381.427\n\n\n\nGroup data\n\u00b6\n\n\n# dataset\nhead(mtcars, 3)\n\n\n\n\n##                mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n\n\n\n# variable grouped by one factor to show a function\nby(mtcars$mpg, mtcars$cyl, FUN = function(x) { c(m = mean(x), s = sd(x)) }) \n\n\n\n\n## mtcars$cyl: 4\n##         m         s \n## 26.663636  4.509828 \n## -------------------------------------------------------- \n## mtcars$cyl: 6\n##         m         s \n## 19.742857  1.453567 \n## -------------------------------------------------------- \n## mtcars$cyl: 8\n##         m         s \n## 15.100000  2.560048\n\n\n\n# description statistics by group\nlibrary(psych)\n\ndescribeBy(mtcars, group = mtcars$am)\n\n\n\n\n## \n##  Descriptive statistics by group \n## group: 0\n##      vars  n   mean     sd median trimmed    mad    min    max  range\n## mpg     1 19  17.15   3.83  17.30   17.12   3.11  10.40  24.40  14.00\n## cyl     2 19   6.95   1.54   8.00    7.06   0.00   4.00   8.00   4.00\n## disp    3 19 290.38 110.17 275.80  289.71 124.83 120.10 472.00 351.90\n## hp      4 19 160.26  53.91 175.00  161.06  77.10  62.00 245.00 183.00\n## drat    5 19   3.29   0.39   3.15    3.28   0.22   2.76   3.92   1.16\n## wt      6 19   3.77   0.78   3.52    3.75   0.45   2.46   5.42   2.96\n## qsec    7 19  18.18   1.75  17.82   18.07   1.19  15.41  22.90   7.49\n## vs      8 19   0.37   0.50   0.00    0.35   0.00   0.00   1.00   1.00\n## am      9 19   0.00   0.00   0.00    0.00   0.00   0.00   0.00   0.00\n## gear   10 19   3.21   0.42   3.00    3.18   0.00   3.00   4.00   1.00\n## carb   11 19   2.74   1.15   3.00    2.76   1.48   1.00   4.00   3.00\n##       skew kurtosis    se\n## mpg   0.01    -0.80  0.88\n## cyl  -0.95    -0.74  0.35\n## disp  0.05    -1.26 25.28\n## hp   -0.01    -1.21 12.37\n## drat  0.50    -1.30  0.09\n## wt    0.98     0.14  0.18\n## qsec  0.85     0.55  0.40\n## vs    0.50    -1.84  0.11\n## am     NaN      NaN  0.00\n## gear  1.31    -0.29  0.10\n## carb -0.14    -1.57  0.26\n## -------------------------------------------------------- \n## group: 1\n##      vars  n   mean    sd median trimmed   mad   min    max  range  skew\n## mpg     1 13  24.39  6.17  22.80   24.38  6.67 15.00  33.90  18.90  0.05\n## cyl     2 13   5.08  1.55   4.00    4.91  0.00  4.00   8.00   4.00  0.87\n## disp    3 13 143.53 87.20 120.30  131.25 58.86 71.10 351.00 279.90  1.33\n## hp      4 13 126.85 84.06 109.00  114.73 63.75 52.00 335.00 283.00  1.36\n## drat    5 13   4.05  0.36   4.08    4.02  0.27  3.54   4.93   1.39  0.79\n## wt      6 13   2.41  0.62   2.32    2.39  0.68  1.51   3.57   2.06  0.21\n## qsec    7 13  17.36  1.79  17.02   17.39  2.34 14.50  19.90   5.40 -0.23\n## vs      8 13   0.54  0.52   1.00    0.55  0.00  0.00   1.00   1.00 -0.14\n## am      9 13   1.00  0.00   1.00    1.00  0.00  1.00   1.00   0.00   NaN\n## gear   10 13   4.38  0.51   4.00    4.36  0.00  4.00   5.00   1.00  0.42\n## carb   11 13   2.92  2.18   2.00    2.64  1.48  1.00   8.00   7.00  0.98\n##      kurtosis    se\n## mpg     -1.46  1.71\n## cyl     -0.90  0.43\n## disp     0.40 24.19\n## hp       0.56 23.31\n## drat     0.21  0.10\n## wt      -1.17  0.17\n## qsec    -1.42  0.50\n## vs      -2.13  0.14\n## am        NaN  0.00\n## gear    -1.96  0.14\n## carb    -0.21  0.60\n\n\n\n# description statistics by group \nlibrary(doBy)\n\nsummaryBy(mpg + wt ~ cyl + vs, data = mtcars, \n  FUN = function(x) { c(m = mean(x), s = sd(x)) } )\n\n\n\n\n##   cyl vs    mpg.m     mpg.s     wt.m      wt.s\n## 1   4  0 26.00000        NA 2.140000        NA\n## 2   4  1 26.73000 4.7481107 2.300300 0.5982073\n## 3   6  0 20.56667 0.7505553 2.755000 0.1281601\n## 4   6  1 19.12500 1.6317169 3.388750 0.1162164\n## 5   8  0 15.10000 2.5600481 3.999214 0.7594047\n\n\n\n2, Frequency Tables, CrossTables, and Independence\n\u00b6\n\n\nCreate frequency and contingency tables from categorical variables. Perform tests of independence, measures of association, and graphically display results.\n\n\n2D frequency tables\n\u00b6\n\n\nmytable <- table(A, B)\n where A are rows, B are columns.\n\n\n# dataset\nmytable <- matrix(c(1,2,3,4), nrow = 2)\nmytable\n\n\n\n\n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n\n\n\n# A frequencies (summed over columns = 1)\nmargin.table(mytable, 1)\n\n\n\n\n## [1] 4 6\n\n\n\n# B frequencies (summed over rows = 2)\nmargin.table(mytable, 2)\n\n\n\n\n## [1] 3 7\n\n\n\n# A/(A + B)\n# cell percentages\nprop.table(mytable)\n\n\n\n\n##      [,1] [,2]\n## [1,]  0.1  0.3\n## [2,]  0.2  0.4\n\n\n\n# row percentages\nprop.table(mytable, 1)\n\n\n\n\n##           [,1]      [,2]\n## [1,] 0.2500000 0.7500000\n## [2,] 0.3333333 0.6666667\n\n\n\n# column percentages \nprop.table(mytable, 2)\n\n\n\n\n##           [,1]      [,2]\n## [1,] 0.3333333 0.4285714\n## [2,] 0.6666667 0.5714286\n\n\n\n3D frequency tables\n\u00b6\n\n\n# dataset\nhead(CO2, 3)\n\n\n\n\n## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8\n\n\n\nA <- as.numeric(CO2[, 'Plant'])\nB <- CO2[, 'conc']\nC <- CO2[, 'uptake']\nmytable <- table(A, B, C)\n\n\n\n\n# several arrays (3D)\ndim(mytable) # the printout is immense\n\n\n\n\n## [1] 12  7 76\n\n\n\n# folded table (2D)\ndim(ftable(mytable)) # the printout is immense\n\n\n\n\n## [1] 84 76\n\n\n\n# 3-Way frequency Table\nmytable <- xtabs(~A + B + C, data = mytable, na.action = na.omit)\n\ndim(ftable(mytable)) # the printout is immense\n\n\n\n\n## [1] 84 76\n\n\n\nCrossTable\n\u00b6\n\n\nA <- as.numeric(CO2[1:8, 'Plant'])\nA\n\n\n\n\n## [1] 1 1 1 1 1 1 1 2\n\n\n\nB <- CO2[1:8, 'conc']\nB\n\n\n\n\n## [1]   95  175  250  350  500  675 1000   95\n\n\n\n# 2-way cross tabulation\nlibrary(gmodels)\n\nCrossTable(A, B) # mydata$myrowvar x mydata$mycolvar\n\n\n\n\n## \n##  \n##    Cell Contents\n## |-------------------------|\n## |                       N |\n## | Chi-square contribution |\n## |           N / Row Total |\n## |           N / Col Total |\n## |         N / Table Total |\n## |-------------------------|\n## \n##  \n## Total Observations in Table:  8 \n## \n##  \n##              | B \n##            A |        95 |       175 |       250 |       350 |       500 |       675 |      1000 | Row Total | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n##            1 |         1 |         1 |         1 |         1 |         1 |         1 |         1 |         7 | \n##              |     0.321 |     0.018 |     0.018 |     0.018 |     0.018 |     0.018 |     0.018 |           | \n##              |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.875 | \n##              |     0.500 |     1.000 |     1.000 |     1.000 |     1.000 |     1.000 |     1.000 |           | \n##              |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n##            2 |         1 |         0 |         0 |         0 |         0 |         0 |         0 |         1 | \n##              |     2.250 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n##              |     1.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.125 | \n##              |     0.500 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |           | \n##              |     0.125 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n## Column Total |         2 |         1 |         1 |         1 |         1 |         1 |         1 |         8 | \n##              |     0.250 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n## \n##\n\n\n\nTests of independence\n\u00b6\n\n\nThere are more tests of independence in section 10, Resampling Statistics.\n\n\n# dataset, a contingency table\ncolors\n\n\n\n\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\n# chi-square test on 2-way tables\n# test independence of the row and column variables, p-value is calculated from the asymptotic chi-squared distribution of the test statistic\nchisq.test(colors)\n\n\n\n\n## \n##  Pearson's Chi-squared test\n## \n## data:  colors\n## X-squared = 1240, df = 12, p-value < 2.2e-16\n\n\n\n# dataset, a contingency table in 2x2 matrix form\nTeaTasting <-\nmatrix(c(3, 1, 1, 3),\n       nrow = 2,\n       dimnames = list(Guess = c(\"Milk\", \"Tea\"),\n                       Truth = c(\"Milk\", \"Tea\")))\nTeaTasting\n\n\n\n\n##       Truth\n## Guess  Milk Tea\n##   Milk    3   1\n##   Tea     1   3\n\n\n\n# Fisher exact test\nfisher.test(TeaTasting, alternative = \"greater\")\n\n\n\n\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  TeaTasting\n## p-value = 0.2429\n## alternative hypothesis: true odds ratio is greater than 1\n## 95 percent confidence interval:\n##  0.3135693       Inf\n## sample estimates:\n## odds ratio \n##   6.408309\n\n\n\n# 3D contingency table, where the last dimension refers to the strata\nRabbits <-\narray(c(0, 0, 6, 5,\n        3, 0, 3, 6,\n        6, 2, 0, 4,\n        5, 6, 1, 0,\n        2, 5, 0, 0),\n      dim = c(2, 2, 5),\n      dimnames = list(\n          Delay = c(\"None\", \"1.5h\"),\n          Response = c(\"Cured\", \"Died\"),\n          Penicillin.Level = c(\"1/8\", \"1/4\", \"1/2\", \"1\", \"4\")))\nRabbits\n\n\n\n\n## , , Penicillin.Level = 1/8\n## \n##       Response\n## Delay  Cured Died\n##   None     0    6\n##   1.5h     0    5\n## \n## , , Penicillin.Level = 1/4\n## \n##       Response\n## Delay  Cured Died\n##   None     3    3\n##   1.5h     0    6\n## \n## , , Penicillin.Level = 1/2\n## \n##       Response\n## Delay  Cured Died\n##   None     6    0\n##   1.5h     2    4\n## \n## , , Penicillin.Level = 1\n## \n##       Response\n## Delay  Cured Died\n##   None     5    1\n##   1.5h     6    0\n## \n## , , Penicillin.Level = 4\n## \n##       Response\n## Delay  Cured Died\n##   None     2    0\n##   1.5h     5    0\n\n\n\n# Mantel-Haenszel test / Cochran-Mantel-Haenszel chi-squared test, hypothesis that two nominal variables are conditionally independent in each stratum, assuming that there is no three-way interaction.\nmantelhaen.test(Rabbits)\n\n\n\n\n## \n##  Mantel-Haenszel chi-squared test with continuity correction\n## \n## data:  Rabbits\n## Mantel-Haenszel X-squared = 3.9286, df = 1, p-value = 0.04747\n## alternative hypothesis: true common odds ratio is not equal to 1\n## 95 percent confidence interval:\n##   1.026713 47.725133\n## sample estimates:\n## common odds ratio \n##                 7\n\n\n\n# dataset\n# 3-way contingency table based on variables A, B, and C\nA <- CO2[, 'Plant']\nB <- CO2[, 'conc']\nC <- CO2[, 'uptake']\nmytable <- xtabs(~A+B+C) # a 3D array\n\n\n\n\n# loglinear Models\n# mutual independence: A, B, and C are pairwise independent\nlibrary(MASS)\n\nloglm(~A + B + C, mytable)\n\n\n\n\n## Call:\n## loglm(formula = ~A + B + C, data = mytable)\n## \n## Statistics:\n##                        X^2   df  P(> X^2)\n## Likelihood Ratio  720.1035 6291 1.0000000\n## Pearson          6300.0000 6291 0.4656775\n\n\n\n# conditional independence: A is independent of B, given C\nloglm(~A + B + C + A * C + B * C, mytable)\n\n\n\n\n## Call:\n## loglm(formula = ~A + B + C + A * C + B * C, data = mytable)\n## \n## Statistics:\n##                       X^2   df P(> X^2)\n## Likelihood Ratio 12.13685 5016        1\n## Pearson               NaN 5016      NaN\n\n\n\n# no three-way interaction\nloglm(~A + B + C + A * B + A * C + B * C, mytable)\n\n\n\n\n## Call:\n## loglm(formula = ~A + B + C + A * B + A * C + B * C, data = mytable)\n## \n## Statistics:\n##                       X^2   df P(> X^2)\n## Likelihood Ratio 1.038376 4950        1\n## Pearson               NaN 4950      NaN\n\n\n\nMeasures of association\n\u00b6\n\n\nAssociation between two nominal variables, giving a value between 0 and +1 (inclusive). It is based on Pearson\u2019s chi-squared statistic.\n\n\n# Dataset\nstr(Arthritis)\n\n\n\n\n## 'data.frame':    84 obs. of  5 variables:\n##  $ ID       : int  57 46 77 17 36 23 75 39 33 55 ...\n##  $ Treatment: Factor w/ 2 levels \"Placebo\",\"Treated\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Sex      : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Age      : int  27 29 30 32 46 58 59 59 63 63 ...\n##  $ Improved : Ord.factor w/ 3 levels \"None\"<\"Some\"<..: 2 1 1 3 3 3 1 3 1 1 ...\n\n\n\ntab <- xtabs(~Improved + Treatment, data = Arthritis)\ntab\n\n\n\n\n##         Treatment\n## Improved Placebo Treated\n##   None        29      13\n##   Some         7       7\n##   Marked       7      21\n\n\n\nsummary(assocstats(tab))\n\n\n\n\n## \n## Call: xtabs(formula = ~Improved + Treatment, data = Arthritis)\n## Number of cases in table: 84 \n## Number of factors: 2 \n## Test for independence of all factors:\n##  Chisq = 13.055, df = 2, p-value = 0.001463\n##                     X^2 df  P(> X^2)\n## Likelihood Ratio 13.530  2 0.0011536\n## Pearson          13.055  2 0.0014626\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.367 \n## Cramer's V        : 0.394\n\n\n\n# phi coefficient, contingency coefficient, and Cram\u00e9r's V for an 2D table\nlibrary(vcd)\n\nassocstats(tab)\n\n\n\n\n##                     X^2 df  P(> X^2)\n## Likelihood Ratio 13.530  2 0.0011536\n## Pearson          13.055  2 0.0014626\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.367 \n## Cramer's V        : 0.394\n\n\n\n# Dataset\nTeaTasting\n\n\n\n\n##       Truth\n## Guess  Milk Tea\n##   Milk    3   1\n##   Tea     1   3\n\n\n\n# Cohen's kappa and weighted kappa for a confusion matrix\nlibrary(vcd)\n\nkappa(TeaTasting)\n\n\n\n\n## [1] 2.333333\n\n\n\n3, Correlations\n\u00b6\n\n\n# dataset\nhead(mtcars, 3)\n\n\n\n\n##                mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n\n\n\n# correlations/covariances among numeric variables in\n# a data frame\ncor(mtcars, use = \"complete.obs\", method = \"kendall\") # method = \"pearson\", \"spearman\" or \"kendall\"\n\n\n\n\n##             mpg        cyl       disp         hp        drat         wt\n## mpg   1.0000000 -0.7953134 -0.7681311 -0.7428125  0.46454879 -0.7278321\n## cyl  -0.7953134  1.0000000  0.8144263  0.7851865 -0.55131785  0.7282611\n## disp -0.7681311  0.8144263  1.0000000  0.6659987 -0.49898277  0.7433824\n## hp   -0.7428125  0.7851865  0.6659987  1.0000000 -0.38262689  0.6113081\n## drat  0.4645488 -0.5513178 -0.4989828 -0.3826269  1.00000000 -0.5471495\n## wt   -0.7278321  0.7282611  0.7433824  0.6113081 -0.54714953  1.0000000\n## qsec  0.3153652 -0.4489698 -0.3008155 -0.4729061  0.03272155 -0.1419881\n## vs    0.5896790 -0.7710007 -0.6033059 -0.6305926  0.37510111 -0.4884787\n## am    0.4690128 -0.4946212 -0.5202739 -0.3039956  0.57554849 -0.6138790\n## gear  0.4331509 -0.5125435 -0.4759795 -0.2794458  0.58392476 -0.5435956\n## carb -0.5043945  0.4654299  0.4137360  0.5959842 -0.09535193  0.3713741\n##             qsec         vs          am        gear        carb\n## mpg   0.31536522  0.5896790  0.46901280  0.43315089 -0.50439455\n## cyl  -0.44896982 -0.7710007 -0.49462115 -0.51254349  0.46542994\n## disp -0.30081549 -0.6033059 -0.52027392 -0.47597955  0.41373600\n## hp   -0.47290613 -0.6305926 -0.30399557 -0.27944584  0.59598416\n## drat  0.03272155  0.3751011  0.57554849  0.58392476 -0.09535193\n## wt   -0.14198812 -0.4884787 -0.61387896 -0.54359562  0.37137413\n## qsec  1.00000000  0.6575431 -0.16890405 -0.09126069 -0.50643945\n## vs    0.65754312  1.0000000  0.16834512  0.26974788 -0.57692729\n## am   -0.16890405  0.1683451  1.00000000  0.77078758 -0.05859929\n## gear -0.09126069  0.2697479  0.77078758  1.00000000  0.09801487\n## carb -0.50643945 -0.5769273 -0.05859929  0.09801487  1.00000000\n\n\n\ncov(mtcars, use = \"complete.obs\") \n\n\n\n\n##              mpg         cyl        disp          hp         drat\n## mpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351\n## cyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694\n## disp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915\n## hp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887\n## drat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135\n## wt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073\n## qsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073\n## vs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919\n## am      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121\n## gear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790\n## carb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726\n##               wt         qsec           vs           am        gear\n## mpg   -5.1166847   4.50914919   2.01713710   1.80393145   2.1356855\n## cyl    1.3673710  -1.88685484  -0.72983871  -0.46572581  -0.6491935\n## disp 107.6842040 -96.05168145 -44.37762097 -36.56401210 -50.8026210\n## hp    44.1926613 -86.77008065 -24.98790323  -8.32056452  -6.3588710\n## drat  -0.3727207   0.08714073   0.11864919   0.19015121   0.2759879\n## wt     0.9573790  -0.30548161  -0.27366129  -0.33810484  -0.4210806\n## qsec  -0.3054816   3.19316613   0.67056452  -0.20495968  -0.2804032\n## vs    -0.2736613   0.67056452   0.25403226   0.04233871   0.0766129\n## am    -0.3381048  -0.20495968   0.04233871   0.24899194   0.2923387\n## gear  -0.4210806  -0.28040323   0.07661290   0.29233871   0.5443548\n## carb   0.6757903  -1.89411290  -0.46370968   0.04637097   0.3266129\n##             carb\n## mpg  -5.36310484\n## cyl   1.52016129\n## disp 79.06875000\n## hp   83.03629032\n## drat -0.07840726\n## wt    0.67579032\n## qsec -1.89411290\n## vs   -0.46370968\n## am    0.04637097\n## gear  0.32661290\n## carb  2.60887097\n\n\n\n# correlations with significance levels\nlibrary(Hmisc)\n\nrcorr(as.matrix(mtcars), type = \"pearson\") # type can be pearson or spearman\n\n\n\n\n##        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n## mpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55\n## cyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53\n## disp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39\n## hp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75\n## drat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09\n## wt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43\n## qsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66\n## vs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57\n## am    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06\n## gear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27\n## carb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00\n## \n## n= 32 \n## \n## \n## P\n##      mpg    cyl    disp   hp     drat   wt     qsec   vs     am     gear  \n## mpg         0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0054\n## cyl  0.0000        0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0022 0.0042\n## disp 0.0000 0.0000        0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0010\n## hp   0.0000 0.0000 0.0000        0.0100 0.0000 0.0000 0.0000 0.1798 0.4930\n## drat 0.0000 0.0000 0.0000 0.0100        0.0000 0.6196 0.0117 0.0000 0.0000\n## wt   0.0000 0.0000 0.0000 0.0000 0.0000        0.3389 0.0010 0.0000 0.0005\n## qsec 0.0171 0.0004 0.0131 0.0000 0.6196 0.3389        0.0000 0.2057 0.2425\n## vs   0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000        0.3570 0.2579\n## am   0.0003 0.0022 0.0004 0.1798 0.0000 0.0000 0.2057 0.3570        0.0000\n## gear 0.0054 0.0042 0.0010 0.4930 0.0000 0.0005 0.2425 0.2579 0.0000       \n## carb 0.0011 0.0019 0.0253 0.0000 0.6212 0.0146 0.0000 0.0007 0.7545 0.1290\n##      carb  \n## mpg  0.0011\n## cyl  0.0019\n## disp 0.0253\n## hp   0.0000\n## drat 0.6212\n## wt   0.0146\n## qsec 0.0000\n## vs   0.0007\n## am   0.7545\n## gear 0.1290\n## carb\n\n\n\n# dataset\nx <- mtcars[1:3]\ny <- mtcars[4:6]\n\n\n\n\n# correlation between two vectors\ncor(x, y)\n\n\n\n\n##              hp       drat         wt\n## mpg  -0.7761684  0.6811719 -0.8676594\n## cyl   0.8324475 -0.6999381  0.7824958\n## disp  0.7909486 -0.7102139  0.8879799\n\n\n\nPolychoric correlation\n\n\nThe correlation between two theorised normally distributed continuous latent variables, from two observed ordinal variables.\n\n\n# dataset\n# 2-way contingency table of counts\ncolors\n\n\n\n\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\n# polychoric correlation\nlibrary(polycor)\n\npolychor(colors)\n\n\n\n\n## [1] 0.4743984\n\n\n\n# heterogeneous correlations in one matrix\n# pearson (numeric-numeric),\n# polyserial (numeric-ordinal),\n# and polychoric (ordinal-ordinal)\n# a data frame with ordered factors and numeric variables\nhetcor(colors)\n\n\n\n\n## \n## Two-Step Estimates\n## \n## Correlations/Type of Correlation:\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## fair.hair           1  Pearson     Pearson   Pearson    Pearson\n## red.hair       0.8333        1     Pearson   Pearson    Pearson\n## medium.hair    0.2597   0.6467           1   Pearson    Pearson\n## dark.hair     -0.7091  -0.2251      0.1911         1    Pearson\n## black.hair     -0.781  -0.3875    -0.06329    0.9674          1\n## \n## Standard Errors:\n##             fair.hair red.hair medium.hair dark.hair\n## fair.hair                                           \n## red.hair      0.03628                               \n## medium.hair    0.3607   0.1383                      \n## dark.hair     0.09977   0.3733      0.3841          \n## black.hair    0.06019   0.3004      0.4092  0.001491\n## \n## n = 4 \n## \n## P-values for Tests of Bivariate Normality:\n##             fair.hair red.hair medium.hair dark.hair\n## fair.hair                                           \n## red.hair       0.8306                               \n## medium.hair    0.6939   0.8609                      \n## dark.hair      0.7465   0.6335      0.7161          \n## black.hair     0.6582   0.5927      0.6711    0.9873\n\n\n\n4, t-tests\n\u00b6\n\n\n# independent 2-group t-test\nt.test(y ~ x) # where y is numeric and x is a binary factor\n\n# independent 2-group t-test\nt.test(y1, y2) # where y1 and y2 are numeric \n\n# paired t-test\nt.test(y1, y2, paired = TRUE) # where y1 & y2 are numeric \n\n# one sample t-test\nt.test(y, mu = 3) # Ho: mu=3\n\n\n\n\n# dataset\n# 2-way contingency table\ncolors\n\n\n\n\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\nmean(as.numeric(colors[1,])) # row 1 average\n\n\n\n\n## [1] 143.6\n\n\n\n# independent 2-group t-test\nt.test(as.numeric(colors[1,]), as.numeric(colors[2,])) # where y1 and y2 are numeric \n\n\n\n\n## \n##  Welch Two Sample t-test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## t = -1.164, df = 5.5777, p-value = 0.2918\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -541.5725  196.7725\n## sample estimates:\n## mean of x mean of y \n##     143.6     316.0\n\n\n\n# one sample t-test\nt.test(as.numeric(colors[1,]), mu = 0) # Ho: mu=0\n\n\n\n\n## \n##  One Sample t-test\n## \n## data:  as.numeric(colors[1, ])\n## t = 2.348, df = 4, p-value = 0.07868\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  -26.2009 313.4009\n## sample estimates:\n## mean of x \n##     143.6\n\n\n\nt.test(as.numeric(colors[1,]), mu = 0, alternative=\"greater\") # Ho: mu=<0\n\n\n\n\n## \n##  One Sample t-test\n## \n## data:  as.numeric(colors[1, ])\n## t = 2.348, df = 4, p-value = 0.03934\n## alternative hypothesis: true mean is greater than 0\n## 95 percent confidence interval:\n##  13.22123      Inf\n## sample estimates:\n## mean of x \n##     143.6\n\n\n\n\n\nalternative=\"less\"\n or \nalternative=\"greater\"\n option to specify a one tailed test.\n\n\nvar.equal = TRUE\n option to specify equal variances and a pooled variance estimate.\n\n\n\n\nFor multivariate tests and ANOVA, see sections 8 and 9.\n\n\n5, Nonparametric statistics\n\u00b6\n\n\nNonnormal distributions.\n\n\nBivariate tests\n\u00b6\n\n\n# independent 2-group Mann-Whitney U test\nwilcox.test(y ~ A) # where y is numeric and A is A binary factor\n\n# independent 2-group Mann-Whitney U test\nwilcox.test(y, x) # where y and x are numeric\n\n# dependent 2-group Wilcoxon signed rank test\nwilcox.test(y1, y2, paired = TRUE) # where y1 and y2 are numeric\n\n\n\n\n# 2-way contingency table\ncolors\n\n\n\n\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\n# independent 2-group Mann-Whitney U Test\nwilcox.test(as.numeric(colors[1,]), as.numeric(colors[2,])) # where y and x are numeric\n\n\n\n\n## \n##  Wilcoxon rank sum test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## W = 8, p-value = 0.4206\n## alternative hypothesis: true location shift is not equal to 0\n\n\n\nalternative=\"less\"\n or \nalternative=\"greater\"\n option to specify a one\n\ntailed test.\n\n\nANOVA\n\u00b6\n\n\n# Kruskal Wallis test one-Way ANOVA by ranks\nkruskal.test(y ~ A) # where y1 is numeric and A is a factor\n\n# randomized block design - Friedman test\nfriedman.test(y ~ A | B) # where y are the data values, A is a grouping factor and B is a blocking factor\n\n# Kruskal Wallis test one-way ANOVA by ranks\nkruskal.test(y, x) # where y and x are numeric\n\n\n\n\n# dataset\n# 2-way contingency table\ncolors\n\n\n\n\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\n# Kruskal Wallis test one-way ANOVA by ranks\nkruskal.test(as.numeric(colors[1,]), as.numeric(colors[2,])) # where y and x are numeric\n\n\n\n\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## Kruskal-Wallis chi-squared = 4, df = 4, p-value = 0.406\n\n\n\n6, Multiple Regressions\n\u00b6\n\n\nFitting the Model\n\u00b6\n\n\n# multiple linear regression\nfit <- lm(y ~ x1 + x2 + x3, data = mydata)\nsummary(fit) # show results\n\n# useful functions\ncoefficients(fit) # model coefficients\nconfint(fit, level = 0.95) # CIs for model parameters\nfitted(fit) # predicted values\nresiduals(fit) # residuals\nanova(fit) # ANOVA table\nvcov(fit) # covariance matrix for model parameters\ninfluence(fit) # regression diagnostics \n\n\n\n\n# dataset\nstr(longley)\n\n\n\n\n## 'data.frame':    16 obs. of  7 variables:\n##  $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...\n##  $ GNP         : num  234 259 258 285 329 ...\n##  $ Unemployed  : num  236 232 368 335 210 ...\n##  $ Armed.Forces: num  159 146 162 165 310 ...\n##  $ Population  : num  108 109 110 111 112 ...\n##  $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...\n##  $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...\n\n\n\n# multiple linear regression example\nfit <- lm(Armed.Forces ~ GNP + Population, data = longley)\nsummary(fit) # show results\n\n\n\n\n## \n## Call:\n## lm(formula = Armed.Forces ~ GNP + Population, data = longley)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -70.349 -33.569   5.076  16.409 104.037 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)   \n## (Intercept) 4123.922   1276.579   3.230  0.00657 **\n## GNP            3.365      0.986   3.413  0.00463 **\n## Population   -44.011     14.089  -3.124  0.00807 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 50.56 on 13 degrees of freedom\n## Multiple R-squared:  0.5426, Adjusted R-squared:  0.4723 \n## F-statistic: 7.712 on 2 and 13 DF,  p-value: 0.006191\n\n\n\n# other useful functions\ncoefficients(fit) # model coefficients\n\n\n\n\n## (Intercept)         GNP  Population \n## 4123.922484    3.365215  -44.010955\n\n\n\nconfint(fit, level = 0.95) # CIs for model parameters\n\n\n\n\n##                   2.5 %      97.5 %\n## (Intercept) 1366.042123 6881.802846\n## GNP            1.235097    5.495333\n## Population   -74.447969  -13.573942\n\n\n\nfitted(fit) # predicted values\n\n\n\n\n##     1947     1948     1949     1950     1951     1952     1953     1954 \n## 176.4245 215.9487 161.1151 199.5681 298.4663 306.5279 288.1248 230.9633 \n##     1955     1956     1957     1958     1959     1960     1961     1962 \n## 295.1332 308.9566 313.0359 252.7794 318.8698 297.7176 240.7975 266.2711\n\n\n\nresiduals(fit) # residuals\n\n\n\n\n##        1947        1948        1949        1950        1951        1952 \n## -17.4245114 -70.3487085   0.4848669 -34.5681071  11.4336564  52.8721086 \n##        1953        1954        1955        1956        1957        1958 \n##  66.5752438 104.0367029   9.6668098 -23.2566323 -33.2359499  10.9205505 \n##        1959        1960        1961        1962 \n## -63.6698197 -46.3175746  16.4025069  16.4288577\n\n\n\nanova(fit) # ANOVA table\n\n\n\n\n## Analysis of Variance Table\n## \n## Response: Armed.Forces\n##            Df Sum Sq Mean Sq F value   Pr(>F)   \n## GNP         1  14479 14478.7  5.6649 0.033301 * \n## Population  1  24941 24940.8  9.7583 0.008068 **\n## Residuals  13  33226  2555.9                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nvcov(fit) # covariance matrix for model parameters\n\n\n\n\n##             (Intercept)          GNP   Population\n## (Intercept) 1629652.882 1239.7478355 -17970.27388\n## GNP            1239.748    0.9721911    -13.76775\n## Population   -17970.274  -13.7677545    198.49444\n\n\n\ninfluence(fit) # regression diagnostics \n\n\n\n\n## $hat\n##       1947       1948       1949       1950       1951       1952 \n## 0.27412252 0.17438942 0.31563199 0.16765687 0.21219668 0.21127212 \n##       1953       1954       1955       1956       1957       1958 \n## 0.11339116 0.08602104 0.10270245 0.12845683 0.13251347 0.11070418 \n##       1959       1960       1961       1962 \n## 0.15598638 0.15164367 0.32488437 0.33842686 \n## \n## $coefficients\n##      (Intercept)          GNP  Population\n## 1947  128.042531  0.131479474 -1.53731106\n## 1948   29.040635  0.121992502 -0.69544934\n## 1949   -6.396740 -0.005738655  0.07379997\n## 1950  177.778426  0.175668519 -2.11609661\n## 1951  133.333008  0.093997366 -1.43810938\n## 1952  638.680267  0.462229807 -6.92955762\n## 1953  422.107967  0.305133565 -4.56222462\n## 1954 -385.998493 -0.325674565  4.42308458\n## 1955   54.458118  0.042127997 -0.59713302\n## 1956 -163.371695 -0.131240587  1.81041089\n## 1957 -212.039316 -0.179084306  2.37664757\n## 1958  -51.395691 -0.033854337  0.55600613\n## 1959 -329.488802 -0.311550864  3.79446941\n## 1960    3.116881 -0.049904757  0.10916689\n## 1961 -242.199361 -0.158976865  2.60043035\n## 1962 -194.416431 -0.113799395  2.04462752\n## \n## $sigma\n##     1947     1948     1949     1950     1951     1952     1953     1954 \n## 52.28757 47.63741 52.61955 51.47046 52.48826 49.73420 48.50003 42.21357 \n##     1955     1956     1957     1958     1959     1960     1961     1962 \n## 52.53729 52.12610 51.60167 52.51353 48.66817 50.57779 52.30331 52.29577 \n## \n## $wt.res\n##        1947        1948        1949        1950        1951        1952 \n## -17.4245114 -70.3487085   0.4848669 -34.5681071  11.4336564  52.8721086 \n##        1953        1954        1955        1956        1957        1958 \n##  66.5752438 104.0367029   9.6668098 -23.2566323 -33.2359499  10.9205505 \n##        1959        1960        1961        1962 \n## -63.6698197 -46.3175746  16.4025069  16.4288577\n\n\n\nDiagnostic plots\n\u00b6\n\n\n# diagnostic plots\nlayout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page\nplot(fit)\n\n\n\n\n\n\nlayout(matrix(c(1,1,1,1),2,2))\n\n\n\n\nComparing two models with ANOVA\n\u00b6\n\n\n# compare models\nfit1 <- lm(y ~ x1 + x2 + x3 + x4, data = mydata)\nfit2 <- lm(y ~ x1 + x2)\n\nanova(fit1, fit2) \n\n\n\n\n# compare models\nfit1 <- lm(Armed.Forces ~ GNP + Population, data = longley)\nfit2 <- lm(Armed.Forces ~ GNP, data = longley)\n\nanova(fit1, fit2) \n\n\n\n\n## Analysis of Variance Table\n## \n## Model 1: Armed.Forces ~ GNP + Population\n## Model 2: Armed.Forces ~ GNP\n##   Res.Df   RSS Df Sum of Sq      F   Pr(>F)   \n## 1     13 33226                                \n## 2     14 58167 -1    -24941 9.7583 0.008068 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCross validation\n\u00b6\n\n\n# dataset\nlibrary(DAAG)\n\ndata(houseprices)\nhead(houseprices, 3)\n\n\n\n\n##    area bedrooms sale.price\n## 9   694        4        192\n## 10  905        4        215\n## 11  802        4        215\n\n\n\n# k-fold cross-validation\nlibrary(DAAG)\n\n# case 1, # 3 fold cross-validation\nCVlm(houseprices, form.lm = formula(sale.price ~ area), m = 3, dots = FALSE, seed = 29, plotit = c(\"Observed\",\"Residual\"), main = \"Small symbols show cross-validation predicted values\", legend.pos = \"topleft\", printit = TRUE)\n\n\n\n\n## Analysis of Variance Table\n## \n## Response: sale.price\n##           Df Sum Sq Mean Sq F value Pr(>F)  \n## area       1  18566   18566       8  0.014 *\n## Residuals 13  30179    2321                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n## \n## fold 1 \n## Observations in test set: 5 \n##              11  20    21     22   23\n## area        802 696 771.0 1006.0 1191\n## cvpred      204 188 199.3  234.7  262\n## sale.price  215 255 260.0  293.0  375\n## CV residual  11  67  60.7   58.3  113\n## \n## Sum of squares = 24351    Mean square = 4870    n = 5 \n## \n## fold 2 \n## Observations in test set: 5 \n##              10   13    14      17     18\n## area        905  716 963.0 1018.00 887.00\n## cvpred      255  224 264.4  273.38 252.06\n## sale.price  215  113 185.0  276.00 260.00\n## CV residual -40 -112 -79.4    2.62   7.94\n## \n## Sum of squares = 20416    Mean square = 4083    n = 5 \n## \n## fold 3 \n## Observations in test set: 5 \n##                 9   12     15    16     19\n## area        694.0 1366 821.00 714.0 790.00\n## cvpred      183.2  388 221.94 189.3 212.49\n## sale.price  192.0  274 212.00 220.0 221.50\n## CV residual   8.8 -114  -9.94  30.7   9.01\n## \n## Sum of squares = 14241    Mean square = 2848    n = 5 \n## \n## Overall (Sum over all 5 folds) \n##   ms \n## 3934\n\n\n\n# dataset\nhead(longley, 3)\n\n\n\n\n##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2\n\n\n\n# case 2, # 3 fold cross-validation\nCVlm(longley, form.lm = formula(Armed.Forces ~ GNP + Population), m = 3, dots = FALSE, seed = 29, plotit = c(\"Observed\",\"Residual\"), main = \"Small symbols show cross-validation predicted values\", legend.pos = \"topleft\", printit = TRUE)\n\n\n\n\n## Analysis of Variance Table\n## \n## Response: Armed.Forces\n##            Df Sum Sq Mean Sq F value Pr(>F)   \n## GNP         1  14479   14479    5.66 0.0333 * \n## Population  1  24941   24941    9.76 0.0081 **\n## Residuals  13  33226    2556                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n## \n## fold 1 \n## Observations in test set: 5 \n##               1953 1954  1957  1960  1962\n## Predicted    288.1  231 313.0 297.7 266.3\n## cvpred       281.4  219 310.6 295.7 263.1\n## Armed.Forces 354.7  335 279.8 251.4 282.7\n## CV residual   73.3  116 -30.8 -44.3  19.6\n## \n## Sum of squares = 22004    Mean square = 4401    n = 5 \n## \n## fold 2 \n## Observations in test set: 6 \n##               1948   1949   1955   1958  1959  1961\n## Predicted    215.9 161.12 295.13 252.78 318.9 240.8\n## cvpred       231.9 171.41 306.33 255.07 324.5 234.9\n## Armed.Forces 145.6 161.60 304.80 263.70 255.2 257.2\n## CV residual  -86.3  -9.81  -1.53   8.63 -69.3  22.3\n## \n## Sum of squares = 12917    Mean square = 2153    n = 6 \n## \n## fold 3 \n## Observations in test set: 5 \n##               1947  1950  1951  1952   1956\n## Predicted    176.4 199.6 298.5 306.5 308.96\n## cvpred       192.8 211.9 282.3 288.9 295.17\n## Armed.Forces 159.0 165.0 309.9 359.4 285.70\n## CV residual  -33.8 -46.9  27.6  70.5  -9.47\n## \n## Sum of squares = 9161    Mean square = 1832    n = 5 \n## \n## Overall (Sum over all 5 folds) \n##   ms \n## 2755\n\n\n\nVariable selection \u2013 Heuristic methods\n\u00b6\n\n\n# dataset\nhead(longley, 3)\n\n\n\n\n##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2\n\n\n\n# Stepwise Regression\nlibrary(MASS)\n\nfit <- lm(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley)\nstep <- stepAIC(fit, direction = \"both\")\n\n\n\n\n## Start:  AIC=125\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n##              Df Sum of Sq   RSS AIC\n## <none>                    21655 125\n## - Unemployed  1      4508 26163 126\n## - Population  1      5522 27177 127\n## - Employed    1     10208 31863 130\n## - GNP         1     15323 36978 132\n\n\n\nstep$anova # display results\n\n\n\n\n## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n## Final Model:\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n## \n##   Step Df Deviance Resid. Df Resid. Dev AIC\n## 1                         11      21655 125\n\n\n\nThe goal is to reduce the AIC. Adding variable does not improve the model. Let\u2019s opt for the most valuable variable.\n\n\nfit <- lm(Armed.Forces ~ Unemployed, data = longley)\nstep <- stepAIC(fit, direction = \"both\")\n\n\n\n\n## Start:  AIC=138\n## Armed.Forces ~ Unemployed\n## \n##              Df Sum of Sq   RSS AIC\n## - Unemployed  1      2287 72646 137\n## <none>                    70359 138\n## \n## Step:  AIC=137\n## Armed.Forces ~ 1\n## \n##              Df Sum of Sq   RSS AIC\n## <none>                    72646 137\n## + Unemployed  1      2287 70359 138\n\n\n\nstep$anova # display results\n\n\n\n\n## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## Armed.Forces ~ Unemployed\n## \n## Final Model:\n## Armed.Forces ~ 1\n## \n## \n##           Step Df Deviance Resid. Df Resid. Dev AIC\n## 1                                 14      70359 138\n## 2 - Unemployed  1     2287        15      72646 137\n\n\n\nVariable selection \u2013 Graphical methods\n\u00b6\n\n\n# model\nfit <- lm(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley)\n\n\n\n\n# all Subsets Regression\nlibrary(leaps)\n\nleaps <- regsubsets(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley, nbest = 10)\n\n# view results\nsummary(leaps)\n\n\n\n\n## Subset selection object\n## Call: regsubsets.formula(Armed.Forces ~ GNP + Population + Employed + \n##     Unemployed, data = longley, nbest = 10)\n## 4 Variables  (and intercept)\n##            Forced in Forced out\n## GNP            FALSE      FALSE\n## Population     FALSE      FALSE\n## Employed       FALSE      FALSE\n## Unemployed     FALSE      FALSE\n## 10 subsets of each size up to 4\n## Selection Algorithm: exhaustive\n##          GNP Population Employed Unemployed\n## 1  ( 1 ) \" \" \" \"        \"*\"      \" \"       \n## 1  ( 2 ) \"*\" \" \"        \" \"      \" \"       \n## 1  ( 3 ) \" \" \"*\"        \" \"      \" \"       \n## 1  ( 4 ) \" \" \" \"        \" \"      \"*\"       \n## 2  ( 1 ) \"*\" \"*\"        \" \"      \" \"       \n## 2  ( 2 ) \"*\" \" \"        \" \"      \"*\"       \n## 2  ( 3 ) \" \" \"*\"        \" \"      \"*\"       \n## 2  ( 4 ) \" \" \" \"        \"*\"      \"*\"       \n## 2  ( 5 ) \" \" \"*\"        \"*\"      \" \"       \n## 2  ( 6 ) \"*\" \" \"        \"*\"      \" \"       \n## 3  ( 1 ) \"*\" \"*\"        \"*\"      \" \"       \n## 3  ( 2 ) \"*\" \" \"        \"*\"      \"*\"       \n## 3  ( 3 ) \"*\" \"*\"        \" \"      \"*\"       \n## 3  ( 4 ) \" \" \"*\"        \"*\"      \"*\"       \n## 4  ( 1 ) \"*\" \"*\"        \"*\"      \"*\"\n\n\n\n# plot a table of models showing variables in each model\n# models are ordered by the selection statistic\nplot(leaps, scale = \"r2\")\n\n\n\n\n\n\n# plot statistic by subset size\nlibrary(car)\n\nsubsets(leaps, statistic = \"rsq\", legend = FALSE) # available criteria are rsq, rss, adjr2, cp, bic\n\n\n\n\n\n\n##            Abbreviation\n## GNP                   G\n## Population            P\n## Employed              E\n## Unemployed            U\n\n\n\nsubsets(leaps, statistic = \"bic\", legend = FALSE) # available criteria are rsq, rss, adjr2, cp, bic\n\n\n\n\n\n\n##            Abbreviation\n## GNP                   G\n## Population            P\n## Employed              E\n## Unemployed            U\n\n\n\nVariable selection \u2013 Relative importance\n\u00b6\n\n\nModel: \nfit <- lm(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley)\n.\n\n\nWarning: \neval=FALSE\n.\n\n\n# calculate the relative importance of each predictor\nlibrary(relaimpo)\n\ncalc.relimp(fit, type = c(\"lmg\", \"last\", \"first\", \"pratt\"), rela = TRUE)\n\n\n\n\nResults.\n\n\nResponse variable: Armed.Forces \nTotal response variance: 4843 \nAnalysis based on 16 observations\n\n4 Regressors: \nGNP Population Employed Unemployed \nProportion of variance explained by model: 70.2%\nMetrics are normalized to sum to 100% (rela=TRUE).\n\nRelative importance metrics:\n\n             lmg  last first  pratt\nGNP        0.328 0.431 0.348  4.566\nPopulation 0.241 0.155 0.232 -1.934\nEmployed   0.217 0.287 0.365 -1.780\nUnemployed 0.215 0.127 0.055  0.148\n\nAverage coefficients for different model sizes:\n\n               1X     2Xs     3Xs     4Xs\nGNP         0.313   1.301   3.641   5.026\nPopulation  3.646 -14.815 -24.637 -37.260\nEmployed    9.062  17.646 -34.249 -54.144\nUnemployed -0.132  -0.511  -0.584  -0.436\n\n\n\nBootstrapping\n\n\nModel: \nfit <- lm(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley)\n.\n\n\nWarning: \neval=FALSE\n.\n\n\n# bootstrap measures of relative importance (1000 samples)\nboot <- boot.relimp(fit, b = 1000, type = c(\"lmg\", \"last\", \"first\", \"pratt\"), rank = TRUE, diff = TRUE, rela = TRUE)\nbooteval.relimp(boot) # print result\n\n\n\n\nResults.\n\n\nResponse variable: Armed.Forces \nTotal response variance: 4843 \nAnalysis based on 16 observations\n\n4 Regressors: \nGNP Population Employed Unemployed \nProportion of variance explained by model: 70.2%\nMetrics are normalized to sum to 100% (rela=TRUE).\n\nRelative importance metrics:\n\n             lmg  last first  pratt\nGNP        0.328 0.431 0.348  4.566\nPopulation 0.241 0.155 0.232 -1.934\nEmployed   0.217 0.287 0.365 -1.780\nUnemployed 0.215 0.127 0.055  0.148\n\nAverage coefficients for different model sizes:\n\n               1X     2Xs     3Xs     4Xs\nGNP         0.313   1.301   3.641   5.026\nPopulation  3.646 -14.815 -24.637 -37.260\nEmployed    9.062  17.646 -34.249 -54.144\nUnemployed -0.132  -0.511  -0.584  -0.436\n\n\n Confidence interval information ( 1000 bootstrap replicates, bty= perc ): \nRelative Contributions with confidence intervals:\n\n                                 Lower  Upper\n                 percentage 0.95 0.95    0.95   \nGNP.lmg           0.3280    ABC_  0.1488  0.4030\nPopulation.lmg    0.2410    _BCD  0.1572  0.3060\nEmployed.lmg      0.2170    ABCD  0.1107  0.3240\nUnemployed.lmg    0.2150    ABCD  0.0586  0.5550\n\nGNP.last          0.4310    ABCD  0.0101  0.5660\nPopulation.last   0.1550    _BCD  0.0003  0.3850\nEmployed.last     0.2870    ABCD  0.0476  0.6280\nUnemployed.last   0.1270    ABCD  0.0009  0.7810\n\nGNP.first         0.3480    ABCD  0.0151  0.3960\nPopulation.first  0.2320    _BCD  0.0070  0.3060\nEmployed.first    0.3650    ABCD  0.0159  0.4020\nUnemployed.first  0.0550    ABCD  0.0004  0.9280\n\nGNP.pratt         4.5660    ABCD -1.0160 10.5380\nPopulation.pratt -1.9340    ABCD -6.0800  2.2050\nEmployed.pratt   -1.7800    _BCD -5.0290  0.8510\nUnemployed.pratt  0.1480    ABC_ -0.3250  1.0910\n\nLetters indicate the ranks covered by bootstrap CIs. \n(Rank bootstrap confidence intervals always obtained by percentile method) \nCAUTION: Bootstrap confidence intervals can be somewhat liberal.\n\n\n Differences between Relative Contributions:\n\n                                            Lower   Upper\n                            difference 0.95 0.95    0.95   \nGNP-Population.lmg           0.0871         -0.0188  0.1521\nGNP-Employed.lmg             0.1106         -0.0479  0.1949\nGNP-Unemployed.lmg           0.1130         -0.4021  0.3281\nPopulation-Employed.lmg      0.0236         -0.1091  0.1124\nPopulation-Unemployed.lmg    0.0259         -0.3904  0.2417\nEmployed-Unemployed.lmg      0.0023         -0.4315  0.2295\n\nGNP-Population.last          0.2756         -0.1062  0.4502\nGNP-Employed.last            0.1438         -0.5564  0.4011\nGNP-Unemployed.last          0.3041         -0.7408  0.5502\nPopulation-Employed.last    -0.1318         -0.6191  0.2653\nPopulation-Unemployed.last   0.0285         -0.7382  0.3622\nEmployed-Unemployed.last     0.1603         -0.6784  0.4852\n\nGNP-Population.first         0.1161         -0.0590  0.1736\nGNP-Employed.first          -0.0172         -0.0893  0.0934\nGNP-Unemployed.first         0.2930         -0.9047  0.3766\nPopulation-Employed.first   -0.1333         -0.2134  0.0688\nPopulation-Unemployed.first  0.1769         -0.8967  0.2969\nEmployed-Unemployed.first    0.3102         -0.8937  0.3823\n\nGNP-Population.pratt         6.4995         -2.2069 15.7978\nGNP-Employed.pratt           6.3461         -1.3879 15.0093\nGNP-Unemployed.pratt         4.4180         -1.9134 10.6456\nPopulation-Employed.pratt   -0.1534         -4.1860  5.9691\nPopulation-Unemployed.pratt -2.0815         -6.1439  2.2348\nEmployed-Unemployed.pratt   -1.9281         -5.2057  0.1725\n\n* indicates that CI for difference does not include 0. \nCAUTION: Bootstrap confidence intervals can be somewhat liberal.\n\n\n\nWarning: \neval=FALSE\n.\n\n\nplot(booteval.relimp(boot, sort = TRUE)) # to plot the results\n\n\n\n\nThe .png file.\n\n\n\n\n\n\n\n\nGoing further\n\u00b6\n\n\n\n\nThe \nnls\n package provides functions for nonlinear regression.\n\n\nPerform robust regression with the \nrlm\n function in the \nMASS\n package.\n\n\nThe \nrobust\n package provides a comprehensive library of robust methods, including regression.\n\n\nThe \nrobustbase\n package also provides basic robust statistics including model selection methods.\n\n\n\n\n7, Regression diagnostics\n\u00b6\n\n\n# assume that we are fitting a multiple linear regression\nfit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\nfit\n\n\n\n\n## \n## Call:\n## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars)\n## \n## Coefficients:\n## (Intercept)         disp           hp           wt         drat  \n##    29.14874      0.00382     -0.03478     -3.47967      1.76805\n\n\n\nOutliers\n\u00b6\n\n\nlibrary(car)\n\n# assessing outliers\noutlierTest(fit) # Bonferonni p-value for most extreme obs\n\n\n\n\n## \n## No Studentized residuals with Bonferonni p < 0.05\n## Largest |rstudent|:\n##                rstudent unadjusted p-value Bonferonni p\n## Toyota Corolla     2.52             0.0184        0.588\n\n\n\nqqPlot(fit, main = \"QQ Plot\") #qq plot for studentized resid\n\n\n\n\n\n\nleveragePlots(fit) # leverage plots\n\n\n\n\n\n\nInfluential observations\n\u00b6\n\n\nModel: \nfit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n.\n\n\n# influential observations\n# added variable plots\nlibrary(car)\n\navPlots(fit)\n\n\n\n\n\n\npar(mfrow = c(1,1))\n\n# Cook's D plot\n# identify D values > 4/(n-k-1)\ncutoff <- 4/((nrow(mtcars) - length(fit$coefficients) - 2))\nplot(fit, which = 4, cook.levels = cutoff)\n\n\n\n\n\n\nWarning: \neval=FALSE\n; interactive function.\n\n\n# influence plot\ninfluencePlot(fit, id.method = \"identify\", main = \"Influence Plot\", sub = \"Circle size is proportial to Cook's Distance\" )\n\n\n\n\n\n\nNonnormality\n\u00b6\n\n\nModel: \nfit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n.\n\n\n# normality of residuals\n# qq plot for studentized resid\nlibrary(car)\n\nqqPlot(fit, main = \"QQ Plot\")\n\n\n\n\n\n\n# distribution of studentized residuals\nlibrary(MASS)\n\nsresid <- studres(fit)\nhist(sresid, freq = FALSE, main = \"Distribution of Studentized Residuals\")\nxfit <- seq(min(sresid), max(sresid),length = 40)\nyfit <- dnorm(xfit)\nlines(xfit, yfit) \n\n\n\n\n\n\nHeteroscedasticity\n\u00b6\n\n\nNonconstant error variance.\n\n\nModel: \nfit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n.\n\n\n# evaluate homoscedasticity\n# non-constant error variance test\nncvTest(fit)\n\n\n\n\n## Non-constant Variance Score Test \n## Variance formula: ~ fitted.values \n## Chisquare = 1.43    Df = 1     p = 0.232\n\n\n\n# plot studentized residuals vs. fitted values\nspreadLevelPlot(fit)\n\n\n\n\n\n\n## \n## Suggested power transformation:  0.662\n\n\n\nMulticollinearity\n\u00b6\n\n\nModel: \nfit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n.\n\n\n# evaluatecCollinearity\nvif(fit)\n\n\n\n\n## disp   hp   wt drat \n## 8.21 2.89 5.10 2.28\n\n\n\nsqrt(vif(fit)) > 2 # benchmark = 1.96, rounded to 2\n\n\n\n\n##  disp    hp    wt  drat \n##  TRUE FALSE  TRUE FALSE\n\n\n\nNonlinearity\n\u00b6\n\n\nModel: \nfit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n.\n\n\n# evaluate nonlinearity\n# component + residual plot\ncrPlots(fit)\n\n\n\n\n\n\n# Ceres plots\nceresPlots(fit)\n\n\n\n\n\n\nAutocorrelation\n\u00b6\n\n\nSerial correlation or non-independence of errors.\n\n\nModel: \nfit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n.\n\n\n# test for autocorrelated errors\ndurbinWatsonTest(fit)\n\n\n\n\n##  lag Autocorrelation D-W Statistic p-value\n##    1           0.101          1.74    0.29\n##  Alternative hypothesis: rho != 0\n\n\n\nGlobal diagnostic\n\u00b6\n\n\nModel: \nfit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n.\n\n\n# global test of model assumptions\nlibrary(gvlma)\n\ngvmodel <- gvlma(fit)\nsummary(gvmodel) \n\n\n\n\n## \n## Call:\n## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.508 -1.905 -0.506  0.982  5.688 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 29.14874    6.29359    4.63  8.2e-05 ***\n## disp         0.00382    0.01080    0.35   0.7268    \n## hp          -0.03478    0.01160   -3.00   0.0058 ** \n## wt          -3.47967    1.07837   -3.23   0.0033 ** \n## drat         1.76805    1.31978    1.34   0.1915    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.6 on 27 degrees of freedom\n## Multiple R-squared:  0.838,  Adjusted R-squared:  0.814 \n## F-statistic: 34.8 on 4 and 27 DF,  p-value: 2.7e-10\n## \n## \n## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\n## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\n## Level of Significance =  0.05 \n## \n## Call:\n##  gvlma(x = fit) \n## \n##                      Value p-value                   Decision\n## Global Stat        13.9382 0.00750 Assumptions NOT satisfied!\n## Skewness            4.3131 0.03782 Assumptions NOT satisfied!\n## Kurtosis            0.0138 0.90654    Assumptions acceptable.\n## Link Function       8.7166 0.00315 Assumptions NOT satisfied!\n## Heteroscedasticity  0.8947 0.34421    Assumptions acceptable.\n\n\n\n8, ANOVA\n\u00b6\n\n\nAnalysis of variance (ANOVA) is an alternative to regressions among other applications.\n\n\n# lower case letters are numeric variables\n# upper case letters are factors\n\n# one-way ANOVA (completely randomized design)\nfit <- aov(y ~ A, data = mydataframe)\n\n# randomized block design (B is the blocking factor)\nfit <- aov(y ~ A + B, data = mydataframe)\n\n# two-way factorial design\nfit <- aov(y ~ A + B + A:B, data = mydataframe)\nfit <- aov(y ~ A*B, data = mydataframe) # same thing\n\n# analysis of covariance\nfit <- aov(y ~ A + x, data = mydataframe) \n\n\n\n\n# dataset\nhead(CO2, 3)\n\n\n\n\n## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8\n\n\n\n# one-way ANOVA (completely randomized design)\nfit <- aov(uptake ~ Plant, data = CO2)\nfit\n\n\n\n\n## Call:\n##    aov(formula = uptake ~ Plant, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## Estimated effects are balanced\n\n\n\nsummary(fit)\n\n\n\n\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# randomized block design (B is the blocking factor)\nfit <- aov(uptake ~ Plant + Type, data = CO2)\nfit\n\n\n\n\n## Call:\n##    aov(formula = uptake ~ Plant + Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 1 out of 13 effects not estimable\n## Estimated effects are balanced\n\n\n\nsummary(fit)\n\n\n\n\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# two-way factorial design\nfit <- aov(uptake ~ Plant + Type + Plant:Type, data = CO2)\nfit\n\n\n\n\n## Call:\n##    aov(formula = uptake ~ Plant + Type + Plant:Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 12 out of 24 effects not estimable\n## Estimated effects are balanced\n\n\n\nsummary(fit)\n\n\n\n\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfit <- aov(uptake ~ Plant*Type, data = CO2) # same thing\nfit\n\n\n\n\n## Call:\n##    aov(formula = uptake ~ Plant * Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 12 out of 24 effects not estimable\n## Estimated effects are balanced\n\n\n\nsummary(fit)\n\n\n\n\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Analysis of Covariance\nfit <- aov(uptake ~ uptake + conc, data = CO2)\nfit\n\n\n\n\n## Call:\n##    aov(formula = uptake ~ uptake + conc, data = CO2)\n## \n## Terms:\n##                 conc Residuals\n## Sum of Squares  2285      7422\n## Deg. of Freedom    1        82\n## \n## Residual standard error: 9.51\n## Estimated effects may be unbalanced\n\n\n\nsummary(fit)\n\n\n\n\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## conc         1   2285    2285    25.2 2.9e-06 ***\n## Residuals   82   7422      91                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nEvaluate model effects\n\u00b6\n\n\n\n\nType I sequential SS: A+B and B+A will produce different results!\n\n\nUse the \ndrop1\n to produce the familiar Type III results; compare each term with the full model.\n\n\n\n\n\n\n\n# display Type I ANOVA table\nfit1 <- aov(uptake ~ Plant + Type, data = CO2)\nsummary(fit1)\n\n\n\n\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# display Type I ANOVA table\nfit2 <- aov(uptake ~ Type + Plant, data = CO2)\nsummary(fit2)\n\n\n\n\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Type         1   3366    3366   50.02 8.1e-10 ***\n## Plant       10   1497     150    2.22   0.026 *  \n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# display type III SS and F tests\ndrop1(fit1, ~ ., test = \"F\")\n\n\n\n\n## Single term deletions\n## \n## Model:\n## uptake ~ Plant + Type\n##        Df Sum of Sq  RSS AIC F value Pr(>F)  \n## <none>              4845 365                 \n## Plant  10      1497 6341 367    2.22  0.026 *\n## Type    0         0 4845 365                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\ndrop1(fit2, ~ ., test = \"F\")\n\n\n\n\n## Single term deletions\n## \n## Model:\n## uptake ~ Type + Plant\n##        Df Sum of Sq  RSS AIC F value Pr(>F)  \n## <none>              4845 365                 \n## Type    0         0 4845 365                 \n## Plant  10      1497 6341 367    2.22  0.026 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCompare nested models directly\n\u00b6\n\n\nfit1 <- aov(uptake ~ Plant + Type, data = CO2)\nfit2 <- aov(uptake ~ Plant, data = CO2)\n\nanova(fit1, fit2)\n\n\n\n\n## Analysis of Variance Table\n## \n## Model 1: uptake ~ Plant + Type\n## Model 2: uptake ~ Plant\n##   Res.Df  RSS Df Sum of Sq F Pr(>F)\n## 1     72 4845                      \n## 2     72 4845  0         0\n\n\n\nfit2 <- aov(uptake ~ Plant + Type, data = CO2)\nfit1 <- aov(uptake ~ Plant, data = CO2)\n\nanova(fit1, fit2)\n\n\n\n\n## Analysis of Variance Table\n## \n## Model 1: uptake ~ Plant\n## Model 2: uptake ~ Plant + Type\n##   Res.Df  RSS Df Sum of Sq F Pr(>F)\n## 1     72 4845                      \n## 2     72 4845  0         0\n\n\n\nMultiple comparisons\n\u00b6\n\n\n\n\nTukey HSD tests for post hoc comparisons on each factor in the model.\n\n\nFactors as an option.\n\n\nType I SS.\n\n\n\n\n\n\n\n# model\nfit <- aov(uptake ~ Plant + Type, data = CO2)\n\n# Tukey honestly significant differences\nTukeyHSD(fit)\n\n\n\n\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = uptake ~ Plant + Type, data = CO2)\n## \n## $Plant\n##            diff    lwr     upr p adj\n## Qn2-Qn1   1.929 -12.88  16.739 1.000\n## Qn3-Qn1   4.386 -10.42  19.196 0.997\n## Qc1-Qn1  -3.257 -18.07  11.553 1.000\n## Qc3-Qn1  -0.643 -15.45  14.168 1.000\n## Qc2-Qn1  -0.529 -15.34  14.282 1.000\n## Mn3-Qn1  -9.114 -23.92   5.696 0.639\n## Mn2-Qn1  -5.886 -20.70   8.925 0.970\n## Mn1-Qn1  -6.829 -21.64   7.982 0.918\n## Mc2-Qn1 -21.086 -35.90  -6.275 0.000\n## Mc3-Qn1 -15.929 -30.74  -1.118 0.024\n## Mc1-Qn1 -15.229 -30.04  -0.418 0.038\n## Qn3-Qn2   2.457 -12.35  17.268 1.000\n## Qc1-Qn2  -5.186 -20.00   9.625 0.989\n## Qc3-Qn2  -2.571 -17.38  12.239 1.000\n## Qc2-Qn2  -2.457 -17.27  12.353 1.000\n## Mn3-Qn2 -11.043 -25.85   3.768 0.346\n## Mn2-Qn2  -7.814 -22.62   6.996 0.822\n## Mn1-Qn2  -8.757 -23.57   6.053 0.694\n## Mc2-Qn2 -23.014 -37.82  -8.204 0.000\n## Mc3-Qn2 -17.857 -32.67  -3.047 0.006\n## Mc1-Qn2 -17.157 -31.97  -2.347 0.010\n## Qc1-Qn3  -7.643 -22.45   7.168 0.842\n## Qc3-Qn3  -5.029 -19.84   9.782 0.991\n## Qc2-Qn3  -4.914 -19.72   9.896 0.993\n## Mn3-Qn3 -13.500 -28.31   1.311 0.108\n## Mn2-Qn3 -10.271 -25.08   4.539 0.457\n## Mn1-Qn3 -11.214 -26.02   3.596 0.323\n## Mc2-Qn3 -25.471 -40.28 -10.661 0.000\n## Mc3-Qn3 -20.314 -35.12  -5.504 0.001\n## Mc1-Qn3 -19.614 -34.42  -4.804 0.002\n## Qc3-Qc1   2.614 -12.20  17.425 1.000\n## Qc2-Qc1   2.729 -12.08  17.539 1.000\n## Mn3-Qc1  -5.857 -20.67   8.953 0.971\n## Mn2-Qc1  -2.629 -17.44  12.182 1.000\n## Mn1-Qc1  -3.571 -18.38  11.239 1.000\n## Mc2-Qc1 -17.829 -32.64  -3.018 0.006\n## Mc3-Qc1 -12.671 -27.48   2.139 0.167\n## Mc1-Qc1 -11.971 -26.78   2.839 0.233\n## Qc2-Qc3   0.114 -14.70  14.925 1.000\n## Mn3-Qc3  -8.471 -23.28   6.339 0.735\n## Mn2-Qc3  -5.243 -20.05   9.568 0.988\n## Mn1-Qc3  -6.186 -21.00   8.625 0.958\n## Mc2-Qc3 -20.443 -35.25  -5.632 0.001\n## Mc3-Qc3 -15.286 -30.10  -0.475 0.037\n## Mc1-Qc3 -14.586 -29.40   0.225 0.057\n## Mn3-Qc2  -8.586 -23.40   6.225 0.719\n## Mn2-Qc2  -5.357 -20.17   9.453 0.985\n## Mn1-Qc2  -6.300 -21.11   8.511 0.952\n## Mc2-Qc2 -20.557 -35.37  -5.747 0.001\n## Mc3-Qc2 -15.400 -30.21  -0.589 0.034\n## Mc1-Qc2 -14.700 -29.51   0.111 0.054\n## Mn2-Mn3   3.229 -11.58  18.039 1.000\n## Mn1-Mn3   2.286 -12.52  17.096 1.000\n## Mc2-Mn3 -11.971 -26.78   2.839 0.233\n## Mc3-Mn3  -6.814 -21.62   7.996 0.919\n## Mc1-Mn3  -6.114 -20.92   8.696 0.961\n## Mn1-Mn2  -0.943 -15.75  13.868 1.000\n## Mc2-Mn2 -15.200 -30.01  -0.389 0.039\n## Mc3-Mn2 -10.043 -24.85   4.768 0.493\n## Mc1-Mn2  -9.343 -24.15   5.468 0.603\n## Mc2-Mn1 -14.257 -29.07   0.553 0.070\n## Mc3-Mn1  -9.100 -23.91   5.711 0.641\n## Mc1-Mn1  -8.400 -23.21   6.411 0.746\n## Mc3-Mc2   5.157  -9.65  19.968 0.989\n## Mc1-Mc2   5.857  -8.95  20.668 0.971\n## Mc1-Mc3   0.700 -14.11  15.511 1.000\n\n\n\nVisualizing results\n\u00b6\n\n\n# two-way interaction plot\nattach(mtcars)\n\ngear <- factor(gear)\ncyl <- factor(cyl)\n\n# two-way interactions\nlibrary(car)\n\ninteraction.plot(cyl, gear, mpg, type = \"b\", col = c(1:3), leg.bty = \"o\", leg.bg = \"beige\", lwd = 2, pch = c(18,24,22), xlab = \"Number of Cylinders\", ylab = \"Mean Miles Per Gallon\", main = \"Interaction Plot\")\n\n\n\n\n\n\n# mean plots for single factors, and includes confidence intervals\nlibrary(gplots)\n\nplotmeans(mpg ~ cyl, xlab = \"Number of Cylinders\", ylab = \"Miles Per Gallon\", main = \"Mean Plot\\nwith 95% CI\")\n\n\n\n\n\n\ndetach(mtcars)\n\n\n\n\nMANOVA\n\u00b6\n\n\nMultivariate analysis of variance (MANOVA) With more than one dependent variable Y. We can run several ANOVA over different Y, or one MANOVA with one Y built with several variables.\n\n\nhead(longley, 3)\n\n\n\n\n##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2\n\n\n\nattach(longley)\n\n# 2x2 factorial MANOVA with 3 dependent variables, Y\nY <- cbind(Unemployed, Armed.Forces, Employed) # Y\nfit <- manova(Y ~ Population*GNP) # Y ~ X\n\n# display type I SS\nsummary(fit, test = \"Pillai\")\n\n\n\n\n##                Df Pillai approx F num Df den Df  Pr(>F)    \n## Population      1  0.997     1074      3     10 7.7e-13 ***\n## GNP             1  0.888       26      3     10 4.6e-05 ***\n## Population:GNP  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# test = \"Wilks\", \"Hotelling-Lawley\", and \"Roy\"\n\n# display univariate statistics\nsummary.aov(fit)\n\n\n\n\n##  Response Unemployed :\n##                Df Sum Sq Mean Sq F value  Pr(>F)    \n## Population      1  61739   61739   45.92   2e-05 ***\n## GNP             1  42841   42841   31.87 0.00011 ***\n## Population:GNP  1  10270   10270    7.64 0.01715 *  \n## Residuals      12  16133    1344                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response Armed.Forces :\n##                Df Sum Sq Mean Sq F value Pr(>F)   \n## Population      1   9647    9647    4.25 0.0617 . \n## GNP             1  29772   29772   13.10 0.0035 **\n## Population:GNP  1   5958    5958    2.62 0.1314   \n## Residuals      12  27268    2272                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response Employed :\n##                Df Sum Sq Mean Sq F value  Pr(>F)    \n## Population      1  170.6   170.6  546.45 2.2e-11 ***\n## GNP             1   10.5    10.5   33.60 8.5e-05 ***\n## Population:GNP  1    0.1     0.1    0.41    0.54    \n## Residuals      12    3.7     0.3                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# display type III SS\nfit1 <- manova(Y ~ Population*GNP)\nfit2 <- manova(Y ~ GNP*Population)\n# type III GNP effect\nsummary(fit1)\n\n\n\n\n##                Df Pillai approx F num Df den Df  Pr(>F)    \n## Population      1  0.997     1074      3     10 7.7e-13 ***\n## GNP             1  0.888       26      3     10 4.6e-05 ***\n## Population:GNP  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# type III Population effect\nsummary(fit2)\n\n\n\n\n##                Df Pillai approx F num Df den Df  Pr(>F)    \n## GNP             1  0.997     1088      3     10 7.2e-13 ***\n## Population      1  0.786       12      3     10  0.0011 ** \n## GNP:Population  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nMultiple comparisons\n\n\nTukeyHSD\n and \nplot\n do work with a MANOVA fit. Run each dependent variable separately to obtain them\u2026 or proceed with ANOVA on each dependent variable.\n\n\nGoing further\n\u00b6\n\n\nPackage \nlme4\n has excellent facilities for fitting linear and generalized linear mixed-effects models.\n\n\n9, (M)ANOVA Assumptions\n\u00b6\n\n\nOutliers\n\u00b6\n\n\nOutliers can severely affect normality and homogeneity of variance.\n\n\n# dataset\nhead(mtcars, 3)\n\n\n\n\n##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n\n\n\n# detect outliers\n# ordered squared robust Mahalanobis distances of the observations against the empirical distribution function of the MD2i\nlibrary(mvoutlier)\n\noutliers <-\naq.plot(mtcars[c(\"mpg\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\")])\n\n\n\n\n## Projection to the first and second robust principal components.\n## Proportion of total variation (explained variance): 0.974\n\n\n\n\n\noutliers # show list of outliers\n\n\n\n\n## $outliers\n##           Mazda RX4       Mazda RX4 Wag          Datsun 710 \n##               FALSE               FALSE               FALSE \n##      Hornet 4 Drive   Hornet Sportabout             Valiant \n##               FALSE               FALSE               FALSE \n##          Duster 360           Merc 240D            Merc 230 \n##                TRUE                TRUE                TRUE \n##            Merc 280           Merc 280C          Merc 450SE \n##               FALSE               FALSE               FALSE \n##          Merc 450SL         Merc 450SLC  Cadillac Fleetwood \n##               FALSE               FALSE               FALSE \n## Lincoln Continental   Chrysler Imperial            Fiat 128 \n##               FALSE               FALSE                TRUE \n##         Honda Civic      Toyota Corolla       Toyota Corona \n##               FALSE               FALSE               FALSE \n##    Dodge Challenger         AMC Javelin          Camaro Z28 \n##               FALSE               FALSE                TRUE \n##    Pontiac Firebird           Fiat X1-9       Porsche 914-2 \n##               FALSE               FALSE               FALSE \n##        Lotus Europa      Ford Pantera L        Ferrari Dino \n##               FALSE                TRUE               FALSE \n##       Maserati Bora          Volvo 142E \n##                TRUE               FALSE\n\n\n\npar(mfrow = c(1,1))\n\n\n\n\nUnivariate normality\n\u00b6\n\n\n# Q-Q plot\nqqnorm(mtcars$mpg)\nqqline(mtcars$mpg)\n\n\n\n\n\n\n# Shapiro-Wilk test of normality\nshapiro.test(mtcars$mpg)\n\n\n\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mtcars$mpg\n## W = 0.9, p-value = 0.1\n\n\n\nlibrary(nortest)\n\n# Anderson-Darling test for normality\nad.test(mtcars$mpg)\n\n\n\n\n## \n##  Anderson-Darling normality test\n## \n## data:  mtcars$mpg\n## A = 0.6, p-value = 0.1\n\n\n\n# Cramer-von Mises test for normality\ncvm.test(mtcars$mpg)\n\n\n\n\n## \n##  Cramer-von Mises normality test\n## \n## data:  mtcars$mpg\n## W = 0.09, p-value = 0.2\n\n\n\n# Lilliefors (Kolmogorov-Smirnov) test for normality\nlillie.test(mtcars$mpg)\n\n\n\n\n## \n##  Lilliefors (Kolmogorov-Smirnov) normality test\n## \n## data:  mtcars$mpg\n## D = 0.1, p-value = 0.2\n\n\n\n# Pearson chi-square test for normality\npearson.test(mtcars$mpg)\n\n\n\n\n## \n##  Pearson chi-square normality test\n## \n## data:  mtcars$mpg\n## P = 8, p-value = 0.2\n\n\n\n# Shapiro-Francia test for normality\nsf.test(mtcars$mpg)\n\n\n\n\n## \n##  Shapiro-Francia normality test\n## \n## data:  mtcars$mpg\n## W = 1, p-value = 0.1\n\n\n\nMultivariate normality\n\u00b6\n\n\nhead(EuStockMarkets, 3)\n\n\n\n\n##       DAX  SMI  CAC FTSE\n## [1,] 1629 1678 1773 2444\n## [2,] 1614 1688 1750 2460\n## [3,] 1607 1679 1718 2448\n\n\n\nlibrary(mvnormtest)\n\n# Shapiro-Wilk test for multivariate normality of numeric matrix\nmshapiro.test(t(EuStockMarkets))\n\n\n\n\n## \n##  Shapiro-Wilk normality test\n## \n## data:  Z\n## W = 0.9, p-value <2e-16\n\n\n\nWith a p x 1 multivariate normal random vector x vector, the squared Mahalanobis distance between x and ?? is going to be chi-square distributed with p degrees of freedom.\n\n\n# graphical assessment of multivariate normality\nx <- as.matrix(EuStockMarkets) # n x p numeric matrix\ncenter <- colMeans(x) # centroid\nn <- nrow(x); p <- ncol(x)\ncov <- cov(x)\nd <- mahalanobis(x, center, cov) # distances\n\nqqplot(qchisq(ppoints(n), df = p), d, main = \"QQ Plot Assessing Multivariate Normality\", ylab = \"Mahalanobis D2\")\nabline(a = 0, b = 1) \n\n\n\n\n\n\nHeteroscedasticity\n\u00b6\n\n\nNonconstant error variance or non-homogeneity of variances.\n\n\n# dataset\nhead(mtcars, 3)\n\n\n\n\n##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n\n\n\n# y is a numeric variable and G is the grouping variable\n# Bartlett parametric test of homogeneity of variances\nbartlett.test(mpg ~ cyl, data=mtcars)\n\n\n\n\n## \n##  Bartlett test of homogeneity of variances\n## \n## data:  mpg by cyl\n## Bartlett's K-squared = 8, df = 2, p-value = 0.02\n\n\n\n# Figner-Killeen non-parametric test of homogeneity of variances\nfligner.test(mpg ~ cyl, data = mtcars) \n\n\n\n\n## \n##  Fligner-Killeen test of homogeneity of variances\n## \n## data:  mpg by cyl\n## Fligner-Killeen:med chi-squared = 7, df = 2, p-value = 0.03\n\n\n\nlibrary(HH)\n\n# y is numeric and G is a grouping factor\n# G must be of type factor\nhov(mpg ~ factor(cyl), data = mtcars)\n\n\n\n\n## \n##  hov: Brown-Forsyth\n## \n## data:  mpg\n## F = 6, df:factor(cyl) = 2, df:Residuals = 30, p-value = 0.009\n## alternative hypothesis: variances are not identical\n\n\n\n# homogeneity of variance plot\n# graphic test of homogeneity of variances based on Brown-Forsyth\nhovPlot(mpg ~ factor(cyl), data = mtcars) \n\n\n\n\n\n\nNon-homogeneity of covariance matrices\n\n\n# dataset\nhead(iris, 3)\n\n\n\n\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n\n\n\nlibrary(biotools)\n\n\n\n\n## ---\n## biotools version 3.0\n\n\n\n# Box's M test\n# very sensitive to violations of normality, leading to rejection in most typical cases\nboxM(iris[, -5], iris[, 5])\n\n\n\n\n## \n##  Box's M-test for Homogeneity of Covariance Matrices\n## \n## data:  iris[, -5]\n## Chi-Sq (approx.) = 100, df = 20, p-value <2e-16\n\n\n\n10, Resampling Statistics\n\u00b6\n\n\nIndependent k-sample location tests\n\u00b6\n\n\n# dataset\nhead(mtcars, 3)\n\n\n\n\n##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n\n\n\n# exact Wilcoxon Mann Whitney rank sum test\n# re-randomization or permutation based statistical tests\n# where y is numeric and A is a binary factor\nlibrary(coin)\n\nwilcox_test(mpg ~ factor(am), data = mtcars, distribution = \"exact\")\n\n\n\n\n## \n##  Exact Wilcoxon-Mann-Whitney Test\n## \n## data:  mpg by factor(am) (0, 1)\n## Z = -3, p-value = 0.001\n## alternative hypothesis: true mu is not equal to 0\n\n\n\n# lower case letters represent numerical variables and upper case letters represent categorical factors\n\n\n\n\n\n\nMonte-Carlo simulations are available for all tests. Exact tests are available for 2 group procedures.\n\n\nThese tests do not assume random sampling from well-defined populations. They can be a reasonable alternative to     classical procedures when test assumptions can not be met.\n\n\n\n\n\n\n\n# one-way permutation test based on 9999 Monte-Carlo\n# resamplings\n# y is numeric and A is a categorical factor\nlibrary(coin)\n\noneway_test(mpg ~ factor(am), data = mtcars, distribution = approximate(B = 9999))\n\n\n\n\n## \n##  Approximative Two-Sample Fisher-Pitman Permutation Test\n## \n## data:  mpg by factor(am) (0, 1)\n## Z = -3, p-value = 5e-04\n## alternative hypothesis: true mu is not equal to 0\n\n\n\nSymmetry of a response for repeated measurements\n\u00b6\n\n\n# exact Wilcoxon signed rank test\n# where y1 and y2 are repeated measures\nlibrary(coin)\n\nwilcoxsign_test(mpg ~ factor(am), data = mtcars, distribution = \"exact\")\n\n\n\n\n## \n##  Exact Wilcoxon-Pratt Signed-Rank Test\n## \n## data:  y by x (pos, neg) \n##   stratified by block\n## Z = 5, p-value = 5e-10\n## alternative hypothesis: true mu is not equal to 0\n\n\n\n# Freidman Test based on 9999 Monte-Carlo resamplings.\n# y is numeric, A is a grouping factor, and B is a\n# blocking factor\nfriedman_test(y ~ A | B, data = mydata,  distribution = approximate(B = 9999))\n\n\n\n\nIndependence of two numeric variables\n\u00b6\n\n\n# Spearman Test of independence based on 9999 Monte-Carlo\n# resamplings. x and y are numeric variables\nlibrary(coin)\n\nspearman_test(mpg ~ am, data = mtcars, distribution = approximate(B = 9999))\n\n\n\n\n## \n##  Approximative Spearman Correlation Test\n## \n## data:  mpg by am\n## Z = 3, p-value = 0.001\n## alternative hypothesis: true rho is not equal to 0\n\n\n\nIndependence in contingency tables\n\u00b6\n\n\n# dataset\nhead(CO2)\n\n\n\n\n## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8\n## 4   Qn1 Quebec nonchilled  350   37.2\n## 5   Qn1 Quebec nonchilled  500   35.3\n## 6   Qn1 Quebec nonchilled  675   39.2\n\n\n\ncases <- c(4, 2, 3, 1, 59)\nn <- sum(cases)\ncochran <- data.frame(\n    diphtheria = factor(\n        unlist(rep(list(c(1, 1, 1, 1),\n                        c(1, 1, 0, 1),\n                        c(0, 1, 1, 1),\n                        c(0, 1, 0, 1),\n                        c(0, 0, 0, 0)),\n                   cases))\n    ),\n    media = factor(rep(LETTERS[1:4], n)),\n    case =  factor(rep(seq_len(n), each = 4))\n)\n\nhead(cochran)\n\n\n\n\n##   diphtheria media case\n## 1          1     A    1\n## 2          1     B    1\n## 3          1     C    1\n## 4          1     D    1\n## 5          1     A    2\n## 6          1     B    2\n\n\n\n# independence in 2-way contingency table based on\n# 9999 Monte-Carlo resamplings. A and B are factors\nlibrary(coin)\n\nchisq_test(Plant ~ Type, data = CO2, distribution = approximate(B = 9999))\n\n\n\n\n## \n##  Approximative Linear-by-Linear Association Test\n## \n## data:  Plant (ordered) by Type (Quebec, Mississippi)\n## Z = -8, p-value <2e-16\n## alternative hypothesis: two.sided\n\n\n\n# Cochran-Mantel-Haenzsel Test of 3-way Contingency Table\n# based on 9999 Monte-Carlo resamplings. A, B, are factors\n# and C is a stratefying factor\nmh_test(diphtheria ~ media | case, data = cochran, distribution = approximate(B = 9999))\n\n\n\n\n## \n##  Approximative Marginal Homogeneity Test\n## \n## data:  diphtheria by\n##   media (A, B, C, D) \n##   stratified by case\n## chi-squared = 8, p-value = 0.05\n\n\n\n# linear by linear association test based on 9999\n# Monte-Carlo resamplings\n# A and B are ordered factors\nlibrary(coin)\n\nlbl_test(Plant ~ Type, data = CO2, distribution = approximate(B = 9999)) \n\n\n\n\n## \n##  Approximative Linear-by-Linear Association Test\n## \n## data:  Plant (ordered) by Type (Quebec, Mississippi)\n## Z = -8, p-value <2e-16\n## alternative hypothesis: two.sided\n\n\n\nMany other univariate and multivariate tests are possible using the functions in the \ncoin\n package.",
            "title": "Statistics with R, Notes"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#group-data",
            "text": "# dataset\nhead(mtcars, 3)  ##                mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  # variable grouped by one factor to show a function\nby(mtcars$mpg, mtcars$cyl, FUN = function(x) { c(m = mean(x), s = sd(x)) })   ## mtcars$cyl: 4\n##         m         s \n## 26.663636  4.509828 \n## -------------------------------------------------------- \n## mtcars$cyl: 6\n##         m         s \n## 19.742857  1.453567 \n## -------------------------------------------------------- \n## mtcars$cyl: 8\n##         m         s \n## 15.100000  2.560048  # description statistics by group\nlibrary(psych)\n\ndescribeBy(mtcars, group = mtcars$am)  ## \n##  Descriptive statistics by group \n## group: 0\n##      vars  n   mean     sd median trimmed    mad    min    max  range\n## mpg     1 19  17.15   3.83  17.30   17.12   3.11  10.40  24.40  14.00\n## cyl     2 19   6.95   1.54   8.00    7.06   0.00   4.00   8.00   4.00\n## disp    3 19 290.38 110.17 275.80  289.71 124.83 120.10 472.00 351.90\n## hp      4 19 160.26  53.91 175.00  161.06  77.10  62.00 245.00 183.00\n## drat    5 19   3.29   0.39   3.15    3.28   0.22   2.76   3.92   1.16\n## wt      6 19   3.77   0.78   3.52    3.75   0.45   2.46   5.42   2.96\n## qsec    7 19  18.18   1.75  17.82   18.07   1.19  15.41  22.90   7.49\n## vs      8 19   0.37   0.50   0.00    0.35   0.00   0.00   1.00   1.00\n## am      9 19   0.00   0.00   0.00    0.00   0.00   0.00   0.00   0.00\n## gear   10 19   3.21   0.42   3.00    3.18   0.00   3.00   4.00   1.00\n## carb   11 19   2.74   1.15   3.00    2.76   1.48   1.00   4.00   3.00\n##       skew kurtosis    se\n## mpg   0.01    -0.80  0.88\n## cyl  -0.95    -0.74  0.35\n## disp  0.05    -1.26 25.28\n## hp   -0.01    -1.21 12.37\n## drat  0.50    -1.30  0.09\n## wt    0.98     0.14  0.18\n## qsec  0.85     0.55  0.40\n## vs    0.50    -1.84  0.11\n## am     NaN      NaN  0.00\n## gear  1.31    -0.29  0.10\n## carb -0.14    -1.57  0.26\n## -------------------------------------------------------- \n## group: 1\n##      vars  n   mean    sd median trimmed   mad   min    max  range  skew\n## mpg     1 13  24.39  6.17  22.80   24.38  6.67 15.00  33.90  18.90  0.05\n## cyl     2 13   5.08  1.55   4.00    4.91  0.00  4.00   8.00   4.00  0.87\n## disp    3 13 143.53 87.20 120.30  131.25 58.86 71.10 351.00 279.90  1.33\n## hp      4 13 126.85 84.06 109.00  114.73 63.75 52.00 335.00 283.00  1.36\n## drat    5 13   4.05  0.36   4.08    4.02  0.27  3.54   4.93   1.39  0.79\n## wt      6 13   2.41  0.62   2.32    2.39  0.68  1.51   3.57   2.06  0.21\n## qsec    7 13  17.36  1.79  17.02   17.39  2.34 14.50  19.90   5.40 -0.23\n## vs      8 13   0.54  0.52   1.00    0.55  0.00  0.00   1.00   1.00 -0.14\n## am      9 13   1.00  0.00   1.00    1.00  0.00  1.00   1.00   0.00   NaN\n## gear   10 13   4.38  0.51   4.00    4.36  0.00  4.00   5.00   1.00  0.42\n## carb   11 13   2.92  2.18   2.00    2.64  1.48  1.00   8.00   7.00  0.98\n##      kurtosis    se\n## mpg     -1.46  1.71\n## cyl     -0.90  0.43\n## disp     0.40 24.19\n## hp       0.56 23.31\n## drat     0.21  0.10\n## wt      -1.17  0.17\n## qsec    -1.42  0.50\n## vs      -2.13  0.14\n## am        NaN  0.00\n## gear    -1.96  0.14\n## carb    -0.21  0.60  # description statistics by group \nlibrary(doBy)\n\nsummaryBy(mpg + wt ~ cyl + vs, data = mtcars, \n  FUN = function(x) { c(m = mean(x), s = sd(x)) } )  ##   cyl vs    mpg.m     mpg.s     wt.m      wt.s\n## 1   4  0 26.00000        NA 2.140000        NA\n## 2   4  1 26.73000 4.7481107 2.300300 0.5982073\n## 3   6  0 20.56667 0.7505553 2.755000 0.1281601\n## 4   6  1 19.12500 1.6317169 3.388750 0.1162164\n## 5   8  0 15.10000 2.5600481 3.999214 0.7594047",
            "title": "Group data"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#2-frequency-tables-crosstables-and-independence",
            "text": "Create frequency and contingency tables from categorical variables. Perform tests of independence, measures of association, and graphically display results.",
            "title": "2, Frequency Tables, CrossTables, and Independence"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#2d-frequency-tables",
            "text": "mytable <- table(A, B)  where A are rows, B are columns.  # dataset\nmytable <- matrix(c(1,2,3,4), nrow = 2)\nmytable  ##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4  # A frequencies (summed over columns = 1)\nmargin.table(mytable, 1)  ## [1] 4 6  # B frequencies (summed over rows = 2)\nmargin.table(mytable, 2)  ## [1] 3 7  # A/(A + B)\n# cell percentages\nprop.table(mytable)  ##      [,1] [,2]\n## [1,]  0.1  0.3\n## [2,]  0.2  0.4  # row percentages\nprop.table(mytable, 1)  ##           [,1]      [,2]\n## [1,] 0.2500000 0.7500000\n## [2,] 0.3333333 0.6666667  # column percentages \nprop.table(mytable, 2)  ##           [,1]      [,2]\n## [1,] 0.3333333 0.4285714\n## [2,] 0.6666667 0.5714286",
            "title": "2D frequency tables"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#3d-frequency-tables",
            "text": "# dataset\nhead(CO2, 3)  ## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8  A <- as.numeric(CO2[, 'Plant'])\nB <- CO2[, 'conc']\nC <- CO2[, 'uptake']\nmytable <- table(A, B, C)  # several arrays (3D)\ndim(mytable) # the printout is immense  ## [1] 12  7 76  # folded table (2D)\ndim(ftable(mytable)) # the printout is immense  ## [1] 84 76  # 3-Way frequency Table\nmytable <- xtabs(~A + B + C, data = mytable, na.action = na.omit)\n\ndim(ftable(mytable)) # the printout is immense  ## [1] 84 76",
            "title": "3D frequency tables"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#crosstable",
            "text": "A <- as.numeric(CO2[1:8, 'Plant'])\nA  ## [1] 1 1 1 1 1 1 1 2  B <- CO2[1:8, 'conc']\nB  ## [1]   95  175  250  350  500  675 1000   95  # 2-way cross tabulation\nlibrary(gmodels)\n\nCrossTable(A, B) # mydata$myrowvar x mydata$mycolvar  ## \n##  \n##    Cell Contents\n## |-------------------------|\n## |                       N |\n## | Chi-square contribution |\n## |           N / Row Total |\n## |           N / Col Total |\n## |         N / Table Total |\n## |-------------------------|\n## \n##  \n## Total Observations in Table:  8 \n## \n##  \n##              | B \n##            A |        95 |       175 |       250 |       350 |       500 |       675 |      1000 | Row Total | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n##            1 |         1 |         1 |         1 |         1 |         1 |         1 |         1 |         7 | \n##              |     0.321 |     0.018 |     0.018 |     0.018 |     0.018 |     0.018 |     0.018 |           | \n##              |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.875 | \n##              |     0.500 |     1.000 |     1.000 |     1.000 |     1.000 |     1.000 |     1.000 |           | \n##              |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n##            2 |         1 |         0 |         0 |         0 |         0 |         0 |         0 |         1 | \n##              |     2.250 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n##              |     1.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.125 | \n##              |     0.500 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |           | \n##              |     0.125 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n## Column Total |         2 |         1 |         1 |         1 |         1 |         1 |         1 |         8 | \n##              |     0.250 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n## \n##",
            "title": "CrossTable"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#tests-of-independence",
            "text": "There are more tests of independence in section 10, Resampling Statistics.  # dataset, a contingency table\ncolors  ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85  # chi-square test on 2-way tables\n# test independence of the row and column variables, p-value is calculated from the asymptotic chi-squared distribution of the test statistic\nchisq.test(colors)  ## \n##  Pearson's Chi-squared test\n## \n## data:  colors\n## X-squared = 1240, df = 12, p-value < 2.2e-16  # dataset, a contingency table in 2x2 matrix form\nTeaTasting <-\nmatrix(c(3, 1, 1, 3),\n       nrow = 2,\n       dimnames = list(Guess = c(\"Milk\", \"Tea\"),\n                       Truth = c(\"Milk\", \"Tea\")))\nTeaTasting  ##       Truth\n## Guess  Milk Tea\n##   Milk    3   1\n##   Tea     1   3  # Fisher exact test\nfisher.test(TeaTasting, alternative = \"greater\")  ## \n##  Fisher's Exact Test for Count Data\n## \n## data:  TeaTasting\n## p-value = 0.2429\n## alternative hypothesis: true odds ratio is greater than 1\n## 95 percent confidence interval:\n##  0.3135693       Inf\n## sample estimates:\n## odds ratio \n##   6.408309  # 3D contingency table, where the last dimension refers to the strata\nRabbits <-\narray(c(0, 0, 6, 5,\n        3, 0, 3, 6,\n        6, 2, 0, 4,\n        5, 6, 1, 0,\n        2, 5, 0, 0),\n      dim = c(2, 2, 5),\n      dimnames = list(\n          Delay = c(\"None\", \"1.5h\"),\n          Response = c(\"Cured\", \"Died\"),\n          Penicillin.Level = c(\"1/8\", \"1/4\", \"1/2\", \"1\", \"4\")))\nRabbits  ## , , Penicillin.Level = 1/8\n## \n##       Response\n## Delay  Cured Died\n##   None     0    6\n##   1.5h     0    5\n## \n## , , Penicillin.Level = 1/4\n## \n##       Response\n## Delay  Cured Died\n##   None     3    3\n##   1.5h     0    6\n## \n## , , Penicillin.Level = 1/2\n## \n##       Response\n## Delay  Cured Died\n##   None     6    0\n##   1.5h     2    4\n## \n## , , Penicillin.Level = 1\n## \n##       Response\n## Delay  Cured Died\n##   None     5    1\n##   1.5h     6    0\n## \n## , , Penicillin.Level = 4\n## \n##       Response\n## Delay  Cured Died\n##   None     2    0\n##   1.5h     5    0  # Mantel-Haenszel test / Cochran-Mantel-Haenszel chi-squared test, hypothesis that two nominal variables are conditionally independent in each stratum, assuming that there is no three-way interaction.\nmantelhaen.test(Rabbits)  ## \n##  Mantel-Haenszel chi-squared test with continuity correction\n## \n## data:  Rabbits\n## Mantel-Haenszel X-squared = 3.9286, df = 1, p-value = 0.04747\n## alternative hypothesis: true common odds ratio is not equal to 1\n## 95 percent confidence interval:\n##   1.026713 47.725133\n## sample estimates:\n## common odds ratio \n##                 7  # dataset\n# 3-way contingency table based on variables A, B, and C\nA <- CO2[, 'Plant']\nB <- CO2[, 'conc']\nC <- CO2[, 'uptake']\nmytable <- xtabs(~A+B+C) # a 3D array  # loglinear Models\n# mutual independence: A, B, and C are pairwise independent\nlibrary(MASS)\n\nloglm(~A + B + C, mytable)  ## Call:\n## loglm(formula = ~A + B + C, data = mytable)\n## \n## Statistics:\n##                        X^2   df  P(> X^2)\n## Likelihood Ratio  720.1035 6291 1.0000000\n## Pearson          6300.0000 6291 0.4656775  # conditional independence: A is independent of B, given C\nloglm(~A + B + C + A * C + B * C, mytable)  ## Call:\n## loglm(formula = ~A + B + C + A * C + B * C, data = mytable)\n## \n## Statistics:\n##                       X^2   df P(> X^2)\n## Likelihood Ratio 12.13685 5016        1\n## Pearson               NaN 5016      NaN  # no three-way interaction\nloglm(~A + B + C + A * B + A * C + B * C, mytable)  ## Call:\n## loglm(formula = ~A + B + C + A * B + A * C + B * C, data = mytable)\n## \n## Statistics:\n##                       X^2   df P(> X^2)\n## Likelihood Ratio 1.038376 4950        1\n## Pearson               NaN 4950      NaN",
            "title": "Tests of independence"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#measures-of-association",
            "text": "Association between two nominal variables, giving a value between 0 and +1 (inclusive). It is based on Pearson\u2019s chi-squared statistic.  # Dataset\nstr(Arthritis)  ## 'data.frame':    84 obs. of  5 variables:\n##  $ ID       : int  57 46 77 17 36 23 75 39 33 55 ...\n##  $ Treatment: Factor w/ 2 levels \"Placebo\",\"Treated\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Sex      : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Age      : int  27 29 30 32 46 58 59 59 63 63 ...\n##  $ Improved : Ord.factor w/ 3 levels \"None\"<\"Some\"<..: 2 1 1 3 3 3 1 3 1 1 ...  tab <- xtabs(~Improved + Treatment, data = Arthritis)\ntab  ##         Treatment\n## Improved Placebo Treated\n##   None        29      13\n##   Some         7       7\n##   Marked       7      21  summary(assocstats(tab))  ## \n## Call: xtabs(formula = ~Improved + Treatment, data = Arthritis)\n## Number of cases in table: 84 \n## Number of factors: 2 \n## Test for independence of all factors:\n##  Chisq = 13.055, df = 2, p-value = 0.001463\n##                     X^2 df  P(> X^2)\n## Likelihood Ratio 13.530  2 0.0011536\n## Pearson          13.055  2 0.0014626\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.367 \n## Cramer's V        : 0.394  # phi coefficient, contingency coefficient, and Cram\u00e9r's V for an 2D table\nlibrary(vcd)\n\nassocstats(tab)  ##                     X^2 df  P(> X^2)\n## Likelihood Ratio 13.530  2 0.0011536\n## Pearson          13.055  2 0.0014626\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.367 \n## Cramer's V        : 0.394  # Dataset\nTeaTasting  ##       Truth\n## Guess  Milk Tea\n##   Milk    3   1\n##   Tea     1   3  # Cohen's kappa and weighted kappa for a confusion matrix\nlibrary(vcd)\n\nkappa(TeaTasting)  ## [1] 2.333333",
            "title": "Measures of association"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#3-correlations",
            "text": "# dataset\nhead(mtcars, 3)  ##                mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1  # correlations/covariances among numeric variables in\n# a data frame\ncor(mtcars, use = \"complete.obs\", method = \"kendall\") # method = \"pearson\", \"spearman\" or \"kendall\"  ##             mpg        cyl       disp         hp        drat         wt\n## mpg   1.0000000 -0.7953134 -0.7681311 -0.7428125  0.46454879 -0.7278321\n## cyl  -0.7953134  1.0000000  0.8144263  0.7851865 -0.55131785  0.7282611\n## disp -0.7681311  0.8144263  1.0000000  0.6659987 -0.49898277  0.7433824\n## hp   -0.7428125  0.7851865  0.6659987  1.0000000 -0.38262689  0.6113081\n## drat  0.4645488 -0.5513178 -0.4989828 -0.3826269  1.00000000 -0.5471495\n## wt   -0.7278321  0.7282611  0.7433824  0.6113081 -0.54714953  1.0000000\n## qsec  0.3153652 -0.4489698 -0.3008155 -0.4729061  0.03272155 -0.1419881\n## vs    0.5896790 -0.7710007 -0.6033059 -0.6305926  0.37510111 -0.4884787\n## am    0.4690128 -0.4946212 -0.5202739 -0.3039956  0.57554849 -0.6138790\n## gear  0.4331509 -0.5125435 -0.4759795 -0.2794458  0.58392476 -0.5435956\n## carb -0.5043945  0.4654299  0.4137360  0.5959842 -0.09535193  0.3713741\n##             qsec         vs          am        gear        carb\n## mpg   0.31536522  0.5896790  0.46901280  0.43315089 -0.50439455\n## cyl  -0.44896982 -0.7710007 -0.49462115 -0.51254349  0.46542994\n## disp -0.30081549 -0.6033059 -0.52027392 -0.47597955  0.41373600\n## hp   -0.47290613 -0.6305926 -0.30399557 -0.27944584  0.59598416\n## drat  0.03272155  0.3751011  0.57554849  0.58392476 -0.09535193\n## wt   -0.14198812 -0.4884787 -0.61387896 -0.54359562  0.37137413\n## qsec  1.00000000  0.6575431 -0.16890405 -0.09126069 -0.50643945\n## vs    0.65754312  1.0000000  0.16834512  0.26974788 -0.57692729\n## am   -0.16890405  0.1683451  1.00000000  0.77078758 -0.05859929\n## gear -0.09126069  0.2697479  0.77078758  1.00000000  0.09801487\n## carb -0.50643945 -0.5769273 -0.05859929  0.09801487  1.00000000  cov(mtcars, use = \"complete.obs\")   ##              mpg         cyl        disp          hp         drat\n## mpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351\n## cyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694\n## disp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915\n## hp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887\n## drat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135\n## wt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073\n## qsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073\n## vs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919\n## am      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121\n## gear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790\n## carb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726\n##               wt         qsec           vs           am        gear\n## mpg   -5.1166847   4.50914919   2.01713710   1.80393145   2.1356855\n## cyl    1.3673710  -1.88685484  -0.72983871  -0.46572581  -0.6491935\n## disp 107.6842040 -96.05168145 -44.37762097 -36.56401210 -50.8026210\n## hp    44.1926613 -86.77008065 -24.98790323  -8.32056452  -6.3588710\n## drat  -0.3727207   0.08714073   0.11864919   0.19015121   0.2759879\n## wt     0.9573790  -0.30548161  -0.27366129  -0.33810484  -0.4210806\n## qsec  -0.3054816   3.19316613   0.67056452  -0.20495968  -0.2804032\n## vs    -0.2736613   0.67056452   0.25403226   0.04233871   0.0766129\n## am    -0.3381048  -0.20495968   0.04233871   0.24899194   0.2923387\n## gear  -0.4210806  -0.28040323   0.07661290   0.29233871   0.5443548\n## carb   0.6757903  -1.89411290  -0.46370968   0.04637097   0.3266129\n##             carb\n## mpg  -5.36310484\n## cyl   1.52016129\n## disp 79.06875000\n## hp   83.03629032\n## drat -0.07840726\n## wt    0.67579032\n## qsec -1.89411290\n## vs   -0.46370968\n## am    0.04637097\n## gear  0.32661290\n## carb  2.60887097  # correlations with significance levels\nlibrary(Hmisc)\n\nrcorr(as.matrix(mtcars), type = \"pearson\") # type can be pearson or spearman  ##        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n## mpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55\n## cyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53\n## disp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39\n## hp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75\n## drat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09\n## wt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43\n## qsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66\n## vs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57\n## am    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06\n## gear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27\n## carb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00\n## \n## n= 32 \n## \n## \n## P\n##      mpg    cyl    disp   hp     drat   wt     qsec   vs     am     gear  \n## mpg         0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0054\n## cyl  0.0000        0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0022 0.0042\n## disp 0.0000 0.0000        0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0010\n## hp   0.0000 0.0000 0.0000        0.0100 0.0000 0.0000 0.0000 0.1798 0.4930\n## drat 0.0000 0.0000 0.0000 0.0100        0.0000 0.6196 0.0117 0.0000 0.0000\n## wt   0.0000 0.0000 0.0000 0.0000 0.0000        0.3389 0.0010 0.0000 0.0005\n## qsec 0.0171 0.0004 0.0131 0.0000 0.6196 0.3389        0.0000 0.2057 0.2425\n## vs   0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000        0.3570 0.2579\n## am   0.0003 0.0022 0.0004 0.1798 0.0000 0.0000 0.2057 0.3570        0.0000\n## gear 0.0054 0.0042 0.0010 0.4930 0.0000 0.0005 0.2425 0.2579 0.0000       \n## carb 0.0011 0.0019 0.0253 0.0000 0.6212 0.0146 0.0000 0.0007 0.7545 0.1290\n##      carb  \n## mpg  0.0011\n## cyl  0.0019\n## disp 0.0253\n## hp   0.0000\n## drat 0.6212\n## wt   0.0146\n## qsec 0.0000\n## vs   0.0007\n## am   0.7545\n## gear 0.1290\n## carb  # dataset\nx <- mtcars[1:3]\ny <- mtcars[4:6]  # correlation between two vectors\ncor(x, y)  ##              hp       drat         wt\n## mpg  -0.7761684  0.6811719 -0.8676594\n## cyl   0.8324475 -0.6999381  0.7824958\n## disp  0.7909486 -0.7102139  0.8879799  Polychoric correlation  The correlation between two theorised normally distributed continuous latent variables, from two observed ordinal variables.  # dataset\n# 2-way contingency table of counts\ncolors  ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85  # polychoric correlation\nlibrary(polycor)\n\npolychor(colors)  ## [1] 0.4743984  # heterogeneous correlations in one matrix\n# pearson (numeric-numeric),\n# polyserial (numeric-ordinal),\n# and polychoric (ordinal-ordinal)\n# a data frame with ordered factors and numeric variables\nhetcor(colors)  ## \n## Two-Step Estimates\n## \n## Correlations/Type of Correlation:\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## fair.hair           1  Pearson     Pearson   Pearson    Pearson\n## red.hair       0.8333        1     Pearson   Pearson    Pearson\n## medium.hair    0.2597   0.6467           1   Pearson    Pearson\n## dark.hair     -0.7091  -0.2251      0.1911         1    Pearson\n## black.hair     -0.781  -0.3875    -0.06329    0.9674          1\n## \n## Standard Errors:\n##             fair.hair red.hair medium.hair dark.hair\n## fair.hair                                           \n## red.hair      0.03628                               \n## medium.hair    0.3607   0.1383                      \n## dark.hair     0.09977   0.3733      0.3841          \n## black.hair    0.06019   0.3004      0.4092  0.001491\n## \n## n = 4 \n## \n## P-values for Tests of Bivariate Normality:\n##             fair.hair red.hair medium.hair dark.hair\n## fair.hair                                           \n## red.hair       0.8306                               \n## medium.hair    0.6939   0.8609                      \n## dark.hair      0.7465   0.6335      0.7161          \n## black.hair     0.6582   0.5927      0.6711    0.9873",
            "title": "3, Correlations"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#4-t-tests",
            "text": "# independent 2-group t-test\nt.test(y ~ x) # where y is numeric and x is a binary factor\n\n# independent 2-group t-test\nt.test(y1, y2) # where y1 and y2 are numeric \n\n# paired t-test\nt.test(y1, y2, paired = TRUE) # where y1 & y2 are numeric \n\n# one sample t-test\nt.test(y, mu = 3) # Ho: mu=3  # dataset\n# 2-way contingency table\ncolors  ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85  mean(as.numeric(colors[1,])) # row 1 average  ## [1] 143.6  # independent 2-group t-test\nt.test(as.numeric(colors[1,]), as.numeric(colors[2,])) # where y1 and y2 are numeric   ## \n##  Welch Two Sample t-test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## t = -1.164, df = 5.5777, p-value = 0.2918\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -541.5725  196.7725\n## sample estimates:\n## mean of x mean of y \n##     143.6     316.0  # one sample t-test\nt.test(as.numeric(colors[1,]), mu = 0) # Ho: mu=0  ## \n##  One Sample t-test\n## \n## data:  as.numeric(colors[1, ])\n## t = 2.348, df = 4, p-value = 0.07868\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  -26.2009 313.4009\n## sample estimates:\n## mean of x \n##     143.6  t.test(as.numeric(colors[1,]), mu = 0, alternative=\"greater\") # Ho: mu=<0  ## \n##  One Sample t-test\n## \n## data:  as.numeric(colors[1, ])\n## t = 2.348, df = 4, p-value = 0.03934\n## alternative hypothesis: true mean is greater than 0\n## 95 percent confidence interval:\n##  13.22123      Inf\n## sample estimates:\n## mean of x \n##     143.6   alternative=\"less\"  or  alternative=\"greater\"  option to specify a one tailed test.  var.equal = TRUE  option to specify equal variances and a pooled variance estimate.   For multivariate tests and ANOVA, see sections 8 and 9.",
            "title": "4, t-tests"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#5-nonparametric-statistics",
            "text": "Nonnormal distributions.",
            "title": "5, Nonparametric statistics"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#bivariate-tests",
            "text": "# independent 2-group Mann-Whitney U test\nwilcox.test(y ~ A) # where y is numeric and A is A binary factor\n\n# independent 2-group Mann-Whitney U test\nwilcox.test(y, x) # where y and x are numeric\n\n# dependent 2-group Wilcoxon signed rank test\nwilcox.test(y1, y2, paired = TRUE) # where y1 and y2 are numeric  # 2-way contingency table\ncolors  ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85  # independent 2-group Mann-Whitney U Test\nwilcox.test(as.numeric(colors[1,]), as.numeric(colors[2,])) # where y and x are numeric  ## \n##  Wilcoxon rank sum test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## W = 8, p-value = 0.4206\n## alternative hypothesis: true location shift is not equal to 0  alternative=\"less\"  or  alternative=\"greater\"  option to specify a one \ntailed test.",
            "title": "Bivariate tests"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#anova",
            "text": "# Kruskal Wallis test one-Way ANOVA by ranks\nkruskal.test(y ~ A) # where y1 is numeric and A is a factor\n\n# randomized block design - Friedman test\nfriedman.test(y ~ A | B) # where y are the data values, A is a grouping factor and B is a blocking factor\n\n# Kruskal Wallis test one-way ANOVA by ranks\nkruskal.test(y, x) # where y and x are numeric  # dataset\n# 2-way contingency table\ncolors  ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85  # Kruskal Wallis test one-way ANOVA by ranks\nkruskal.test(as.numeric(colors[1,]), as.numeric(colors[2,])) # where y and x are numeric  ## \n##  Kruskal-Wallis rank sum test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## Kruskal-Wallis chi-squared = 4, df = 4, p-value = 0.406",
            "title": "ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#6-multiple-regressions",
            "text": "",
            "title": "6, Multiple Regressions"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#fitting-the-model",
            "text": "# multiple linear regression\nfit <- lm(y ~ x1 + x2 + x3, data = mydata)\nsummary(fit) # show results\n\n# useful functions\ncoefficients(fit) # model coefficients\nconfint(fit, level = 0.95) # CIs for model parameters\nfitted(fit) # predicted values\nresiduals(fit) # residuals\nanova(fit) # ANOVA table\nvcov(fit) # covariance matrix for model parameters\ninfluence(fit) # regression diagnostics   # dataset\nstr(longley)  ## 'data.frame':    16 obs. of  7 variables:\n##  $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...\n##  $ GNP         : num  234 259 258 285 329 ...\n##  $ Unemployed  : num  236 232 368 335 210 ...\n##  $ Armed.Forces: num  159 146 162 165 310 ...\n##  $ Population  : num  108 109 110 111 112 ...\n##  $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...\n##  $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...  # multiple linear regression example\nfit <- lm(Armed.Forces ~ GNP + Population, data = longley)\nsummary(fit) # show results  ## \n## Call:\n## lm(formula = Armed.Forces ~ GNP + Population, data = longley)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -70.349 -33.569   5.076  16.409 104.037 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)   \n## (Intercept) 4123.922   1276.579   3.230  0.00657 **\n## GNP            3.365      0.986   3.413  0.00463 **\n## Population   -44.011     14.089  -3.124  0.00807 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 50.56 on 13 degrees of freedom\n## Multiple R-squared:  0.5426, Adjusted R-squared:  0.4723 \n## F-statistic: 7.712 on 2 and 13 DF,  p-value: 0.006191  # other useful functions\ncoefficients(fit) # model coefficients  ## (Intercept)         GNP  Population \n## 4123.922484    3.365215  -44.010955  confint(fit, level = 0.95) # CIs for model parameters  ##                   2.5 %      97.5 %\n## (Intercept) 1366.042123 6881.802846\n## GNP            1.235097    5.495333\n## Population   -74.447969  -13.573942  fitted(fit) # predicted values  ##     1947     1948     1949     1950     1951     1952     1953     1954 \n## 176.4245 215.9487 161.1151 199.5681 298.4663 306.5279 288.1248 230.9633 \n##     1955     1956     1957     1958     1959     1960     1961     1962 \n## 295.1332 308.9566 313.0359 252.7794 318.8698 297.7176 240.7975 266.2711  residuals(fit) # residuals  ##        1947        1948        1949        1950        1951        1952 \n## -17.4245114 -70.3487085   0.4848669 -34.5681071  11.4336564  52.8721086 \n##        1953        1954        1955        1956        1957        1958 \n##  66.5752438 104.0367029   9.6668098 -23.2566323 -33.2359499  10.9205505 \n##        1959        1960        1961        1962 \n## -63.6698197 -46.3175746  16.4025069  16.4288577  anova(fit) # ANOVA table  ## Analysis of Variance Table\n## \n## Response: Armed.Forces\n##            Df Sum Sq Mean Sq F value   Pr(>F)   \n## GNP         1  14479 14478.7  5.6649 0.033301 * \n## Population  1  24941 24940.8  9.7583 0.008068 **\n## Residuals  13  33226  2555.9                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  vcov(fit) # covariance matrix for model parameters  ##             (Intercept)          GNP   Population\n## (Intercept) 1629652.882 1239.7478355 -17970.27388\n## GNP            1239.748    0.9721911    -13.76775\n## Population   -17970.274  -13.7677545    198.49444  influence(fit) # regression diagnostics   ## $hat\n##       1947       1948       1949       1950       1951       1952 \n## 0.27412252 0.17438942 0.31563199 0.16765687 0.21219668 0.21127212 \n##       1953       1954       1955       1956       1957       1958 \n## 0.11339116 0.08602104 0.10270245 0.12845683 0.13251347 0.11070418 \n##       1959       1960       1961       1962 \n## 0.15598638 0.15164367 0.32488437 0.33842686 \n## \n## $coefficients\n##      (Intercept)          GNP  Population\n## 1947  128.042531  0.131479474 -1.53731106\n## 1948   29.040635  0.121992502 -0.69544934\n## 1949   -6.396740 -0.005738655  0.07379997\n## 1950  177.778426  0.175668519 -2.11609661\n## 1951  133.333008  0.093997366 -1.43810938\n## 1952  638.680267  0.462229807 -6.92955762\n## 1953  422.107967  0.305133565 -4.56222462\n## 1954 -385.998493 -0.325674565  4.42308458\n## 1955   54.458118  0.042127997 -0.59713302\n## 1956 -163.371695 -0.131240587  1.81041089\n## 1957 -212.039316 -0.179084306  2.37664757\n## 1958  -51.395691 -0.033854337  0.55600613\n## 1959 -329.488802 -0.311550864  3.79446941\n## 1960    3.116881 -0.049904757  0.10916689\n## 1961 -242.199361 -0.158976865  2.60043035\n## 1962 -194.416431 -0.113799395  2.04462752\n## \n## $sigma\n##     1947     1948     1949     1950     1951     1952     1953     1954 \n## 52.28757 47.63741 52.61955 51.47046 52.48826 49.73420 48.50003 42.21357 \n##     1955     1956     1957     1958     1959     1960     1961     1962 \n## 52.53729 52.12610 51.60167 52.51353 48.66817 50.57779 52.30331 52.29577 \n## \n## $wt.res\n##        1947        1948        1949        1950        1951        1952 \n## -17.4245114 -70.3487085   0.4848669 -34.5681071  11.4336564  52.8721086 \n##        1953        1954        1955        1956        1957        1958 \n##  66.5752438 104.0367029   9.6668098 -23.2566323 -33.2359499  10.9205505 \n##        1959        1960        1961        1962 \n## -63.6698197 -46.3175746  16.4025069  16.4288577",
            "title": "Fitting the Model"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#diagnostic-plots",
            "text": "# diagnostic plots\nlayout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page\nplot(fit)   layout(matrix(c(1,1,1,1),2,2))",
            "title": "Diagnostic plots"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#comparing-two-models-with-anova",
            "text": "# compare models\nfit1 <- lm(y ~ x1 + x2 + x3 + x4, data = mydata)\nfit2 <- lm(y ~ x1 + x2)\n\nanova(fit1, fit2)   # compare models\nfit1 <- lm(Armed.Forces ~ GNP + Population, data = longley)\nfit2 <- lm(Armed.Forces ~ GNP, data = longley)\n\nanova(fit1, fit2)   ## Analysis of Variance Table\n## \n## Model 1: Armed.Forces ~ GNP + Population\n## Model 2: Armed.Forces ~ GNP\n##   Res.Df   RSS Df Sum of Sq      F   Pr(>F)   \n## 1     13 33226                                \n## 2     14 58167 -1    -24941 9.7583 0.008068 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
            "title": "Comparing two models with ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#cross-validation",
            "text": "# dataset\nlibrary(DAAG)\n\ndata(houseprices)\nhead(houseprices, 3)  ##    area bedrooms sale.price\n## 9   694        4        192\n## 10  905        4        215\n## 11  802        4        215  # k-fold cross-validation\nlibrary(DAAG)\n\n# case 1, # 3 fold cross-validation\nCVlm(houseprices, form.lm = formula(sale.price ~ area), m = 3, dots = FALSE, seed = 29, plotit = c(\"Observed\",\"Residual\"), main = \"Small symbols show cross-validation predicted values\", legend.pos = \"topleft\", printit = TRUE)  ## Analysis of Variance Table\n## \n## Response: sale.price\n##           Df Sum Sq Mean Sq F value Pr(>F)  \n## area       1  18566   18566       8  0.014 *\n## Residuals 13  30179    2321                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   ## \n## fold 1 \n## Observations in test set: 5 \n##              11  20    21     22   23\n## area        802 696 771.0 1006.0 1191\n## cvpred      204 188 199.3  234.7  262\n## sale.price  215 255 260.0  293.0  375\n## CV residual  11  67  60.7   58.3  113\n## \n## Sum of squares = 24351    Mean square = 4870    n = 5 \n## \n## fold 2 \n## Observations in test set: 5 \n##              10   13    14      17     18\n## area        905  716 963.0 1018.00 887.00\n## cvpred      255  224 264.4  273.38 252.06\n## sale.price  215  113 185.0  276.00 260.00\n## CV residual -40 -112 -79.4    2.62   7.94\n## \n## Sum of squares = 20416    Mean square = 4083    n = 5 \n## \n## fold 3 \n## Observations in test set: 5 \n##                 9   12     15    16     19\n## area        694.0 1366 821.00 714.0 790.00\n## cvpred      183.2  388 221.94 189.3 212.49\n## sale.price  192.0  274 212.00 220.0 221.50\n## CV residual   8.8 -114  -9.94  30.7   9.01\n## \n## Sum of squares = 14241    Mean square = 2848    n = 5 \n## \n## Overall (Sum over all 5 folds) \n##   ms \n## 3934  # dataset\nhead(longley, 3)  ##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2  # case 2, # 3 fold cross-validation\nCVlm(longley, form.lm = formula(Armed.Forces ~ GNP + Population), m = 3, dots = FALSE, seed = 29, plotit = c(\"Observed\",\"Residual\"), main = \"Small symbols show cross-validation predicted values\", legend.pos = \"topleft\", printit = TRUE)  ## Analysis of Variance Table\n## \n## Response: Armed.Forces\n##            Df Sum Sq Mean Sq F value Pr(>F)   \n## GNP         1  14479   14479    5.66 0.0333 * \n## Population  1  24941   24941    9.76 0.0081 **\n## Residuals  13  33226    2556                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   ## \n## fold 1 \n## Observations in test set: 5 \n##               1953 1954  1957  1960  1962\n## Predicted    288.1  231 313.0 297.7 266.3\n## cvpred       281.4  219 310.6 295.7 263.1\n## Armed.Forces 354.7  335 279.8 251.4 282.7\n## CV residual   73.3  116 -30.8 -44.3  19.6\n## \n## Sum of squares = 22004    Mean square = 4401    n = 5 \n## \n## fold 2 \n## Observations in test set: 6 \n##               1948   1949   1955   1958  1959  1961\n## Predicted    215.9 161.12 295.13 252.78 318.9 240.8\n## cvpred       231.9 171.41 306.33 255.07 324.5 234.9\n## Armed.Forces 145.6 161.60 304.80 263.70 255.2 257.2\n## CV residual  -86.3  -9.81  -1.53   8.63 -69.3  22.3\n## \n## Sum of squares = 12917    Mean square = 2153    n = 6 \n## \n## fold 3 \n## Observations in test set: 5 \n##               1947  1950  1951  1952   1956\n## Predicted    176.4 199.6 298.5 306.5 308.96\n## cvpred       192.8 211.9 282.3 288.9 295.17\n## Armed.Forces 159.0 165.0 309.9 359.4 285.70\n## CV residual  -33.8 -46.9  27.6  70.5  -9.47\n## \n## Sum of squares = 9161    Mean square = 1832    n = 5 \n## \n## Overall (Sum over all 5 folds) \n##   ms \n## 2755",
            "title": "Cross validation"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#variable-selection-heuristic-methods",
            "text": "# dataset\nhead(longley, 3)  ##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2  # Stepwise Regression\nlibrary(MASS)\n\nfit <- lm(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley)\nstep <- stepAIC(fit, direction = \"both\")  ## Start:  AIC=125\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n##              Df Sum of Sq   RSS AIC\n## <none>                    21655 125\n## - Unemployed  1      4508 26163 126\n## - Population  1      5522 27177 127\n## - Employed    1     10208 31863 130\n## - GNP         1     15323 36978 132  step$anova # display results  ## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n## Final Model:\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n## \n##   Step Df Deviance Resid. Df Resid. Dev AIC\n## 1                         11      21655 125  The goal is to reduce the AIC. Adding variable does not improve the model. Let\u2019s opt for the most valuable variable.  fit <- lm(Armed.Forces ~ Unemployed, data = longley)\nstep <- stepAIC(fit, direction = \"both\")  ## Start:  AIC=138\n## Armed.Forces ~ Unemployed\n## \n##              Df Sum of Sq   RSS AIC\n## - Unemployed  1      2287 72646 137\n## <none>                    70359 138\n## \n## Step:  AIC=137\n## Armed.Forces ~ 1\n## \n##              Df Sum of Sq   RSS AIC\n## <none>                    72646 137\n## + Unemployed  1      2287 70359 138  step$anova # display results  ## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## Armed.Forces ~ Unemployed\n## \n## Final Model:\n## Armed.Forces ~ 1\n## \n## \n##           Step Df Deviance Resid. Df Resid. Dev AIC\n## 1                                 14      70359 138\n## 2 - Unemployed  1     2287        15      72646 137",
            "title": "Variable selection -- Heuristic methods"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#variable-selection-graphical-methods",
            "text": "# model\nfit <- lm(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley)  # all Subsets Regression\nlibrary(leaps)\n\nleaps <- regsubsets(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley, nbest = 10)\n\n# view results\nsummary(leaps)  ## Subset selection object\n## Call: regsubsets.formula(Armed.Forces ~ GNP + Population + Employed + \n##     Unemployed, data = longley, nbest = 10)\n## 4 Variables  (and intercept)\n##            Forced in Forced out\n## GNP            FALSE      FALSE\n## Population     FALSE      FALSE\n## Employed       FALSE      FALSE\n## Unemployed     FALSE      FALSE\n## 10 subsets of each size up to 4\n## Selection Algorithm: exhaustive\n##          GNP Population Employed Unemployed\n## 1  ( 1 ) \" \" \" \"        \"*\"      \" \"       \n## 1  ( 2 ) \"*\" \" \"        \" \"      \" \"       \n## 1  ( 3 ) \" \" \"*\"        \" \"      \" \"       \n## 1  ( 4 ) \" \" \" \"        \" \"      \"*\"       \n## 2  ( 1 ) \"*\" \"*\"        \" \"      \" \"       \n## 2  ( 2 ) \"*\" \" \"        \" \"      \"*\"       \n## 2  ( 3 ) \" \" \"*\"        \" \"      \"*\"       \n## 2  ( 4 ) \" \" \" \"        \"*\"      \"*\"       \n## 2  ( 5 ) \" \" \"*\"        \"*\"      \" \"       \n## 2  ( 6 ) \"*\" \" \"        \"*\"      \" \"       \n## 3  ( 1 ) \"*\" \"*\"        \"*\"      \" \"       \n## 3  ( 2 ) \"*\" \" \"        \"*\"      \"*\"       \n## 3  ( 3 ) \"*\" \"*\"        \" \"      \"*\"       \n## 3  ( 4 ) \" \" \"*\"        \"*\"      \"*\"       \n## 4  ( 1 ) \"*\" \"*\"        \"*\"      \"*\"  # plot a table of models showing variables in each model\n# models are ordered by the selection statistic\nplot(leaps, scale = \"r2\")   # plot statistic by subset size\nlibrary(car)\n\nsubsets(leaps, statistic = \"rsq\", legend = FALSE) # available criteria are rsq, rss, adjr2, cp, bic   ##            Abbreviation\n## GNP                   G\n## Population            P\n## Employed              E\n## Unemployed            U  subsets(leaps, statistic = \"bic\", legend = FALSE) # available criteria are rsq, rss, adjr2, cp, bic   ##            Abbreviation\n## GNP                   G\n## Population            P\n## Employed              E\n## Unemployed            U",
            "title": "Variable selection -- Graphical methods"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#variable-selection-relative-importance",
            "text": "Model:  fit <- lm(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley) .  Warning:  eval=FALSE .  # calculate the relative importance of each predictor\nlibrary(relaimpo)\n\ncalc.relimp(fit, type = c(\"lmg\", \"last\", \"first\", \"pratt\"), rela = TRUE)  Results.  Response variable: Armed.Forces \nTotal response variance: 4843 \nAnalysis based on 16 observations\n\n4 Regressors: \nGNP Population Employed Unemployed \nProportion of variance explained by model: 70.2%\nMetrics are normalized to sum to 100% (rela=TRUE).\n\nRelative importance metrics:\n\n             lmg  last first  pratt\nGNP        0.328 0.431 0.348  4.566\nPopulation 0.241 0.155 0.232 -1.934\nEmployed   0.217 0.287 0.365 -1.780\nUnemployed 0.215 0.127 0.055  0.148\n\nAverage coefficients for different model sizes:\n\n               1X     2Xs     3Xs     4Xs\nGNP         0.313   1.301   3.641   5.026\nPopulation  3.646 -14.815 -24.637 -37.260\nEmployed    9.062  17.646 -34.249 -54.144\nUnemployed -0.132  -0.511  -0.584  -0.436  Bootstrapping  Model:  fit <- lm(Armed.Forces ~ GNP + Population + Employed + Unemployed, data = longley) .  Warning:  eval=FALSE .  # bootstrap measures of relative importance (1000 samples)\nboot <- boot.relimp(fit, b = 1000, type = c(\"lmg\", \"last\", \"first\", \"pratt\"), rank = TRUE, diff = TRUE, rela = TRUE)\nbooteval.relimp(boot) # print result  Results.  Response variable: Armed.Forces \nTotal response variance: 4843 \nAnalysis based on 16 observations\n\n4 Regressors: \nGNP Population Employed Unemployed \nProportion of variance explained by model: 70.2%\nMetrics are normalized to sum to 100% (rela=TRUE).\n\nRelative importance metrics:\n\n             lmg  last first  pratt\nGNP        0.328 0.431 0.348  4.566\nPopulation 0.241 0.155 0.232 -1.934\nEmployed   0.217 0.287 0.365 -1.780\nUnemployed 0.215 0.127 0.055  0.148\n\nAverage coefficients for different model sizes:\n\n               1X     2Xs     3Xs     4Xs\nGNP         0.313   1.301   3.641   5.026\nPopulation  3.646 -14.815 -24.637 -37.260\nEmployed    9.062  17.646 -34.249 -54.144\nUnemployed -0.132  -0.511  -0.584  -0.436\n\n\n Confidence interval information ( 1000 bootstrap replicates, bty= perc ): \nRelative Contributions with confidence intervals:\n\n                                 Lower  Upper\n                 percentage 0.95 0.95    0.95   \nGNP.lmg           0.3280    ABC_  0.1488  0.4030\nPopulation.lmg    0.2410    _BCD  0.1572  0.3060\nEmployed.lmg      0.2170    ABCD  0.1107  0.3240\nUnemployed.lmg    0.2150    ABCD  0.0586  0.5550\n\nGNP.last          0.4310    ABCD  0.0101  0.5660\nPopulation.last   0.1550    _BCD  0.0003  0.3850\nEmployed.last     0.2870    ABCD  0.0476  0.6280\nUnemployed.last   0.1270    ABCD  0.0009  0.7810\n\nGNP.first         0.3480    ABCD  0.0151  0.3960\nPopulation.first  0.2320    _BCD  0.0070  0.3060\nEmployed.first    0.3650    ABCD  0.0159  0.4020\nUnemployed.first  0.0550    ABCD  0.0004  0.9280\n\nGNP.pratt         4.5660    ABCD -1.0160 10.5380\nPopulation.pratt -1.9340    ABCD -6.0800  2.2050\nEmployed.pratt   -1.7800    _BCD -5.0290  0.8510\nUnemployed.pratt  0.1480    ABC_ -0.3250  1.0910\n\nLetters indicate the ranks covered by bootstrap CIs. \n(Rank bootstrap confidence intervals always obtained by percentile method) \nCAUTION: Bootstrap confidence intervals can be somewhat liberal.\n\n\n Differences between Relative Contributions:\n\n                                            Lower   Upper\n                            difference 0.95 0.95    0.95   \nGNP-Population.lmg           0.0871         -0.0188  0.1521\nGNP-Employed.lmg             0.1106         -0.0479  0.1949\nGNP-Unemployed.lmg           0.1130         -0.4021  0.3281\nPopulation-Employed.lmg      0.0236         -0.1091  0.1124\nPopulation-Unemployed.lmg    0.0259         -0.3904  0.2417\nEmployed-Unemployed.lmg      0.0023         -0.4315  0.2295\n\nGNP-Population.last          0.2756         -0.1062  0.4502\nGNP-Employed.last            0.1438         -0.5564  0.4011\nGNP-Unemployed.last          0.3041         -0.7408  0.5502\nPopulation-Employed.last    -0.1318         -0.6191  0.2653\nPopulation-Unemployed.last   0.0285         -0.7382  0.3622\nEmployed-Unemployed.last     0.1603         -0.6784  0.4852\n\nGNP-Population.first         0.1161         -0.0590  0.1736\nGNP-Employed.first          -0.0172         -0.0893  0.0934\nGNP-Unemployed.first         0.2930         -0.9047  0.3766\nPopulation-Employed.first   -0.1333         -0.2134  0.0688\nPopulation-Unemployed.first  0.1769         -0.8967  0.2969\nEmployed-Unemployed.first    0.3102         -0.8937  0.3823\n\nGNP-Population.pratt         6.4995         -2.2069 15.7978\nGNP-Employed.pratt           6.3461         -1.3879 15.0093\nGNP-Unemployed.pratt         4.4180         -1.9134 10.6456\nPopulation-Employed.pratt   -0.1534         -4.1860  5.9691\nPopulation-Unemployed.pratt -2.0815         -6.1439  2.2348\nEmployed-Unemployed.pratt   -1.9281         -5.2057  0.1725\n\n* indicates that CI for difference does not include 0. \nCAUTION: Bootstrap confidence intervals can be somewhat liberal.  Warning:  eval=FALSE .  plot(booteval.relimp(boot, sort = TRUE)) # to plot the results  The .png file.",
            "title": "Variable selection -- Relative importance"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#going-further",
            "text": "The  nls  package provides functions for nonlinear regression.  Perform robust regression with the  rlm  function in the  MASS  package.  The  robust  package provides a comprehensive library of robust methods, including regression.  The  robustbase  package also provides basic robust statistics including model selection methods.",
            "title": "Going further"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#7-regression-diagnostics",
            "text": "# assume that we are fitting a multiple linear regression\nfit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\nfit  ## \n## Call:\n## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars)\n## \n## Coefficients:\n## (Intercept)         disp           hp           wt         drat  \n##    29.14874      0.00382     -0.03478     -3.47967      1.76805",
            "title": "7, Regression diagnostics"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#outliers",
            "text": "library(car)\n\n# assessing outliers\noutlierTest(fit) # Bonferonni p-value for most extreme obs  ## \n## No Studentized residuals with Bonferonni p < 0.05\n## Largest |rstudent|:\n##                rstudent unadjusted p-value Bonferonni p\n## Toyota Corolla     2.52             0.0184        0.588  qqPlot(fit, main = \"QQ Plot\") #qq plot for studentized resid   leveragePlots(fit) # leverage plots",
            "title": "Outliers"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#influential-observations",
            "text": "Model:  fit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars) .  # influential observations\n# added variable plots\nlibrary(car)\n\navPlots(fit)   par(mfrow = c(1,1))\n\n# Cook's D plot\n# identify D values > 4/(n-k-1)\ncutoff <- 4/((nrow(mtcars) - length(fit$coefficients) - 2))\nplot(fit, which = 4, cook.levels = cutoff)   Warning:  eval=FALSE ; interactive function.  # influence plot\ninfluencePlot(fit, id.method = \"identify\", main = \"Influence Plot\", sub = \"Circle size is proportial to Cook's Distance\" )",
            "title": "Influential observations"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#nonnormality",
            "text": "Model:  fit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars) .  # normality of residuals\n# qq plot for studentized resid\nlibrary(car)\n\nqqPlot(fit, main = \"QQ Plot\")   # distribution of studentized residuals\nlibrary(MASS)\n\nsresid <- studres(fit)\nhist(sresid, freq = FALSE, main = \"Distribution of Studentized Residuals\")\nxfit <- seq(min(sresid), max(sresid),length = 40)\nyfit <- dnorm(xfit)\nlines(xfit, yfit)",
            "title": "Nonnormality"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#heteroscedasticity",
            "text": "Nonconstant error variance.  Model:  fit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars) .  # evaluate homoscedasticity\n# non-constant error variance test\nncvTest(fit)  ## Non-constant Variance Score Test \n## Variance formula: ~ fitted.values \n## Chisquare = 1.43    Df = 1     p = 0.232  # plot studentized residuals vs. fitted values\nspreadLevelPlot(fit)   ## \n## Suggested power transformation:  0.662",
            "title": "Heteroscedasticity"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#multicollinearity",
            "text": "Model:  fit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars) .  # evaluatecCollinearity\nvif(fit)  ## disp   hp   wt drat \n## 8.21 2.89 5.10 2.28  sqrt(vif(fit)) > 2 # benchmark = 1.96, rounded to 2  ##  disp    hp    wt  drat \n##  TRUE FALSE  TRUE FALSE",
            "title": "Multicollinearity"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#nonlinearity",
            "text": "Model:  fit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars) .  # evaluate nonlinearity\n# component + residual plot\ncrPlots(fit)   # Ceres plots\nceresPlots(fit)",
            "title": "Nonlinearity"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#autocorrelation",
            "text": "Serial correlation or non-independence of errors.  Model:  fit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars) .  # test for autocorrelated errors\ndurbinWatsonTest(fit)  ##  lag Autocorrelation D-W Statistic p-value\n##    1           0.101          1.74    0.29\n##  Alternative hypothesis: rho != 0",
            "title": "Autocorrelation"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#global-diagnostic",
            "text": "Model:  fit <- lm(mpg ~ disp + hp + wt + drat, data = mtcars) .  # global test of model assumptions\nlibrary(gvlma)\n\ngvmodel <- gvlma(fit)\nsummary(gvmodel)   ## \n## Call:\n## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.508 -1.905 -0.506  0.982  5.688 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 29.14874    6.29359    4.63  8.2e-05 ***\n## disp         0.00382    0.01080    0.35   0.7268    \n## hp          -0.03478    0.01160   -3.00   0.0058 ** \n## wt          -3.47967    1.07837   -3.23   0.0033 ** \n## drat         1.76805    1.31978    1.34   0.1915    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.6 on 27 degrees of freedom\n## Multiple R-squared:  0.838,  Adjusted R-squared:  0.814 \n## F-statistic: 34.8 on 4 and 27 DF,  p-value: 2.7e-10\n## \n## \n## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\n## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\n## Level of Significance =  0.05 \n## \n## Call:\n##  gvlma(x = fit) \n## \n##                      Value p-value                   Decision\n## Global Stat        13.9382 0.00750 Assumptions NOT satisfied!\n## Skewness            4.3131 0.03782 Assumptions NOT satisfied!\n## Kurtosis            0.0138 0.90654    Assumptions acceptable.\n## Link Function       8.7166 0.00315 Assumptions NOT satisfied!\n## Heteroscedasticity  0.8947 0.34421    Assumptions acceptable.",
            "title": "Global diagnostic"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#8-anova",
            "text": "Analysis of variance (ANOVA) is an alternative to regressions among other applications.  # lower case letters are numeric variables\n# upper case letters are factors\n\n# one-way ANOVA (completely randomized design)\nfit <- aov(y ~ A, data = mydataframe)\n\n# randomized block design (B is the blocking factor)\nfit <- aov(y ~ A + B, data = mydataframe)\n\n# two-way factorial design\nfit <- aov(y ~ A + B + A:B, data = mydataframe)\nfit <- aov(y ~ A*B, data = mydataframe) # same thing\n\n# analysis of covariance\nfit <- aov(y ~ A + x, data = mydataframe)   # dataset\nhead(CO2, 3)  ## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8  # one-way ANOVA (completely randomized design)\nfit <- aov(uptake ~ Plant, data = CO2)\nfit  ## Call:\n##    aov(formula = uptake ~ Plant, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## Estimated effects are balanced  summary(fit)  ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # randomized block design (B is the blocking factor)\nfit <- aov(uptake ~ Plant + Type, data = CO2)\nfit  ## Call:\n##    aov(formula = uptake ~ Plant + Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 1 out of 13 effects not estimable\n## Estimated effects are balanced  summary(fit)  ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # two-way factorial design\nfit <- aov(uptake ~ Plant + Type + Plant:Type, data = CO2)\nfit  ## Call:\n##    aov(formula = uptake ~ Plant + Type + Plant:Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 12 out of 24 effects not estimable\n## Estimated effects are balanced  summary(fit)  ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  fit <- aov(uptake ~ Plant*Type, data = CO2) # same thing\nfit  ## Call:\n##    aov(formula = uptake ~ Plant * Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 12 out of 24 effects not estimable\n## Estimated effects are balanced  summary(fit)  ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # Analysis of Covariance\nfit <- aov(uptake ~ uptake + conc, data = CO2)\nfit  ## Call:\n##    aov(formula = uptake ~ uptake + conc, data = CO2)\n## \n## Terms:\n##                 conc Residuals\n## Sum of Squares  2285      7422\n## Deg. of Freedom    1        82\n## \n## Residual standard error: 9.51\n## Estimated effects may be unbalanced  summary(fit)  ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## conc         1   2285    2285    25.2 2.9e-06 ***\n## Residuals   82   7422      91                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
            "title": "8, ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#evaluate-model-effects",
            "text": "Type I sequential SS: A+B and B+A will produce different results!  Use the  drop1  to produce the familiar Type III results; compare each term with the full model.    # display Type I ANOVA table\nfit1 <- aov(uptake ~ Plant + Type, data = CO2)\nsummary(fit1)  ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # display Type I ANOVA table\nfit2 <- aov(uptake ~ Type + Plant, data = CO2)\nsummary(fit2)  ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Type         1   3366    3366   50.02 8.1e-10 ***\n## Plant       10   1497     150    2.22   0.026 *  \n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # display type III SS and F tests\ndrop1(fit1, ~ ., test = \"F\")  ## Single term deletions\n## \n## Model:\n## uptake ~ Plant + Type\n##        Df Sum of Sq  RSS AIC F value Pr(>F)  \n## <none>              4845 365                 \n## Plant  10      1497 6341 367    2.22  0.026 *\n## Type    0         0 4845 365                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  drop1(fit2, ~ ., test = \"F\")  ## Single term deletions\n## \n## Model:\n## uptake ~ Type + Plant\n##        Df Sum of Sq  RSS AIC F value Pr(>F)  \n## <none>              4845 365                 \n## Type    0         0 4845 365                 \n## Plant  10      1497 6341 367    2.22  0.026 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
            "title": "Evaluate model effects"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#compare-nested-models-directly",
            "text": "fit1 <- aov(uptake ~ Plant + Type, data = CO2)\nfit2 <- aov(uptake ~ Plant, data = CO2)\n\nanova(fit1, fit2)  ## Analysis of Variance Table\n## \n## Model 1: uptake ~ Plant + Type\n## Model 2: uptake ~ Plant\n##   Res.Df  RSS Df Sum of Sq F Pr(>F)\n## 1     72 4845                      \n## 2     72 4845  0         0  fit2 <- aov(uptake ~ Plant + Type, data = CO2)\nfit1 <- aov(uptake ~ Plant, data = CO2)\n\nanova(fit1, fit2)  ## Analysis of Variance Table\n## \n## Model 1: uptake ~ Plant\n## Model 2: uptake ~ Plant + Type\n##   Res.Df  RSS Df Sum of Sq F Pr(>F)\n## 1     72 4845                      \n## 2     72 4845  0         0",
            "title": "Compare nested models directly"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#multiple-comparisons",
            "text": "Tukey HSD tests for post hoc comparisons on each factor in the model.  Factors as an option.  Type I SS.    # model\nfit <- aov(uptake ~ Plant + Type, data = CO2)\n\n# Tukey honestly significant differences\nTukeyHSD(fit)  ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = uptake ~ Plant + Type, data = CO2)\n## \n## $Plant\n##            diff    lwr     upr p adj\n## Qn2-Qn1   1.929 -12.88  16.739 1.000\n## Qn3-Qn1   4.386 -10.42  19.196 0.997\n## Qc1-Qn1  -3.257 -18.07  11.553 1.000\n## Qc3-Qn1  -0.643 -15.45  14.168 1.000\n## Qc2-Qn1  -0.529 -15.34  14.282 1.000\n## Mn3-Qn1  -9.114 -23.92   5.696 0.639\n## Mn2-Qn1  -5.886 -20.70   8.925 0.970\n## Mn1-Qn1  -6.829 -21.64   7.982 0.918\n## Mc2-Qn1 -21.086 -35.90  -6.275 0.000\n## Mc3-Qn1 -15.929 -30.74  -1.118 0.024\n## Mc1-Qn1 -15.229 -30.04  -0.418 0.038\n## Qn3-Qn2   2.457 -12.35  17.268 1.000\n## Qc1-Qn2  -5.186 -20.00   9.625 0.989\n## Qc3-Qn2  -2.571 -17.38  12.239 1.000\n## Qc2-Qn2  -2.457 -17.27  12.353 1.000\n## Mn3-Qn2 -11.043 -25.85   3.768 0.346\n## Mn2-Qn2  -7.814 -22.62   6.996 0.822\n## Mn1-Qn2  -8.757 -23.57   6.053 0.694\n## Mc2-Qn2 -23.014 -37.82  -8.204 0.000\n## Mc3-Qn2 -17.857 -32.67  -3.047 0.006\n## Mc1-Qn2 -17.157 -31.97  -2.347 0.010\n## Qc1-Qn3  -7.643 -22.45   7.168 0.842\n## Qc3-Qn3  -5.029 -19.84   9.782 0.991\n## Qc2-Qn3  -4.914 -19.72   9.896 0.993\n## Mn3-Qn3 -13.500 -28.31   1.311 0.108\n## Mn2-Qn3 -10.271 -25.08   4.539 0.457\n## Mn1-Qn3 -11.214 -26.02   3.596 0.323\n## Mc2-Qn3 -25.471 -40.28 -10.661 0.000\n## Mc3-Qn3 -20.314 -35.12  -5.504 0.001\n## Mc1-Qn3 -19.614 -34.42  -4.804 0.002\n## Qc3-Qc1   2.614 -12.20  17.425 1.000\n## Qc2-Qc1   2.729 -12.08  17.539 1.000\n## Mn3-Qc1  -5.857 -20.67   8.953 0.971\n## Mn2-Qc1  -2.629 -17.44  12.182 1.000\n## Mn1-Qc1  -3.571 -18.38  11.239 1.000\n## Mc2-Qc1 -17.829 -32.64  -3.018 0.006\n## Mc3-Qc1 -12.671 -27.48   2.139 0.167\n## Mc1-Qc1 -11.971 -26.78   2.839 0.233\n## Qc2-Qc3   0.114 -14.70  14.925 1.000\n## Mn3-Qc3  -8.471 -23.28   6.339 0.735\n## Mn2-Qc3  -5.243 -20.05   9.568 0.988\n## Mn1-Qc3  -6.186 -21.00   8.625 0.958\n## Mc2-Qc3 -20.443 -35.25  -5.632 0.001\n## Mc3-Qc3 -15.286 -30.10  -0.475 0.037\n## Mc1-Qc3 -14.586 -29.40   0.225 0.057\n## Mn3-Qc2  -8.586 -23.40   6.225 0.719\n## Mn2-Qc2  -5.357 -20.17   9.453 0.985\n## Mn1-Qc2  -6.300 -21.11   8.511 0.952\n## Mc2-Qc2 -20.557 -35.37  -5.747 0.001\n## Mc3-Qc2 -15.400 -30.21  -0.589 0.034\n## Mc1-Qc2 -14.700 -29.51   0.111 0.054\n## Mn2-Mn3   3.229 -11.58  18.039 1.000\n## Mn1-Mn3   2.286 -12.52  17.096 1.000\n## Mc2-Mn3 -11.971 -26.78   2.839 0.233\n## Mc3-Mn3  -6.814 -21.62   7.996 0.919\n## Mc1-Mn3  -6.114 -20.92   8.696 0.961\n## Mn1-Mn2  -0.943 -15.75  13.868 1.000\n## Mc2-Mn2 -15.200 -30.01  -0.389 0.039\n## Mc3-Mn2 -10.043 -24.85   4.768 0.493\n## Mc1-Mn2  -9.343 -24.15   5.468 0.603\n## Mc2-Mn1 -14.257 -29.07   0.553 0.070\n## Mc3-Mn1  -9.100 -23.91   5.711 0.641\n## Mc1-Mn1  -8.400 -23.21   6.411 0.746\n## Mc3-Mc2   5.157  -9.65  19.968 0.989\n## Mc1-Mc2   5.857  -8.95  20.668 0.971\n## Mc1-Mc3   0.700 -14.11  15.511 1.000",
            "title": "Multiple comparisons"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#visualizing-results",
            "text": "# two-way interaction plot\nattach(mtcars)\n\ngear <- factor(gear)\ncyl <- factor(cyl)\n\n# two-way interactions\nlibrary(car)\n\ninteraction.plot(cyl, gear, mpg, type = \"b\", col = c(1:3), leg.bty = \"o\", leg.bg = \"beige\", lwd = 2, pch = c(18,24,22), xlab = \"Number of Cylinders\", ylab = \"Mean Miles Per Gallon\", main = \"Interaction Plot\")   # mean plots for single factors, and includes confidence intervals\nlibrary(gplots)\n\nplotmeans(mpg ~ cyl, xlab = \"Number of Cylinders\", ylab = \"Miles Per Gallon\", main = \"Mean Plot\\nwith 95% CI\")   detach(mtcars)",
            "title": "Visualizing results"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#manova",
            "text": "Multivariate analysis of variance (MANOVA) With more than one dependent variable Y. We can run several ANOVA over different Y, or one MANOVA with one Y built with several variables.  head(longley, 3)  ##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2  attach(longley)\n\n# 2x2 factorial MANOVA with 3 dependent variables, Y\nY <- cbind(Unemployed, Armed.Forces, Employed) # Y\nfit <- manova(Y ~ Population*GNP) # Y ~ X\n\n# display type I SS\nsummary(fit, test = \"Pillai\")  ##                Df Pillai approx F num Df den Df  Pr(>F)    \n## Population      1  0.997     1074      3     10 7.7e-13 ***\n## GNP             1  0.888       26      3     10 4.6e-05 ***\n## Population:GNP  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # test = \"Wilks\", \"Hotelling-Lawley\", and \"Roy\"\n\n# display univariate statistics\nsummary.aov(fit)  ##  Response Unemployed :\n##                Df Sum Sq Mean Sq F value  Pr(>F)    \n## Population      1  61739   61739   45.92   2e-05 ***\n## GNP             1  42841   42841   31.87 0.00011 ***\n## Population:GNP  1  10270   10270    7.64 0.01715 *  \n## Residuals      12  16133    1344                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response Armed.Forces :\n##                Df Sum Sq Mean Sq F value Pr(>F)   \n## Population      1   9647    9647    4.25 0.0617 . \n## GNP             1  29772   29772   13.10 0.0035 **\n## Population:GNP  1   5958    5958    2.62 0.1314   \n## Residuals      12  27268    2272                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response Employed :\n##                Df Sum Sq Mean Sq F value  Pr(>F)    \n## Population      1  170.6   170.6  546.45 2.2e-11 ***\n## GNP             1   10.5    10.5   33.60 8.5e-05 ***\n## Population:GNP  1    0.1     0.1    0.41    0.54    \n## Residuals      12    3.7     0.3                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # display type III SS\nfit1 <- manova(Y ~ Population*GNP)\nfit2 <- manova(Y ~ GNP*Population)\n# type III GNP effect\nsummary(fit1)  ##                Df Pillai approx F num Df den Df  Pr(>F)    \n## Population      1  0.997     1074      3     10 7.7e-13 ***\n## GNP             1  0.888       26      3     10 4.6e-05 ***\n## Population:GNP  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # type III Population effect\nsummary(fit2)  ##                Df Pillai approx F num Df den Df  Pr(>F)    \n## GNP             1  0.997     1088      3     10 7.2e-13 ***\n## Population      1  0.786       12      3     10  0.0011 ** \n## GNP:Population  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  Multiple comparisons  TukeyHSD  and  plot  do work with a MANOVA fit. Run each dependent variable separately to obtain them\u2026 or proceed with ANOVA on each dependent variable.",
            "title": "MANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#going-further_1",
            "text": "Package  lme4  has excellent facilities for fitting linear and generalized linear mixed-effects models.",
            "title": "Going further"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#9-manova-assumptions",
            "text": "",
            "title": "9, (M)ANOVA Assumptions"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#outliers_1",
            "text": "Outliers can severely affect normality and homogeneity of variance.  # dataset\nhead(mtcars, 3)  ##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1  # detect outliers\n# ordered squared robust Mahalanobis distances of the observations against the empirical distribution function of the MD2i\nlibrary(mvoutlier)\n\noutliers <-\naq.plot(mtcars[c(\"mpg\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\")])  ## Projection to the first and second robust principal components.\n## Proportion of total variation (explained variance): 0.974   outliers # show list of outliers  ## $outliers\n##           Mazda RX4       Mazda RX4 Wag          Datsun 710 \n##               FALSE               FALSE               FALSE \n##      Hornet 4 Drive   Hornet Sportabout             Valiant \n##               FALSE               FALSE               FALSE \n##          Duster 360           Merc 240D            Merc 230 \n##                TRUE                TRUE                TRUE \n##            Merc 280           Merc 280C          Merc 450SE \n##               FALSE               FALSE               FALSE \n##          Merc 450SL         Merc 450SLC  Cadillac Fleetwood \n##               FALSE               FALSE               FALSE \n## Lincoln Continental   Chrysler Imperial            Fiat 128 \n##               FALSE               FALSE                TRUE \n##         Honda Civic      Toyota Corolla       Toyota Corona \n##               FALSE               FALSE               FALSE \n##    Dodge Challenger         AMC Javelin          Camaro Z28 \n##               FALSE               FALSE                TRUE \n##    Pontiac Firebird           Fiat X1-9       Porsche 914-2 \n##               FALSE               FALSE               FALSE \n##        Lotus Europa      Ford Pantera L        Ferrari Dino \n##               FALSE                TRUE               FALSE \n##       Maserati Bora          Volvo 142E \n##                TRUE               FALSE  par(mfrow = c(1,1))",
            "title": "Outliers"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#univariate-normality",
            "text": "# Q-Q plot\nqqnorm(mtcars$mpg)\nqqline(mtcars$mpg)   # Shapiro-Wilk test of normality\nshapiro.test(mtcars$mpg)  ## \n##  Shapiro-Wilk normality test\n## \n## data:  mtcars$mpg\n## W = 0.9, p-value = 0.1  library(nortest)\n\n# Anderson-Darling test for normality\nad.test(mtcars$mpg)  ## \n##  Anderson-Darling normality test\n## \n## data:  mtcars$mpg\n## A = 0.6, p-value = 0.1  # Cramer-von Mises test for normality\ncvm.test(mtcars$mpg)  ## \n##  Cramer-von Mises normality test\n## \n## data:  mtcars$mpg\n## W = 0.09, p-value = 0.2  # Lilliefors (Kolmogorov-Smirnov) test for normality\nlillie.test(mtcars$mpg)  ## \n##  Lilliefors (Kolmogorov-Smirnov) normality test\n## \n## data:  mtcars$mpg\n## D = 0.1, p-value = 0.2  # Pearson chi-square test for normality\npearson.test(mtcars$mpg)  ## \n##  Pearson chi-square normality test\n## \n## data:  mtcars$mpg\n## P = 8, p-value = 0.2  # Shapiro-Francia test for normality\nsf.test(mtcars$mpg)  ## \n##  Shapiro-Francia normality test\n## \n## data:  mtcars$mpg\n## W = 1, p-value = 0.1",
            "title": "Univariate normality"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#multivariate-normality",
            "text": "head(EuStockMarkets, 3)  ##       DAX  SMI  CAC FTSE\n## [1,] 1629 1678 1773 2444\n## [2,] 1614 1688 1750 2460\n## [3,] 1607 1679 1718 2448  library(mvnormtest)\n\n# Shapiro-Wilk test for multivariate normality of numeric matrix\nmshapiro.test(t(EuStockMarkets))  ## \n##  Shapiro-Wilk normality test\n## \n## data:  Z\n## W = 0.9, p-value <2e-16  With a p x 1 multivariate normal random vector x vector, the squared Mahalanobis distance between x and ?? is going to be chi-square distributed with p degrees of freedom.  # graphical assessment of multivariate normality\nx <- as.matrix(EuStockMarkets) # n x p numeric matrix\ncenter <- colMeans(x) # centroid\nn <- nrow(x); p <- ncol(x)\ncov <- cov(x)\nd <- mahalanobis(x, center, cov) # distances\n\nqqplot(qchisq(ppoints(n), df = p), d, main = \"QQ Plot Assessing Multivariate Normality\", ylab = \"Mahalanobis D2\")\nabline(a = 0, b = 1)",
            "title": "Multivariate normality"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#heteroscedasticity_1",
            "text": "Nonconstant error variance or non-homogeneity of variances.  # dataset\nhead(mtcars, 3)  ##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1  # y is a numeric variable and G is the grouping variable\n# Bartlett parametric test of homogeneity of variances\nbartlett.test(mpg ~ cyl, data=mtcars)  ## \n##  Bartlett test of homogeneity of variances\n## \n## data:  mpg by cyl\n## Bartlett's K-squared = 8, df = 2, p-value = 0.02  # Figner-Killeen non-parametric test of homogeneity of variances\nfligner.test(mpg ~ cyl, data = mtcars)   ## \n##  Fligner-Killeen test of homogeneity of variances\n## \n## data:  mpg by cyl\n## Fligner-Killeen:med chi-squared = 7, df = 2, p-value = 0.03  library(HH)\n\n# y is numeric and G is a grouping factor\n# G must be of type factor\nhov(mpg ~ factor(cyl), data = mtcars)  ## \n##  hov: Brown-Forsyth\n## \n## data:  mpg\n## F = 6, df:factor(cyl) = 2, df:Residuals = 30, p-value = 0.009\n## alternative hypothesis: variances are not identical  # homogeneity of variance plot\n# graphic test of homogeneity of variances based on Brown-Forsyth\nhovPlot(mpg ~ factor(cyl), data = mtcars)    Non-homogeneity of covariance matrices  # dataset\nhead(iris, 3)  ##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa  library(biotools)  ## ---\n## biotools version 3.0  # Box's M test\n# very sensitive to violations of normality, leading to rejection in most typical cases\nboxM(iris[, -5], iris[, 5])  ## \n##  Box's M-test for Homogeneity of Covariance Matrices\n## \n## data:  iris[, -5]\n## Chi-Sq (approx.) = 100, df = 20, p-value <2e-16",
            "title": "Heteroscedasticity"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#10-resampling-statistics",
            "text": "",
            "title": "10, Resampling Statistics"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#independent-k-sample-location-tests",
            "text": "# dataset\nhead(mtcars, 3)  ##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1  # exact Wilcoxon Mann Whitney rank sum test\n# re-randomization or permutation based statistical tests\n# where y is numeric and A is a binary factor\nlibrary(coin)\n\nwilcox_test(mpg ~ factor(am), data = mtcars, distribution = \"exact\")  ## \n##  Exact Wilcoxon-Mann-Whitney Test\n## \n## data:  mpg by factor(am) (0, 1)\n## Z = -3, p-value = 0.001\n## alternative hypothesis: true mu is not equal to 0  # lower case letters represent numerical variables and upper case letters represent categorical factors   Monte-Carlo simulations are available for all tests. Exact tests are available for 2 group procedures.  These tests do not assume random sampling from well-defined populations. They can be a reasonable alternative to     classical procedures when test assumptions can not be met.    # one-way permutation test based on 9999 Monte-Carlo\n# resamplings\n# y is numeric and A is a categorical factor\nlibrary(coin)\n\noneway_test(mpg ~ factor(am), data = mtcars, distribution = approximate(B = 9999))  ## \n##  Approximative Two-Sample Fisher-Pitman Permutation Test\n## \n## data:  mpg by factor(am) (0, 1)\n## Z = -3, p-value = 5e-04\n## alternative hypothesis: true mu is not equal to 0",
            "title": "Independent k-sample location tests"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#symmetry-of-a-response-for-repeated-measurements",
            "text": "# exact Wilcoxon signed rank test\n# where y1 and y2 are repeated measures\nlibrary(coin)\n\nwilcoxsign_test(mpg ~ factor(am), data = mtcars, distribution = \"exact\")  ## \n##  Exact Wilcoxon-Pratt Signed-Rank Test\n## \n## data:  y by x (pos, neg) \n##   stratified by block\n## Z = 5, p-value = 5e-10\n## alternative hypothesis: true mu is not equal to 0  # Freidman Test based on 9999 Monte-Carlo resamplings.\n# y is numeric, A is a grouping factor, and B is a\n# blocking factor\nfriedman_test(y ~ A | B, data = mydata,  distribution = approximate(B = 9999))",
            "title": "Symmetry of a response for repeated measurements"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#independence-of-two-numeric-variables",
            "text": "# Spearman Test of independence based on 9999 Monte-Carlo\n# resamplings. x and y are numeric variables\nlibrary(coin)\n\nspearman_test(mpg ~ am, data = mtcars, distribution = approximate(B = 9999))  ## \n##  Approximative Spearman Correlation Test\n## \n## data:  mpg by am\n## Z = 3, p-value = 0.001\n## alternative hypothesis: true rho is not equal to 0",
            "title": "Independence of two numeric variables"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#independence-in-contingency-tables",
            "text": "# dataset\nhead(CO2)  ## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8\n## 4   Qn1 Quebec nonchilled  350   37.2\n## 5   Qn1 Quebec nonchilled  500   35.3\n## 6   Qn1 Quebec nonchilled  675   39.2  cases <- c(4, 2, 3, 1, 59)\nn <- sum(cases)\ncochran <- data.frame(\n    diphtheria = factor(\n        unlist(rep(list(c(1, 1, 1, 1),\n                        c(1, 1, 0, 1),\n                        c(0, 1, 1, 1),\n                        c(0, 1, 0, 1),\n                        c(0, 0, 0, 0)),\n                   cases))\n    ),\n    media = factor(rep(LETTERS[1:4], n)),\n    case =  factor(rep(seq_len(n), each = 4))\n)\n\nhead(cochran)  ##   diphtheria media case\n## 1          1     A    1\n## 2          1     B    1\n## 3          1     C    1\n## 4          1     D    1\n## 5          1     A    2\n## 6          1     B    2  # independence in 2-way contingency table based on\n# 9999 Monte-Carlo resamplings. A and B are factors\nlibrary(coin)\n\nchisq_test(Plant ~ Type, data = CO2, distribution = approximate(B = 9999))  ## \n##  Approximative Linear-by-Linear Association Test\n## \n## data:  Plant (ordered) by Type (Quebec, Mississippi)\n## Z = -8, p-value <2e-16\n## alternative hypothesis: two.sided  # Cochran-Mantel-Haenzsel Test of 3-way Contingency Table\n# based on 9999 Monte-Carlo resamplings. A, B, are factors\n# and C is a stratefying factor\nmh_test(diphtheria ~ media | case, data = cochran, distribution = approximate(B = 9999))  ## \n##  Approximative Marginal Homogeneity Test\n## \n## data:  diphtheria by\n##   media (A, B, C, D) \n##   stratified by case\n## chi-squared = 8, p-value = 0.05  # linear by linear association test based on 9999\n# Monte-Carlo resamplings\n# A and B are ordered factors\nlibrary(coin)\n\nlbl_test(Plant ~ Type, data = CO2, distribution = approximate(B = 9999))   ## \n##  Approximative Linear-by-Linear Association Test\n## \n## data:  Plant (ordered) by Type (Quebec, Mississippi)\n## Z = -8, p-value <2e-16\n## alternative hypothesis: two.sided  Many other univariate and multivariate tests are possible using the functions in the  coin  package.",
            "title": "Independence in contingency tables"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/",
            "text": "Documentation\n\n\n1, Introduction\n\n\n2, Data Exploration\n\n\n3, Data Manipulation\n\n\n4, Data Analysis\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets only.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\n\n\nMicrosoft R Server: previously called Revolution R Enterprise for Hadoop, Linux and Teradata and included new Microsoft enterprise support and purchasing options. Microsoft R Server was further made available to students through the Microsoft DreamSpark programme.\n\n\nMicrosoft R Server Developer Edition: a gratis version for developers that with a feature set akin to the commercial edition.\n\n\nMicrosoft Data Science Virtual Machine: an analytics tool developed by the Revolution Analytics division premiered in January 2015.\n\n\nMicrosoft R Open: a rebranded version of Revolution R Open.\n\n\n\n\n1, Introduction\n\u00b6\n\n\nImporting data with \nrxImport\n function\n\n\n# Declare the file paths for the csv and xdf files\n# find the path or directory where the file is, load the path variable\nmyAirlineCsv <- file.path(rxGetOption('sampleDataDir'), '2007_subset.csv')\n\n# fin the data in this directory and load the data variable\nmyAirlineXdf <- '2007_subset.xdf'\n\n# Use rxImport to import the data into xdf format\n# rxImport(inData = myAirlineCsv, outFile = myAirlineXdf, overwrite = TRUE)\n# or\n# function within a function for more stats\nsystem.time(rxImport(inData = myAirlineCsv, \n                     outFile = myAirlineXdf, \n                     overwrite = TRUE))\nlist.files()\n\n\n\n\nFunctions for summarizing data\n\n\n# Get basic information about your data\nrxGetInfo(data = myAirlineXdf, \n          getVarInfo = TRUE, \n          numRows = 10)\n\n# Summarize the variables corresponding to actual elapsed time, time in the air, departure delay, flight Distance\nrxSummary(formula = ~ ActualElapsedTime + AirTime + DepDelay + Distance, \n          data = myAirlineXdf)\n\n# Histogram of departure delays\nrxHistogram(formula = ~DepDelay, \n            data = myAirlineXdf)\n\n# Use parameters similar to a regular histogram to zero in on the interesting area\nrxHistogram(formula = ~DepDelay, \n            data = myAirlineXdf, \n            xAxisMinMax = c(-100, 400), \n            numBreaks = 500,\n            xNumTicks = 10)\n\n\n\n\nCreating new variables using \nrxDataStep\n\n\n# Calculate an additional variable: airspeed (distance traveled / time in the air)\nrxDataStep(inData = myAirlineXdf, \n           outFile = myAirlineXdf, \n           varsToKeep = c('Distance', 'AirTime'),\n           transforms = list(airSpeed = Distance / Airtime),\n           append = 'cols',\n           overwrite = TRUE)\n\n# Get Variable Information for airspeed\nrxGetInfo(data = myAirlineXdf, \n          getVarInfo = TRUE,\n          varsToKeep = 'airSpeed')\n\n# Summary for the airspeed variable\nrxSummary(~airSpeed, \n          data = myAirlineXdf)\n\n# Construct a histogtam for airspeed\n# We can use the xAxisMinMax argument to limit the X-axis\nrxHistogram(~airSpeed, \n            data = myAirlineXdf)\n\nrxHistogram(~airSpeed, \n            data = myAirlineXdf,\n            xNumTicks = 10,\n            numBreaks = 1500,\n            xAxisMinMax = c(0,12))\n\n\n\n\nTransforming variables using \nrxDataStep\n\n\n# Conversion to miles per hour\nrxDataStep(inData = myAirlineXdf, \n         outFile = myAirlineXdf, \n         varsToKeep = c('airSpeed'),\n           transforms = list(airSpeed = airSpeed * 60),\n         overwrite = TRUE)\n\n# Histogram for airspeed after conversion\nrxHistogram(~airSpeed, \n            data = myAirlineXdf)\n\n\n\n\nCorrelations\n\n\n# Correlation for departure delay, arrival delay, and air speed\nrxCor(formula = ~ DepDelay + ArrDelay + airSpeed,\n      data = myAirlineXdf,\n      rowSelection = (airSpeed > 50) & (airSpeed < 800))\n\n\n\n\nLinear regression\n\n\n# Regression for airSpeed based on departure delay\nmyLMobj <- rxLinMod(formula = airSpeed ~ DepDelay, \n         data = myAirlineXdf,\n         rowSelection = (airSpeed > 50) & (airSpeed < 800))\n\nsummary(myLMobj)\n\n\n\n\n2, Data Exploration\n\u00b6\n\n\nRevoScaleR\n options\n\n\n# Extract the names of the possible options\nnames(rxOptions())\n\n# Extract the sample data directory\nrxGetOption('sampleDataDir')\n\n# View the current value of the reportProgress option\nrxGetOption('reportProgress')\n\n# Set the value of the reportProgress option to 0\nrxOptions(reportProgress = 0)\n\n\n\n\nImport and explore Dow Jones data\n\n\n# Set up the variable that has the address of the relevant data file\ndjiXdf <- file.path(rxGetOption('sampleDataDir'), 'DJIAdaily.xdf')\n\n# Get information about that dataset\nrxGetInfo(djiXdf, getVarInfo = TRUE)\n\n\n\n\nExtracting meta data about a variable using \nrxGetVarInfo\n\n\n# Get variable information for the dataset\ndjiVarInfo <- rxGetVarInfo(djiXdf)\nnames(djiVarInfo)\n\n# Extract information about the closing cost variable\n(closeVarInfo <- djiVarInfo[['Close']])\n\n# Get the class of the closeVarInfo object\nclass(closeVarInfo)\n\n# Examine the structure of the closeVarInfo object\nstr(closeVarInfo)\n\n# Extract the global maximum of the closing cost variable\ncloseMax <- closeVarInfo[['high']]\n\n\n\n\nSummarizing variables with \nrxSummary\n\n\n# Basic summary statistics\nrxSummary(~ DayOfWeek + Close + Volume, \n          data = djiXdf)\n\n# Frequency weighted\nrxSummary(~ DayOfWeek + Close, \n          data = djiXdf, \n          fweights = 'Volume')\n\n# Basic frequency count\nrxCrossTabs(~ DayOfWeek, \n            data = djiXdf)\n\n\n\n\nExploring a distribution with \nrxHistogram\n\n\n# Numeric Variables\nrxHistogram(~ Close, \n            data = djiXdf)\n\n# Categorical Variable\nrxHistogram(~ DayOfWeek, \n            data = djiXdf)\n\n# Different panels for different days of the week\nrxHistogram(~ Close | DayOfWeek, \n            data = djiXdf)\n\n# Numeric Variables with a frequency weighting\nrxHistogram(~ Close, data = djiXdf, \n            fweights = 'Volume')\n\n\n\n\nPlotting bivariate relationships with \nrxLinePlot\n\n\n# Simple bivariate line plot\nrxLinePlot(Close ~ DaysSince1928, \n           data = djiXdf)\n\n# Using different panels for different days of the week\nrxLinePlot(Close ~ DaysSince1928 | DayOfWeek, \n           data = djiXdf)\n\n# Using different groups\nrxLinePlot(Close ~ DaysSince1928, \n           groups = DayOfWeek, \n           data = djiXdf)\n\n# Simple bivariate line plot, after taking the log() of the ordinate (y) variable\nrxLinePlot(log(Close) ~ DaysSince1928, \n           data = djiXdf)\n\n\n\n\nSummarzing variables with \nrxCrossTabs\n\n\n# Compute the the summed volume for each day of the week\nrxCrossTabs(formula = Volume ~ DayOfWeek, \n            data = djiXdf)\n\n# Compute the the summed volume for each day of the week for each month\nrxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, \n            data = djiXdf)\n\n# Compute the the average volume for each day of the week for each month\nrxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, \n            data = djiXdf, \n            means = TRUE)\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\nrxCrossTabs(formula = Close ~ F(Month):DayOfWeek, \n            data = djiXdf, \n            means = TRUE, \n            fweights = 'Volume')\n\n\n\n\nSummarzing variables with \nrxCube\n\n\n# Compute the the summed volume for each day of the week\nrxCrossTabs(Volume ~ DayOfWeek, \n            data = djiXdf)\n\nrxCube(Volume ~ DayOfWeek, \n       data = djiXdf, \n       means = FALSE)\n\n# Compute the the summed volume for each day of the week for each month\nrxCrossTabs(Volume ~ F(Month):DayOfWeek, \n            data = djiXdf)\n\nrxCube(Volume ~ F(Month):DayOfWeek, \n       data = djiXdf, \n       means = FALSE)\n\n# Compute the the average volume for each day of the week for each month\nrxCube(Volume ~ F(Month):DayOfWeek, \n       data = djiXdf)\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\nrxCube(Close ~ DayOfWeek, \n       data = djiXdf, \n       fweights = 'Volume')\n\n\n\n\n3, Data Manipulation\n\u00b6\n\n\nUsing \nrxDataStep\n to transform data\n\n\n# Get information on mortData\nrxGetInfo(mortData)\n\n## Set up my personal copy of the data\nmyMortData <- 'myMD.xdf'\n\n# Create the transform\nrxDataStep(inData = mortData, \n           outFile = myMortData, \n           transforms = list(highDebtRow = ccDebt > 8000), \n           overwrite = TRUE)\n\n#rxDataStep(inData = mortData, outFile = myMortData, transforms = list(highDebtRow = ccDebt > 8000))\n# Get the variable information\nrxGetVarInfo(myMortData)\n\n# Get the proportion of values that are 1\nrxSummary( ~ highDebtRow, \n           data = myMortData)\n\n# Compute multiple transforms!\nrxDataStep(inData = myMortData, outFile = myMortData,\n           transforms = list(\n             newHouse = houseAge < 10,\n             ccsXhd = creditScore * highDebtRow),\n           append = 'cols',\n           overwrite = TRUE)\n\n\n\n\nMore complex transforms using \ntransformFuncs\n\n\n# Compute the summary statistics\n(csSummary <- rxSummary(~ creditScore, data = mortData))\n\n# Extract the mean and std. deviation\nmeanCS <- csSummary$sDataFrame$Mean[1]\nsdCS <- csSummary$sDataFrame$StdDev[1]\n\n# Create a function to compute the scaled variable\nscaleCS <- function(mylist){\n  mylist[['scaledCreditScore']] <- (mylist[['creditScore']] - myCenter) / myScale\n  return(mylist)\n}\n\n# Run it with rxDataStep (A above in B below)\nmyMortData <- 'myMD.xdf'\nrxDataStep(inData = mortData, outFile = myMortData,\n           transformFunc = scaleCS,\n           transformObjects = list(myCenter = meanCS, myScale = sdCS))\n\n# Check the new variable\nrxGetVarInfo(myMortData)\nrxSummary(~ scaledCreditScore, \n          data = myMortData)\n\n\n\n\n4, Data Analysis\n\u00b6\n\n\nPreparing data for analysis: import\n\n\n# Declare the file paths for the csv and xdf files\nmyAirlineCsv <- file.path(rxGetOption('sampleDataDir'), 'AirlineDemoSmall.csv')\nmyAirlineXdf <- 'ADS.xdf'\n\n# Use rxImport to import the data into xdf format\nrxImport(inData = myAirlineCsv, \n         outFile = myAirlineXdf, \n         overwrite = TRUE,\n         colInfo = list( \n           DayOfWeek = list(\n            type = 'factor', \n            levels = c('Monday', 'Tuesday', 'Wednesday', \n                       'Thursday', 'Friday', 'Saturday', 'Sunday'))))\n\n\n\n\nPreparing data Ffor analysis: exploration\n\n\n# Summarize arrival delay for each day of the week\nrxSummary(formula = ArrDelay ~ DayOfWeek, \n          data = myAirlineXdf)\n\n# Vizualize the arrival delay histogram\nrxHistogram(formula = ~ArrDelay, \n            data = myAirlineXdf)\n\n\n\n\nConstruct a linear model\n\n\n# predict arrival delay by day of the week\nmyLM1 <- rxLinMod(ArrDelay ~ DayOfWeek, \n                  data = myAirlineXdf)\n\n# summarize the model\nsmmary(myLM1)\n\n# Use the transforms argument to create a factor variable associated with departure time 'on the fly,'\n# predict Arrival Delay by the interaction between Day of the week and that new factor variable\nmyLM2 <- rxLinMod(ArrDelay ~ DayOfWeek, \n                  data = myAirlineXdf,\n                  transforms = list(\n                    catDepTime = cut(CRSDepTime, breaks = seq(from = 5, to = 23, by = 2))),\n                    cube = TRUE)\n\n# summarize the model\nsummary(myLM2)\n\n\n\n\nGenerating predictions and residuals\n\n\n# Summarize model first\nsummary(myLM2)\n\n# Path to new dataset storing predictions\nmyNewADS <- 'myNEWADS.xdf'\n\n# Generate predictions\nrxPredict(modelObject = myLM2, \n          data = myAirlineXdf, \n          outData = myNewADS, \n          writeModelVars = TRUE)\n\n# Get information on the new dataset\nrxGetInfo(myNewADS, getVarInfo = TRUE)\n\n# Generate residuals.\nrxPredict(modelObject = myLM2, \n          data = myAirlineXdf, \n          outData = myNewADS, \n          writeModelVars = TRUE, \n          computeResiduals = TRUE, \n          overwrite = TRUE)\n\n# Get information on the new dataset\nrxGetInfo(myNewADS, getVarInfo = TRUE)\n\n\n\n\nLogistic regression\n\n\n# look at the meta data\nls()\nrxGetInfo(data = mortData, getVarInfo = TRUE)\n\n# Construct the logit model\nlogitModel <- rxLogit(formula = default ~ houseAge + F(year) + ccDebt + creditScore + yearsEmploy, \n                      data = mortData)\n\n# Summarize the result contained in logitModel\nsummary(logitModel\n\n\n\n\nIndividual mortgage information\n\n\n# Summarize the model\nsummary(logitModel)\n\n# view the first few rows\nhead(newData)\n\n# Make predictions\ndataWithPredictions <- rxPredict(modelObject = logitModel, \n                                 data = newData, \n                                 outData = newData, \n                                 type = 'response')\n\n# view the predictions\ndataWithPredictions\n\n\n\n\nComputing k-means with \nrxKmeans\n\n\n# Examine the mortData dataset\nrxGetInfo(mortData, getVarInfo = TRUE)\n\n# Set up a path to a new xdf file\nmyNewMortData = 'myMDwithKMeans.xdf'\n\n# Run k-means\nKMout <- rxKmeans(formula = ~ ccDebt + creditScore + houseAge, \n         data = mortData,\n         outFile = myNewMortData,\n         rowSelection = year == 2000,\n         numClusters = 4,\n         writeModelVars = TRUE)\n\nprint(KMout)\n\n# Examine the variables in the new dataset:\nrxGetInfo(myNewMortData, getVarInfo = TRUE)\n\n# Summarize the cluster variable:\nrxSummary(~ F(.rxCluster), data = myNewMortData)\n\n# Read into memory 10% of the data:\nmydf <- rxXdfToDataFrame(myNewMortData,\n                         rowSelection = randSamp == 1,\n                         varsToDrop = 'year',\n                         transforms = list(randSamp = sample(10, size = .rxNumRows, replace = TRUE)))\n\n## Visualize the clusters\nplot(mydf[-1], col = mydf$.rxCluster)\n\n\n\n\nCreate some decision trees\n\n\n# regression tree\nregTreeOut <- rxDTree(default ~ creditScore + ccDebt + yearsEmploy + houseAge, \n                      rowSelection = year == 2000, \n                      data = mortData, maxdepth = 5)\n\n# print out the object\nprint(regTreeOut)\n\n# plot a dendrogram, and add node labels\nplot(rxAddInheritance(regTreeOut))\ntext(rxAddInheritance(regTreeOut))\n\n# Another visualization\n#library(RevoTreeView)\n#createTreeView(regTreeOut)\n# predict values\nmyNewData = 'myNewMortData.xdf'\n\nrxPredict(regTreeOut,\n          data = mortData,\n          outData = myNewData,\n          writeModelVars = TRUE,\n          predVarNames = 'default_RegPred')\n\n# visualize ROC curve\nrxRocCurve(actualVarName = 'default', \n           predVarNames = 'default_RegPred', \n           data = myNewData)",
            "title": "Big Data Analysis with Revolution R Enterprise"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#1-introduction",
            "text": "Importing data with  rxImport  function  # Declare the file paths for the csv and xdf files\n# find the path or directory where the file is, load the path variable\nmyAirlineCsv <- file.path(rxGetOption('sampleDataDir'), '2007_subset.csv')\n\n# fin the data in this directory and load the data variable\nmyAirlineXdf <- '2007_subset.xdf'\n\n# Use rxImport to import the data into xdf format\n# rxImport(inData = myAirlineCsv, outFile = myAirlineXdf, overwrite = TRUE)\n# or\n# function within a function for more stats\nsystem.time(rxImport(inData = myAirlineCsv, \n                     outFile = myAirlineXdf, \n                     overwrite = TRUE))\nlist.files()  Functions for summarizing data  # Get basic information about your data\nrxGetInfo(data = myAirlineXdf, \n          getVarInfo = TRUE, \n          numRows = 10)\n\n# Summarize the variables corresponding to actual elapsed time, time in the air, departure delay, flight Distance\nrxSummary(formula = ~ ActualElapsedTime + AirTime + DepDelay + Distance, \n          data = myAirlineXdf)\n\n# Histogram of departure delays\nrxHistogram(formula = ~DepDelay, \n            data = myAirlineXdf)\n\n# Use parameters similar to a regular histogram to zero in on the interesting area\nrxHistogram(formula = ~DepDelay, \n            data = myAirlineXdf, \n            xAxisMinMax = c(-100, 400), \n            numBreaks = 500,\n            xNumTicks = 10)  Creating new variables using  rxDataStep  # Calculate an additional variable: airspeed (distance traveled / time in the air)\nrxDataStep(inData = myAirlineXdf, \n           outFile = myAirlineXdf, \n           varsToKeep = c('Distance', 'AirTime'),\n           transforms = list(airSpeed = Distance / Airtime),\n           append = 'cols',\n           overwrite = TRUE)\n\n# Get Variable Information for airspeed\nrxGetInfo(data = myAirlineXdf, \n          getVarInfo = TRUE,\n          varsToKeep = 'airSpeed')\n\n# Summary for the airspeed variable\nrxSummary(~airSpeed, \n          data = myAirlineXdf)\n\n# Construct a histogtam for airspeed\n# We can use the xAxisMinMax argument to limit the X-axis\nrxHistogram(~airSpeed, \n            data = myAirlineXdf)\n\nrxHistogram(~airSpeed, \n            data = myAirlineXdf,\n            xNumTicks = 10,\n            numBreaks = 1500,\n            xAxisMinMax = c(0,12))  Transforming variables using  rxDataStep  # Conversion to miles per hour\nrxDataStep(inData = myAirlineXdf, \n         outFile = myAirlineXdf, \n         varsToKeep = c('airSpeed'),\n           transforms = list(airSpeed = airSpeed * 60),\n         overwrite = TRUE)\n\n# Histogram for airspeed after conversion\nrxHistogram(~airSpeed, \n            data = myAirlineXdf)  Correlations  # Correlation for departure delay, arrival delay, and air speed\nrxCor(formula = ~ DepDelay + ArrDelay + airSpeed,\n      data = myAirlineXdf,\n      rowSelection = (airSpeed > 50) & (airSpeed < 800))  Linear regression  # Regression for airSpeed based on departure delay\nmyLMobj <- rxLinMod(formula = airSpeed ~ DepDelay, \n         data = myAirlineXdf,\n         rowSelection = (airSpeed > 50) & (airSpeed < 800))\n\nsummary(myLMobj)",
            "title": "1, Introduction"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#2-data-exploration",
            "text": "RevoScaleR  options  # Extract the names of the possible options\nnames(rxOptions())\n\n# Extract the sample data directory\nrxGetOption('sampleDataDir')\n\n# View the current value of the reportProgress option\nrxGetOption('reportProgress')\n\n# Set the value of the reportProgress option to 0\nrxOptions(reportProgress = 0)  Import and explore Dow Jones data  # Set up the variable that has the address of the relevant data file\ndjiXdf <- file.path(rxGetOption('sampleDataDir'), 'DJIAdaily.xdf')\n\n# Get information about that dataset\nrxGetInfo(djiXdf, getVarInfo = TRUE)  Extracting meta data about a variable using  rxGetVarInfo  # Get variable information for the dataset\ndjiVarInfo <- rxGetVarInfo(djiXdf)\nnames(djiVarInfo)\n\n# Extract information about the closing cost variable\n(closeVarInfo <- djiVarInfo[['Close']])\n\n# Get the class of the closeVarInfo object\nclass(closeVarInfo)\n\n# Examine the structure of the closeVarInfo object\nstr(closeVarInfo)\n\n# Extract the global maximum of the closing cost variable\ncloseMax <- closeVarInfo[['high']]  Summarizing variables with  rxSummary  # Basic summary statistics\nrxSummary(~ DayOfWeek + Close + Volume, \n          data = djiXdf)\n\n# Frequency weighted\nrxSummary(~ DayOfWeek + Close, \n          data = djiXdf, \n          fweights = 'Volume')\n\n# Basic frequency count\nrxCrossTabs(~ DayOfWeek, \n            data = djiXdf)  Exploring a distribution with  rxHistogram  # Numeric Variables\nrxHistogram(~ Close, \n            data = djiXdf)\n\n# Categorical Variable\nrxHistogram(~ DayOfWeek, \n            data = djiXdf)\n\n# Different panels for different days of the week\nrxHistogram(~ Close | DayOfWeek, \n            data = djiXdf)\n\n# Numeric Variables with a frequency weighting\nrxHistogram(~ Close, data = djiXdf, \n            fweights = 'Volume')  Plotting bivariate relationships with  rxLinePlot  # Simple bivariate line plot\nrxLinePlot(Close ~ DaysSince1928, \n           data = djiXdf)\n\n# Using different panels for different days of the week\nrxLinePlot(Close ~ DaysSince1928 | DayOfWeek, \n           data = djiXdf)\n\n# Using different groups\nrxLinePlot(Close ~ DaysSince1928, \n           groups = DayOfWeek, \n           data = djiXdf)\n\n# Simple bivariate line plot, after taking the log() of the ordinate (y) variable\nrxLinePlot(log(Close) ~ DaysSince1928, \n           data = djiXdf)  Summarzing variables with  rxCrossTabs  # Compute the the summed volume for each day of the week\nrxCrossTabs(formula = Volume ~ DayOfWeek, \n            data = djiXdf)\n\n# Compute the the summed volume for each day of the week for each month\nrxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, \n            data = djiXdf)\n\n# Compute the the average volume for each day of the week for each month\nrxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, \n            data = djiXdf, \n            means = TRUE)\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\nrxCrossTabs(formula = Close ~ F(Month):DayOfWeek, \n            data = djiXdf, \n            means = TRUE, \n            fweights = 'Volume')  Summarzing variables with  rxCube  # Compute the the summed volume for each day of the week\nrxCrossTabs(Volume ~ DayOfWeek, \n            data = djiXdf)\n\nrxCube(Volume ~ DayOfWeek, \n       data = djiXdf, \n       means = FALSE)\n\n# Compute the the summed volume for each day of the week for each month\nrxCrossTabs(Volume ~ F(Month):DayOfWeek, \n            data = djiXdf)\n\nrxCube(Volume ~ F(Month):DayOfWeek, \n       data = djiXdf, \n       means = FALSE)\n\n# Compute the the average volume for each day of the week for each month\nrxCube(Volume ~ F(Month):DayOfWeek, \n       data = djiXdf)\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\nrxCube(Close ~ DayOfWeek, \n       data = djiXdf, \n       fweights = 'Volume')",
            "title": "2, Data Exploration"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#3-data-manipulation",
            "text": "Using  rxDataStep  to transform data  # Get information on mortData\nrxGetInfo(mortData)\n\n## Set up my personal copy of the data\nmyMortData <- 'myMD.xdf'\n\n# Create the transform\nrxDataStep(inData = mortData, \n           outFile = myMortData, \n           transforms = list(highDebtRow = ccDebt > 8000), \n           overwrite = TRUE)\n\n#rxDataStep(inData = mortData, outFile = myMortData, transforms = list(highDebtRow = ccDebt > 8000))\n# Get the variable information\nrxGetVarInfo(myMortData)\n\n# Get the proportion of values that are 1\nrxSummary( ~ highDebtRow, \n           data = myMortData)\n\n# Compute multiple transforms!\nrxDataStep(inData = myMortData, outFile = myMortData,\n           transforms = list(\n             newHouse = houseAge < 10,\n             ccsXhd = creditScore * highDebtRow),\n           append = 'cols',\n           overwrite = TRUE)  More complex transforms using  transformFuncs  # Compute the summary statistics\n(csSummary <- rxSummary(~ creditScore, data = mortData))\n\n# Extract the mean and std. deviation\nmeanCS <- csSummary$sDataFrame$Mean[1]\nsdCS <- csSummary$sDataFrame$StdDev[1]\n\n# Create a function to compute the scaled variable\nscaleCS <- function(mylist){\n  mylist[['scaledCreditScore']] <- (mylist[['creditScore']] - myCenter) / myScale\n  return(mylist)\n}\n\n# Run it with rxDataStep (A above in B below)\nmyMortData <- 'myMD.xdf'\nrxDataStep(inData = mortData, outFile = myMortData,\n           transformFunc = scaleCS,\n           transformObjects = list(myCenter = meanCS, myScale = sdCS))\n\n# Check the new variable\nrxGetVarInfo(myMortData)\nrxSummary(~ scaledCreditScore, \n          data = myMortData)",
            "title": "3, Data Manipulation"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#4-data-analysis",
            "text": "Preparing data for analysis: import  # Declare the file paths for the csv and xdf files\nmyAirlineCsv <- file.path(rxGetOption('sampleDataDir'), 'AirlineDemoSmall.csv')\nmyAirlineXdf <- 'ADS.xdf'\n\n# Use rxImport to import the data into xdf format\nrxImport(inData = myAirlineCsv, \n         outFile = myAirlineXdf, \n         overwrite = TRUE,\n         colInfo = list( \n           DayOfWeek = list(\n            type = 'factor', \n            levels = c('Monday', 'Tuesday', 'Wednesday', \n                       'Thursday', 'Friday', 'Saturday', 'Sunday'))))  Preparing data Ffor analysis: exploration  # Summarize arrival delay for each day of the week\nrxSummary(formula = ArrDelay ~ DayOfWeek, \n          data = myAirlineXdf)\n\n# Vizualize the arrival delay histogram\nrxHistogram(formula = ~ArrDelay, \n            data = myAirlineXdf)  Construct a linear model  # predict arrival delay by day of the week\nmyLM1 <- rxLinMod(ArrDelay ~ DayOfWeek, \n                  data = myAirlineXdf)\n\n# summarize the model\nsmmary(myLM1)\n\n# Use the transforms argument to create a factor variable associated with departure time 'on the fly,'\n# predict Arrival Delay by the interaction between Day of the week and that new factor variable\nmyLM2 <- rxLinMod(ArrDelay ~ DayOfWeek, \n                  data = myAirlineXdf,\n                  transforms = list(\n                    catDepTime = cut(CRSDepTime, breaks = seq(from = 5, to = 23, by = 2))),\n                    cube = TRUE)\n\n# summarize the model\nsummary(myLM2)  Generating predictions and residuals  # Summarize model first\nsummary(myLM2)\n\n# Path to new dataset storing predictions\nmyNewADS <- 'myNEWADS.xdf'\n\n# Generate predictions\nrxPredict(modelObject = myLM2, \n          data = myAirlineXdf, \n          outData = myNewADS, \n          writeModelVars = TRUE)\n\n# Get information on the new dataset\nrxGetInfo(myNewADS, getVarInfo = TRUE)\n\n# Generate residuals.\nrxPredict(modelObject = myLM2, \n          data = myAirlineXdf, \n          outData = myNewADS, \n          writeModelVars = TRUE, \n          computeResiduals = TRUE, \n          overwrite = TRUE)\n\n# Get information on the new dataset\nrxGetInfo(myNewADS, getVarInfo = TRUE)  Logistic regression  # look at the meta data\nls()\nrxGetInfo(data = mortData, getVarInfo = TRUE)\n\n# Construct the logit model\nlogitModel <- rxLogit(formula = default ~ houseAge + F(year) + ccDebt + creditScore + yearsEmploy, \n                      data = mortData)\n\n# Summarize the result contained in logitModel\nsummary(logitModel  Individual mortgage information  # Summarize the model\nsummary(logitModel)\n\n# view the first few rows\nhead(newData)\n\n# Make predictions\ndataWithPredictions <- rxPredict(modelObject = logitModel, \n                                 data = newData, \n                                 outData = newData, \n                                 type = 'response')\n\n# view the predictions\ndataWithPredictions  Computing k-means with  rxKmeans  # Examine the mortData dataset\nrxGetInfo(mortData, getVarInfo = TRUE)\n\n# Set up a path to a new xdf file\nmyNewMortData = 'myMDwithKMeans.xdf'\n\n# Run k-means\nKMout <- rxKmeans(formula = ~ ccDebt + creditScore + houseAge, \n         data = mortData,\n         outFile = myNewMortData,\n         rowSelection = year == 2000,\n         numClusters = 4,\n         writeModelVars = TRUE)\n\nprint(KMout)\n\n# Examine the variables in the new dataset:\nrxGetInfo(myNewMortData, getVarInfo = TRUE)\n\n# Summarize the cluster variable:\nrxSummary(~ F(.rxCluster), data = myNewMortData)\n\n# Read into memory 10% of the data:\nmydf <- rxXdfToDataFrame(myNewMortData,\n                         rowSelection = randSamp == 1,\n                         varsToDrop = 'year',\n                         transforms = list(randSamp = sample(10, size = .rxNumRows, replace = TRUE)))\n\n## Visualize the clusters\nplot(mydf[-1], col = mydf$.rxCluster)  Create some decision trees  # regression tree\nregTreeOut <- rxDTree(default ~ creditScore + ccDebt + yearsEmploy + houseAge, \n                      rowSelection = year == 2000, \n                      data = mortData, maxdepth = 5)\n\n# print out the object\nprint(regTreeOut)\n\n# plot a dendrogram, and add node labels\nplot(rxAddInheritance(regTreeOut))\ntext(rxAddInheritance(regTreeOut))\n\n# Another visualization\n#library(RevoTreeView)\n#createTreeView(regTreeOut)\n# predict values\nmyNewData = 'myNewMortData.xdf'\n\nrxPredict(regTreeOut,\n          data = mortData,\n          outData = myNewData,\n          writeModelVars = TRUE,\n          predVarNames = 'default_RegPred')\n\n# visualize ROC curve\nrxRocCurve(actualVarName = 'default', \n           predVarNames = 'default_RegPred', \n           data = myNewData)",
            "title": "4, Data Analysis"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/",
            "text": "CONTENT\n\n\n1, Introduction to eXtensible Time\n\n\n2, First Order of Business - Basic Manipulations\n\n\n3, Merging and modifying time series\n\n\n4, Apply and aggregate by time\n\n\n5, Extra features of xts\n\n\n\n\n\n\n\n\nForeword\n\n\nCourse notes and snippets.\n\n\n\n\n1, Introduction to eXtensible Time\n\u00b6\n\n\nWhat is an \nxts\n object\n\n\nxts\n, a constructor or a subclass that inherits behavior from parents. \nxts\n (as a subclass) extends the popular \nzoo\n class (as a parent). Most \nzoo\n methods work for \nxts\n. \n\n\nxts\n is a matrix objects; subsets always preserve the matrix form. \n\n\nxts\n are indexed by a formal time object. You can time-stamp the data.\n\n\nThe main \nxts\n constructor two most important arguments are \nx\n for the data and \norder.by\n for the index. \nx\n must be a vector or matrix. \norder.by\n is a vector of the same length or number of rows of \nx\n; it must be a proper time or date object and it must be in increasing order.\n\n\nxts\n also allows you to bind arbitrary key-value attributes to your data. This lets you keep metadata about your object inside your object.\n\n\n# Load xts\nlibrary(xts)\n\n\n\n\nA first \nxts\n object\n\n\ncore <- matrix(c(rep(1,3), rep(2,3)), nrow = 3, ncol = 2)\ncore\n\nidx <- as.Date(c(\"2016-06-01\",\"2016-06-02\", \"2016-06-03\"))\nidx\n\nex_matrix <- xts(core, order.by = idx)\nex_matrix\n\n\n\n\n\n\n\n    \n1\n2\n\n    \n1\n2\n\n    \n1\n2\n\n\n\n\n\n\n\n[1] \"2016-06-01\" \"2016-06-02\" \"2016-06-03\"\n\n\n\n           [,1] [,2]\n2016-06-01    1    2\n2016-06-02    1    2\n2016-06-03    1    2\n\n\n\nMore than a matrix\n\n\nxts\n objects are normal R matrices, but with special powers.\n\n\n# View the structure of ex_matrix\nstr(ex_matrix)\n\n\n\n\nAn 'xts' object on 2016-06-01/2016-06-03 containing:\n  Data: num [1:3, 1:2] 1 1 1 2 2 2\n  Indexed by objects of class: [Date] TZ: UTC\n  xts Attributes:  \n NULL\n\n\n\n# Extract the 3rd observation of the 2nd column of ex_matrix\nex_matrix[3, 2]\n\n# Extract the 3rd observation of the 2nd column of core \ncore[3, 2]\n\n\n\n\n           [,1]\n2016-06-03    2\n\n\n\n2\n\n\nYour first xts object\n\n\nVector must be of the same length or number of rows as x and, this is very important: it must be a proper time or date object and it must be in increasing order.\n\n\n# Create the object data using 5 random numbers\ndata <- rnorm(5)\ndata\n\n# Create dates as a Date class object starting from 2016-01-01\ndates <- seq(as.Date(\"2016-01-01\"), length = 5, by = \"days\")\ndates\n\n# Use xts() to create smith\nsmith <- xts(x = data, order.by = dates)\nsmith\n\n# Create bday (1899-05-08) using a POSIXct date class object\nbday <- as.POSIXct(\"1899-05-08\")\nbday\n\n# Create hayek and add a new attribute called born\nhayek <- xts(x = data, order.by = dates, born = bday)\nhayek\n\n\n\n\n\n    \n0.384581498733952\n\n    \n-0.423869463715667\n\n    \n-0.49549963387221\n\n    \n1.49946058244083\n\n    \n-0.92906780054414\n\n\n\n\n\n[1] \"2016-01-01\" \"2016-01-02\" \"2016-01-03\" \"2016-01-04\" \"2016-01-05\"\n\n\n\n                 [,1]\n2016-01-01  0.3845815\n2016-01-02 -0.4238695\n2016-01-03 -0.4954996\n2016-01-04  1.4994606\n2016-01-05 -0.9290678\n\n\n\n[1] \"1899-05-08 EST\"\n\n\n\n                 [,1]\n2016-01-01  0.3845815\n2016-01-02 -0.4238695\n2016-01-03 -0.4954996\n2016-01-04  1.4994606\n2016-01-05 -0.9290678\n\n\n\nDeconstructing xts\n\n\nSeparate a time series into its core data and index attributes for additional analysis and manipulation.\n\n\nFunctions are methods from the \nzoo\n class.\n\n\n# Extract the core data of hayek\nhayek_core <- coredata(hayek)\n\n# View the class of hayek_core\nclass(hayek_core)\n\n# Extract the index of hayek\nhayek_index <- index(hayek)\n\n# View the class of hayek_index\nclass(hayek_index)\n\n\n\n\n\u2018matrix\u2019\n\n\n\u2018Date\u2019\n\n\nTime-based indices\n\n\nxts\n objects get their power from the index attribute that holds the time dimension. One major difference between \nxts\n and most other time series objects in R is the ability to use any one of various classes that are used to represent time. Whether \nPOSIXct\n, \nDate\n, or some other class, xts will convert this into an internal form to make subsetting as natural to the user as possible.\n\n\n# Create dates\ndates <- as.Date(\"2016-01-01\") + 0:4\n\n# Create ts_a\nts_a <- xts(x = 1:5, order.by = dates)\n\n# Create ts_b\nts_b <- xts(x = 1:5, order.by = as.POSIXct(dates))\n\n# Extract the rows of ts_a using the index of ts_b\nts_a[index(ts_a)]\n\n# Extract the rows of ts_b using the index of ts_a\nts_b[index(ts_a)]\n\n\n\n\n           [,1]\n2016-01-01    1\n2016-01-02    2\n2016-01-03    3\n2016-01-04    4\n2016-01-05    5\n\n\n\n     [,1]\n\n\n\n2, First Order of Business - Basic Manipulations\n\u00b6\n\n\nConverting xts objects\n\n\nxts\n provides methods to convert all of the major objects you are likely to come across - suitable native R types like matrix, data.frame, and ts are supported, as well as contributed ones such as \ntimeSeries\n, \nfts\n and of course \nzoo\n. \nas.xts\n is the workhorse function to do the conversions to \nxts\n, and similar functions will provide the reverse behavior.\n\n\n# using a R dataset\nhead(austres)\n\n\n\n\n\n    \n13067.3\n\n    \n13130.5\n\n    \n13198.4\n\n    \n13254.2\n\n    \n13303.7\n\n    \n13353.9\n\n\n\n\n\n# Convert austres to an xts object called au\nau <- as.xts(austres)\nhead(au, 5)\n\n# Convert your xts object (au) into a matrix am\nam <- as.matrix(au)\nhead(am, 5)\n\n# Convert the original austres into a matrix am2\nam2 <- as.matrix(austres)\nhead(am2, 5)\n\n\n\n\n           [,1]\n1971 Q2 13067.3\n1971 Q3 13130.5\n1971 Q4 13198.4\n1972 Q1 13254.2\n1972 Q2 13303.7\n\n\n\n\n\n\n    \n1971 Q2\n13067.3\n\n    \n1971 Q3\n13130.5\n\n    \n1971 Q4\n13198.4\n\n    \n1972 Q1\n13254.2\n\n    \n1972 Q2\n13303.7\n\n\n\n\n\n\n\n\n\n\n    \n13067.3\n\n    \n13130.5\n\n    \n13198.4\n\n    \n13254.2\n\n    \n13303.7\n\n\n\n\n\n\n\nImporting data\n\n\nRead raw data from files on disk or the web and convert data to \nxts\n.\n\n\nRead the same data into a \nzoo\n object using read.zoo and convert the \nzoo\n into an \nxts\n object.\n\n\n# Create dat by reading tmp_file\ndat <- as.matrix(read.csv('tmp_file.csv', sep = ',', header = TRUE))\ndat\n\n\n\n\n\n\na\nb\n\n\n\n    \n1/02/2015\n1\n3\n\n    \n2/03/2015\n2\n4\n\n\n\n\n\n\n\n# Convert dat into xts\nxts(dat, order.by = as.Date(rownames(dat), \"%m%d%Y\"))\n\n\n\n\n     a b\n<NA> 1 3\n<NA> 2 4\n\n\n\n# Read tmp_file using read.zoo and as.xts\ndat_xts <- as.xts(read.zoo('tmp_file.csv', index.column = 0, sep = \",\", format = \"%m/%d/%Y\"))\ndat_xts\n\n\n\n\n           a b\n2015-01-02 1 3\n2015-02-03 2 4\n\n\n\nExporting xts objects\n\n\nhead(sunspots, 5)\n\n# Convert sunspots to xts\nsunspots_xts <- as.xts(sunspots)\n\n\n\n\n\n    \n58\n\n    \n62.6\n\n    \n70\n\n    \n55.7\n\n    \n85\n\n\n\n\n\n# Get the temporary file name\ntmp <- tempfile()\n\n# Write the xts object using zoo to tmp \nwrite.zoo(sunspots, sep = \",\", file = tmp)\n\ntmp\n\n\n\n\n\u2018C:\\Users\\Dell\\AppData\\Local\\Temp\\RtmpEdYcAQ\\file1a706c8c732b\u2019\n\n\n# Read the tmp file\n# FUN=as.yearmon will convert strings such as Jan 1749 into a proper time class\nsun <- read.zoo(tmp, sep = \",\", FUN = as.yearmon)\n\n# Convert sun into xts\nsun_xts <- as.xts(sun)\nhead(sun_xts, 5)\n\n\n\n\nError in eval(expr, envir, enclos): impossible de trouver la fonction \"read.zoo\"\nTraceback:\n\n\n\nThe ISO8601 Standard\n\n\nThe ISO8601 standard is the internationally recognized and accepted way to represent dates and times. From the biggest to the smallest, left to right, YYYY-MM-DDTHH:MM:SS.\n\n\nThe standard allows for a common format to not only describe dates, but also a way to represent ranges and repeating intervals:\n\n\n\n\n2004 or 2001/2015\n\n\n201402/03\n\n\n2014-02-22 08:30:00\n\n\nT08:00/T09:00\n\n\n\n\nQuerying for dates\n\n\nDate ranges can be extracted from xts objects by simply specifying the period(s) you want using special character strings in your subset.\n\n\nCode:\n\n\nA[\"201601\"]         ## Jan 2016\nA[\"20160125\"]       ## Jan 25, 2016\nA[\"201203/201212\"]  ## Feb to Dec 2012\n\n# Select all of 2016 from x\nx_2016 <- x[\"2016\"]\n\n# Select January 2016 to March 22\njan_march <- x[\"201601/20160322\"]\n\n\n\n\nExtracting recurring intraday intervals\n\n\nMost common data \u201cin the wild\u201d is daily. On ocassion you may find yourself working with intraday data, that is date plus times.\n\n\nYou can slice days easily by using special notation in the \ni =\n argument to the single bracket extraction (i.e. \n[i,j]\n).\n\n\nCode:\n\n\n# Intraday times for all days\nNYSE[\"T09:30/T16:00\"]\n\n# Extract all data between 8AM and 10AM\nmorn_2010 <- irreg[\"T8:00/T10:00\"]\n\n# Extract the observations for January 13th\nmorn_2010[\"2010-01-13\"]\n\n\n\n\nAlternating extraction techniques\n\n\nOften times you may need to subset an existing time series with a set of Dates, or time-based objects. These might be from \nas.Date()\n, or \nas.POSIXct()\n, or a variety of other classes.\n\n\nCode:\n\n\n# Subset x using the vector dates\nx[dates]\n\n# Subset x using dates as POSIXct\nx[as.POSIXct(dates)]\n\n\n\n\nExtracting recurring intraday intervals\n\n\nThe most common time series data is daily, intraday data, which contains both dates and times. \n\n\n# Extract all data between 8AM and 10AM\nmorn_2010 <- irreg[\"T8:00/T10:00\"]\n\n# Extract the observations for January 13th\nmorn_2010[\"2010-01-13\"]\n\n\n\n\nRow selection with time objects\n\n\nSubset an existing time series with a set of Dates, or time-based objects.\n\n\n# Subset x using the vector dates\nx[dates]\n\n# Subset x using dates as POSIXct\nx[as.POSIXct(dates)]\n\n\n\n\nUpdate and replace elements\n\n\nReplace known intervals or observations with an NA, say due to a malfunctioning sensor on a particular day or a set of outliers given a holiday.\n\n\nCode:\n\n\n# Replace all values in x on dates with NA\nx[dates] <- NA\n\n# Replace dates from 2016-06-09 and on with 0\nx[\"20160609/\"] <- 0\n\n\n\n\nFind the \nfirst\n or \nlast\n period of time\n\n\nSometimes you need to locate data by relative time. Instead of using an absolute offset, you describe a position relative in time. A simple example would be something like the \nlast 3 weeks\n of a series, or the \nfirst day of current month\n.\n\n\nCode:\n\n\n# Create lastweek with using the last 1 week of temps\nlastweek <- last(temps, \"1 week\")\n\n# Print the last 2 observations in lastweek\nlast(lastweek, 2)\n\n# Extract all but the last two days of lastweek\nlast(lastweek, \"-2 day\")\n\n\n\n\nCombining \nfirst\n and \nlast\n\n\nCode:\n\n\n# Last 3 days of first week\nlast(first(Temps, '1 week'),'3 days') \n\n# Extract the first three days of the second week of temps\nfirst(last(first(temps, \"2 week\"), \"1 week\"), \"3 day\")\n\n\n\n\nMatrix arithmetic - add, subtract, multiply and divide in time!\n\n\nWhen you perform any binary operation using two xts objects, these objects are first aligned using the intersection of the indexes to preserve the point-in-time aspect of your data, assuring that you don\u2019t introduce accidental look ahead (or look behind!) bias into your calculations.\n\n\nYour options include:\n\n\n\n\ncoredata() or as.numeric() (drop one to a matrix or vector).\n\n\nManually shift index values - i.e. use lag().\n\n\nReindex your data (before or after the calculation).\n\n\n\n\nxts\n respects time and will only return the intersection of times when doing various mathematical operations. First option:\n\n\n# Add a and b\na + b\n\n# Add a with the numeric value of b\na + as.numeric(b)\n\n\n\n\nMath with non-overlapping indexes\n\n\nThis third way involves modifying the two series you want by aligning assuring you have some union of dates - the dates you require in your final output. This makes it possibly to preserve dimensionality of the data:\n\n\nmerge(b, index(a))\n\n# Add a to b, and fill all missing rows of b with 0\na + merge(b, index(a), fill = 0)\n\n# Add a to b and fill NAs with the last observation\na + merge(b, index(a), fill = na.locf)\n\n\n\n\n3, Merging and modifying time series\n\u00b6\n\n\nCombining \nxts\n by column with \nmerge\n\n\nxts\n makes it easy to join data by column and row using a few different functions.  \n\n\nxts\n objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types.\n\n\nOne of the most important functions to accomplish this is \nmerge\n. It works like a \ncbind\n or and SQL \njoin\n:\n\n\n\n\ninner join (and)\n\n\nouter join (or)\n\n\nleft join\n\n\nright join\n\n\n\n\n# Basic argument use\nmerge(a, b, join = \"right\", fill = 9999)\n\n# Perform an inner join of a and b\nmerge(a, b, join = \"inner\")\n\n# Perform of a left-join of a and b, fill mising values with 0\nmerge(a, b, join = \"left\", fill = 0)\n\n# fill = na.locf, fill = NA\n\n\n\n\nCombining \nxts\n by row with \nrbind\n\n\nEasy to add new rows to your data.\n\n\n# Row bind temps_june30 to temps, assign this to temps2\ntemps2 <- rbind(temps, temps_june30)\n\n# Row bind temps_july17 and temps_july18 to temps2, call this temps3\ntemps3 <- rbind(temps2, temps_july17, temps_july18)\n\n\n\n\nFill missing values using last or previous observation\n\n\nThe \nna.locf\n function takes the last-observation-carried-forward approach.\n\n\nThe \nxts\n package leverages the power of \nzoo\n for help with this. \nzoo\n provides a variety of missing data handling functions which are usable by \nxts\n, and very handy.\n\n\n# Last obs. carried forward\nna.locf(x)                \n\n# Next obs. carried backward\nna.locf(x, fromLast = TRUE)\n\n# Fill missing values using the last observation\nna.locf(temps, na.rm = TRUE,\n                fromLast = TRUE)\n\n# Fill missing values using the next observation\nna.locf(temps, na.rm = TRUE)\n\n# maxgap = Inf\n# rule = 2,\n\n\n\n\nNA interpolation\n\n\nFrom \nzoo\n.\n\n\nOn occassion, a simple carry forward approach to missingness isn\u2019t appropriate. It may be that a series is missing an observation due to a higher frequency sampling than the generating process. You might also encounter an observation that is in error, yet expected to be somewhere between the values of its neighboring observations.\n\n\nBoth of these cases are examples of where interpolation is useful.\n\n\n# Interpolate NAs using linear approximation\nna.approx(AirPass)\n\n\n\n\nCombine a leading and lagging time series\n\n\nAnother common modification for time series is the ability to lag a series.\n\n\n# Your final object\nlag(x, k = 1, na.pad = TRUE)\n\n# k = -1  leading\n# k = 1   lagging\n# zoo uses the other way around\n\n# k = c(0, 1, 5)  multiple leads or lags\n\n# Create a leading object called lead_x\nlead_x <- lag(x, k = -1)\n\n# Create a lagging object called lag_x\nlag_x <- lag(x, k = 1)\n\n# Merge your three series together and assign to z\nz <- merge(lead_x, x, lag_x)\n\n\n\n\nCalculate a difference of a series with \ndiff\n\n\nAnother common operation on time series, typically on those that are non-stationary, is to take a difference of the series.\n\n\ndifferences\n is the order of the difference (how many times \ndiff\n is called). A simple way to view a single (or \nfirst order\n) difference is to see it as \nx(t) - x(t-k)\n where \nk\n is the number of lags to go back. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference).\n\n\ndiff(x, lag = 1, difference = 1, log = FALSE, na.pad = TRUE)\n\n# calculate the first difference of AirPass using lag and subtraction\nAirPass - lag(AirPass, k = 1)\n\n# calculate the first order 12-month difference if AirPass\ndiff(AirPass, lag = 12, differences = 1)\n\n\n\n\nWhat is the key difference in lag between xts and zoo? The \nk\n argument in \nzoo\n uses positive values for shifting past observations forward.\n\n\n4, Apply and aggregate by time\n\u00b6\n\n\nFind intervals by time in \nxts\n\n\nThe main function in xts to facilitate this is endpoints. It takes a time series (or a vector of times) and returns the locations of the last observations in each interval.\n\n\nFor example, the code below locates the last observation of each year for the \nAirPass\n dataset:\n\n\nendpoints(AirPass, on = \"years\")\n\n# The argument on supports a variety of periods, including \"years\", \"quarters\", \"months\", as well as intraday intervals such as \"hours\", and \"minutes\"\n\n# on = \"weeks\", k = 2, the result would be the final day of every other week in your data\n\n# Locate the final day of every week\nendpoints(temps, on = \"weeks\")\n\n# Locate the final day of every two weeks\nendpoints(temps, on = \"weeks\", k = 2)\n\n\n\n\nApply a function by time period(s)\n\n\nxts\n provides \nperiod.apply\n, which takes a time series, an \nINDEX\n of endpoints, and a function to \napply\n:\n\n\nperiod.apply(x, INDEX, FUN, ...)\n\n# Calculate the weekly endpoints\nep <- endpoints(temps, on = \"weeks\")\n\n# Now calculate the weekly mean and display the results\nperiod.apply(temps[, \"Temp.Mean\"], INDEX = ep, FUN = mean)\n\n\n\n\nUsing \nlapply\n and \nsplit\n to apply functions on intervals\n\n\nOften it is useful to physically split your data into disjoint chunks by time and perform some calculation on these periods.\n\n\n# Split temps by week\ntemps_weekly <- split(temps, f = \"weeks\")\n\n# Create a list of weekly means\nlapply(X = temps_weekly, FUN = mean)\n\n\n\n\nSelection by \nendpoints\n vs. \nsplit-lapply-rbind\n\n\n# use the proper combination of split, lapply and rbind.\n#T1 <- do.call(rbind, ___(___(Temps, \"weeks\"), function(w) last(w, n=\"___\")))\nT1 <- do.call(rbind, lapply(split(Temps, \"weeks\"), function(w) last(w, n= \"1 day\")))\n\n# now subset Temps using the results of 'endpoints'\nlast_day_of_weeks <- endpoints(Temps, on = \"weeks\")\nT2 <- Temps[last_day_of_weeks]\n\n\n\n\nConvert univariate series to OHLC data\n\n\nIn financial series it is common to find Open-High-Low-Close data calculated over some repeating and regular interval.\n\n\nAlso known as range bars, aggregating a series based on some regular window can make analysis easier amongst series that have varying frequencies. For example, a weekly economic series and a daily stock series can be compared more easily if the daily is converted to weekly.\n\n\nOHLC means Opening, High, Low, Closing values.\n\n\nto.period(x,\n          period = \"months\", \n          k = 1, \n          indexAt, \n          name=NULL,\n          OHLC = TRUE,\n          ...)\n\n# Convert USDEUR to weekly\n# OHLC = FALSE by default\nUSDEUR_weekly <- to.period(USDEUR, period = \"weeks\")\nhead(USDEUR)\nhead(USDEUR_weekly)\n\n# Convert USDEUR_weekly to monthly\nUSDEUR_monthly <- to.period(USDEUR_weekly, period = \"months\")\n\n# Convert USDEUR_monthly to yearly univariate\nUSDEUR_yearly <- to.period(USDEUR_monthly, period = \"years\", OHLC = FALSE)\n\n\n\n\nConvert a series to a lower frequency\n\n\nBesides converting univariate time series to OHLC series, to.period() also lets you convert OHLC to lower regularized frequency - something like subsampling your data.\n\n\nFor example, when using the shortcut function \nto.quarterly()\n xts will convert your index to the \nyearqtr\n class to make periods more obvious.\n\n\n# Convert EqMktNeutral to quarterly OHLC\nmkt_quarterly <- to.period(EqMktNeutral, period = \"quarters\")\n\n# Convert EqMktNeutral to quarterly using shortcut function\n# Change the base name of each OHLC column to EDHEC.Equity and change the index to \"firstof\"\nmkt_quarterly2 <- to.quarterly(EqMktNeutral, name = \"EDHEC.Equity\", indexAt = \"first\")\n\n\n\n\nCalculate basic rolling value of series by month\n\n\nRolling windows can be discrete: use \nlapply\n with (\ncumsum, cumprod, cummax, cummin\n) and \nsplit\n:\n\n\n# split-lapply-rbind pattern\n\nx_split <- split(x, f = \"months\")\nx_list <- lapply(x_split, cummax)\nx_list_rbind <- do.call(rbind, x_list)\n\n# Split edhec into years\nedhec_years <- split(edhec , f = \"years\")\n\n# Use lapply to calculate the cumsum for each year\nedhec_ytd <- lapply(edhec_years, FUN = cumsum)\n\n# Use do.call to rbind the results\nedhec_xts <- do.call(rbind, edhec_ytd)\n\n\n\n\nCalculate the rolling standard deviation of a time series\n\n\nRolling windows can be continuous: use \nrollapply\n:\n\n\nrollapply(x, 10, FUN = max, na.rm = TRUE)\n\n# Use rollapply to calculate the rolling 3 period sd of EqMktNeutral\neq_monthly <- rollapply(EqMktNeutral, 3, FUN = sd)\n\n\n\n\n5, Extra features of xts\n\u00b6\n\n\nIndex, attributes, and time zones\n\n\nAll time is stored in seconds since 1970-01-01. Time via \n.index()\n () and \nindex()\n (raw seconds since 1970-01-01).\n\n\nindex()\n returns the time of an \nxts\n object, as a vector of class \nindexClass\n.\n\n\nSys.setenv()\n.\n\n\nClass attributes - \ntclass\n, \ntzone\n and \ntformat\n\n\nxts\n objects are somewhat tricky when it comes to times. Internally, we have now seen that the index attribute is really a vector of numeric values corresponding to the seconds since the UNIX epoch (1970-01-01).\n\n\nHow these values are displayed on printing and how they are returned to the user when using the \nindex()\n function is dependent on a few key internal attributes.\n\n\nThe information that controls this behavior can be viewed and even changed through a set of accessor functions detailed below:\n\n\n\n\nThe index class using indexClass (e.g. from Date to chron)\n\n\nThe time zone using indexTZ (e.g. from America/Chicago to Europe/London)\n\n\nThe time format to be displayed via indexFormat (e.g. YYYY/MM/DD)\n\n\n\n\n# Get the index class of temps\nindexClass(temps)\n\n# Get the timezone of temps\nindexTZ(temps)\n\n# Change the format of the time display\nindexFormat(temps) <- \"M/DD/YYYY\"\n\n# Extract the new index\ntime(temps)\n\n\n\n\nTime zones - (and why you should care!)\n\n\nR provides time zone support in native classes \nPOSIXct\n and \nPOSIXlt\n. xts extends this power to the entire object, allowing you to have multiple timezones across various objects. One very important thing to remember is that some internal operation system functions require a timezone to do date math, and if not set one is chosen for you! Be careful to always set a timezone in your environment to prevent very hard to debug errors when working with dates and times. \n\n\nxts\n provides the function \ntzone\n. This function allows to you to extract, or set timezones. \ntzone(x) <- \"Time_Zone\"\n In this exercise you will work with an object called times to practice constructing your own xts objects with custom time zones.\n\n\n# Construct times_xts with TZ set to America/Chicago\ntimes_xts <- xts(1:10, order.by = times, tzone = \"America/Chicago\")\n\n# Change the time zone to Asia/Hong_Kong\ntzone(times_xts) <- \"Asia/Hong_Kong\"\n\n# Extract the current timezone \nindexTZ(times_xts)\n\n\n\n\nPeriods, periodicity, and timestamps\n\n\nPeriods (yearly or intradays time index?): \nperiodicity()\n\n\nBroken downtime with \n.index*\n, \n.index()\n, \n.indexmday()\n, \nindexyday()\n, \nindexyear()\n.\n\n\nTimestamps.\n\n\nalign.time()\n round time stamps to another period.\n\n\nmake.index.unique()\n removes observations from duplicate time stamps.\n\n\nDetermining periodicity\n\n\nThe idea of periodicity is pretty simple; with what regularity does your data repeat? For stock market data, you might have hourly prices or maybe daily open-high-low-close bars. For macro economic series, it might be monthly or weekly survey numbers.\n\n\n# Calculate the periodicity of temps\nperiodicity(temps)\n\n# Calculate the periodicity of edhec\nperiodicity(edhec)\n\n# Convert edhec to yearly\nedhec_yearly <- to.yearly(edhec, 12)\n\n# Calculate the periodicity of edhec_yearly\nperiodicity(edhec_yearly)\n\n\n\n\nFind the number of periods in your data\n\n\nOften times it is handy to know not just the range of your time series index, but also have an idea of how many discrete irregular periods this covers. xts provides a set of functions to do just that. If you have a time series, it is now easy to see how many days, weeks or years your data contains.\n\n\nCount: \nnseconds()\n, \nnminutes()\n, \nnhours()\n, etc.\n\n\n`# Count the months\nnmonths(edhec)\n\n# Count the quarters\nnquarters(edhec)\n\n# Count the years\nnyears(edhec)  \n\n\n\n\nSecret index tools\n\n\nNormally you want to access the times you stored. \nindex()\n does this magically for you by using your \nindexClass\n. To get to the raw vector another function is provided, \n.index()\n. Note the critical dot before the function name.\n\n\nMore useful than extracting raw seconds is the ability to extract time components similar to the \nPOSIXlt\n class, which mirrors closely the underlying POSIX internal compiled structure \ntm\n. This is provided by a handful of functions such as \n.indexday()\n, \n.indexmon()\n, \n.indexyear()\n and more.\n\n\n# Explore underlying units of temps\n.index(temps) # in seconds\n.indexwday(temps) # (0 for Sun, 1, 2,..., 6 for Sat) day of the week\n\n# Create an index using which (Sunday has a value of 0, and Saturday has a value of 6)\nindex <- which(.indexwday(temps) == 0 | .indexwday(temps) == 6)\n\n# Select the index\ntemps[index]\n\n\n\n\nModifying timestamps\n\n\nDepending on your field you might encounter higher frequency data - think intraday trading intervals, or sensor data from medical equipment.\n\n\nIf you find that you have observations with identical timestamps, it might be useful to perturb or remove these times to allow for uniqueness.\n\n\nOn other ocassions you might find your timestamps a bit too precise. In these instances it might be better to round up to some fixed interval, for example an observation may occur at any point in an hour, but you want to record the latest as of beginning of the next hour.\n\n\nmake.index.unique(x, eps = 1e-4)  # Perturb\nmake.index.unique(x, drop = TRUE) # Drop duplicates\nalign.time(x, n = 60) # Round to the minute\n\n# Make z have unique timestamps\nmake.index.unique(z, eps = 1e-4)\n\n# Remove duplicate times in z\nmake.index.unique(z, drop = TRUE)\n\n# Round observations to the next time\nalign.time(z, n = 3600) # next hour",
            "title": "Time Series in R, The Power of xts and zoo"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#2-first-order-of-business-basic-manipulations",
            "text": "Converting xts objects  xts  provides methods to convert all of the major objects you are likely to come across - suitable native R types like matrix, data.frame, and ts are supported, as well as contributed ones such as  timeSeries ,  fts  and of course  zoo .  as.xts  is the workhorse function to do the conversions to  xts , and similar functions will provide the reverse behavior.  # using a R dataset\nhead(austres)  \n     13067.3 \n     13130.5 \n     13198.4 \n     13254.2 \n     13303.7 \n     13353.9   # Convert austres to an xts object called au\nau <- as.xts(austres)\nhead(au, 5)\n\n# Convert your xts object (au) into a matrix am\nam <- as.matrix(au)\nhead(am, 5)\n\n# Convert the original austres into a matrix am2\nam2 <- as.matrix(austres)\nhead(am2, 5)             [,1]\n1971 Q2 13067.3\n1971 Q3 13130.5\n1971 Q4 13198.4\n1972 Q1 13254.2\n1972 Q2 13303.7   \n     1971 Q2 13067.3 \n     1971 Q3 13130.5 \n     1971 Q4 13198.4 \n     1972 Q1 13254.2 \n     1972 Q2 13303.7     \n     13067.3 \n     13130.5 \n     13198.4 \n     13254.2 \n     13303.7    Importing data  Read raw data from files on disk or the web and convert data to  xts .  Read the same data into a  zoo  object using read.zoo and convert the  zoo  into an  xts  object.  # Create dat by reading tmp_file\ndat <- as.matrix(read.csv('tmp_file.csv', sep = ',', header = TRUE))\ndat   a b  \n     1/02/2015 1 3 \n     2/03/2015 2 4    # Convert dat into xts\nxts(dat, order.by = as.Date(rownames(dat), \"%m%d%Y\"))       a b\n<NA> 1 3\n<NA> 2 4  # Read tmp_file using read.zoo and as.xts\ndat_xts <- as.xts(read.zoo('tmp_file.csv', index.column = 0, sep = \",\", format = \"%m/%d/%Y\"))\ndat_xts             a b\n2015-01-02 1 3\n2015-02-03 2 4  Exporting xts objects  head(sunspots, 5)\n\n# Convert sunspots to xts\nsunspots_xts <- as.xts(sunspots)  \n     58 \n     62.6 \n     70 \n     55.7 \n     85   # Get the temporary file name\ntmp <- tempfile()\n\n# Write the xts object using zoo to tmp \nwrite.zoo(sunspots, sep = \",\", file = tmp)\n\ntmp  \u2018C:\\Users\\Dell\\AppData\\Local\\Temp\\RtmpEdYcAQ\\file1a706c8c732b\u2019  # Read the tmp file\n# FUN=as.yearmon will convert strings such as Jan 1749 into a proper time class\nsun <- read.zoo(tmp, sep = \",\", FUN = as.yearmon)\n\n# Convert sun into xts\nsun_xts <- as.xts(sun)\nhead(sun_xts, 5)  Error in eval(expr, envir, enclos): impossible de trouver la fonction \"read.zoo\"\nTraceback:  The ISO8601 Standard  The ISO8601 standard is the internationally recognized and accepted way to represent dates and times. From the biggest to the smallest, left to right, YYYY-MM-DDTHH:MM:SS.  The standard allows for a common format to not only describe dates, but also a way to represent ranges and repeating intervals:   2004 or 2001/2015  201402/03  2014-02-22 08:30:00  T08:00/T09:00   Querying for dates  Date ranges can be extracted from xts objects by simply specifying the period(s) you want using special character strings in your subset.  Code:  A[\"201601\"]         ## Jan 2016\nA[\"20160125\"]       ## Jan 25, 2016\nA[\"201203/201212\"]  ## Feb to Dec 2012\n\n# Select all of 2016 from x\nx_2016 <- x[\"2016\"]\n\n# Select January 2016 to March 22\njan_march <- x[\"201601/20160322\"]  Extracting recurring intraday intervals  Most common data \u201cin the wild\u201d is daily. On ocassion you may find yourself working with intraday data, that is date plus times.  You can slice days easily by using special notation in the  i =  argument to the single bracket extraction (i.e.  [i,j] ).  Code:  # Intraday times for all days\nNYSE[\"T09:30/T16:00\"]\n\n# Extract all data between 8AM and 10AM\nmorn_2010 <- irreg[\"T8:00/T10:00\"]\n\n# Extract the observations for January 13th\nmorn_2010[\"2010-01-13\"]  Alternating extraction techniques  Often times you may need to subset an existing time series with a set of Dates, or time-based objects. These might be from  as.Date() , or  as.POSIXct() , or a variety of other classes.  Code:  # Subset x using the vector dates\nx[dates]\n\n# Subset x using dates as POSIXct\nx[as.POSIXct(dates)]  Extracting recurring intraday intervals  The most common time series data is daily, intraday data, which contains both dates and times.   # Extract all data between 8AM and 10AM\nmorn_2010 <- irreg[\"T8:00/T10:00\"]\n\n# Extract the observations for January 13th\nmorn_2010[\"2010-01-13\"]  Row selection with time objects  Subset an existing time series with a set of Dates, or time-based objects.  # Subset x using the vector dates\nx[dates]\n\n# Subset x using dates as POSIXct\nx[as.POSIXct(dates)]  Update and replace elements  Replace known intervals or observations with an NA, say due to a malfunctioning sensor on a particular day or a set of outliers given a holiday.  Code:  # Replace all values in x on dates with NA\nx[dates] <- NA\n\n# Replace dates from 2016-06-09 and on with 0\nx[\"20160609/\"] <- 0  Find the  first  or  last  period of time  Sometimes you need to locate data by relative time. Instead of using an absolute offset, you describe a position relative in time. A simple example would be something like the  last 3 weeks  of a series, or the  first day of current month .  Code:  # Create lastweek with using the last 1 week of temps\nlastweek <- last(temps, \"1 week\")\n\n# Print the last 2 observations in lastweek\nlast(lastweek, 2)\n\n# Extract all but the last two days of lastweek\nlast(lastweek, \"-2 day\")  Combining  first  and  last  Code:  # Last 3 days of first week\nlast(first(Temps, '1 week'),'3 days') \n\n# Extract the first three days of the second week of temps\nfirst(last(first(temps, \"2 week\"), \"1 week\"), \"3 day\")  Matrix arithmetic - add, subtract, multiply and divide in time!  When you perform any binary operation using two xts objects, these objects are first aligned using the intersection of the indexes to preserve the point-in-time aspect of your data, assuring that you don\u2019t introduce accidental look ahead (or look behind!) bias into your calculations.  Your options include:   coredata() or as.numeric() (drop one to a matrix or vector).  Manually shift index values - i.e. use lag().  Reindex your data (before or after the calculation).   xts  respects time and will only return the intersection of times when doing various mathematical operations. First option:  # Add a and b\na + b\n\n# Add a with the numeric value of b\na + as.numeric(b)  Math with non-overlapping indexes  This third way involves modifying the two series you want by aligning assuring you have some union of dates - the dates you require in your final output. This makes it possibly to preserve dimensionality of the data:  merge(b, index(a))\n\n# Add a to b, and fill all missing rows of b with 0\na + merge(b, index(a), fill = 0)\n\n# Add a to b and fill NAs with the last observation\na + merge(b, index(a), fill = na.locf)",
            "title": "2, First Order of Business - Basic Manipulations"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#3-merging-and-modifying-time-series",
            "text": "Combining  xts  by column with  merge  xts  makes it easy to join data by column and row using a few different functions.    xts  objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types.  One of the most important functions to accomplish this is  merge . It works like a  cbind  or and SQL  join :   inner join (and)  outer join (or)  left join  right join   # Basic argument use\nmerge(a, b, join = \"right\", fill = 9999)\n\n# Perform an inner join of a and b\nmerge(a, b, join = \"inner\")\n\n# Perform of a left-join of a and b, fill mising values with 0\nmerge(a, b, join = \"left\", fill = 0)\n\n# fill = na.locf, fill = NA  Combining  xts  by row with  rbind  Easy to add new rows to your data.  # Row bind temps_june30 to temps, assign this to temps2\ntemps2 <- rbind(temps, temps_june30)\n\n# Row bind temps_july17 and temps_july18 to temps2, call this temps3\ntemps3 <- rbind(temps2, temps_july17, temps_july18)  Fill missing values using last or previous observation  The  na.locf  function takes the last-observation-carried-forward approach.  The  xts  package leverages the power of  zoo  for help with this.  zoo  provides a variety of missing data handling functions which are usable by  xts , and very handy.  # Last obs. carried forward\nna.locf(x)                \n\n# Next obs. carried backward\nna.locf(x, fromLast = TRUE)\n\n# Fill missing values using the last observation\nna.locf(temps, na.rm = TRUE,\n                fromLast = TRUE)\n\n# Fill missing values using the next observation\nna.locf(temps, na.rm = TRUE)\n\n# maxgap = Inf\n# rule = 2,  NA interpolation  From  zoo .  On occassion, a simple carry forward approach to missingness isn\u2019t appropriate. It may be that a series is missing an observation due to a higher frequency sampling than the generating process. You might also encounter an observation that is in error, yet expected to be somewhere between the values of its neighboring observations.  Both of these cases are examples of where interpolation is useful.  # Interpolate NAs using linear approximation\nna.approx(AirPass)  Combine a leading and lagging time series  Another common modification for time series is the ability to lag a series.  # Your final object\nlag(x, k = 1, na.pad = TRUE)\n\n# k = -1  leading\n# k = 1   lagging\n# zoo uses the other way around\n\n# k = c(0, 1, 5)  multiple leads or lags\n\n# Create a leading object called lead_x\nlead_x <- lag(x, k = -1)\n\n# Create a lagging object called lag_x\nlag_x <- lag(x, k = 1)\n\n# Merge your three series together and assign to z\nz <- merge(lead_x, x, lag_x)  Calculate a difference of a series with  diff  Another common operation on time series, typically on those that are non-stationary, is to take a difference of the series.  differences  is the order of the difference (how many times  diff  is called). A simple way to view a single (or  first order ) difference is to see it as  x(t) - x(t-k)  where  k  is the number of lags to go back. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference).  diff(x, lag = 1, difference = 1, log = FALSE, na.pad = TRUE)\n\n# calculate the first difference of AirPass using lag and subtraction\nAirPass - lag(AirPass, k = 1)\n\n# calculate the first order 12-month difference if AirPass\ndiff(AirPass, lag = 12, differences = 1)  What is the key difference in lag between xts and zoo? The  k  argument in  zoo  uses positive values for shifting past observations forward.",
            "title": "3, Merging and modifying time series"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#4-apply-and-aggregate-by-time",
            "text": "Find intervals by time in  xts  The main function in xts to facilitate this is endpoints. It takes a time series (or a vector of times) and returns the locations of the last observations in each interval.  For example, the code below locates the last observation of each year for the  AirPass  dataset:  endpoints(AirPass, on = \"years\")\n\n# The argument on supports a variety of periods, including \"years\", \"quarters\", \"months\", as well as intraday intervals such as \"hours\", and \"minutes\"\n\n# on = \"weeks\", k = 2, the result would be the final day of every other week in your data\n\n# Locate the final day of every week\nendpoints(temps, on = \"weeks\")\n\n# Locate the final day of every two weeks\nendpoints(temps, on = \"weeks\", k = 2)  Apply a function by time period(s)  xts  provides  period.apply , which takes a time series, an  INDEX  of endpoints, and a function to  apply :  period.apply(x, INDEX, FUN, ...)\n\n# Calculate the weekly endpoints\nep <- endpoints(temps, on = \"weeks\")\n\n# Now calculate the weekly mean and display the results\nperiod.apply(temps[, \"Temp.Mean\"], INDEX = ep, FUN = mean)  Using  lapply  and  split  to apply functions on intervals  Often it is useful to physically split your data into disjoint chunks by time and perform some calculation on these periods.  # Split temps by week\ntemps_weekly <- split(temps, f = \"weeks\")\n\n# Create a list of weekly means\nlapply(X = temps_weekly, FUN = mean)  Selection by  endpoints  vs.  split-lapply-rbind  # use the proper combination of split, lapply and rbind.\n#T1 <- do.call(rbind, ___(___(Temps, \"weeks\"), function(w) last(w, n=\"___\")))\nT1 <- do.call(rbind, lapply(split(Temps, \"weeks\"), function(w) last(w, n= \"1 day\")))\n\n# now subset Temps using the results of 'endpoints'\nlast_day_of_weeks <- endpoints(Temps, on = \"weeks\")\nT2 <- Temps[last_day_of_weeks]  Convert univariate series to OHLC data  In financial series it is common to find Open-High-Low-Close data calculated over some repeating and regular interval.  Also known as range bars, aggregating a series based on some regular window can make analysis easier amongst series that have varying frequencies. For example, a weekly economic series and a daily stock series can be compared more easily if the daily is converted to weekly.  OHLC means Opening, High, Low, Closing values.  to.period(x,\n          period = \"months\", \n          k = 1, \n          indexAt, \n          name=NULL,\n          OHLC = TRUE,\n          ...)\n\n# Convert USDEUR to weekly\n# OHLC = FALSE by default\nUSDEUR_weekly <- to.period(USDEUR, period = \"weeks\")\nhead(USDEUR)\nhead(USDEUR_weekly)\n\n# Convert USDEUR_weekly to monthly\nUSDEUR_monthly <- to.period(USDEUR_weekly, period = \"months\")\n\n# Convert USDEUR_monthly to yearly univariate\nUSDEUR_yearly <- to.period(USDEUR_monthly, period = \"years\", OHLC = FALSE)  Convert a series to a lower frequency  Besides converting univariate time series to OHLC series, to.period() also lets you convert OHLC to lower regularized frequency - something like subsampling your data.  For example, when using the shortcut function  to.quarterly()  xts will convert your index to the  yearqtr  class to make periods more obvious.  # Convert EqMktNeutral to quarterly OHLC\nmkt_quarterly <- to.period(EqMktNeutral, period = \"quarters\")\n\n# Convert EqMktNeutral to quarterly using shortcut function\n# Change the base name of each OHLC column to EDHEC.Equity and change the index to \"firstof\"\nmkt_quarterly2 <- to.quarterly(EqMktNeutral, name = \"EDHEC.Equity\", indexAt = \"first\")  Calculate basic rolling value of series by month  Rolling windows can be discrete: use  lapply  with ( cumsum, cumprod, cummax, cummin ) and  split :  # split-lapply-rbind pattern\n\nx_split <- split(x, f = \"months\")\nx_list <- lapply(x_split, cummax)\nx_list_rbind <- do.call(rbind, x_list)\n\n# Split edhec into years\nedhec_years <- split(edhec , f = \"years\")\n\n# Use lapply to calculate the cumsum for each year\nedhec_ytd <- lapply(edhec_years, FUN = cumsum)\n\n# Use do.call to rbind the results\nedhec_xts <- do.call(rbind, edhec_ytd)  Calculate the rolling standard deviation of a time series  Rolling windows can be continuous: use  rollapply :  rollapply(x, 10, FUN = max, na.rm = TRUE)\n\n# Use rollapply to calculate the rolling 3 period sd of EqMktNeutral\neq_monthly <- rollapply(EqMktNeutral, 3, FUN = sd)",
            "title": "4, Apply and aggregate by time"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#5-extra-features-of-xts",
            "text": "Index, attributes, and time zones  All time is stored in seconds since 1970-01-01. Time via  .index()  () and  index()  (raw seconds since 1970-01-01).  index()  returns the time of an  xts  object, as a vector of class  indexClass .  Sys.setenv() .  Class attributes -  tclass ,  tzone  and  tformat  xts  objects are somewhat tricky when it comes to times. Internally, we have now seen that the index attribute is really a vector of numeric values corresponding to the seconds since the UNIX epoch (1970-01-01).  How these values are displayed on printing and how they are returned to the user when using the  index()  function is dependent on a few key internal attributes.  The information that controls this behavior can be viewed and even changed through a set of accessor functions detailed below:   The index class using indexClass (e.g. from Date to chron)  The time zone using indexTZ (e.g. from America/Chicago to Europe/London)  The time format to be displayed via indexFormat (e.g. YYYY/MM/DD)   # Get the index class of temps\nindexClass(temps)\n\n# Get the timezone of temps\nindexTZ(temps)\n\n# Change the format of the time display\nindexFormat(temps) <- \"M/DD/YYYY\"\n\n# Extract the new index\ntime(temps)  Time zones - (and why you should care!)  R provides time zone support in native classes  POSIXct  and  POSIXlt . xts extends this power to the entire object, allowing you to have multiple timezones across various objects. One very important thing to remember is that some internal operation system functions require a timezone to do date math, and if not set one is chosen for you! Be careful to always set a timezone in your environment to prevent very hard to debug errors when working with dates and times.   xts  provides the function  tzone . This function allows to you to extract, or set timezones.  tzone(x) <- \"Time_Zone\"  In this exercise you will work with an object called times to practice constructing your own xts objects with custom time zones.  # Construct times_xts with TZ set to America/Chicago\ntimes_xts <- xts(1:10, order.by = times, tzone = \"America/Chicago\")\n\n# Change the time zone to Asia/Hong_Kong\ntzone(times_xts) <- \"Asia/Hong_Kong\"\n\n# Extract the current timezone \nindexTZ(times_xts)  Periods, periodicity, and timestamps  Periods (yearly or intradays time index?):  periodicity()  Broken downtime with  .index* ,  .index() ,  .indexmday() ,  indexyday() ,  indexyear() .  Timestamps.  align.time()  round time stamps to another period.  make.index.unique()  removes observations from duplicate time stamps.  Determining periodicity  The idea of periodicity is pretty simple; with what regularity does your data repeat? For stock market data, you might have hourly prices or maybe daily open-high-low-close bars. For macro economic series, it might be monthly or weekly survey numbers.  # Calculate the periodicity of temps\nperiodicity(temps)\n\n# Calculate the periodicity of edhec\nperiodicity(edhec)\n\n# Convert edhec to yearly\nedhec_yearly <- to.yearly(edhec, 12)\n\n# Calculate the periodicity of edhec_yearly\nperiodicity(edhec_yearly)  Find the number of periods in your data  Often times it is handy to know not just the range of your time series index, but also have an idea of how many discrete irregular periods this covers. xts provides a set of functions to do just that. If you have a time series, it is now easy to see how many days, weeks or years your data contains.  Count:  nseconds() ,  nminutes() ,  nhours() , etc.  `# Count the months\nnmonths(edhec)\n\n# Count the quarters\nnquarters(edhec)\n\n# Count the years\nnyears(edhec)    Secret index tools  Normally you want to access the times you stored.  index()  does this magically for you by using your  indexClass . To get to the raw vector another function is provided,  .index() . Note the critical dot before the function name.  More useful than extracting raw seconds is the ability to extract time components similar to the  POSIXlt  class, which mirrors closely the underlying POSIX internal compiled structure  tm . This is provided by a handful of functions such as  .indexday() ,  .indexmon() ,  .indexyear()  and more.  # Explore underlying units of temps\n.index(temps) # in seconds\n.indexwday(temps) # (0 for Sun, 1, 2,..., 6 for Sat) day of the week\n\n# Create an index using which (Sunday has a value of 0, and Saturday has a value of 6)\nindex <- which(.indexwday(temps) == 0 | .indexwday(temps) == 6)\n\n# Select the index\ntemps[index]  Modifying timestamps  Depending on your field you might encounter higher frequency data - think intraday trading intervals, or sensor data from medical equipment.  If you find that you have observations with identical timestamps, it might be useful to perturb or remove these times to allow for uniqueness.  On other ocassions you might find your timestamps a bit too precise. In these instances it might be better to round up to some fixed interval, for example an observation may occur at any point in an hour, but you want to record the latest as of beginning of the next hour.  make.index.unique(x, eps = 1e-4)  # Perturb\nmake.index.unique(x, drop = TRUE) # Drop duplicates\nalign.time(x, n = 60) # Round to the minute\n\n# Make z have unique timestamps\nmake.index.unique(z, eps = 1e-4)\n\n# Remove duplicate times in z\nmake.index.unique(z, drop = TRUE)\n\n# Round observations to the next time\nalign.time(z, n = 3600) # next hour",
            "title": "5, Extra features of xts"
        }
    ]
}