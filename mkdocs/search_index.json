{
    "docs": [
        {
            "location": "/", 
            "text": "Let there be light!\n\n\nA R documentation website.\n\n\nLayout\n\n\n\n\nugo_r_doc\n is a corpus: a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc.\n\n\nA Mkdocs site is automatically indexed. The \ndocs\n is a searchable knowledge-based system. \n\n\nYou type a keyword, it leads to several sources, you identify the document the source belongs to, and you go retrieve the document; whether it is a digital or a material document. Fast and easy!\n\n\nFor that matter, \ngenerously\n adding keywords to the \ndocs\n is crucial (adding them in subscript makes them stand apart).\n\n\nCitations, keywords, links, etc.; they all provide leads.\n\n\nThe corpus is unstructured. There is no unique chapter dedicated to one topic.\n\n\nKnowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information. \n\n\nInformation may be repeted among many documents, with different explanations, or some more comprehensive.\n\n\nNewer entries might also supplement or contradict older entries.", 
            "title": "Home"
        }, 
        {
            "location": "/#layout", 
            "text": "ugo_r_doc  is a corpus: a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc.  A Mkdocs site is automatically indexed. The  docs  is a searchable knowledge-based system.   You type a keyword, it leads to several sources, you identify the document the source belongs to, and you go retrieve the document; whether it is a digital or a material document. Fast and easy!  For that matter,  generously  adding keywords to the  docs  is crucial (adding them in subscript makes them stand apart).  Citations, keywords, links, etc.; they all provide leads.  The corpus is unstructured. There is no unique chapter dedicated to one topic.  Knowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information.   Information may be repeted among many documents, with different explanations, or some more comprehensive.  Newer entries might also supplement or contradict older entries.", 
            "title": "Layout"
        }, 
        {
            "location": "/An Introduction to R/", 
            "text": "An Introduction to R\n\n\nForeword\n\n\nNotes, leads, and ideas on what R can do. More at:\n\n\n\n\nwww.statmethods.net (Quick-R, searchable R guide)\n\n\nR-intro\n\n\ncran.r-project.org/manuals (series of official manuals)\n\n\n\n\n\n\nCONTENT\n\n\nAn Introduction to R\n\n\n1, Introduction and Preliminaries\n\n\n2, Simple Manipulations; Numbers and Vectors\n\n\n3, Objects, their Modes and Attributes\n\n\n4, Ordered and Unordered Factors\n\n\n5, Arrays and Matrices\n\n\n6, Lists and Data Frames\n\n\n7, Reading/Writing Data from/to Files (Input/Output)\n\n\n8, Probability Distributions\n\n\n9, Grouping, Loops and Conditional Execution\n\n\n10, Writing you own Functions\n\n\n11, Statistical models in R\n\n\n12, Graphical Procedures\n\n\n13, Packages\n\n\n14, OS Facilities\n\n\n\n\n\n\n\n\n\n\n\n\n1, Introduction and Preliminaries\n\n\n\n\nRun R (CLI).\n\n\nQuit with \nq()\n.\n\n\n\n\n\n\nRun R in an IDE (GUI); like RStudio.\n\n\nCreate a working directory \nwork\n.\n\n\nAsk help.\n\n\nhelp(function)\n; open a web documentation in a browser or in the IDE.\n\n\n?function\n; idem.\n\n\n??function\n; idem, but showing concordances.\n\n\nhelp(\"[[\")\n; idem, searching with a string.\n\n\nhelp.start()\n; show the entire manual.\n\n\n\n\n\n\nsink()\n; divert output from the console to a connection; restore the output to the console.\n\n\nobjects()\n, \nls()\n; see the objects stored in a session.\n\n\nObjects are in a file with \n.RData\n extension.\n\n\n\n\n\n\nrm(x, y, ink, ...)\n; remove stored objects.\n\n\nAll commands entered or run are recorded in a file with \n.Rhistory\n extension.\n\n\n\n\n2, Simple Manipulations; Numbers and Vectors\n\n\nCreate a vector\n\n\nx \n- c(1, 2) # assignment (universal).\nx\n[1] 1 2\n\nc(2, 1) -\n y # alternative assignment.\ny\n[1] 2 1\n\nx = c(1, 2) # alternative assignment (some limitations).\nx \n- c(1, 2) # permanent assignment.\n\n# examples\nab \n- 9\nab\n[1] 9\n\nassign(\nab\n, 10)\nab \n[1] 10\n\n\n\n\nCreate a sequence (vector)\n\n\nx \n- 2 * 1:15\n\nx\n[1]  2  4  6  8 10 12 14 16 18 20 22 24 26 28 30\n\nn \n- 10\nx \n- 1:n-1\n\nx\n[1] 1 2 3 4 5 6 7 8 9\n\nx \n- 30:1\nx\n[1] 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n[29]  2  1\n\nseq(from = 1, to = 30)\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n[29] 29 30\n\nseq(1, 30)\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n[29] 29 30\n\nseq(30, 1)\n[1] 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n[29]  2  1\n\nseq(1, 30, by = 2)\n[1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29\n\n\n\n\nRepetition\n\n\nrep(2, times = 5)\n[1] 2 2 2 2 2\n\n\n\n\nx \n- c(\nx\n, \ny\n)[rep(c(1, 2, 2, 1), times = 4)]\n\nx\n[1] \nx\n \ny\n \ny\n \nx\n \nx\n \ny\n \ny\n \nx\n \nx\n \ny\n \ny\n \nx\n \nx\n \ny\n \ny\n \nx\n\n\n\n\n\nLength (vector)\n\n\n\n\nlength(vector)\n\n\n\n\nBoolean\n\n\n\n\nTRUE\n or \nT\n; \nT == 1\n.\n\n\nFALSE\n or \nF\n; \nF == 0\n.\n\n\n\n\nSome operators\n\n\n\n\n\n\n=\n\n\n\n\n=\n\n\n==\n\n\n!=\n\n\n\n\n\n\n\n\n|\n\n\n||\n\n\nand many more.\n\n\n\n\nMissing data and more\n\n\n\n\nNA; not available.\n\n\nNaN; not a number.\n\n\nInf - Inf == NaN == 0/0; infinite number.\n\n\n\n\nx \n- c(1:3, NA)\n\nx\n[1] 1 2 3 NA\n\n\n\n\n\n\nis.na(var)\n.\n\n\nvar == NA\n.\n\n\nis.na(x)\n.\n\n\n!is.na(x)\n.\n\n\n\n\nx \n- c(-1:3, NA)\ny \n- x[!is.na(x) \n x \n 0]\n\nx\n[1]  -1  0  1  2  3 NA\n\ny\n[1] 1 2 3\n\n\n\n\nExtract, subset (vector)\n\n\n\n\nx[i]\n; index.\n\n\n\n\nBackslash use for some characters\n\n\n\n\n\\\\\n; backslash.\n\n\n\\n\n; new line.\n\n\n\\t\n; tab.\n\n\n\\b\n; backspace.\n\n\n\n\nConcatenate, paste (vector)\n\n\nlabs \n- paste(c(\nX\n, \nY\n), 1:10, sep = \n)\n\nlabs\n[1] \nX1\n  \nY2\n  \nX3\n  \nY4\n  \nX5\n  \nY6\n  \nX7\n  \nY8\n  \nX9\n  \nY10\n\n\n\n\n\nA note of data handling and manipulations\n\n\nYou can also \nsplit()\n, \nmerge()\n, \nrbind()\n, \ncbind()\n vectors. \n\n\nIt is also possible with other objects such as factors, lists, arrays, matrices, and data frames.\n\n\nThere are built-in functions to extract, exclude, subset, replace, transform or convert (\n.as\n), concatenate, paste, group, and bind.\n\n\n\\sub\nslice, extract, exclude, subset, replace, convert,  concatenate, paste, group, bind,   \n\n\nExclude, remove (vector)\n\n\nz \n- 1:20\n\nz\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\nzz \n- z[-(1:5)]\n\nzz\n[1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\n\n\nReplace (vector)\n\n\nx \n- c(1, 1, 1, NA)\n\nx\n[1]  1  1  1 NA\n\nx[is.na(x)] \n- 0\n\nx\n[1] 1 1 1 0\n# replaces any missing values in x by zeros\n\n\n\n\nAbsolute value\n\n\n\n\ny \n- abs(y)\n.\n\n\n\n\nConvert (\n.as\n)\n\n\n\n\nas.vector()\n.\n\n\nas.integer()\n.\n\n\nas.numeric()\n\n\nas.factor()\n.\n\n\nas.character()\n.\n\n\nand many more.\n\n\n\n\n3, Objects, their Modes and Attributes\n\n\nObject type\n\n\nobj \n- 1\n\nobj1 \n- numeric(obj)\nmode(obj1)\n[1] \nnumeric\n\nobj2 \n- character(obj)\nmode(obj2)\n[1] \ncharacter\n\n\nx \n- 1\n\n# class() != mode(), but almost\nmode(x)\n[1] \nnumeric\n\n\nx \n- factor(x)\nx\n[1] 1\nLevels: 1\n\nx \n- numeric(x)\nx\n[1] 0\n\n# load key:value\nobj[3] \n- 17\n\nobj\n[1]  1 NA 17\n\n\n\n\nClasses\n\n\nclass\n\\sub\n\n\n\n\nnumeric\n.\n\n\nlogical\n.\n\n\ncharacter\n.\n\n\nlist\n.\n\n\nmatrix\n.\n\n\narray\n.\n\n\nfactor\n.\n\n\ndata.frame\n.\n\n\n\n\nclass(obj)\n[1] \nnumeric\n\n\n# print the object as ordinary\nunclass(obj)\n[1]  1 NA 17\n\n\n\n\n4, Ordered and Unordered Factors\n\n\nstate \n- c(\ntas\n,\nqld\n,\nsa\n,\nsa\n,\nsa\n,\nvic\n,\nnt\n,\nact\n,\nqld\n,\nnsw\n,\nwa\n,\nnsw\n,\nnsw\n,\nvic\n,\nvic\n,\nvic\n,\nnsw\n,\nqld\n,\nqld\n,\nvic\n,\nnt\n,\nwa\n,\nwa\n,\nqld\n,\nsa\n, \ntas\n,\nnsw\n, \nnsw\n, \nwa\n,\nact\n)\n\nstatef \n- factor(state)\n\n# class != mode()\nclass(statef)\n[1] \nfactor\n\nmode(statef)\n[1] \nnumeric\n\n\nlevels(statef)\n[1] \nact\n \nnsw\n \nnt\n  \nqld\n \nsa\n  \ntas\n \nvic\n \nwa\n \n\nincomes \n- c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\n\nincmeans \n- tapply(incomes, statef, mean)\n\nincmeans\n  act   nsw    nt   qld    sa   tas   vic    wa \n48.50 55.00 54.00 51.60 54.25 53.00 61.60 54.50 \n\nstderr \n- function(x) { sqrt(var(x) / length(x)) }\n\nstderr(incomes)\n[1] 1.524462\n\n# alternatively\nincster \n- tapply(incomes, statef, stderr)\n\nincster\n      act       nsw        nt       qld        sa       tas       vic        wa \n5.5000000 3.9665266 5.0000000 2.6570661 5.3909647 7.0000000 0.8717798 6.2249498  \n\n\n\n\n# a vector of characters\nstateff \n- c(\na\n, \nb\n, \nc\n)\n[1] \na\n \nb\n \nc\n\n\nas.factor(stateff)\n[1] a b c\nLevels: a b c\n\n\n\n\n# Create a factor with the wrong order of levels\nsizes \n- factor(c(\nsmall\n, \nlarge\n, \nlarge\n, \nsmall\n, \nmedium\n))\n\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# levels can be specified explicitly\nsizes \n- factor(sizes, levels = c(\nsmall\n, \nmedium\n, \nlarge\n))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# do the same with an ordered factor\nsizes \n- ordered(c(\nsmall\n, \nlarge\n, \nlarge\n, \nsmall\n, \nmedium\n))\n\nsizes \n- ordered(sizes, levels = c(\nsmall\n, \nmedium\n, \nlarge\n))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small \n medium \n large\n\n# use relevel() to make a particular level first in the list. (This will not work for ordered factors.)\n\n# Create a factor with the wrong order\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# make medium first\nsizes \n- relevel(sizes, \nmedium\n)\n\nsizes\n[1] small  large  large  small  medium\nLevels: medium large small\n\n# make small first\nsizes \n- relevel(sizes, \nsmall\n)\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# specify the proper order when the factor is created\nsizes \n- factor(c(\nsmall\n, \nlarge\n, \nlarge\n, \nsmall\n, \nmedium\n),\n                  levels = c(\nsmall\n, \nmedium\n, \nlarge\n))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# Create a factor with the wrong order of levels\nsizes \n- factor(c(\nsmall\n, \nlarge\n, \nlarge\n, \nsmall\n, \nmedium\n))\n\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# reverse the order of levels in a factor\nsizes \n- factor(sizes, levels=rev(levels(sizes)))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n\n\n\nConvert (\n.as\n)\n\n\n\n\nas.factor()\n.\n\n\n\n\n5, Arrays and Matrices\n\n\nDimension\n\n\nz \n- 1:1500\ndim(z) \n- c(3, 5, 100)\n\n# gives 100 arrays of 3 lines by 5 columns\n\n\n\n\nCreate a matrix, an array\n\n\nx \n- array(1:20, dim=c(4, 5))\n\nx\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20\n\nx \n- array(1:20, dim=c(2, 5, 2))\n\nx\n, , 1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n, , 2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   11   13   15   17   19\n[2,]   12   14   16   18   20\n\n\ni \n- array(c(1:3, 3:1), dim = c(3, 2))\n\ni\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    2\n[3,]    3    1\n\nj \n- array(c(1:8), dim = c(2, 2, 2))\n\nj\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nk \n- array(1:27, c(3, 3, 3))\n\n\n k\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   19   22   25\n[2,]   20   23   26\n[3,]   21   24   27\n\n\na \n- matrix(1, 2, 2)\n\na\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n\nb \n- matrix(2, 2, 2)\n\nb\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n\n\n\nExtract, subset (matrix)\n\n\n\n\na[2, 1]\n; rows, columns.\n\n\n\n\nExtract, subset (array)\n\n\n\n\na[2, 1, 1]\n; rows, columns, matrix.\n\n\n\n\nCross product vs multiplication\n\n\na \n- 1:5\nb \n- seq(10, 6, -1)\n\na\n[1] 1 2 3 4 5\nb\n[1] 10  9  8  7  6\n\na * b\n[1] 10 18 24 28 30\n\ncrossprod(a, b)\n     [,1]\n[1,]  110\n\nab \n- a %o% b\nab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   10    9    8    7    6\n[2,]   20   18   16   14   12\n[3,]   30   27   24   21   18\n[4,]   40   36   32   28   24\n[5,]   50   45   40   35   30\n\n\n\n\nMatrix operation\n\n\nA et B are 2x2 matrices:\n\n\n\n\nA * B\n; scalar multiplication.\n\n\nA %*% B\n; matrix multiplication\n\n\nx %*% A %*% y\n; matrix multiplication.\n\n\ncrossproduct(A, B)\n; cross multiplication.\n\n\nab \n- outer(A,B,\"*\")\n; \na * b\n.\n\n\nab \n- outer(A,B,\"+\")\n; \na + b\n.\n\n\nab \n- outer(A,B,\"-\")\n; \na - b\n.\n\n\nand many more.\n\n\n\n\nDiagonal and triangle\n\n\nab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   11   10    9    8    7\n[2,]   12   11   10    9    8\n[3,]   13   12   11   10    9\n[4,]   14   13   12   11   10\n[5,]   15   14   13   12   11\n\ndiag(ab)\n[1] 11 11 11 11 11\n\nlower.tri(ab)\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE FALSE FALSE FALSE FALSE\n[2,]  TRUE FALSE FALSE FALSE FALSE\n[3,]  TRUE  TRUE FALSE FALSE FALSE\n[4,]  TRUE  TRUE  TRUE FALSE FALSE\n[5,]  TRUE  TRUE  TRUE  TRUE FALSE\n\nlower.tri(ab) * ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]   12    0    0    0    0\n[3,]   13   12    0    0    0\n[4,]   14   13   12    0    0\n[5,]   15   14   13   12    0\n\nupper.tri(ab)\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE  TRUE  TRUE  TRUE  TRUE\n[2,] FALSE FALSE  TRUE  TRUE  TRUE\n[3,] FALSE FALSE FALSE  TRUE  TRUE\n[4,] FALSE FALSE FALSE FALSE  TRUE\n[5,] FALSE FALSE FALSE FALSE FALSE\n\nupper.tri(ab) * ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0   10    9    8    7\n[2,]    0    0   10    9    8\n[3,]    0    0    0   10    9\n[4,]    0    0    0    0   10\n[5,]    0    0    0    0    0\n\n\n\n\nSolving a matrix system\n\n\nb \n- A %*% x\n\n# or\nsolve(A, b)\n\n\n\n\n\n\nsolve(A)\n; inverse the matrix.\n\n\n\n\nSymmetrical matrix and eigen value\n\n\nb \n- matrix(2, 2, 2)\n\nb\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\ne \n- eigen(b, only.values = TRUE)\ne\n$values\n[1] 4 0\n\n$vectors\nNULL\n\n\n\n\nSingular value decomposition (matrix)\n\n\nsvd(b)\n$d\n[1] 4 0\n\n$u\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068\n\n$v\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068\n\n\n\n\nDeterminant (matrix)\n\n\ndet(ab)\n[1] 0\n\n\n\n\nLeast square fitting (matrix)\n\n\n\n\nlsfit()\n or least squares estimate of \nb\n in the model: \ny = X b + e\n.\n\n\n\n\nQR decomposition (matrix)\n\n\nqr(ab)\n$qr\n            [,1]        [,2]          [,3]          [,4]          [,5]\n[1,] -29.2403830 -27.0174299 -2.479448e+01 -2.257152e+01 -2.034857e+01\n[2,]   0.4103913   0.2418254  4.836508e-01  7.254763e-01  9.673017e-01\n[3,]   0.4445906  -0.1703815  3.717512e-15  1.134003e-14  1.749210e-14\n[4,]   0.4787899  -0.5015812  3.957070e-01  1.553722e-15  1.295516e-15\n[5,]   0.5129892  -0.8327809  5.076995e-01  6.936403e-01 -1.685698e-16\n\n$rank\n[1] 2\n\n$qraux\n[1] 1.376192e+00 1.160818e+00 1.765282e+00 1.720322e+00 1.685698e-16\n\n$pivot\n[1] 1 2 3 4 5\n\nattr(,\nclass\n)\n[1] \nqr\n\n\n\n\n\nAlso:\n\n\n\n\nqr.coef()\n.\n\n\nqr.fitted()\n.\n\n\nqr.resid()\n.\n\n\n\n\nConvert (\n.as\n)\n\n\n\n\nas.array()\n.\n\n\nas.matrix()\n.\n\n\n\n\n6, Lists and Data Frames\n\n\nCrate a list\n\n\nLst \n- list(name = \nFred\n, wife = \nMary\n, no.children = 3, child.ages = c(4,7,9))\n\nLst\n$name\n[1] \nFred\n\n\n$wife\n[1] \nMary\n\n\n$no.children\n[1] 3\n\n$child.ages\n[1] 4 7 9\n\n\n\n\nExtract, subset (list)\n\n\nLst$name\n[1] \nFred\n\n\nLst[[\nname\n]] == Lst[[1]]\n[1] TRUE\n\n\nLst[5] \n- list(\n) # or list()  \nLst[\nnew\n] \n- list()  # key, value\n\nLst[1]\n$name\n[1] \nFred\n\n\nLst$child.ages[1]\n[1] 4\nLst[[4]][1]\n[1] 4\n\n\n\n\nConcatenate, paste\n\n\nx \n- c(1,2)\ny \n- c(3,4)\n\nc(x, y)\n[1] 1 2 3 4\n\npaste(x, y)\n[1] \n1 3\n \n2 4\n\n\ndata.frame(x, y)\n  x y\n1 1 3\n2 2 4\n\na \n- list(1, 2)\nb \n- list(\na\n, \nb\n)\n\nlist(a, b)\n[[1]]\n[[1]][[1]]\n[1] 1\n\n[[1]][[2]]\n[1] 2\n\n\n[[2]]\n[[2]][[1]]\n[1] \na\n\n\n[[2]][[2]]\n[1] \nb\n\n\nh \n- matrix(2, 2, 2)\ng \n- matrix(1, 2, 2)\n\nc(h, g)\n[1] 2 2 2 2 1 1 1 1\n\npaste(h, g)\n[1] \n2 1\n \n2 1\n \n2 1\n \n2 1\n\n\nlist(h, g)\n[[1]]\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n[[2]]\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n\n\n\n\nConvert (\nas.\n)\n\n\n\n\nas.matrix()\n\n\n\n\nData frame\n\n\n\n\nA data frame can hold other data frames.\n\n\nA list can hold other lists.\n\n\nA vector can hold other vectors.\n\n\n\n\n\n\nEach variable can be numeric, character, logical, factor, numeric matrix, list, data.frame.\n\n\n\n\nstate \n- c(\ntas\n,\nqld\n,\nsa\n,\nsa\n,\nsa\n,\nvic\n,\nnt\n,\nact\n,\nqld\n,\nnsw\n,\nwa\n,\nnsw\n,\nnsw\n,\nvic\n,\nvic\n,\nvic\n,\nnsw\n,\nqld\n,\nqld\n,\nvic\n,\nnt\n,\nwa\n,\nwa\n,\nqld\n,\nsa\n, \ntas\n,\nnsw\n, \nnsw\n, \nwa\n,\nact\n)\n\nstatef \n- factor(state)\n\nincomes \n- c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\n\nincomef \n- factor(incomes)\n\naccountants \n- data.frame(home=statef, loot=incomes, shot=incomef)\n\nhead(accountants, 10)\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n\n# class() != mode()\nclass(accountants)\n[1] \ndata.frame\n\n\nmode(accountants)\n[1] \nlist\n\n\n\n\n\nConcatenate, paste (data frame)\n\n\n# accountants == acc\n\nc(accountants, acc)\n$home\n [1] tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n[14] vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n[27] nsw nsw wa  act\nLevels: act nsw nt qld sa tas vic wa\n\n$loot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n\n$shot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n20 Levels: 40 41 42 43 46 48 49 51 52 54 56 58 59 ... 70\n\n$home\n [1] tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n[14] vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n[27] nsw nsw wa  act\nLevels: act nsw nt qld sa tas vic wa\n\n$loot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n\n$shot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n20 Levels: 40 41 42 43 46 48 49 51 52 54 56 58 59 ... 70\n\n\npaste(accountants, acc)\n[1] \nc(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1) c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1)\n                                                            \n[2] \nc(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43) c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\n\n[3] \nc(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4) c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4)\n                          \n\nlist(accountants, acc)\n[[1]]\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n...\n\n[[2]]\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n...\n\naccountants + acc\n   home loot shot\n1    NA  120   NA\n2    NA   98   NA\n3    NA   80   NA\n4    NA  122   NA\n5    NA  128   NA\n6    NA  120   NA\n7    NA  118   NA\n8    NA  108   NA\n9    NA  124   NA\n10   NA  138   NA\n...\n\n\n\n\nConvert (\nas.\n)\n\n\n\n\nas.data.frame()\n\n\n\n\nLoad data into R\n\n\n\n\nread.table()\n; produce a data frame with inputs.\n\n\n\n\nA note on environment objects\n\n\n\n\nsearch()\n; objects in .GlobalEnv; including packages.\n\n\nls()\n; list these objects.\n\n\n\n\n\n\nattach()\n; attach an object to .GlobalEnv.\n\n\ndetach()\n.\n\n\n\n\n7, Reading/Writing Data from/to Files (Input/Output)\n\n\nA lot more can be found on I/O: depending on the file type, the data format, the desired R object, many commands are available. Depending on a file format, look for the dedicated package on CRAN.\n\n\nread, reading, write, writing, input, output, i/o\n\\sub\n\n\nExamples\n\n\n# read from a file\nHousePrice \n- read.table(\nhouse.data\n, header = TRUE, sep = \n;\n)\n\n# read a 3-variable list\ninput \n- scan(\ninput.dat\n, list(\n, 0, 0))  \n\n# read a3-variable list\ninput \n- scan(\ninput.dat\n, list(id=\n, x = 0, y = 0))\n\n# make 3 separate vectors\nlabel \n- input[[1]]\nx \n- input[[2]]\ny \n- input[[3]]\n\n# label them\nlabel \n- inp$id\nx \n- inp$x\ny \n- inp$y\n\n# read a 5-variable list and transform it into a matrix\nX \n- matrix(scan(\nlight.dat\n, 0), ncol = 5, byrow = TRUE)\n\n\n\n\nInputing from file types\n\n\n\n\n.txt.\n\n\n.csv.\n\n\n.tsv.\n\n\n.hdf5.\n\n\n.bmp.\n\n\n.jpeg.\n\n\n.png.\n\n\n.tiff.\n\n\n.zip.\n\n\n.xls, spreadsheet.\n\n\ndatabases.\n\n\nstatistical programs.\n\n\nbinary files.\n\n\nand many more (some are up and coming such as julia files).\n\n\n\n\nCleaning parameter\n\n\n\n\nstrip.white = TRUE\n; remove unnecessary spaces.\n\n\n\n\nChange the data frame or matrix format\n\n\n\n\nstack()\n; for example, take a 6-column, 4-variable data frame and stack the 4 variables into one long column.\n\n\nunstack()\n; vice-versa.\n\n\n\n\n# 6 columns or variables\nStatus Age  V1    V2    V3    V4\n P 23646 45190 50333 55166 56271\nCC 26174 35535 38227 37911 41184\nCC 27723 25691 25712 26144 26398\nCC 27193 30949 29693 29754 30772\nCC 24370 50542 51966 54341 54273\nCC 28359 58591 58803 59435 61292\nCC 25136 45801 45389 47197 47126\n...\n\n# 4 columns, more rows\nStatus Age values ind\nX1  P 23646 45190 V1\nX2 CC 26174 35535 V1\nX3 CC 27723 25691 V1\nX4 CC 27193 30949 V1\nX5 CC 24370 50542 V1\nX6 CC 28359 58591 V1\nX7 CC 25136 45801 V1\nX11 P 23646 50333 V2\n...\n\n\n\n\n\n\nreshape()\n; stack and unstack variables, according to parameters.\n\n\n\n\nreshape(zz, idvar = \nid\n, timevar = \nvar\n, varying = list(c(\nV1\n, \nV2\n, \nV3\n, \nV4\n)), direction = \nlong\n)\n\n\n\n\n\n\nftable()\n; flatten multidimensional matrices (arrays);\n\n\nread.ftable()\n.\n\n\ndata()\n; list of datasets.\n\n\ndata(package = \"rpart\")\n; load package.\n\n\nedit(input)\n; open a mini-spreadsheet.\n\n\nxnew \n- edit(input)\n; open and save it as a new dataset.\n\n\nand many more.\n\n\n\n\nEncoding\n\n\n\n\nutf-8 for Linux and OS X.\n\n\nUCS-2LE and UTF-16 for Windows.\n\n\n\n\n# examples\nreadLines(\nfilename.txt\n, encoding = \nUTF-8\n)\nreadLines(\nfilename.txt\n, encoding = \nUCS2LE\n)\nread.delim(\nclipboard\n, fileEncoding=\nUTF-16\n)\n\n\n\n\nOutput\n\n\n\n\nwrite.table()\n; many parameters.\n\n\nwrite.matrix()\n.\n\n\nwrite.foreign()\n.\n\n\n\n\nCheck a file\n\n\n\n\nreadLines(\"aab.txt\")\n.\n\n\nreadLines(\"aab.txt\", 1)\n.\n\n\nunlink(\"aab.txt\")\n; delete the file on the working directory.\n\n\n\n\nDirectory management\n\n\n\n\ngetwd()\n; get the current working directory.\n\n\nsetwd()\n; set the current working directory.\n\n\nfile.show(filepath)\n.\n\n\nsystem.file()\n\n\n\n\nSpreadsheet editor and edition in R\n\n\n\n\nfix(c)\n; edit object c; a vetor, list, data frame, etc. in a mini-spreadsheet.\n\n\ndata.entry(c, mode=NULL, Names=NULL), dataentry(c), de(c, Modes=list(), Names=NULL)\n; idem.\n\n\nvi(file= )\n; invoke a text editor.\n\n\nemacs(file= )\n; idem.\n\n\npico(file= )\n; idem.\n\n\nxemacs(file= )\n; idem.\n\n\nxedit(file=)\n; idem.\n\n\n\n\n8, Probability Distributions\n\n\nDistribution\n\n\n\n\nbeta\n.\n\n\nbinom\n.\n\n\ncauchy\n.\n\n\nchisq\n.\n\n\nexp\n.\n\n\nf\n.\n\n\ngamma\n.\n\n\ngeom\n.\n\n\nhyper\n.\n\n\nlnorm\n.\n\n\nlogis\n.\n\n\nnbinom\n.\n\n\nnorm\n.\n\n\npois\n.\n\n\nsignrank\n.\n\n\nt\n.\n\n\nunif\n.\n\n\nweibull\n.\n\n\nwilcox\n.\n\n\nand many more.\n\n\n\n\nPrefixes\n\n\n\n\nd\n; density.\n\n\np\n; CDF or cumulative density function.\n\n\nq\n; quantile.\n\n\nr\n; random deviates or simulation or number generation.\n\n\n\n\nDistribution operation\n\n\nCommands, prefix + distribution, examples:\n\n\n\n\ndbeta(x= )\n.\n\n\npbinom(q= , lower.tail= , log.p= )\n.\n\n\nqcauchy(p= , lower.tail= , log.p= )\n.\n\n\nrchisq(n or r= )\n.\n\n\nptukey()\n; studentized (t) distribution\n\n\nqtukey()\n.\n\n\ndmultinom()\n.\n\n\nltinomial\n.\n\n\nrmultinom()\n.\n\n\netc.\n\n\n\n\nDescriptive statistics\n\n\n# overview\nsummary(cars)\nstr(cars)\nboxplot(cars$speed)\nboxplot(cars$dist)\n\n# Tukey five-number summaries\nfivenum(cars$speed)\n\n# histograms and bar charts\nstem(cars$speed)\nhist(cars$speed)\nbarplot(cars$speed)\ndotchart(cars$speed)\n\n# scatter plot\nplot(cars$speed, cars$dist)\nlines(cars$speed, cars$dist)\n\n# add a (1-d) representation\nplot(cars$speed, cars$dist)\nrug(cars$speed, ticksize = 0.03)\nplot(cars$speed, cars$dist)\nrug(cars$dist, ticksize = 0.03)\n\n# normality of the residuals, linearity\nqqnorm(cars$speed)\nqqline(cars$speed)\n\nqqnorm(cars$dist)\nqqline(cars$dist)\n\nqqplot(cars$speed, cars$dist)\n\n# normality test\nshapiro.test(cars$speed)\n\n# Kolmogorov-Smirnov test on a sample\nks.test(cars$speed[10], \npnorm\n, mean = mean(cars$speed), sd = sqrt(var(cars$speed)))\nks.test(cars$speed[40], \npnorm\n, mean = mean(cars$speed), sd = sqrt(var(cars$speed)))\n\n\n\n\nTest\n\n\n\n\nt.test(A,B)\n; true difference of means is not 0, difference means.\n\n\nvar.test(A,B)\n; true ratio of variances is not 1, difference variances.\n\n\nt.test(A,B, var.equal=TRUE)\n; true difference of means is not 0.\n\n\nwilcox.test(A,B)\n; rank sum with continuity correction, continuous distribution.\n\n\nand many more.\n\n\n\n\nNormality\n\n\n\n\nplot(ecdf(A), do.points=FALSE, verticals=TRUE, xlim=range(A, B))\n\n\nplot(ecdf(B), do.points=FALSE, verticals=TRUE, add=TRUE)\n\n\nks.test(A,B); maximal vertical distance between the two ecdf\n\n\n\n\n9, Grouping, Loops and Conditional Execution\n\n\n\n\n; AND.\n\n\n; AND; evaluates left to right, examining only the first element of each vector.\n\n\n|\n; OR.\n\n\n||\n; OR; evaluates left to right, examining only the first element of each vector.\n\n\nxor()\n; elementwise exclusive OR.\n\n\nisTRUE(x)\n; is true if and only if x is a length-one logical vector whose only element is TRUE and which has no attributes.\n\n\nifelse(condition, a, b)\n; if \ncondition\n is proven true, return \na\n, or else, return \nb\n.\n\n\n\n\nLoop\n\n\nlooping\n\\sub\n\n\n\n\nfor(var in seq) expr\n.\n\n\nwhile(condition is true) expr\n.\n\n\nrepeat expr\n.\n\n\nbreak\n; break out of a for, while or repeat loop; control is transferred to the first statement outside the inner-most loop.\n\n\nnext\n; halt the processing of the current iteration and advance the looping index.\n\n\n\n\n10, Writing you own Functions\n\n\nSimple custom function\n\n\n# define\nmyfunction \n- function(arg1, arg2, ...){\nstatements\nreturn(object)\n}\n\n# call\nmyfunction(arg1, arg2, ...)\n\n\n\n\nIt is possible to remplace function arguments with variables and objects.\n\n\nComplex custom function\n\n\nInvolve conditions and loops.\n\n\ntwosam \n- function(y1, y2) {\n    n1 \n- length(y1); n2 \n- length(y2)\n    yb1 \n- mean(y1);\n    yb2 \n- mean(y2)\n    s1 \n- var(y1);\n    s2 \n- var(y2)\n    s \n- ((n1 - 1) * s1 + (n2-1) * s2) / (n1 + n2 - 2)\n    tst \n- (yb1 - yb2) / sqrt(s * (1 / n1 + 1 / n2))\n    tst\n}\n\n\n\n\nDefine and call on the same line\n\n\n\n\ntstat \n- twosam(data$male, data$female); tstat\n\n\n\n\nSeveral commands and functions can be called on the same line with:\n\n\n\n\ncommand1; command2; function1; function2\n \n\n\n\n\nTwo ways of defining and calling a function\n\n\nmulty \n- function(x, y) {\n    x * y\n    }\n\n# with a binary operator\n\n%!%\n \n- function(x, y) {\n    x * y\n    }\n\nx \n- 2\ny \n- 2\n\nmulty(x, y)\n[1] 4\n\nx %!% y\n[1] 4\n\n\n\n\n\nMatrix multiplication operator, \n%*%\n, and the outer product matrix operator, \n%o%\n, are other examples of binary operators.\n\n\nFunction in a function\n\n\n# case 1\narea \n- function(f, a, b, eps = 1.0e-06, lim = 10) {\n    fun1 \n- function(f, a, b, fa, fb, a0, eps, lim, fun) {\n        d \n- (a + b)/2\n        h \n- (b - a)/4\n        fd \n- f(d)\n        a1 \n- h * (fa + fd)\n        a2 \n- h * (fd + fb)\n        if(abs(a0 - a1 - a2) \n eps || lim == 0)\n            return(a1 + a2)\n        else {\n            return(fun(f, a, d, fa, fd, a1, eps, lim - 1, fun) \n                fun(f, d, b, fd, fb, a2, eps, lim - 1, fun))\n        }\n    }\n    fa \n- f(a)\n    fb \n- f(b)\n    a0 \n- ((fa + fb) * (b - a))/2\n    fun1(f, a, b, fa, fb, a0, eps, lim, fun1)\n}\n\n# case 2\nf \n- function(x) {\n    y \n- 2*x\n    print(x)\n    print(y)\n    print(z)\n}\n\n# case 3\ncube \n- function(n) {\n    sq \n- function() n*n\n    n*sq()\n}\n\n# case 4\nopen.account \n- function(total) {\n    list(\n        deposit = function(amount) {\n            if(amount \n= 0)\n                stop(\nDeposits must be positive!\\n\n)\n            total \n- total + amount\n            cat(amount, \ndeposited. Your balance is\n, total, \n\\n\\n\n)\n        },\n        withdraw = function(amount) {\n            if(amount \n total)\n                stop(\nYou don\u2019t have that much money!\\n\n)\n            total \n- total - amount\n            cat(amount, \nwithdrawn. Your balance is\n, total, \n\\n\\n\n)\n        },\n        balance = function() {\n            cat(\nYour balance is\n, total, \n\\n\\n\n)\n        }\n    )\n}\n\nross \n- open.account(100)\nrobert \n- open.account(200)\n\nross$withdraw(30)\nross$balance()\nrobert$balance()\nross$deposit(50)\nross$balance()\nross$withdraw(500)\n\n\n\n\nName\n\n\nAdd,  modify, and remove (with \nnames(x) \n- NA or 0\n) names.\n\n\n\n\nnames()\n.\n\n\nrownames()\n.\n\n\ncolnames()\n.\n\n\ndimnames()\n.\n\n\n\n\nCustomizing startup\n\n\nCustomize the R environment through a directory initialization file; commands that you want to execute every time R is started under your system.\n\n\nR will always source the Rprofile.site file first. \n\n\nOn Windows, the file is in C:\\Program Files\\R\\R-n.n.n\\etc. You can also place a .Rprofile file in any directory that you are going to run R from or in the user home directory. \n\n\nIndividual users control over their workspace and allows for different startup procedures in different working directories.\n\n\nIf no .Rprofile file is found in the startup directory, then R looks for a .Rprofile file in the user\u2019s home directory and uses that (if it exists) environment variable R_PROFILE_USER is set. The file it points to is used instead of the .Rprofile files. \n\n\nFunction named .First() in either of the two profile files or in the .RData image has a special status: initialize the environment\n\n\nSequence in which files are executed is:\n\n\n\n\nRprofile.site\n\n\nthe user profile\n\n\n.RData\n\n\n.First()\n\n\n.Last(), if defined, is (normally) executed at the very end of the session.\n\n\n\n\nList function and method\n\n\n\n\nmethods(class = \"data.frame\")\n; list methods associated with the class.\n\n\nmethods(plot)\n; list methods specific to the object.\n\n\n\n\nmethods(class = \ndata.frame\n)\n [1] $             $\n-           [             [[            [[\n-         \n [6] [\n-           aggregate     anyDuplicated as.data.frame as.list      \n[11] as.matrix     by            cbind         coerce        dim          \n[16] dimnames      dimnames\n-    droplevels    duplicated    edit         \n[21] format        formula       head          initialize    is.na        \n[26] Math          merge         na.exclude    na.omit       Ops          \n[31] plot          print         prompt        rbind         row.names    \n[36] row.names\n-   rowsum        show          slotsFromS3   split        \n[41] split\n-       stack         str           subset        summary      \n[46] Summary       t             tail          transform     unique       \n[51] unstack       within       \nsee '?methods' for accessing help and source code\n\nmethods(plot)\n [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*\n [4] plot.default        plot.dendrogram*    plot.density*      \n [7] plot.ecdf           plot.factor*        plot.formula*      \n[10] plot.function       plot.hclust*        plot.histogram*    \n[13] plot.HoltWinters*   plot.isoreg*        plot.lm*           \n[16] plot.medpolish*     plot.mlm*           plot.ppr*          \n[19] plot.prcomp*        plot.princomp*      plot.profile.nls*  \n[22] plot.R6*            plot.raster*        plot.spec*         \n[25] plot.stepfun        plot.stl*           plot.table*        \n[28] plot.ts             plot.tskernel*      plot.TukeyHSD*     \nsee '?methods' for accessing help and source code\n\n\n\n\nDifference:\n\n\n\n\nplot()\n; a generic method.\n\n\nplot.\n ; a specific method such as \nplot.ts()\n for example.\n\n\n\n\n11, Statistical models in R\n\n\nRegression\n\n\ny \n- 1:10\nx \n- 1:10\n\na \n- lm(y \nsub\nx)\na\nCall:\nlm(formula = y \nsub\nx)\n\nCoefficients:\n(Intercept)            x  \n  1.123e-15    1.000e+00  \n\n\n# no intercept, through the origin\nb \n- lm(y \nsub\n0 + x)\nb\nCall:\nlm(formula = y \nsub\n0 + x)\n\nCoefficients:\nx  \n1  \n\n\n# no intercept\nc \n- lm(y \nsub\n-1 + x)\nc\nCall:\nlm(formula = y \nsub\n-1 + x)\n\nCoefficients:\nx  \n1  \n\n\nd \n- lm(y \nsub\nx - 1)\nd\nCall:\nlm(formula = y \nsub\nx - 1)\n\nCoefficients:\nx  \n1  \n\n\n# log\ne \n- lm(log(y) \nsub\nx)\ne\nCall:\nlm(formula = log(y) \nsub\nx)\n\nCoefficients:\n(Intercept)            x  \n     0.2432       0.2304  \n\n\n# quadratic\nf \n- lm(y \nsub\n1 + x + I(x^2))\nf\nCall:\nlm(formula = y \nsub\n1 + x + I(x^2))\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n  1.123e-15    1.000e+00    5.699e-18  \n\n\n# polynomial\ng \n- lm(y \nsub\nX + poly(x, 2))\n\n\n\n# weighted regression\nfm1 \n- lm(y \nsub\nx, data = dummy, weight = 1 / w^2)\n\n\n# and more\nnew.model \n- update(old.model, new.formula)\n\nfm05 \n- lm(y \nsub\nx1 + x2 + x3 + x4 + x5, data = production)\n\nfm6 \n- update(fm06, . \nsub\n. + x6)\nsmf6 \n- update(fm6, sqrt(,) \nsub\n.)\n\n\n\n\nExplore the results\n\n\n\n\nsummary(regression results)\n.\n\n\nvcov()\n; variance-covariance matrix.\n\n\naov(formula, data = data.frame)\n.\n\n\nanova(fitted.model.1, fitted.model.2, ...)\n\n\nand many more.\n\n\n\n\nanova, coeficient, coef, deviance, residuals, effects, formula, model, kappa, labels, plot, predict, proj, projection\n\n\nStepwise Regression\n\n\nSelect a suitable model by adding or dropping variables and preserving hierarchies. The best model with the smallest AIC (Akaike\u2019s Information Criterion) is discovered with the search.\n\n\nGeneralized least squares\n\n\ngls, binomial, logit, probit, log, cloglog, gaussian, identity, log, inverse, gamma, identity, inverse, log, inverse.gaussian, 1/mu^2, identity, inverse, log, poisson, identity, log, sqrt, quasi-likelihood, logit, probit\n\\sub\n\n\nfitted.model \n- glm(formula, famili=family.generator, data = data.frame)\n\n\n\n\nNonlinear least squares\n\n\nnls\n\\sub\n\n\n\n\nnlm(function)\n\n\n\n\nMaximum likehood\n\n\nml\n\\sub\n\n\nWhen errors are not normal.\n\n\n\n\nml(function)\n\n\n\n\nMixed model\n\n\n\n\nnlme\n package.\n\n\n\n\nLocal Approximating Regression\n\n\nNonparametric local regression function.\n\n\n\n\nlrf \n- lowess(x, y)\n.\n\n\n\n\nRobust regression\n\n\nMASS\n package.\n\n\nAdditive model\n\n\n\n\nacepack\n package.\n\n\nmda\n package.\n\n\ngam\n package.\n\n\nmgcv\n package.\n\n\n\n\nTree-based model\n\n\ndecision, classification tree, random forest\n\\sub\n\n\n\n\nrpart\n package.\n\n\ntree\n package.\n\n\n\n\n12, Graphical Procedures\n\n\nwww.statmethods.net/advgraphs\n\n\nGraphic, packages\n\n\n\n\nlattice\n package.\n\n\nggplot2\n package.\n\n\ngrid\n package.\n\n\nggobi\n, \nrgl\n packages; for interactive graphics, 3D, and surfaces.\n\n\nand many more.\n\n\n\n\nBasic plot\n\n\n\n\nplot()\n.\n\n\nboxplot()\n..\n\n\nlines(x,y)\n; add a line to a basic plot.\n\n\npairs()\n; multivariate, pairwise scatterplot matrix. \n\n\ncoplot(a \nsub\nb | c)\n; scatter plot of a \nb given c, a factor vector (levels)\n\n\ncoplot(a \nsub\nv | c + d)\n.\n\n\npie()\n.\n\n\nhist(x)\n; where \nnclass = n\n and \nbreaks = b\n.\n\n\nbarplot()\n; can be horizontal or vertical.\n\n\ndotchart(x, ...)\n; a case of bar chart.\n\n\nand many more with lots of options.\n\n\n\n\nqq plot\n\n\n\n\nqqnorm(x)\n.\n\n\nqqline(x)\n.\n\n\nqqplot(x, y)\n; comparison.\n\n\n\n\nPicture\n\n\n\n\nimage(x, y, z, ...)\n; grid of rectangles with colors corresponding to the values in z.\n\n\ncontour(x, y, z, ...)\n; z add contour lines (even to an existing plot).\n\n\npersp(x, y, z, ...)\n; perspective plots of a surface over the x\u2013y plane.\n\n\n\n\nGraphic arguments and parameters\n\n\nwww.statmethods.net/advgraphs/parameters\n\n\nx1 \n- rnorm(1000, 0.4, 0.8)\nx2 \n- rnorm(1000, 0.0, 1.0)\nx3 \n- rnorm(1000, -1.0, 1.0)\nhist(x1, width = 0.33, offset = 0.00, col = \nblue\n, xlim = c(-4,4),\n     main = \nHistogram of x1, x2 \n x3\n,\n     xlab = \nx1 - blue, x2 - red, x3 - green\n)\nhist(x2, width = 0.33, offset = 0.33, col = \nred\n, add = TRUE)\nhist(x3, width = 0.33, offset = 0.66, col = \ngreen\n, add = TRUE)\n\n\n\n\n\n\nadd = TRUE\n; superimpose a plot on another plot.\n\n\naxes = FALSE\n; no axes.\n\n\naxis(side,...)\n; 1 to 4, bottom, left, top, right.\n\n\nlog = \"x\", \"y\", \"xy\"\n; difference scale.\n\n\ntype = \"p\"\n (points); \n\"l\"\n (lines), \n\"b\"\n (p+l), \n\"o\"\n (l+p), \n\"h\"\n (vertical lines from points to the zero axis), \n\"s\"\n (step-function), \n\"n\"\n (not plotting).\n\n\nxlab = \"bla\"\n.\n\n\nylab = \"bla\"\n.\n\n\nmain = \"bla\"\n.\n\n\nsub = \"bla\"\n.\n\n\ntitle(main, sub)\n.\n\n\npoints(x, y)\n; add points on top of \nplot()\n.\n\n\ntext(x, y, labels,...)\n; add text to each point.\n\n\nplot(x, t, type = \"n\"); text(x, y, names)\n; replace the dot with text.\n\n\nabline(a, b)\n; add a line.\n\n\nabline(0, 1, lty = 3)\n.\n\n\nabline(coef(fm))\n.\n\n\nabline(coef(fm1), col = \"red\")\n.\n\n\nabline(h = y)\n; or \nh = value\n; add a horizontal line.\n\n\nabline(v = x)\n; or \nv = value\n; add a vertical line.\n\n\nabline(lm.obj)\n.\n\n\nlegend(x, y, legend, ...)\n; add a legend at a specified position.\n\n\nfill = v\n; add a vector of the same length as a legend.\n\n\nlty = 2\n; line style.\n\n\nlwd = 1.5\n; line width.\n\n\npch = 0\n; dot style.\n\n\npar()\n; list of permanent graphic parameters.\n\n\npar(c(\"col\", \"lty\"))\n; limit the list of parameters.\n\n\npar(col = 4, lty = 2)\n; set the parameters for all plots.\n\n\nplot(x, y, pch = \"+\")\n; will set a temporary parameter inside a plot.\n\n\npch = \"+\"\n, \npch = 4\n.\n\n\ncol = \"red\"\n; dot color.\n\n\ncol =  \"red\"\n.\n\n\ncol.axis = \"red\"\n.\n\n\ncol.lab = \"red\"\n.\n\n\ncol.main = \"red\"\n.\n\n\ncol.sub = \"red\"\n.\n\n\nfont = \"red\"\n.\n\n\nfont.axis = \"red\"\n.\n\n\nfont.lab = \"red\"\n.\n\n\nfont.main = \"red\"\n.\n\n\nfont.sub = \"red\"\n.\n\n\nadj = -0.1\n; adjust the text to the plotting position (-1\n-0.5\n0\n0.5\n1), from left to right, 0 being the center.\n\n\ncex = 1.5\n; character 50% larger.\n\n\ncex.axis = 1.5\n.\n\n\ncex.lab = 1.5\n.\n\n\ncex.main = 1.5\n.\n\n\ncex.sub = 1.5\n.\n\n\nlab = c(5, 7, 12)\n; the first two numbers are the desired number of tick intervals on the x and y axes respectively. The third number is the desired length of axis labels.\n\n\nlas = 1\n; orientation of axis labels (text and numbers). 0 means always parallel to axis, 1 means always horizontal, and 2 means always perpendicular to the axis.\n\n\nmgp = c(3, 1, 0)\n; position of the axis components.\n\n\ntck = 0.01\n; length of tick marks.\n\n\nxaxs = \"r\"\n; axis style.\n\n\nxaxs = \"i\"\n; inside.\n\n\nmai = c(1, 0.5, 0.5, 0)\n; margins in inches around the plot.\n\n\nmar = c(4, 2, 2, 1)\n; in text lines.\n\n\nand many more.\n\n\n\n\nR allows you to create an n by m array of plots on a single page:\n\n\n\n\nmfcol = c(3.2)\n; 3 rows, 2 columns, 6 plotting areas.\n\n\nmfrow = c(3,2)\n; idem but filled by rows.\n\n\nomi = c(1, 0.5, 0.5, 0)\n; margins between plots.\n\n\noma = c(1, 0.5, 0.5, 0)\n; margins outside plots.\n\n\nmfg = c(2, 2, 3, 2)\n; position of the current figure in a multiple figure environment. The first two numbers are the row and column of the current figure; the last two are the total number of rows and columns in the multiple figure array; in other words, 2nd row and 2nd column in a 3x2 panel.\n\n\nfig = c(4, 9, 1, 4) / 10\n; position of the current figure on the page. Values are the positions of the left, right, bottom and top edges, respectively, as a percentage of the page measured from the bottom left corner.\n\n\nand many more.\n\n\n\n\nGeometric shapes\n\n\n\n\npolygon(x, y, ...)\n; x, y are vectors containing the coordinates of the vertices of the polygon.\n\n\n\n\nGraphic help\n\n\n\n\nhelp(plotmath)\n.\n\n\nexample(plotmath)\n.\n\n\ndemo(plotmath)\n.\n\n\nhelp(Hershey)\n.\n\n\ndemo(Hershey)\n.\n\n\nhelp(Japanese)\n.\n\n\ndemo(Japanese)\n.\n\n\n\n\nGraphic text and mouse\n\n\nLeave graphic marks and texts.\n\n\n\n\nlocator(n, type)\n; select with mouse a max of n locations to mark on the graph (left click, right click to stop).\n\n\ntext(locator(1), \"Outlier\", adj=0)\n; select with mouse a location to add a string on the graph.\n\n\nidentify(x, y)\n; add a label to a dot with mouse.\n\n\nidentify(x, y, labels)\n.\n\n\nidentify(x, y, \"yes\")\n.\n\n\n\n\nGraphic device\n\n\n\n\nsplit.screen()\n; FALSE or number of regions within the current device which can, to some extent, be treated as separate graphics devices. It is useful for generating multiple plots on a single device.\n\n\nlayout()\n; divides the device up into as many rows and columns as there are in matrix mat.\n\n\n\n\nGraphic device driver\n\n\nOpen a special graphics window.\n\n\n\n\nX11()\n; under UNIX.\n\n\nwindows()\n; under Windows.\n\n\nwin.printer()\n.\n\n\nwin.metafile()\n.\n\n\n\n\n\n\nquartz()\n; under OS X.\n\n\ndev.new()\n; returns the return value of the device opened, usually invisible NULL.\n\n\ngrid()\n adds an rectangular grid to an existing plot.\n\n\npostscript()\n; starts the graphics device driver for producing PostScript graphics.\n\n\npostscript(\"file.ps\", horizontal=FALSE, height=5, pointsize=10)\n.\n\n\npostscript(\"plot1.eps\", horizontal=FALSE, onefile=FALSE, height=8, width=6, pointsize=10)\n.\n\n\n\n\n\n\npdf()\n; starts the graphics device driver for producing PDF graphics.\n\n\npng()\n; idem.\n\n\njpeg()\n; idem.\n\n\ntiff()\n; idem.\n\n\nbitmap()\n; idem.\n\n\ndev.off()\n; shuts down the specified (by default the current) device.\n\n\ndev.set()\n; dev.set makes the specified device the active device.\n\n\ndev.list()\n; returns the numbers of all open devices.\n\n\ndev.next()\n, \ndev.prev()\n; return the number and name of the next / previous device in the list of devices.\n\n\ngraphics.off(); terminate all devices.\n\n\n\n\n13, Packages\n\n\n\n\ninstall.package()\n; on the computer.\n\n\nlibrary()\n; load it.\n\n\nsearch()\n; check what is loaded.\n\n\nloadedNamespaces()\n; idem.\n\n\nhelp.start()\n; start the HTML help system, package section.\n\n\n\n\nFind out about packages:\n\n\nCRAN.R-project.org\n\n\nwww.bioconductor.org\n\n\nwww.omegahat.org\n\n\n14, OS Facilities\n\n\nManage files with Linux or Windows or RStudio.", 
            "title": "An Introduction to R"
        }, 
        {
            "location": "/An Introduction to R/#1-introduction-and-preliminaries", 
            "text": "Run R (CLI).  Quit with  q() .    Run R in an IDE (GUI); like RStudio.  Create a working directory  work .  Ask help.  help(function) ; open a web documentation in a browser or in the IDE.  ?function ; idem.  ??function ; idem, but showing concordances.  help(\"[[\") ; idem, searching with a string.  help.start() ; show the entire manual.    sink() ; divert output from the console to a connection; restore the output to the console.  objects() ,  ls() ; see the objects stored in a session.  Objects are in a file with  .RData  extension.    rm(x, y, ink, ...) ; remove stored objects.  All commands entered or run are recorded in a file with  .Rhistory  extension.", 
            "title": "1, Introduction and Preliminaries"
        }, 
        {
            "location": "/An Introduction to R/#2-simple-manipulations-numbers-and-vectors", 
            "text": "Create a vector  x  - c(1, 2) # assignment (universal).\nx\n[1] 1 2\n\nc(2, 1) -  y # alternative assignment.\ny\n[1] 2 1\n\nx = c(1, 2) # alternative assignment (some limitations).\nx  - c(1, 2) # permanent assignment.\n\n# examples\nab  - 9\nab\n[1] 9\n\nassign( ab , 10)\nab \n[1] 10  Create a sequence (vector)  x  - 2 * 1:15\n\nx\n[1]  2  4  6  8 10 12 14 16 18 20 22 24 26 28 30\n\nn  - 10\nx  - 1:n-1\n\nx\n[1] 1 2 3 4 5 6 7 8 9\n\nx  - 30:1\nx\n[1] 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n[29]  2  1\n\nseq(from = 1, to = 30)\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n[29] 29 30\n\nseq(1, 30)\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n[29] 29 30\n\nseq(30, 1)\n[1] 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3\n[29]  2  1\n\nseq(1, 30, by = 2)\n[1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29  Repetition  rep(2, times = 5)\n[1] 2 2 2 2 2  x  - c( x ,  y )[rep(c(1, 2, 2, 1), times = 4)]\n\nx\n[1]  x   y   y   x   x   y   y   x   x   y   y   x   x   y   y   x   Length (vector)   length(vector)   Boolean   TRUE  or  T ;  T == 1 .  FALSE  or  F ;  F == 0 .   Some operators    =   =  ==  !=     |  ||  and many more.   Missing data and more   NA; not available.  NaN; not a number.  Inf - Inf == NaN == 0/0; infinite number.   x  - c(1:3, NA)\n\nx\n[1] 1 2 3 NA   is.na(var) .  var == NA .  is.na(x) .  !is.na(x) .   x  - c(-1:3, NA)\ny  - x[!is.na(x)   x   0]\n\nx\n[1]  -1  0  1  2  3 NA\n\ny\n[1] 1 2 3  Extract, subset (vector)   x[i] ; index.   Backslash use for some characters   \\\\ ; backslash.  \\n ; new line.  \\t ; tab.  \\b ; backspace.   Concatenate, paste (vector)  labs  - paste(c( X ,  Y ), 1:10, sep =  )\n\nlabs\n[1]  X1    Y2    X3    Y4    X5    Y6    X7    Y8    X9    Y10   A note of data handling and manipulations  You can also  split() ,  merge() ,  rbind() ,  cbind()  vectors.   It is also possible with other objects such as factors, lists, arrays, matrices, and data frames.  There are built-in functions to extract, exclude, subset, replace, transform or convert ( .as ), concatenate, paste, group, and bind.  \\sub slice, extract, exclude, subset, replace, convert,  concatenate, paste, group, bind,     Exclude, remove (vector)  z  - 1:20\n\nz\n[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\nzz  - z[-(1:5)]\n\nzz\n[1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20  Replace (vector)  x  - c(1, 1, 1, NA)\n\nx\n[1]  1  1  1 NA\n\nx[is.na(x)]  - 0\n\nx\n[1] 1 1 1 0\n# replaces any missing values in x by zeros  Absolute value   y  - abs(y) .   Convert ( .as )   as.vector() .  as.integer() .  as.numeric()  as.factor() .  as.character() .  and many more.", 
            "title": "2, Simple Manipulations; Numbers and Vectors"
        }, 
        {
            "location": "/An Introduction to R/#3-objects-their-modes-and-attributes", 
            "text": "Object type  obj  - 1\n\nobj1  - numeric(obj)\nmode(obj1)\n[1]  numeric \nobj2  - character(obj)\nmode(obj2)\n[1]  character \n\nx  - 1\n\n# class() != mode(), but almost\nmode(x)\n[1]  numeric \n\nx  - factor(x)\nx\n[1] 1\nLevels: 1\n\nx  - numeric(x)\nx\n[1] 0\n\n# load key:value\nobj[3]  - 17\n\nobj\n[1]  1 NA 17  Classes  class \\sub   numeric .  logical .  character .  list .  matrix .  array .  factor .  data.frame .   class(obj)\n[1]  numeric \n\n# print the object as ordinary\nunclass(obj)\n[1]  1 NA 17", 
            "title": "3, Objects, their Modes and Attributes"
        }, 
        {
            "location": "/An Introduction to R/#4-ordered-and-unordered-factors", 
            "text": "state  - c( tas , qld , sa , sa , sa , vic , nt , act , qld , nsw , wa , nsw , nsw , vic , vic , vic , nsw , qld , qld , vic , nt , wa , wa , qld , sa ,  tas , nsw ,  nsw ,  wa , act )\n\nstatef  - factor(state)\n\n# class != mode()\nclass(statef)\n[1]  factor \nmode(statef)\n[1]  numeric \n\nlevels(statef)\n[1]  act   nsw   nt    qld   sa    tas   vic   wa  \n\nincomes  - c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\n\nincmeans  - tapply(incomes, statef, mean)\n\nincmeans\n  act   nsw    nt   qld    sa   tas   vic    wa \n48.50 55.00 54.00 51.60 54.25 53.00 61.60 54.50 \n\nstderr  - function(x) { sqrt(var(x) / length(x)) }\n\nstderr(incomes)\n[1] 1.524462\n\n# alternatively\nincster  - tapply(incomes, statef, stderr)\n\nincster\n      act       nsw        nt       qld        sa       tas       vic        wa \n5.5000000 3.9665266 5.0000000 2.6570661 5.3909647 7.0000000 0.8717798 6.2249498    # a vector of characters\nstateff  - c( a ,  b ,  c )\n[1]  a   b   c \n\nas.factor(stateff)\n[1] a b c\nLevels: a b c  # Create a factor with the wrong order of levels\nsizes  - factor(c( small ,  large ,  large ,  small ,  medium ))\n\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# levels can be specified explicitly\nsizes  - factor(sizes, levels = c( small ,  medium ,  large ))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# do the same with an ordered factor\nsizes  - ordered(c( small ,  large ,  large ,  small ,  medium ))\n\nsizes  - ordered(sizes, levels = c( small ,  medium ,  large ))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small   medium   large\n\n# use relevel() to make a particular level first in the list. (This will not work for ordered factors.)\n\n# Create a factor with the wrong order\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# make medium first\nsizes  - relevel(sizes,  medium )\n\nsizes\n[1] small  large  large  small  medium\nLevels: medium large small\n\n# make small first\nsizes  - relevel(sizes,  small )\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# specify the proper order when the factor is created\nsizes  - factor(c( small ,  large ,  large ,  small ,  medium ),\n                  levels = c( small ,  medium ,  large ))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large\n\n# Create a factor with the wrong order of levels\nsizes  - factor(c( small ,  large ,  large ,  small ,  medium ))\n\nsizes\n[1] small  large  large  small  medium\nLevels: large medium small\n\n# reverse the order of levels in a factor\nsizes  - factor(sizes, levels=rev(levels(sizes)))\n\nsizes\n[1] small  large  large  small  medium\nLevels: small medium large  Convert ( .as )   as.factor() .", 
            "title": "4, Ordered and Unordered Factors"
        }, 
        {
            "location": "/An Introduction to R/#5-arrays-and-matrices", 
            "text": "Dimension  z  - 1:1500\ndim(z)  - c(3, 5, 100)\n\n# gives 100 arrays of 3 lines by 5 columns  Create a matrix, an array  x  - array(1:20, dim=c(4, 5))\n\nx\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20\n\nx  - array(1:20, dim=c(2, 5, 2))\n\nx\n, , 1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n, , 2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   11   13   15   17   19\n[2,]   12   14   16   18   20\n\n\ni  - array(c(1:3, 3:1), dim = c(3, 2))\n\ni\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    2\n[3,]    3    1\n\nj  - array(c(1:8), dim = c(2, 2, 2))\n\nj\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\nk  - array(1:27, c(3, 3, 3))  k\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   19   22   25\n[2,]   20   23   26\n[3,]   21   24   27\n\n\na  - matrix(1, 2, 2)\n\na\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n\nb  - matrix(2, 2, 2)\n\nb\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2  Extract, subset (matrix)   a[2, 1] ; rows, columns.   Extract, subset (array)   a[2, 1, 1] ; rows, columns, matrix.   Cross product vs multiplication  a  - 1:5\nb  - seq(10, 6, -1)\n\na\n[1] 1 2 3 4 5\nb\n[1] 10  9  8  7  6\n\na * b\n[1] 10 18 24 28 30\n\ncrossprod(a, b)\n     [,1]\n[1,]  110\n\nab  - a %o% b\nab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   10    9    8    7    6\n[2,]   20   18   16   14   12\n[3,]   30   27   24   21   18\n[4,]   40   36   32   28   24\n[5,]   50   45   40   35   30  Matrix operation  A et B are 2x2 matrices:   A * B ; scalar multiplication.  A %*% B ; matrix multiplication  x %*% A %*% y ; matrix multiplication.  crossproduct(A, B) ; cross multiplication.  ab  - outer(A,B,\"*\") ;  a * b .  ab  - outer(A,B,\"+\") ;  a + b .  ab  - outer(A,B,\"-\") ;  a - b .  and many more.   Diagonal and triangle  ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   11   10    9    8    7\n[2,]   12   11   10    9    8\n[3,]   13   12   11   10    9\n[4,]   14   13   12   11   10\n[5,]   15   14   13   12   11\n\ndiag(ab)\n[1] 11 11 11 11 11\n\nlower.tri(ab)\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE FALSE FALSE FALSE FALSE\n[2,]  TRUE FALSE FALSE FALSE FALSE\n[3,]  TRUE  TRUE FALSE FALSE FALSE\n[4,]  TRUE  TRUE  TRUE FALSE FALSE\n[5,]  TRUE  TRUE  TRUE  TRUE FALSE\n\nlower.tri(ab) * ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]   12    0    0    0    0\n[3,]   13   12    0    0    0\n[4,]   14   13   12    0    0\n[5,]   15   14   13   12    0\n\nupper.tri(ab)\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE  TRUE  TRUE  TRUE  TRUE\n[2,] FALSE FALSE  TRUE  TRUE  TRUE\n[3,] FALSE FALSE FALSE  TRUE  TRUE\n[4,] FALSE FALSE FALSE FALSE  TRUE\n[5,] FALSE FALSE FALSE FALSE FALSE\n\nupper.tri(ab) * ab\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0   10    9    8    7\n[2,]    0    0   10    9    8\n[3,]    0    0    0   10    9\n[4,]    0    0    0    0   10\n[5,]    0    0    0    0    0  Solving a matrix system  b  - A %*% x\n\n# or\nsolve(A, b)   solve(A) ; inverse the matrix.   Symmetrical matrix and eigen value  b  - matrix(2, 2, 2)\n\nb\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\ne  - eigen(b, only.values = TRUE)\ne\n$values\n[1] 4 0\n\n$vectors\nNULL  Singular value decomposition (matrix)  svd(b)\n$d\n[1] 4 0\n\n$u\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068\n\n$v\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,] -0.7071068  0.7071068  Determinant (matrix)  det(ab)\n[1] 0  Least square fitting (matrix)   lsfit()  or least squares estimate of  b  in the model:  y = X b + e .   QR decomposition (matrix)  qr(ab)\n$qr\n            [,1]        [,2]          [,3]          [,4]          [,5]\n[1,] -29.2403830 -27.0174299 -2.479448e+01 -2.257152e+01 -2.034857e+01\n[2,]   0.4103913   0.2418254  4.836508e-01  7.254763e-01  9.673017e-01\n[3,]   0.4445906  -0.1703815  3.717512e-15  1.134003e-14  1.749210e-14\n[4,]   0.4787899  -0.5015812  3.957070e-01  1.553722e-15  1.295516e-15\n[5,]   0.5129892  -0.8327809  5.076995e-01  6.936403e-01 -1.685698e-16\n\n$rank\n[1] 2\n\n$qraux\n[1] 1.376192e+00 1.160818e+00 1.765282e+00 1.720322e+00 1.685698e-16\n\n$pivot\n[1] 1 2 3 4 5\n\nattr(, class )\n[1]  qr   Also:   qr.coef() .  qr.fitted() .  qr.resid() .   Convert ( .as )   as.array() .  as.matrix() .", 
            "title": "5, Arrays and Matrices"
        }, 
        {
            "location": "/An Introduction to R/#6-lists-and-data-frames", 
            "text": "Crate a list  Lst  - list(name =  Fred , wife =  Mary , no.children = 3, child.ages = c(4,7,9))\n\nLst\n$name\n[1]  Fred \n\n$wife\n[1]  Mary \n\n$no.children\n[1] 3\n\n$child.ages\n[1] 4 7 9  Extract, subset (list)  Lst$name\n[1]  Fred \n\nLst[[ name ]] == Lst[[1]]\n[1] TRUE\n\n\nLst[5]  - list( ) # or list()  \nLst[ new ]  - list()  # key, value\n\nLst[1]\n$name\n[1]  Fred \n\nLst$child.ages[1]\n[1] 4\nLst[[4]][1]\n[1] 4  Concatenate, paste  x  - c(1,2)\ny  - c(3,4)\n\nc(x, y)\n[1] 1 2 3 4\n\npaste(x, y)\n[1]  1 3   2 4 \n\ndata.frame(x, y)\n  x y\n1 1 3\n2 2 4\n\na  - list(1, 2)\nb  - list( a ,  b )\n\nlist(a, b)\n[[1]]\n[[1]][[1]]\n[1] 1\n\n[[1]][[2]]\n[1] 2\n\n\n[[2]]\n[[2]][[1]]\n[1]  a \n\n[[2]][[2]]\n[1]  b \n\nh  - matrix(2, 2, 2)\ng  - matrix(1, 2, 2)\n\nc(h, g)\n[1] 2 2 2 2 1 1 1 1\n\npaste(h, g)\n[1]  2 1   2 1   2 1   2 1 \n\nlist(h, g)\n[[1]]\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n[[2]]\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1  Convert ( as. )   as.matrix()   Data frame   A data frame can hold other data frames.  A list can hold other lists.  A vector can hold other vectors.    Each variable can be numeric, character, logical, factor, numeric matrix, list, data.frame.   state  - c( tas , qld , sa , sa , sa , vic , nt , act , qld , nsw , wa , nsw , nsw , vic , vic , vic , nsw , qld , qld , vic , nt , wa , wa , qld , sa ,  tas , nsw ,  nsw ,  wa , act )\n\nstatef  - factor(state)\n\nincomes  - c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\n\nincomef  - factor(incomes)\n\naccountants  - data.frame(home=statef, loot=incomes, shot=incomef)\n\nhead(accountants, 10)\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n\n# class() != mode()\nclass(accountants)\n[1]  data.frame \n\nmode(accountants)\n[1]  list   Concatenate, paste (data frame)  # accountants == acc\n\nc(accountants, acc)\n$home\n [1] tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n[14] vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n[27] nsw nsw wa  act\nLevels: act nsw nt qld sa tas vic wa\n\n$loot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n\n$shot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n20 Levels: 40 41 42 43 46 48 49 51 52 54 56 58 59 ... 70\n\n$home\n [1] tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n[14] vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n[27] nsw nsw wa  act\nLevels: act nsw nt qld sa tas vic wa\n\n$loot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n\n$shot\n [1] 60 49 40 61 64 60 59 54 62 69 70 42 56 61 61 61 58 51\n[19] 48 65 49 49 41 48 52 46 59 46 58 43\n20 Levels: 40 41 42 43 46 48 49 51 52 54 56 58 59 ... 70\n\n\npaste(accountants, acc)\n[1]  c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1) c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1)                                                             \n[2]  c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43) c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43) \n[3]  c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4) c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4)                           \n\nlist(accountants, acc)\n[[1]]\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n...\n\n[[2]]\n   home loot shot\n1   tas   60   60\n2   qld   49   49\n3    sa   40   40\n4    sa   61   61\n5    sa   64   64\n6   vic   60   60\n7    nt   59   59\n8   act   54   54\n9   qld   62   62\n10  nsw   69   69\n...\n\naccountants + acc\n   home loot shot\n1    NA  120   NA\n2    NA   98   NA\n3    NA   80   NA\n4    NA  122   NA\n5    NA  128   NA\n6    NA  120   NA\n7    NA  118   NA\n8    NA  108   NA\n9    NA  124   NA\n10   NA  138   NA\n...  Convert ( as. )   as.data.frame()   Load data into R   read.table() ; produce a data frame with inputs.   A note on environment objects   search() ; objects in .GlobalEnv; including packages.  ls() ; list these objects.    attach() ; attach an object to .GlobalEnv.  detach() .", 
            "title": "6, Lists and Data Frames"
        }, 
        {
            "location": "/An Introduction to R/#7-readingwriting-data-fromto-files-inputoutput", 
            "text": "A lot more can be found on I/O: depending on the file type, the data format, the desired R object, many commands are available. Depending on a file format, look for the dedicated package on CRAN.  read, reading, write, writing, input, output, i/o \\sub  Examples  # read from a file\nHousePrice  - read.table( house.data , header = TRUE, sep =  ; )\n\n# read a 3-variable list\ninput  - scan( input.dat , list( , 0, 0))  \n\n# read a3-variable list\ninput  - scan( input.dat , list(id= , x = 0, y = 0))\n\n# make 3 separate vectors\nlabel  - input[[1]]\nx  - input[[2]]\ny  - input[[3]]\n\n# label them\nlabel  - inp$id\nx  - inp$x\ny  - inp$y\n\n# read a 5-variable list and transform it into a matrix\nX  - matrix(scan( light.dat , 0), ncol = 5, byrow = TRUE)  Inputing from file types   .txt.  .csv.  .tsv.  .hdf5.  .bmp.  .jpeg.  .png.  .tiff.  .zip.  .xls, spreadsheet.  databases.  statistical programs.  binary files.  and many more (some are up and coming such as julia files).   Cleaning parameter   strip.white = TRUE ; remove unnecessary spaces.   Change the data frame or matrix format   stack() ; for example, take a 6-column, 4-variable data frame and stack the 4 variables into one long column.  unstack() ; vice-versa.   # 6 columns or variables\nStatus Age  V1    V2    V3    V4\n P 23646 45190 50333 55166 56271\nCC 26174 35535 38227 37911 41184\nCC 27723 25691 25712 26144 26398\nCC 27193 30949 29693 29754 30772\nCC 24370 50542 51966 54341 54273\nCC 28359 58591 58803 59435 61292\nCC 25136 45801 45389 47197 47126\n...\n\n# 4 columns, more rows\nStatus Age values ind\nX1  P 23646 45190 V1\nX2 CC 26174 35535 V1\nX3 CC 27723 25691 V1\nX4 CC 27193 30949 V1\nX5 CC 24370 50542 V1\nX6 CC 28359 58591 V1\nX7 CC 25136 45801 V1\nX11 P 23646 50333 V2\n...   reshape() ; stack and unstack variables, according to parameters.   reshape(zz, idvar =  id , timevar =  var , varying = list(c( V1 ,  V2 ,  V3 ,  V4 )), direction =  long )   ftable() ; flatten multidimensional matrices (arrays);  read.ftable() .  data() ; list of datasets.  data(package = \"rpart\") ; load package.  edit(input) ; open a mini-spreadsheet.  xnew  - edit(input) ; open and save it as a new dataset.  and many more.   Encoding   utf-8 for Linux and OS X.  UCS-2LE and UTF-16 for Windows.   # examples\nreadLines( filename.txt , encoding =  UTF-8 )\nreadLines( filename.txt , encoding =  UCS2LE )\nread.delim( clipboard , fileEncoding= UTF-16 )  Output   write.table() ; many parameters.  write.matrix() .  write.foreign() .   Check a file   readLines(\"aab.txt\") .  readLines(\"aab.txt\", 1) .  unlink(\"aab.txt\") ; delete the file on the working directory.   Directory management   getwd() ; get the current working directory.  setwd() ; set the current working directory.  file.show(filepath) .  system.file()   Spreadsheet editor and edition in R   fix(c) ; edit object c; a vetor, list, data frame, etc. in a mini-spreadsheet.  data.entry(c, mode=NULL, Names=NULL), dataentry(c), de(c, Modes=list(), Names=NULL) ; idem.  vi(file= ) ; invoke a text editor.  emacs(file= ) ; idem.  pico(file= ) ; idem.  xemacs(file= ) ; idem.  xedit(file=) ; idem.", 
            "title": "7, Reading/Writing Data from/to Files (Input/Output)"
        }, 
        {
            "location": "/An Introduction to R/#8-probability-distributions", 
            "text": "Distribution   beta .  binom .  cauchy .  chisq .  exp .  f .  gamma .  geom .  hyper .  lnorm .  logis .  nbinom .  norm .  pois .  signrank .  t .  unif .  weibull .  wilcox .  and many more.   Prefixes   d ; density.  p ; CDF or cumulative density function.  q ; quantile.  r ; random deviates or simulation or number generation.   Distribution operation  Commands, prefix + distribution, examples:   dbeta(x= ) .  pbinom(q= , lower.tail= , log.p= ) .  qcauchy(p= , lower.tail= , log.p= ) .  rchisq(n or r= ) .  ptukey() ; studentized (t) distribution  qtukey() .  dmultinom() .  ltinomial .  rmultinom() .  etc.   Descriptive statistics  # overview\nsummary(cars)\nstr(cars)\nboxplot(cars$speed)\nboxplot(cars$dist)\n\n# Tukey five-number summaries\nfivenum(cars$speed)\n\n# histograms and bar charts\nstem(cars$speed)\nhist(cars$speed)\nbarplot(cars$speed)\ndotchart(cars$speed)\n\n# scatter plot\nplot(cars$speed, cars$dist)\nlines(cars$speed, cars$dist)\n\n# add a (1-d) representation\nplot(cars$speed, cars$dist)\nrug(cars$speed, ticksize = 0.03)\nplot(cars$speed, cars$dist)\nrug(cars$dist, ticksize = 0.03)\n\n# normality of the residuals, linearity\nqqnorm(cars$speed)\nqqline(cars$speed)\n\nqqnorm(cars$dist)\nqqline(cars$dist)\n\nqqplot(cars$speed, cars$dist)\n\n# normality test\nshapiro.test(cars$speed)\n\n# Kolmogorov-Smirnov test on a sample\nks.test(cars$speed[10],  pnorm , mean = mean(cars$speed), sd = sqrt(var(cars$speed)))\nks.test(cars$speed[40],  pnorm , mean = mean(cars$speed), sd = sqrt(var(cars$speed)))  Test   t.test(A,B) ; true difference of means is not 0, difference means.  var.test(A,B) ; true ratio of variances is not 1, difference variances.  t.test(A,B, var.equal=TRUE) ; true difference of means is not 0.  wilcox.test(A,B) ; rank sum with continuity correction, continuous distribution.  and many more.   Normality   plot(ecdf(A), do.points=FALSE, verticals=TRUE, xlim=range(A, B))  plot(ecdf(B), do.points=FALSE, verticals=TRUE, add=TRUE)  ks.test(A,B); maximal vertical distance between the two ecdf", 
            "title": "8, Probability Distributions"
        }, 
        {
            "location": "/An Introduction to R/#9-grouping-loops-and-conditional-execution", 
            "text": "; AND.  ; AND; evaluates left to right, examining only the first element of each vector.  | ; OR.  || ; OR; evaluates left to right, examining only the first element of each vector.  xor() ; elementwise exclusive OR.  isTRUE(x) ; is true if and only if x is a length-one logical vector whose only element is TRUE and which has no attributes.  ifelse(condition, a, b) ; if  condition  is proven true, return  a , or else, return  b .   Loop  looping \\sub   for(var in seq) expr .  while(condition is true) expr .  repeat expr .  break ; break out of a for, while or repeat loop; control is transferred to the first statement outside the inner-most loop.  next ; halt the processing of the current iteration and advance the looping index.", 
            "title": "9, Grouping, Loops and Conditional Execution"
        }, 
        {
            "location": "/An Introduction to R/#10-writing-you-own-functions", 
            "text": "Simple custom function  # define\nmyfunction  - function(arg1, arg2, ...){\nstatements\nreturn(object)\n}\n\n# call\nmyfunction(arg1, arg2, ...)  It is possible to remplace function arguments with variables and objects.  Complex custom function  Involve conditions and loops.  twosam  - function(y1, y2) {\n    n1  - length(y1); n2  - length(y2)\n    yb1  - mean(y1);\n    yb2  - mean(y2)\n    s1  - var(y1);\n    s2  - var(y2)\n    s  - ((n1 - 1) * s1 + (n2-1) * s2) / (n1 + n2 - 2)\n    tst  - (yb1 - yb2) / sqrt(s * (1 / n1 + 1 / n2))\n    tst\n}  Define and call on the same line   tstat  - twosam(data$male, data$female); tstat   Several commands and functions can be called on the same line with:   command1; command2; function1; function2     Two ways of defining and calling a function  multy  - function(x, y) {\n    x * y\n    }\n\n# with a binary operator %!%   - function(x, y) {\n    x * y\n    }\n\nx  - 2\ny  - 2\n\nmulty(x, y)\n[1] 4\n\nx %!% y\n[1] 4  Matrix multiplication operator,  %*% , and the outer product matrix operator,  %o% , are other examples of binary operators.  Function in a function  # case 1\narea  - function(f, a, b, eps = 1.0e-06, lim = 10) {\n    fun1  - function(f, a, b, fa, fb, a0, eps, lim, fun) {\n        d  - (a + b)/2\n        h  - (b - a)/4\n        fd  - f(d)\n        a1  - h * (fa + fd)\n        a2  - h * (fd + fb)\n        if(abs(a0 - a1 - a2)   eps || lim == 0)\n            return(a1 + a2)\n        else {\n            return(fun(f, a, d, fa, fd, a1, eps, lim - 1, fun) \n                fun(f, d, b, fd, fb, a2, eps, lim - 1, fun))\n        }\n    }\n    fa  - f(a)\n    fb  - f(b)\n    a0  - ((fa + fb) * (b - a))/2\n    fun1(f, a, b, fa, fb, a0, eps, lim, fun1)\n}\n\n# case 2\nf  - function(x) {\n    y  - 2*x\n    print(x)\n    print(y)\n    print(z)\n}\n\n# case 3\ncube  - function(n) {\n    sq  - function() n*n\n    n*sq()\n}\n\n# case 4\nopen.account  - function(total) {\n    list(\n        deposit = function(amount) {\n            if(amount  = 0)\n                stop( Deposits must be positive!\\n )\n            total  - total + amount\n            cat(amount,  deposited. Your balance is , total,  \\n\\n )\n        },\n        withdraw = function(amount) {\n            if(amount   total)\n                stop( You don\u2019t have that much money!\\n )\n            total  - total - amount\n            cat(amount,  withdrawn. Your balance is , total,  \\n\\n )\n        },\n        balance = function() {\n            cat( Your balance is , total,  \\n\\n )\n        }\n    )\n}\n\nross  - open.account(100)\nrobert  - open.account(200)\n\nross$withdraw(30)\nross$balance()\nrobert$balance()\nross$deposit(50)\nross$balance()\nross$withdraw(500)  Name  Add,  modify, and remove (with  names(x)  - NA or 0 ) names.   names() .  rownames() .  colnames() .  dimnames() .   Customizing startup  Customize the R environment through a directory initialization file; commands that you want to execute every time R is started under your system.  R will always source the Rprofile.site file first.   On Windows, the file is in C:\\Program Files\\R\\R-n.n.n\\etc. You can also place a .Rprofile file in any directory that you are going to run R from or in the user home directory.   Individual users control over their workspace and allows for different startup procedures in different working directories.  If no .Rprofile file is found in the startup directory, then R looks for a .Rprofile file in the user\u2019s home directory and uses that (if it exists) environment variable R_PROFILE_USER is set. The file it points to is used instead of the .Rprofile files.   Function named .First() in either of the two profile files or in the .RData image has a special status: initialize the environment  Sequence in which files are executed is:   Rprofile.site  the user profile  .RData  .First()  .Last(), if defined, is (normally) executed at the very end of the session.   List function and method   methods(class = \"data.frame\") ; list methods associated with the class.  methods(plot) ; list methods specific to the object.   methods(class =  data.frame )\n [1] $             $ -           [             [[            [[ -         \n [6] [ -           aggregate     anyDuplicated as.data.frame as.list      \n[11] as.matrix     by            cbind         coerce        dim          \n[16] dimnames      dimnames -    droplevels    duplicated    edit         \n[21] format        formula       head          initialize    is.na        \n[26] Math          merge         na.exclude    na.omit       Ops          \n[31] plot          print         prompt        rbind         row.names    \n[36] row.names -   rowsum        show          slotsFromS3   split        \n[41] split -       stack         str           subset        summary      \n[46] Summary       t             tail          transform     unique       \n[51] unstack       within       \nsee '?methods' for accessing help and source code\n\nmethods(plot)\n [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*\n [4] plot.default        plot.dendrogram*    plot.density*      \n [7] plot.ecdf           plot.factor*        plot.formula*      \n[10] plot.function       plot.hclust*        plot.histogram*    \n[13] plot.HoltWinters*   plot.isoreg*        plot.lm*           \n[16] plot.medpolish*     plot.mlm*           plot.ppr*          \n[19] plot.prcomp*        plot.princomp*      plot.profile.nls*  \n[22] plot.R6*            plot.raster*        plot.spec*         \n[25] plot.stepfun        plot.stl*           plot.table*        \n[28] plot.ts             plot.tskernel*      plot.TukeyHSD*     \nsee '?methods' for accessing help and source code  Difference:   plot() ; a generic method.  plot.  ; a specific method such as  plot.ts()  for example.", 
            "title": "10, Writing you own Functions"
        }, 
        {
            "location": "/An Introduction to R/#11-statistical-models-in-r", 
            "text": "Regression  y  - 1:10\nx  - 1:10\n\na  - lm(y  sub x)\na\nCall:\nlm(formula = y  sub x)\n\nCoefficients:\n(Intercept)            x  \n  1.123e-15    1.000e+00  \n\n\n# no intercept, through the origin\nb  - lm(y  sub 0 + x)\nb\nCall:\nlm(formula = y  sub 0 + x)\n\nCoefficients:\nx  \n1  \n\n\n# no intercept\nc  - lm(y  sub -1 + x)\nc\nCall:\nlm(formula = y  sub -1 + x)\n\nCoefficients:\nx  \n1  \n\n\nd  - lm(y  sub x - 1)\nd\nCall:\nlm(formula = y  sub x - 1)\n\nCoefficients:\nx  \n1  \n\n\n# log\ne  - lm(log(y)  sub x)\ne\nCall:\nlm(formula = log(y)  sub x)\n\nCoefficients:\n(Intercept)            x  \n     0.2432       0.2304  \n\n\n# quadratic\nf  - lm(y  sub 1 + x + I(x^2))\nf\nCall:\nlm(formula = y  sub 1 + x + I(x^2))\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n  1.123e-15    1.000e+00    5.699e-18  \n\n\n# polynomial\ng  - lm(y  sub X + poly(x, 2))\n\n\n\n# weighted regression\nfm1  - lm(y  sub x, data = dummy, weight = 1 / w^2)\n\n\n# and more\nnew.model  - update(old.model, new.formula)\n\nfm05  - lm(y  sub x1 + x2 + x3 + x4 + x5, data = production)\n\nfm6  - update(fm06, .  sub . + x6)\nsmf6  - update(fm6, sqrt(,)  sub .)  Explore the results   summary(regression results) .  vcov() ; variance-covariance matrix.  aov(formula, data = data.frame) .  anova(fitted.model.1, fitted.model.2, ...)  and many more.   anova, coeficient, coef, deviance, residuals, effects, formula, model, kappa, labels, plot, predict, proj, projection  Stepwise Regression  Select a suitable model by adding or dropping variables and preserving hierarchies. The best model with the smallest AIC (Akaike\u2019s Information Criterion) is discovered with the search.  Generalized least squares  gls, binomial, logit, probit, log, cloglog, gaussian, identity, log, inverse, gamma, identity, inverse, log, inverse.gaussian, 1/mu^2, identity, inverse, log, poisson, identity, log, sqrt, quasi-likelihood, logit, probit \\sub  fitted.model  - glm(formula, famili=family.generator, data = data.frame)  Nonlinear least squares  nls \\sub   nlm(function)   Maximum likehood  ml \\sub  When errors are not normal.   ml(function)   Mixed model   nlme  package.   Local Approximating Regression  Nonparametric local regression function.   lrf  - lowess(x, y) .   Robust regression  MASS  package.  Additive model   acepack  package.  mda  package.  gam  package.  mgcv  package.   Tree-based model  decision, classification tree, random forest \\sub   rpart  package.  tree  package.", 
            "title": "11, Statistical models in R"
        }, 
        {
            "location": "/An Introduction to R/#12-graphical-procedures", 
            "text": "www.statmethods.net/advgraphs  Graphic, packages   lattice  package.  ggplot2  package.  grid  package.  ggobi ,  rgl  packages; for interactive graphics, 3D, and surfaces.  and many more.   Basic plot   plot() .  boxplot() ..  lines(x,y) ; add a line to a basic plot.  pairs() ; multivariate, pairwise scatterplot matrix.   coplot(a  sub b | c) ; scatter plot of a  b given c, a factor vector (levels)  coplot(a  sub v | c + d) .  pie() .  hist(x) ; where  nclass = n  and  breaks = b .  barplot() ; can be horizontal or vertical.  dotchart(x, ...) ; a case of bar chart.  and many more with lots of options.   qq plot   qqnorm(x) .  qqline(x) .  qqplot(x, y) ; comparison.   Picture   image(x, y, z, ...) ; grid of rectangles with colors corresponding to the values in z.  contour(x, y, z, ...) ; z add contour lines (even to an existing plot).  persp(x, y, z, ...) ; perspective plots of a surface over the x\u2013y plane.   Graphic arguments and parameters  www.statmethods.net/advgraphs/parameters  x1  - rnorm(1000, 0.4, 0.8)\nx2  - rnorm(1000, 0.0, 1.0)\nx3  - rnorm(1000, -1.0, 1.0)\nhist(x1, width = 0.33, offset = 0.00, col =  blue , xlim = c(-4,4),\n     main =  Histogram of x1, x2   x3 ,\n     xlab =  x1 - blue, x2 - red, x3 - green )\nhist(x2, width = 0.33, offset = 0.33, col =  red , add = TRUE)\nhist(x3, width = 0.33, offset = 0.66, col =  green , add = TRUE)   add = TRUE ; superimpose a plot on another plot.  axes = FALSE ; no axes.  axis(side,...) ; 1 to 4, bottom, left, top, right.  log = \"x\", \"y\", \"xy\" ; difference scale.  type = \"p\"  (points);  \"l\"  (lines),  \"b\"  (p+l),  \"o\"  (l+p),  \"h\"  (vertical lines from points to the zero axis),  \"s\"  (step-function),  \"n\"  (not plotting).  xlab = \"bla\" .  ylab = \"bla\" .  main = \"bla\" .  sub = \"bla\" .  title(main, sub) .  points(x, y) ; add points on top of  plot() .  text(x, y, labels,...) ; add text to each point.  plot(x, t, type = \"n\"); text(x, y, names) ; replace the dot with text.  abline(a, b) ; add a line.  abline(0, 1, lty = 3) .  abline(coef(fm)) .  abline(coef(fm1), col = \"red\") .  abline(h = y) ; or  h = value ; add a horizontal line.  abline(v = x) ; or  v = value ; add a vertical line.  abline(lm.obj) .  legend(x, y, legend, ...) ; add a legend at a specified position.  fill = v ; add a vector of the same length as a legend.  lty = 2 ; line style.  lwd = 1.5 ; line width.  pch = 0 ; dot style.  par() ; list of permanent graphic parameters.  par(c(\"col\", \"lty\")) ; limit the list of parameters.  par(col = 4, lty = 2) ; set the parameters for all plots.  plot(x, y, pch = \"+\") ; will set a temporary parameter inside a plot.  pch = \"+\" ,  pch = 4 .  col = \"red\" ; dot color.  col =  \"red\" .  col.axis = \"red\" .  col.lab = \"red\" .  col.main = \"red\" .  col.sub = \"red\" .  font = \"red\" .  font.axis = \"red\" .  font.lab = \"red\" .  font.main = \"red\" .  font.sub = \"red\" .  adj = -0.1 ; adjust the text to the plotting position (-1 -0.5 0 0.5 1), from left to right, 0 being the center.  cex = 1.5 ; character 50% larger.  cex.axis = 1.5 .  cex.lab = 1.5 .  cex.main = 1.5 .  cex.sub = 1.5 .  lab = c(5, 7, 12) ; the first two numbers are the desired number of tick intervals on the x and y axes respectively. The third number is the desired length of axis labels.  las = 1 ; orientation of axis labels (text and numbers). 0 means always parallel to axis, 1 means always horizontal, and 2 means always perpendicular to the axis.  mgp = c(3, 1, 0) ; position of the axis components.  tck = 0.01 ; length of tick marks.  xaxs = \"r\" ; axis style.  xaxs = \"i\" ; inside.  mai = c(1, 0.5, 0.5, 0) ; margins in inches around the plot.  mar = c(4, 2, 2, 1) ; in text lines.  and many more.   R allows you to create an n by m array of plots on a single page:   mfcol = c(3.2) ; 3 rows, 2 columns, 6 plotting areas.  mfrow = c(3,2) ; idem but filled by rows.  omi = c(1, 0.5, 0.5, 0) ; margins between plots.  oma = c(1, 0.5, 0.5, 0) ; margins outside plots.  mfg = c(2, 2, 3, 2) ; position of the current figure in a multiple figure environment. The first two numbers are the row and column of the current figure; the last two are the total number of rows and columns in the multiple figure array; in other words, 2nd row and 2nd column in a 3x2 panel.  fig = c(4, 9, 1, 4) / 10 ; position of the current figure on the page. Values are the positions of the left, right, bottom and top edges, respectively, as a percentage of the page measured from the bottom left corner.  and many more.   Geometric shapes   polygon(x, y, ...) ; x, y are vectors containing the coordinates of the vertices of the polygon.   Graphic help   help(plotmath) .  example(plotmath) .  demo(plotmath) .  help(Hershey) .  demo(Hershey) .  help(Japanese) .  demo(Japanese) .   Graphic text and mouse  Leave graphic marks and texts.   locator(n, type) ; select with mouse a max of n locations to mark on the graph (left click, right click to stop).  text(locator(1), \"Outlier\", adj=0) ; select with mouse a location to add a string on the graph.  identify(x, y) ; add a label to a dot with mouse.  identify(x, y, labels) .  identify(x, y, \"yes\") .   Graphic device   split.screen() ; FALSE or number of regions within the current device which can, to some extent, be treated as separate graphics devices. It is useful for generating multiple plots on a single device.  layout() ; divides the device up into as many rows and columns as there are in matrix mat.   Graphic device driver  Open a special graphics window.   X11() ; under UNIX.  windows() ; under Windows.  win.printer() .  win.metafile() .    quartz() ; under OS X.  dev.new() ; returns the return value of the device opened, usually invisible NULL.  grid()  adds an rectangular grid to an existing plot.  postscript() ; starts the graphics device driver for producing PostScript graphics.  postscript(\"file.ps\", horizontal=FALSE, height=5, pointsize=10) .  postscript(\"plot1.eps\", horizontal=FALSE, onefile=FALSE, height=8, width=6, pointsize=10) .    pdf() ; starts the graphics device driver for producing PDF graphics.  png() ; idem.  jpeg() ; idem.  tiff() ; idem.  bitmap() ; idem.  dev.off() ; shuts down the specified (by default the current) device.  dev.set() ; dev.set makes the specified device the active device.  dev.list() ; returns the numbers of all open devices.  dev.next() ,  dev.prev() ; return the number and name of the next / previous device in the list of devices.  graphics.off(); terminate all devices.", 
            "title": "12, Graphical Procedures"
        }, 
        {
            "location": "/An Introduction to R/#13-packages", 
            "text": "install.package() ; on the computer.  library() ; load it.  search() ; check what is loaded.  loadedNamespaces() ; idem.  help.start() ; start the HTML help system, package section.   Find out about packages:  CRAN.R-project.org  www.bioconductor.org  www.omegahat.org", 
            "title": "13, Packages"
        }, 
        {
            "location": "/An Introduction to R/#14-os-facilities", 
            "text": "Manage files with Linux or Windows or RStudio.", 
            "title": "14, OS Facilities"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/", 
            "text": "Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques\n\n\nForeword\n\n\nNotes, leads, and ideas on what R can do. \nREF: reference(s) to the book.\n From Springer, 2014.\n\n\n\n\nCONTENT\n\n\nLe logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques\n\n\nA, Presentation\n\n\nB, Datasets\n\n\nPart 1, Basics\n\n\nChapter 1, Basic Concept, Organizing Data\n\n\nChapter 2, Import-Export and Producing Data\n\n\nChapter 3, Data Manipulation\n\n\nChapter 4, R Documentation\n\n\nChapter 5, Techniques for Plotting Graphics\n\n\nChapter 6, Initiation to R Programming\n\n\nChapter 7, Session Management\n\n\nPart 2, Mathematics and Basic Statistics\n\n\nChapter 8, Basic Mathematics, matrix algebra, integration, optimization\n\n\nChapter 9, Descriptive Statistics\n\n\nChapter 10, Random Variables, Laws, and Simulation\n\n\nChapter 11, Confidence Intervals and Hypothesis Testing\n\n\nChapter 12, Simple and Multiple Linear Regression\n\n\nChapter 13, Elementary Variance Analysis\n\n\nAppendix, Installing the R Software and Packages\n\n\nAnswers to the Exercises\n\n\n\n\n\n\n\n\n\n\n\n\nA, Presentation\n\n\nGUI\n\n\n\n\n\n\nRcommander; \nRcmdr\n pachage.\n\n\n\n\n\n\nThe R Commander: A Basic-Statistics GUI for R\n\n\n\n\n\n\nREF: p.3-5\n\n\nB, Datasets\n\n\n\n\nDatasets.\n\n\n\n\nPart 1, Basics\n\n\nChapter 1, Basic Concept, Organizing Data\n\n\nEditors\n\n\n\n\nRStudio.\n\n\nTinn-R.\n\n\nJGR.\n\n\nEmacs/ESS.\n\n\n\n\nData entry\n\n\n\n\nx \n- 2\n.\n\n\n2 -\n 2\n.\n\n\n\n\nExponent\n\n\n\n\nexp(1)\n.\n\n\n\n\nLogarithm\n\n\n\n\nlog(3)\n.\n\n\nlog(x = 3)\n.\n\n\nlog(x = 3, base(exp(1))\n.\n\n\nlog(3, exp(1))\n.\n\n\n\n\nFactorial\n\n\n\n\nfactorial(2)\n.\n\n\n\n\nFind out about an object, a variable (\nis.\n)\n\n\n\n\nis.character()\n.\n\n\nis.vector()\n.\n\n\nis.character()\n.\n\n\nis.character()\n\n\nand many more.\n\n\n\n\nConvert (\nas.\n)\n\n\n\n\nas.character()\n.\n\n\nas.raw()\n.\n\n\nas.date()\n.\n\n\nand many more.\n\n\n\n\nArray and matrix\n\n\nmatrix(1:12, nrow = 4, ncol = 3, byrow = FALSE)\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\nmatrix(1:12, nrow = 4, ncol = 3, byrow = TRUE)\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n\narray(1:12, dim = c(2, 2, 3))\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 3\n\n     [,1] [,2]\n[1,]    9   11\n[2,]   10   12\n\n\n\n\nVector\n\n\n\n\nvec \n- c(1.1, 2.2, 3.5)\n.\n\n\nvec \n- 1:3\n.\n\n\n\n\nSequence\n\n\n\n\nseq(1:3)\n.\n\n\n1:3\n.\n\n\n\n\nList\n\n\nc(1:3) # vector\n[1] 1 2 3\n\n# vs\n\nlist(1:3) # list\n[[1]]\n[1] 1 2 3\n\n\n\n\nData Frame\n\n\ntable, tabular\n\n\n\n\ndata.frame(name = c(), name = c(), name = c(), etc)\n; each column is a vector with a name.\n\n\n\n\nTime series\n\n\n ts(1:10, frequency=4, start=c(1959,2))\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         1    2    3\n1960    4    5    6    7\n1961    8    9   10     \n\n\n\n\nClass and mode\n\n\ntype, data, variable, object\n\n\n\n\nmode()\n.\n\n\nclass()\n; mode != class.\n\n\ntypeof()\n; type of storage.\n\n\n\n\nChapter 2, Import-Export and Producing Data\n\n\nimport, export, i/o\n\n\nInput data from files\n\n\nread.table(file = path/file.txt, header = TRUE, sep= \n\\t\n, dec=\n.\n, row.names = 1)\n\n\n\n\n\n\nattach(data)\n; dataset is attached to .GlobalEnv.\n\n\nsearch()\n; search objects in .GlobalEnv, including attached dataset,\n\n\ndetach(data)\n.\n\n\n\n\nMore about .GlobalEnv in \nChapter 7, Session Management\n.\n\n\nRead .csv and .tsv\n\n\n\n\nread.csv()\n.\n\n\nread.csv2()\n.\n\n\nread.delim()\n.\n\n\nread.delim2()\n.\n\n\nand many more.\n\n\n\n\nRead text files\n\n\n\n\nread.ftable(\"file.txt\", row.var.names = c(...), col.vars = list())\n.\n\n\nscan(\"file.txt\", skip = 9, nlines = 1, what = \"\", dec = \"\")\n.\n\n\nand many more.\n\n\n\n\nRead software files\n\n\n\n\n.sav\n; SPSS.\n\n\nread.spss\n.\n\n\n\n\n\n\n.mtp\n; Minitab.\n\n\nread.mtp\n.\n\n\n\n\n\n\n.xpt\n; SAS en data.frame.\n\n\nread.xport\n.\n\n\n\n\n\n\n.mat\n; Matlab.\n\n\nreadMat()\n.\n\n\n\n\n\n\nand many other formats and commands.\n\n\n\n\nOuput data and export\n\n\n\n\nlookup.xport()\n; for SAS.\n\n\nwrite.table(data, file=\"file.txt\", sep=\"\\t\")\n.\n\n\nxlsReadWrite()\n.\n\n\nand many more.\n\n\n\n\nMeasure computation time\n\n\n# start the timer\ntmps \n- Sys.time()\n\n# run\ndbsnp \n- read.table(\nfile\n)\n\n# stop the timer\nSys.time() - tmps\n\n\n\n\nProduce by repetition\n\n\nrepeat\n\n\nrep(1:4, reach = 2, len = 10)\n[1] 1 2 3 4 1 2 3 4 1 2\n\n\n\n\nProduce random numbers\n\n\n# generate random numbers between 0 and 1\nrunif(5)\n[1] 0.2424283 0.6140730 0.4824881 0.7263319 0.1381030\n\nrunif(5, min = 2, max = 7)\n[1] 4.588744 5.522278 4.307162 6.248397 3.854982\n\n\n\n\nMore on random number in \nChapter 10, Random Variables, Laws, and Simulation\n.\n\n\nProduce random number following a distribution\n\n\n# generate random numbers following the normal distribution\nrnorm(5)\n[1]  0.8752170  1.3869022 -0.4419174 -0.6129075 -1.6987139\n\n\n\n\nGenerate numbers with other distributions.\n\n\nProduce random number by sampling a population\n\n\nurne \n- 0:9\n\n# 20 draws from 'urne'\nsample(urne, 20, replace = TRUE)\n[1] 5 4 2 2 9 7 4 6 2 2 7 8 3 3 9 6 6 1 1 0\n\n\n\n\nProduce data by manual input (vector-like)\n\n\n\n\nFirst,  \nz \n- scan()\n.\n\n\nSecond, input data in the prompt.\n\n\n\n\nProduce data with a mini-spreadsheet (tabular-like)\n\n\n\n\nx \n- as.data.frame(de(\"\"))\n; open a spreadsheet.\n\n\ndata.entry(\"\")\n; alternatively.\n\n\nInput data; column are variables like in a data frame.\n\n\n\n\n\n\nfix(x)\n; invokes edit on x, then assigns the new (edited) version of x to the user\ns workspace.\n\n\n\n\nList of objects\n\n\n\n\nls()\n; list of objects.\n\n\nrm(list = ls())\n; remove all the objects.\n\n\n\n\nRead from and write to a database\n\n\n\n\nRODBC\n package.\n\n\nodbcConnect()\n.\n\n\nsqlQuery()\n.\n\n\nodbcClose()\n.\n\n\n\n\nREF: p.82-84\n\n\nFile management\n\n\n\n\nfile.choose()\n; open a window.\n\n\n\n\nRead from the clipboard\n\n\n\n\nFirst, copy from a spreadsheet or a table.\n\n\nSecond, \nread.clipboard()\n.\n\n\n\n\nChapter 3, Data Manipulation\n\n\nArithmetics\n\n\nx \n- c(1,2,3)\ny \n- c(4,5,6)\n\nx + y\n[1] 5 7 9\n\n\n\n\nBuilt-in functions\n\n\n\n\nlength(vec)\n; length.\n\n\nsort(vec, decreasing = TRUE)\n; sort.\n\n\nrev(vec)\n; inverse sorting.\n\n\norder(vec)\n; sort a vector according to the names or a list of strings.\n\n\nnames(vec) \n- 1:9\n; attribute names.\n\n\nrank(vec)\n; rank the elements.\n\n\nunique(vec)\n; remove doubles.\n\n\nduplicated(vec)\n; create a TRUE/FALSE vector indicating doubles.\n\n\nx %% y\n;  modulus (x mod y).\n\n\nx %/% y\n; integer division.\n\n\n\n\nDimension functions\n\n\nnumber, row, column, dimension\n\n\n\n\ndim(df)\n.\n\n\nnrow(df)\n.\n\n\nncol(df)\n.\n\n\n\n\nName functions\n\n\n\n\ndimnames(df)\n.\n\n\nnames(df)\n, \ncolnames(df)\n.\n\n\nrownames(df)\n.\n\n\n\n\nMerge functions\n\n\ncombine\n\n\n\n\ncbind()\n.\n\n\nrbind()\n.\n\n\n\n\nREF: p.98\n\n\ny \n- array(1:12, dim = c(4, 3))\n\ny\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\ny \n- cbind(y, c(100, 101, 102, 103))\n\ny\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9  100\n[2,]    2    6   10  101\n[3,]    3    7   11  102\n[4,]    4    8   12  103\n\nmerge(x, y)\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n# idem with rows\n\n\n\n\nREF: p.96-98\n\n\n\n\ngtools\n package.\n\n\nsmartbind(x,y)\n; for two data frames, similar to merge.\n\n\n\n\nApply functions and family\n\n\nexcel, wrangle\n\n\nAmong the most useful function for \nwrangling\n data. Excel-like power. When and how to use them.\n\n\n\n\napply()\n.\n\n\nlapply()\n.\n\n\nsapply()\n.\n\n\nmapply()\n.\n\n\nby()\n.\n\n\nwith()\n.\n\n\nreplicate()\n.\n\n\ntransform()\n.\n\n\nrowSums(df)\n.\n\n\ncolSums(df)\n.\n\n\nrowMeans(df)\n.\n\n\ncolMeans(df)\n.\n\n\nsweep()\n.\n\n\nstack()\n.\n\n\nunstack()\n.\n\n\naggregate()\n\n\n\n\nREF: p.99\n\n\nMore help online such as this article: \nTutorial on the apply family\n, and complementary notes below.\n\n\nSweep functions\n\n\nu\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n# removes pattern '3, 5, 3, 5, etc.'\nsweep(u, MARGIN = 1, STATS = c(3, 5), FUN = \n-\n)\n  V1 V2 V3 V4\n1 -2  2  6 97\n2 -3  1  5 96\n3  0  4  8 99\n4 -1  3  7 98\n\n# divide by a vector\nsweep(u, MARGIN = 2, STATS = c(2, 2, 3, 3), FUN = \n/\n)\n   V1  V2       V3       V4\n1 0.5 2.5 3.000000 33.33333\n2 1.0 3.0 3.333333 33.66667\n3 1.5 3.5 3.666667 34.00000\n4 2.0 4.0 4.000000 34.33333\n\n\n\n\nREF: p.100-101\n\n\nStack functions\n\n\nstack, unstack\n\n\nu\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\nv\n- stack(u)\nv\n   values ind\n1       1  V1\n2       2  V1\n3       3  V1\n4       4  V1\n5       5  V2\n6       6  V2\n7       7  V2\n8       8  V2\n9       9  V3\n10     10  V3\n11     11  V3\n12     12  V3\n13    100  V4\n14    101  V4\n15    102  V4\n16    103  V4\n\nw \n- unstack(v)\nw\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n\n\n\nAggregation functions\n\n\naggregate\n\n\nw\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\nfac \n- c(\na\n, \nb\n, \nb\n, \na\n)\nx \n- cbind(w, fac)\nx\n  V1 V2 V3  V4 fac\n1  1  5  9 100   a\n2  2  6 10 101   b\n3  3  7 11 102   b\n4  4  8 12 103   a\n\naggregate(w, by = list(x$fac), sum)\n  Group.1 V1 V2 V3  V4\n1       a  5 13 21 203\n2       b  5 13 21 203\n\n\n\n\nREF: p.101\n\n\nBoolean \n logical functions\n\n\n\n\nlogical(2)\n; generate two FALSE in a vector; change the length.\n\n\n!logical(2)\n; generate two TRUE.\n\n\nas.logical(vec)\n.\n\n\nis.logical(vec)\n.\n\n\nisTRUE()\n.\n\n\n; AND.\n\n\n; sequential AND.\n\n\n|\n; OR.\n\n\n||\n; sequential OR.\n\n\nPrefer \n over \n, and \n||\n over \n|\n. Assessments go from left to right, and keep on going as long as the conditions are TRUE.\n\n\nxor()\n; exclusive OR.\n\n\nif\n, \nelse\n.\n\n\nany()\n; if one or another is TRUE.\n\n\nall()\n; if all are TRUE.\n\n\nidentical()\n; if all are identical.\n\n\nall.equal()\n.\n\n\n==\n or \nall.equal\n (and \n!=\n) can yield a FALSE because decimal are different on large numbers.\n\n\nall.equal(x, y, tolerance = 10^-6)\n fixes the problem.\n\n\n\n\n\n\nifelse(cond, a, b)\n; if \ncond\n is TRUE, \na\n, else, \nb\n.\n\n\nx \n- c(3:-2); sqrt(ifelse(x \n= 0, x, NA)\n.\n\n\n\n\n\n\n\n\nREF: p.126-127\n\n\nVenn functions\n\n\nA \n- 1:3\nB \n- 3:6\n\nis.element(1, A)\n[1] TRUE\nis.element(4, A)\n[1] FALSE\nis.element(4, B)\n[1] TRUE\n\nall(A %in% B)\n[1] FALSE\nall(B %in% A)\n[1] FALSE\n\nintersect(A, B)\n[1] 3\nunion(A, B)\n[1] 1 2 3 4 5 6\n\nsetdiff(A, B)\n[1] 1 2\nsetdiff(B, A)\n[1] 4 5 6\nintersect(A, B)\n[1] 3\n\n\n\n\nVector functions\n\n\n\n\nvec[2]\n; extract.\n\n\nvec[2:5]\n; extract.\n\n\nvec[c(T, F, T)]\n; extraction with filter.\n\n\nvec[vec \n 4]\n; conditional extraction.\n\n\nvec[vec == 3]\n.\n\n\nvec[which.max(z)]\n; extract the maximum value.\n\n\nvec[which.min(z)]\n.\n\n\nvec \n 4\n; yield a vector of TRUE or FALSE.\n\n\nvec[-2]\n; exclude.\n\n\nvec[-c(1,5)]\n; exclude.\n\n\n\n\nSearch functions\n\n\n\n\nmasque \n- c(TRUE, FALSE)\n.\n\n\nwhich(masque)\n; return the TRUE indices.\n\n\nwhich.min(x)\n; return the index with minimum value.\n\n\nwhich.max(x)\n.\n\n\n\n\nReplace functions\n\n\n\n\nz[c(1, 5)] \n- 1\n; replace value 1 and 5 by 1.\n\n\nz[which.max(z)] \n- 0\n; replace the maximum value.\n\n\nz[z == 0] \n- 8\n; replace zeros and FALSE.\n\n\n\n\nExtend a vector\n\n\n\n\nvecA\n.\n\n\nvecB \n- c(vecA, 4, 5)\n.\n\n\nvecC \n- c(vecA[1:4], 8, 5, vecA[5:9])\n.\n\n\n\n\nMatrix and array\n\n\n\n\nmat[r, c]\n; extract.\n\n\nmat[1, 2]\n.\n\n\nmat[,2]\n; all rows, column 2 only.\n\n\nmat[1, ]\n; all columns, row 1 only.\n\n\nmat[c(1, 3), c(4:5)]\n.\n\n\nmat[, 1, drop = FALSE]\n; avoid making a (horizontal) row with a (vertical) column.\n\n\nmat[ind]\n; matrix index.\n\n\narray[r, c, m]\n; extract.\n\n\n\n\nREF: p.110-113\n\n\nLists\n\n\nchar \n- c(\na\n, \nb\n, \nc\n)\nnumb \n- c(1, 2, 3)\ngreek \n- c(\nalpha\n, \nbeta\n, \ngamma\n)\n\nx \n- list(char, numb, greek)\nx\n[[1]]\n[1] \na\n \nb\n \nc\n\n\n[[2]]\n[1] 1 2 3\n\n[[3]]\n[1] \nalpha\n \nbeta\n  \ngamma\n\n\nnames(x) \n- c(\nchar\n, \nnumb\n, \ngreek\n)\nx\n$char\n[1] \na\n \nb\n \nc\n\n\n$numb\n[1] 1 2 3\n\n$greek\n[1] \nalpha\n \nbeta\n  \ngamma\n\n\nx[2]\n$numb\n[1] 1 2 3\n\nx[[2]][2]\n[1] 2\n\nx$numb[2]\n[1] 2\n\n\n\n\nREF: p.113-115\n\n\nString\n\n\nbla bla bla\n\n[1] \nbla bla bla\n\n\nnoquote(\nbla bla bla\n)\n[1] bla bla bla\n\nsQuote(\nbla bla bla\n)\n[1] \n\u2018bla bla bla\u2019\n\n\ndQuote(\nbla bla bla\n)\n[1] \n\u201cbla bla bla\u201d\n\n\n\n\n\nText\n\n\nwrangle, text, string, character, natural language processing, nlp\n\n\n\n\n\n\nformat()\n; and arguments:\n\n\n\n\ndigits\n.\n\n\ntrim\n.\n\n\ndigit\n.\n\n\nnsmall\n.\n\n\njustify\n.\n\n\nwidth\n.\n\n\nna.encode\n.\n\n\ndecimal.mark\n.\n\n\ndrop0trailing\n.\n\n\nand many more.\n\n\n\n\n\n\n\n\ncat(\"current working dir: \", wd)\n; print the objects, concatenate the representations.\n\n\n\n\nprintf(\"hello %d\\n\", 56)\n; mix text and data; pythonic print.\n\n\nprint(paste0(\"current working dir: \", wd))\n.\n\n\nnchar()\n; number of characters.\n\n\nx[nchar(x) \n 2]\n.\n\n\nx[x %n% c(letters, LETTERS)]\n; retrieve letters, patterns or strings in a text object; alike Venn.\n\n\npaste(ch1, ch2, sep = \"-\")\n; concatenate.\n\n\npaste0(ch1, ch2)\n; concatenate.\n\n\nsubstring(\"abcdef\", first = 1:3, last = 2:4)\n; create subsets \nab\n, \nbc\n, \ncd\n.\n\n\nstrsplit(c(\"\",\"\"), split=\" \")\n; break down a string.\n\n\ngrep(\"i\", c())\n; extract an object index.\n\n\ngsub(\"i\", \"L\", c())\n; substitute.\n\n\nsub()\n; substitute the first occurrence.\n\n\ntolower()\n.\n\n\ntoupper()\n.\n\n\n\n\nDate and time\n\n\nconvert, extract\n\n\n\n\nSys.time()\n.\n\n\ndate()\n.\n\n\nSys.setlocale()\n.\n\n\nas.numeric()\n.\n\n\nstrptime()\n; extract time.\n\n\nas.POSIXlt()\n.\n\n\n\n\nREF: p.120-123\n\n\nCustom functions (two examples)\n\n\na \n- 2\nb \n- -3\n\n# quadratic\nf \n- function (x) { x**2 - 2*x - 2 }\n\nf(a)\n[1] -2\n\nf(b)\n[1] 13\n\n# test\ng \n- function (x, y) {\n  if (x \n= y) {\n    z \n- y - x\n    print(\nx smaller\n)\n  } else {\n    z \n- x - y\n    print(\nx larger\n)\n  }\n} \n\ng(a, b)\n[1] \nx larger\n\n\ng(a, abs(b))\n[1] \nx smaller\n\n\n\n\n\nMore about custom functions in \nChapter 6, Initiation to R Programming\n.\n\n\nLoops structure\n\n\n\n\nfor\n.\n\n\nwhile\n.\n\n\nrepeat\n.\n\n\n\n\n# while\nwhile(x + y \n 7) { x \n- x + y }\n\n# for\nfor (i in 1:4) {\n    if (i == 3) break\n    for (j in 6:8) {\n        if (j == 7) next\n        j \n- i + j\n    }\n}\n\n# repeat\ni \n- 0\nrepeat {\n    i \n- i + 1\n    if (i == 4) break\n}\n\n\n\n\nLoop example\n\n\nIMC \n- function (poids, taille) {\n  imc \n- poids / taille^2\n  names(imc) \n- \nIMC\n\n  return(imc)\n}\n\nIMC(100, 1.90)\n     IMC \n27.70083 \n\np \n- c(100, 101, 95, 97)\nt \n- c(1.90, 1.75, 1.68, 1.92)\n\nIMC(p, t)\n     IMC     \nNA\n     \nNA\n     \nNA\n \n27.70083 32.97959 33.65930 26.31293 \n\nfor (i in 1:4) {\n  print(IMC(data[i,1], data[i,2]))\n}\n     IMC \n27.70083 \n     IMC \n32.97959 \n    IMC \n33.6593 \n     IMC \n26.31293 \n\n\n\n\nLoops increase computation time\n\n\nsystem.time(for (i in 1:1000000) sqrt(i))\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       0.19        0.01        0.22 \n\nsystem.time(sqrt(1:1000000))\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       0.07        0.00        0.07 \n\n\n\n\nREF: p.133-136\n\n\nBinary and decimal\n\n\nconvert\n\n\n\n\nbin2dec()\n.\n\n\ndec2bin()\n.\n\n\n\n\nREF: p.136-144, 147-152\n\n\nChapter 4, R Documentation\n\n\nHelp\n\n\n\n\n?functionname\n.\n\n\nhelp(functionname)\n.\n\n\nhelp.start()\n; user manual in the browser.\n\n\napropos('mean')\n; object with the name \nmean\n.\n\n\n\n\nLibrary\n\n\n\n\ninstall.packages('package')\n; should be done as an administrator or superuser directly in R, not an editor. Once the package is installed, it can be loaded on the PC by any user in R or an editor (such as RStudio).\n\n\nlibrary(package = 'package')\n; load a package.\n\n\nlibrary(help = package)\n; help on a specific package.\n\n\nlibrary(help = base)\n; for example.\n\n\nlibrary(help = utils)\n; for example.\n\n\nlibrary(help = datasets)\n; for example.\n\n\nlibrary(help = graphics)\n; for example.\n\n\nlibrary(help = grDevices)\n; for example.\n\n\n\n\n\n\nlibrary(lib.loc = .Library)\n; find which package (system packages, but not user packages).\n\n\nfind('function')\n; find which package (system).\n\n\ndata()\n; datasets available.\n\n\ndemo()\n; available demos by package.\n\n\ndemo(graphics)\n; give examples.\n\n\nexample(mean)\n; give examples.\n\n\n\n\nResources\n\n\n\n\nnmz\n; search the content of the R functions, package vignette, and task views.\n\n\nR-Project Mailing Lists\n; mailing lists.\n\n\nRSeek\n; search engine.\n\n\nStackOverflow\n; forum.\n\n\nAbcd\nR\n; scripts.\n\n\nCIRAD\n; forum.\n\n\nDeveloppez.net\n; forum.\n\n\nETHZ\n; mailing list.\n\n\nETHZ Manual\n.\n\n\nhttp://a\n; manuals and resources.\n\nstat.ethz.ch/mailman/listinfo/r-annonce\n\n\nETHZ List Info\n.\n\n\nR Programming Wikibooks\n; wiki.\n\n\nCRAN-R\n.\n\n\nCRAN-R Doc\n; PDF and documentation.\n\n\nCRAN-R Views\n; projects.\n\n\nR Journal\n; publication.\n\n\nJournal of Statistical Software\n; publication.\n\n\n\n\nChapter 5, Techniques for Plotting Graphics\n\n\nGraphic windows\n\n\nOf course, these commands are automated in an editor (RStudio).\n\n\n\n\ndev.new()\n; graphic window on Windows.\n\n\nwindows()\n; graphic window on Windows.\n\n\nwin.graph()\n; graphic window on Windows.\n\n\nX11()\n; graphic window on Linux.\n\n\nAdd parameters in the command:\n\n\nwidth = , height =\n.\n\n\npointsize =\n.\n\n\nxpinch = , ypinch =\n; pixels per inch.\n\n\nxpos = , ypos =\n; position of the upper left corner, in pixel.\n\n\n\n\n\n\ndev.set(num)\n; activate window number \nnum\n; there can be several graphic windows.\n\n\ndev.off(num)\n; close a window.\n\n\ngraphics.off()\n; close all windows.\n\n\ndev.list()\n; return window numbers.\n\n\ndev.cur()\n; return the current number, active window.\n\n\ndev.print(png, file = , width = , height = )\n; print.\n\n\nsavePlot(filename = , type = \"png\")\n; save.\n\n\npng(file=\" \", width = , height = )\n; save directly in .png.\n\n\njpeg()\n.\n\n\nbitmap()\n.\n\n\npostscript()\n.\n\n\npdf()\n.\n\n\n\n\n# examples\n\ndev.new(width = 200, height = 200, xpos = 100, ypos = 100)\nNULL\n\ndev.list()\nRStudioGD       png   windows \n        2         3         4 \n\ndev.cur()\nwindows \n      4\n\nhist(runif(100)) # create a graphic\n\n# in the current working directory\ndev.print(png, file = \nmygraph.png\n, width = 480, height = 480)\nsavePlot(filename = \nmygraph.png\n, type = \npng\n)\npdf(file = \nmygraph.pdf\n)\n\ndev.off()\n\n\n\n\nMultiple windows\n\n\n\n\npar(mfrow = c(3, 2))\n; create 6 graphic boxes, 3 rows X 2 columns.\n\n\n\n\nREF: p.166-167\n\n\ndev.new()\nmat \n- matrix(c(2, 3, 0, 1, 0, 0, 0, 4, 0, 0, 0, 5), 4, 3, byrow = TRUE)\nlayout(mat)\n\n\ndev.new()\nlayout(mat, widths = c(1, 5, 14), heights = c(1,2,4,1))\nlayout.show(5)\n\n\n\n\nREF: p.168\n\n\nDraw on a graphic\n\n\n\n\nsegments(x0 = 0, y0 = 0, x1 = 1, y1 = 1)\n; draw lines on a plot.\n\n\nlines(x = c(1,0), y = c(0,1))\n.\n\n\nabline(h = 0, v = 0)\n; add a line to a plot.\n\n\nabline(a = 1, b = 1)\n.\n\n\n\n\nRandom numbers\n\n\nx \n- runif(12)\nx\n[1] 0.03344539 0.22711659 0.66696650 0.02840671 0.61067995 0.92957527\n[7] 0.26962190 0.60013387 0.04831111 0.12603905 0.41913598 0.13142315\n\n# ranking\ni \n- order(x)\ni\n[1]  4  1  9 10 12  2  7 11  8  5  3  6\n\n# reorder\nx \n- x[i]\nx\n[1] 0.02840671 0.03344539 0.04831111 0.12603905 0.13142315 0.22711659\n[7] 0.26962190 0.41913598 0.60013387 0.61067995 0.66696650 0.92957527\n\n\n\n\nMore on random number in \nChapter 10, Random Variables, Laws, and Simulation\n.\n\n\nExamples\n\n\n\n\nexample(polygon)\n; see examples.\n\n\ncurve(x**3 - 3*x, from = -2, to = 2)\n; trace a curve according to a function.\n\n\nhist(rnorm(10000), prob = TRUE, breaks = 100)\n.\n\n\nplot(runif(7), type = \"h\", axes = F); box(lty = \"1373\")\n\n\nplot(1:10, runif(10), type = \"l\", col = \"orangered\")\n\n\n\n\nHelp\n\n\n\n\ncolors()[grep(\"orange\", colors())]\n; list of tones.\n\n\n\n\nColors\n\n\n\n\nrgb(red = 26, green = 204, blue = 76, maxColorValue = 255)\n; return the code.\n\n\nrgb(red = 0.1, green = 0.8, blue = 0.3)\n.\n\n\ncol2rgb(\"#1AVV4C\")\n; inversely.\n\n\nR generates 256\u00b3 colors, 15M.\n\n\nA site about colors\n.\n\n\nrainbow(#)\n; show 1, 8, 17 or more colors; R can generate 256\u00b3 colors (over 15M),\n\n\npie(rep(1,200), labels = \"\", col = rainbow(200), border = NA)\n; show the colors.\n\n\nplot(1:40, col = rainbow(n), pch = 19, cex = 2),     grid(col = \"grey50\")\n; show.\n\n\nRColorBrewer\n package.\n\n\n\n\nlibrary('RColorBrewer')\ndisplay.brewer.all()\n\n\n\n\nImages\n\n\n\n\ncaTools\n package to manage images.\n\n\nread.gif()\n.\n\n\nimage()\n.\n\n\n\n\nWrite and add marks on a graphic\n\n\n\n\ntext(coord x, coord y, 'text')\n; write.\n\n\ndemo(plotmath)\n; see examples.\n\n\nmtext('bas', side = 1)\n; write \nbas\n on the graphic box; 4 sides of the box (bottom, left, top, right).\n\n\nlocator()\n; point with the mouse on a graphic to record coordinates; \nesc\n to show the coordinates.\n\nplot(1,1); locate area to be annotated with \ntext()\n.\n\n\ntext(locator(1), label=\"ici\")\n; add a label, one or several occurrences, by pointing the location with the mouse.\n\n\nidentify(occurrence, label)\n; add one or more labels by pointing the location with the mouse. \n\n\n\n\nGraphic parameters and graphic windows\n\n\nmargin, border, box, square, space, height, width, color, text, title, label, mark, font, police, character, language, size, length, size, tick, coordinate, x, y, log, scale, axis, axes, format, alignment, point\n\n\nAdvanced graphic package\n\n\n\n\nrgl\n.\n\n\nlattice\n.\n\n\nggplot2\n.\n\n\n\n\nREF: p.190-201\n\n\nChapter 6, Initiation to R Programming\n\n\nfunction (\nparameters\n) {\n    \nbody\n\n}\n\n\n\n\nlance \n- function (nom) {\n  cat(\nBonjour\n, nom, \n!\n)\n}\n\nlance('allo')\nBonjour Alex !\n\nbonjour \n- function (nom=\nPierre\n, langue=\nfr\n) {\n  cat(switch(langue, fr=\nBonjour\n, esp=\nHola\n, ang=\nHi\n), nom, \n!\n)\n}\n\nbonjour()\nBonjour Pierre !\nbonjour(nom=\nBen\n) # replace the default value.\nBonjour Ben !\nbonjour(langue=\nang\n)\nHi Pierre !\nbonjour(lang = \nang\n) # partial call\nHi Pierre !\nbonjour(l=\nang\n)\nHi Pierre !\nbonjour(l=\na\n)\n Pierre !\n\n\n\n\nREF: p.218-219\n\n\nUnion\n\n\n%union%\n \n- function (A,B) { union(A,B) }\nA \n- c(4,5,2,7)\nB \n- c(2,1,7,3)\n\nA %union% B\n[1] 4 5 2 7 1 3\n\n\n\n\nClass\n\n\nobj \n- 1:10\n\nclass(obj)\n[1] \ninteger\n\n\nclass(obj) \n- \nTheClass\n\n\nclass(obj)\n[1] \nTheClass\n\n\ninherits(obj, \nTheClass\n)\n[1] TRUE\n\n\n\n\nMethods\n\n\nx \n- 1:10\n\nprint.default(x)\n[1] 1 2 3 4 5 6 7 8 9 10\n\n\n\n\nREF: p.227-231\n\n\nCombine and permute\n\n\n\n\ncombinat\n package.\n\n\ncombn(5,3)\n; combine 3 numbers from 1:5.\n\n\nchoose(200,3)\n; choose 3 numbers from 1:200.\n\n\npermn(n,m)\n.\n\n\n\n\nVery time consuming!\n\n\nMore power, speed\n\n\n\n\nThe core of R in programmed in C. Converting a R function into C is easy. C is faster.\n\n\nCall it through API C.\n\n\nWith R graphic interface and C computation speed, it is the best of both world.\n\n\nEasier to set up on Linux or OS X (the OS has default compilers. Use the \nRcpp\n package. \n\n\nOn Windows, there is a need for \nRtools\n. \n\n\nR can be compiled (byte compiler) with the \nRevoScaleR\n package (parallel computing).\n\n\nbigmemory\n, \nff\n, packages; split a matrix into sub-matrices, perform computation and combine the results (parallel computing). \n\n\nUse a multi-core architecture with the \nparallel\n package.\n\n\nHighPerformanceComputing list of packages\n.\n\n\nUse the \nRmpi\n package and the MPI protocol (OpenMPI or mpich2 software).\n\n\nBuild a cluster or a collection of workstations in parallel; the \nsnow\n package\n.\n\n\nsnow\n or Simple Network of Workstation\n.\n\n\nDeveloping parallel programs using snowfall\n.\n\n\n\n\n\n\n\n\nTry parallel computing with a Monte Carlo with the \nparallel\n package\n\n\nmyfunc \n- function(M=1000) {\n    decision \n- 0\n    for (i in 1:M) {\n      x \n- rnorm(100)\n      if (shapiro.test(x)$p \n 0.05) decision \n- decision +1\n    }\nreturn(decision)\n}\n\nsystem.time({\n    M \n- 60000\n    decision \n- myfunc(M)\n    print(decision/M)\n})\n\n\n\n\n\n\nFor a parallel execution.\n\n\nStart menu.\n\n\nType devmgmt.msc\n\n\nUnder Processors in Linux, type \ntop\n in the terminal.\n\n\nThen, type \n1\n in R.\n\n\nEnter \ndetectCores()\n from the \nparallel\n package.\n\n\n\n\nrequire(\nparallel\n)\nsystem.time({\n    nbcores \n- 6 # less than detectCores() - 1\n    M \n- 60000\n    cl \n- makeCluster(nbcores, type = \nPSOCK\n)\n    out \n- clusterCall(cl, myfunc, round(M/nbcores))\n    stopCluster(cl)\n    decision \n- 0\n    for (clus in 1:nbcores) {\n        decision \n- decision + out[[clus]]\n    }\n    print(decision/(round(M/nbcores)*nbcores))\n})\n\n\n\n\n\n\nThe process number (PID) of each computation node (core) in the cluster.\n\n\n\n\nrequire(\nparallel\n)\nSys.getpid()\ncl \n- makeCluster(4,type=\nPSOCK\n)\nout \n- clusterCall(cl, Sys.getpid))\n\n\n\n\nInvolve the graphical card for more power\n\n\n\n\nRun computations with the graphical card, the GPU.\n\n\ngputools\n package\n\n\nCUDA Education \n Training, Accelerate Your Applications \n\n\n\n\n\n\n\n\nREF: p.303-308\n\n\nChapter 7, Session Management\n\n\nwork, session, save, object, instruction, graphic, create, package\n\n\nEnvironment\n\n\n\n\nglobalenv()\n; .GlobalEnv.\n\n\nnew.env()\n; new environment with its own functions, variable, etc.\n\n\nls()\n; list of objects in the environment.\n\n\nobjects()\n; idem.\n\n\nrm()\n; remove one or more objects.\n\n\nrm(list = ls())\n; remove all.\n\n\n.RData; workspace file extension.\n\n\nfile.RData; R file.\n\n\nsave.image(\"file.RData\")\n; save a R workspace to the current working directory. Several workspace can have their own objects.\n\n\nload(\"file.RData\")\n; load a R workspace from the current working directory.\n\n\nload(file.choose())\n; open the current working directory.\n\n\ngetwd()\n; get the current working directory.\n\n\nsetwd()\n; set the current working directory.\n\n\n.Rhistory; history file extension.\n\n\nhistory()\n; consult the R log.\n\n\nsavehistory()\n; save the log to a file.\n\n\nloadhistory()\n; load the log from a file.\n\n\nsearch()\n; list of attached packages.\n\n\nsearchpaths()\n;  list of paths.\n\n\nlibrary()\n; list of packages in memory.\n\n\nrequire(\"packages\")\n, \nlibrary(\"packages\")\n; load a package.\n\n\nattach(data)\n; attach a dataset to .GlobalEnv..\n\n\ndetach(data)\n.\n\n\nls(pos = 1)\n, \nls()\n; object list in database 1.\n\n\nls(pos = 2)\n; object list in database 2.\n\n\nls(pos = match(\"package:datasets\", search()))\n; list datasets.\n\n\nls(data)\n; list object related to the dataset.\n\n\nfix(data)\n; open a spreadsheet with the data.\n\n\nsink(file = \"sortie.txt\")\n; save a file on the current working directory.\n\n\nsink()\n; stop the recording.\n\n\n\n\nFile manipulation\n\n\n\n\nLow-level interface to the computer\ns file system.\n\n\nCreate a file.\n\n\nExecute the command.\n\n\nCheck out the result in the text files themselves.\n\n\nfile.create(\"sorty.txt\", showWarnings = TRUE)\n.\n\n\nfile.exists(\"sorty.txt\")\n.\n\n\nfile.remove(\"sorty.txt\")\n.\n\n\nfile.rename(\"sorty.txt\", \"sorti.txt\")\n.\n\n\nfile.append(\"sorty.txt\", \"sorti2.txt\")\n.\n\n\nfile.copy(\"sorty.txt\", \"sorti2.txt\")\n.\n\n\nfile.symlink(\"sorty.txt\", \"sorti2.txt\")\n.\n\n\nfile.path(\"sorty.txt\")\n.\n\n\nfile.show()\n.\n\n\nlist.files()\n.\n\n\nunlink()\n.\n\n\nbasename()\n.\n\n\npath.expand()\n.\n\n\nlist.files()\n.\n\n\nfile.exists()\n.\n\n\nmemory.size()\n.\n\n\nmemory.limit()\n.\n\n\n\n\nREF: p.320-330\n\n\nMemory management\n\n\n\n\nThe KSysGuard software analyzes memory usage in real time. It is a task manager and performance monitor for UNIX-like OS.\n\n\n\n\nCreate a package\n\n\n\n\nHow to: build a list of R instruction, load a dataset, create function, and output the results.\n\n\nRun a series of scripts in batch, run a script at a distance.\n\n\nSet the PATH to an executable Rgui.exe.\n\n\nOpen a R script without opening R.\n\n\nCreate a runthis script, apply \nchmod\n to use and execute it. The bash script launch R commands (it must begin with #!/in/bash).\n\n\nand much more.\n\n\n\n\nREF: p.332-338\n\n\nPart 2, Mathematics and Basic Statistics\n\n\nChapter 8, Basic Mathematics, matrix algebra, integration, optimization\n\n\nMath functions\n\n\nREF: p.342\n\n\nMatrix calculation\n\n\nA \n- matrix(c(2,3,5,4), nrow = 2, ncol = 2)\nB \n- matrix(c(1,2,2,7), nrow = 2, ncol = 2)\n\nA\n     [,1] [,2]\n[1,]    2    5\n[2,]    3    4\n\nB\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    7\n\nA + B\n     [,1] [,2]\n[1,]    3    7\n[2,]    5   11\n\nA - B\n     [,1] [,2]\n[1,]    1    3\n[2,]    1   -3\n\n# scalar multiplication\nA * B\n     [,1] [,2]\n[1,]    2   10\n[2,]    6   28\n\n# matrix multiplication\nA %*% B\n     [,1] [,2]\n[1,]   12   39\n[2,]   11   34\n\n# scalar multiplication\na \n- 10\na * A\n     [,1] [,2]\n[1,]   20   50\n[2,]   30   40\n\n# transpose\nt(A)\n     [,1] [,2]\n[1,]    2    3\n[2,]    5    4\n\n# inverse\nsolve(B)\n           [,1]       [,2]\n[1,]  2.3333333 -0.6666667\n[2,] -0.6666667  0.3333333\n\nsolve(A) %*% B\n           [,1]      [,2]\n[1,]  0.8571429  3.857143\n[2,] -0.1428571 -1.142857\n\nt(A) * B\n     [,1] [,2]\n[1,]    2    6\n[2,]   10   28\n\nx \n- seq(1,4)\ny \n- seq(4,7)\n\nx\n[1] 1 2 3 4\ny\n[1] 4 5 6 7\n\nouter(x, y, FUN = \n*\n)\n     [,1] [,2] [,3] [,4]\n[1,]    4    5    6    7\n[2,]    8   10   12   14\n[3,]   12   15   18   21\n[4,]   16   20   24   28\n\n# Kronecker\nkronecker(A, B)\n     [,1] [,2] [,3] [,4]\n[1,]    2    4    5   10\n[2,]    4   14   10   35\n[3,]    3    6    4    8\n[4,]    6   21    8   28\n\n# triangle\nlower.tri(A)\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,]  TRUE FALSE\n\nlower.tri(A, diag = TRUE)\n     [,1]  [,2]\n[1,] TRUE FALSE\n[2,] TRUE  TRUE\n\nlower.tri(A) * A\n     [,1] [,2]\n[1,]    0    0\n[2,]    3    0\n\n# diagonal\ndiag(A)\n[1] 2 4\n\nsum(diag(A))\n[1] 6\n\n# kappa\nkappa(A, exact = TRUE)\n[1] 7.582401\n\n# matrix reduction\nscale(A, scale = FALSE)\n     [,1] [,2]\n[1,] -0.5  0.5\n[2,]  0.5 -0.5\nattr(,\nscaled:center\n)\n[1] 2.5 4.5\n\nscale(A, center = FALSE, scale = apply(A, 2, sd))\n         [,1]     [,2]\n[1,] 2.828427 7.071068\n[2,] 4.242641 5.656854\nattr(,\nscaled:scale\n)\n[1] 0.7071068 0.7071068\n\n# eigenvalue\neigen(A)\n$values\n[1]  7 -1\n\n$vectors\n           [,1]       [,2]\n[1,] -0.7071068 -0.8574929\n[2,] -0.7071068  0.5144958\n\n# singular value vector\nsvd(A)\n$d\n[1] 7.285383 0.960828\n\n$u\n           [,1]       [,2]\n[1,] -0.7337222 -0.6794496\n[2,] -0.6794496  0.7337222\n\n$v\n           [,1]       [,2]\n[1,] -0.4812092  0.8766058\n[2,] -0.8766058 -0.4812092\n\n# Cholesky\nchol2inv(A) \n          [,1]     [,2]\n[1,]  0.640625 -0.15625\n[2,] -0.156250  0.06250\n\n# QR\nqr(A)\n$qr\n           [,1]      [,2]\n[1,] -3.6055513 -6.101702\n[2,]  0.8320503 -1.941451\n\n$rank\n[1] 2\n\n$qraux\n[1] 1.554700 1.941451\n\n$pivot\n[1] 1 2\n\nattr(,\nclass\n)\n[1] \nqr\n\n\n\n\n\nIntegral calculus\n\n\nintegration\n\n\nmyf \n- function(x) { exp(-x^2 / 2) / sqrt(2  *pi) }\n\nintegrate(myf, lower = -Inf, upper = Inf)$value\n[1] 1\n\n\n\n\nDifferential calculus\n\n\nderivative\n\n\nD(expression(sin(cos(x + y^2))), \nx\n)\n-(cos(cos(x + y^2)) * sin(x + y^2))\n\nf \n- deriv(\nsub\nx^2, \nx\n, TRUE)\nf(3)\n[1] 9\nattr(,\ngradient\n)\n     x\n[1,] 6\n\n\n\n\n\n\nnumDeriv\n package.\n\n\ngrad()\n; first-degree derivative.\n\n\nhessian()\n; second-degree derivative.\n\n\nand many more.\n\n\n\n\n\n\n\n\nOptimisation\n\n\nlinear, programming, constraints, min-max\n\n\n# compute a 1-variable min, max (y \nsub\nx)\n\noptimize(function(x) { cos(x^2) }, lower = 0, upper = 2, maximum = FALSE)\n\nsource('\n/sub\n/.active-rstudio-document', echo = TRUE)\n\n\n\n\n\n\nnlm()\n; compute a 2-variable min, max (z \nx + y).\n\n\nnlminb()\n; add contraints on x and y, add parameters \nupper\n and \nlower\n.\n\n\noptim()\n.\n\n\nconstrOptim()\n.\n\n\nand many more.\n\n\n\n\nUnit root\n\n\nunitroot(f=function(x) { cos(x^2) }, lower = 0,upper = 2,tol = 0.00001)$root\n\npolyroot(x(3, -8, 1)) # for p(x) = 3 - 8x + x\u00b2\n\n\n\n\n\n\ncummax()\n.\n\n\ncummin()\n.\n\n\ncumprod()\n.\n\n\ncumsum()\n.\n\n\nand many more.\n\n\n\n\nChapter 9, Descriptive Statistics\n\n\nFactor, levels, labels\n\n\n\n\nfactor(c())\n\n\nas.factor()\n.\n\n\nis.factor()\n.\n\n\nlevels(var) \n- c()\n.\n\n\nlabels(var) \n- c()\n.\n\n\nlevels(var)\n; output the levels.\n\n\nlabels(var)\n; output the labels.\n\n\nnlevels(var)\n; output the number of levels.\n\n\n\n\nmydata \n- factor(mydata,\n    levels = c(1,2,3),\n    labels = c(\nred\n, \nblue\n, \ngreen\n)\n    ) \n\nmydata \n- ordered(mydata,\n    levels = c(1,3, 5),\n    labels = c(\nLow\n, \nMedium\n, \nHigh\n)\n    ) \n\n\n\n\nNames\n\n\n\n\nnames(var) \n- c()\n; add names to a vector, data frame, list.\n\n\ncolnames(var) \n- c()\n; idem.\n\n\nrownames()\n; left-most column.\n\n\ndimnames()\n; add names to an array.\n\n\n\n\nOrder\n\n\n\n\nsort(vec, decreasing = TRUE)\n.\n\n\nrev(vec)\n; inverse sorting.\n\n\norder(vec)\n; sort a vector with names or a list of strings.\n\n\nordered(vec)\n.\n\n\nas.ordered()\n.\n\n\nis.ordered()\n.\n\n\nand many more.\n\n\n\n\nConsult keywords \narithmetics\n and \nrandom numbers\n, where ordering data is commonly used.\n\n\nConvert (\nas.\n)\n\n\n\n\nis.integer()\n.\n\n\nas.integer()\n.\n\n\nas.double()\n.\n\n\nis.double()\n.\n\n\nis.numeric()\n.\n\n\nas.numeric()\n.\n\n\nis.character()\n.\n\n\nas.character()\n.\n\n\nand many more.\n\n\n\n\nTable, proportion table\n\n\ntabular, comparison, 2-dimensional, 2, two, dimensions\n\n\n\n\ntable(var1, var2)\n; 2-dimensional view; cross table.\n\n\nas.table(var)\n; convert.\n\n\ncut()\n; divide the range into intervals.\n\n\ntable(cut(x, res$breaks, include.lowest = TRUE))\n.\n\n\n\n\n\n\naddmargins(x, FUN = sum, quiet = TRUE)\n; add a column or row with sums, means, etc.\n\n\nread.ftable()\n; frequencies.\n\n\ntablefreq \n- mytable / sum(mytable)\n.\n\n\nmargin.table(tablefreq, 1)\n; margin, right and bottom.\n\n\ntablefreq[ ,ncol()]\n; extract the total.\n\n\ntablefreq[nrow(), ]\n; extract the total.\n\n\n\n\n\n\nprop.table(mytable, 1)\n; percentage view; 1, row sum = 100%.\n\n\nprop.table(mytable, 2)\n; percentage view; 2, row sum = 100%.\n\n\nwhich.max(table)\n; find the max, min, mean, etc.\n\n\n\n\nDescriptive statistics\n\n\n\n\nmean(x)\n.\n\n\nmedian(x)\n.\n\n\nquantile(x, probs = c(0.1, 0.9))\n.\n\n\nprobs = 1:10 / 10\n.\n\n\n\n\n\n\nmax(x)\n.\n\n\nmin(x)\n.\n\n\ndiff(range(x))\n.\n\n\nIQR(x)\n.\n\n\nvar.pop(x)\n, \nvar(x)\n.\n\n\nsd.pop(x)\n, \nsd(x)\n.\n\n\nco.var(x)\n.\n\n\nmad(x)\n; absolute deviation from the median.\n\n\nmean(abs(x - mean(x)))\n.\n\n\n\n\n\n\nskew(x)\n.\n\n\nkurt(x)\n.\n\n\nchisq.test()\n.\n\n\nround()\n.\n\n\nsum()\n.\n\n\nnrow()\n.\n\n\nncol()\n.\n\n\ncor(var1, var2)\n.\n\n\nmethod = \"kendall\", \"spearman\"\n\n\n\n\n\n\nrank()\n.\n\n\nrgrs\n package .\n\n\ncramer.v()\n.\n\n\n\n\n\n\n\n\nREF: p.378-379\n\n\nGraphic descriptive statistics\n\n\n\n\nplot()\n.\n\n\ndotchart(table(), col = c(\"\", \"\", \"\" ,\"\") ,pch = , main = , lcolor = )\n.\n\n\nbarplot()\n.\n\n\nbarplot(var, col = , pareto = TRUE)\n.\n\n\nbarplot(sort(table(var)), TRUE))\n.\n\n\nbarplot(xlim = , width = , space = , names.arg = , legend = , density = , ylab = , lwd = )\n.\n\n\npie()\n.\n\n\npoints(barplot(), cumsum(var), type=\"l\")\n.\n\n\nboxplot()\n.\n\n\nstem()\n.\n\n\naplpack\n package.\n\n\nstem.left()\n\n\n\n\n\n\nhist()\n.\n\n\nsegments()\n.\n\n\n\n\nREF: p.392-394, 397-398, 400-410\n\n\nChapter 10, Random Variables, Laws, and Simulation\n\n\n\n\nx %% m\n; modulo or modulus.\n\n\n\n\nRandomness\n\n\n\n\nrunif(1)\n; generate a pseudo-random number between 0 and 1.\n\n\nset.seed()\n; \nshuffle the dice!\n.\n\n\nx \n- function() { runif(1) }\n; generate random numbers, following the uniform distribution.\n\n\nrnorm(1)\n; generate a random numbers, following the normal distribution.\n\n\ngenerate random numbers with a randomness function and \ncurve()\n.\n\n\n\n\nx \n- function() { rnorm(1, 7, 1) }\n# avg = 7, sd = 1, 1 obs\n\ncurve(rnorm(x, 7, 1), xlim = c(-1,10))\n\nplot(density(rnorm(1000, 7, 1)),xlim = c(-1, 10), main=\nDensity Curve\n)\n\n\n\n\nREF: p.422-423\n\n\nmean(runif(1))\n[1] 0.6586903\n\nmean(runif(10))\n[1] 0.5196868\n\nmean(runif(100))\n[1] 0.5345603\n\nmean(runif(1000))\n[1] 0.5042301\n\nmean(runif(10000))\n[1] 0.5021896\n\n# getting closer to 0.5 as the sample increases\n\n\n\n\n\n\nConvergenceConcepts\n package to view the law of great numbers.\n\n\n\n\nDice\n\n\nREF: p.430-431\n\n\nBootstrap\n\n\nREF: p.436\n\n\nLaws\n\n\nREF: p.437-447\n\n\nChapter 11, Confidence Intervals and Hypothesis Testing\n\n\nConfidence intervals\n\n\n\n\nUse t-values with at least 30 observation in the sample.\n\n\nWith smaller samples (or larger too), use bootstraps to simulate populations from the \nboot\n package (\nboot()\n and \nboot.ci()\n).\n\n\nFor proportion with large samples, use the \nepitools\n package and \nbinom.approx()\n. With smaller samples, go with \nbinom.test()\n.\n\n\nVariance confidence intervals; test for normality with \nsigma2.test()\n.\n\n\nFor non-parametric sample, again, simulate populations with \nboot()\n and \nboot.ci()\n.\n\n\nFor median, use \nqbinom()\n.\n\n\nFor correlation, use \ncor.test()\n.\n\n\n\n\nREF: p.450-456\n\n\nTest\n\n\n\n\nIn a test, \n\\alpha\n is the signification threshold, \nH_0\n is the tested hypothesis.\n\n\nAverage tests;  compare the theoretical average to a reference value with the \nt.test()\n.\n\n\nCompare two theoretical averages with a \nt.test()\n.\n\n\nCompare pair samples with \nt.test(paired=TRUE)\n.\n\n\nTest variance(s) with ANOVA.\n\n\nCompare a theoretical variance with a reference value with \nsigma2.test()\n.\n\n\nCompare two theoretical variances with \nvar.test()\n.\n\n\nCompare a theoretical proportion with a reference value with \nprop.test()\n.\n\n\nCompare two theoretical proportions with \nprop.test(\n).\n\n\nTest the theoretical correlation coefficient vs a reference value with \ncor.test()\n and \ncor-.test()\n.\n\n\nTest two theoretical correlation coefficients vs a reference value with \ncor.test.2.sample()\n.\n\n\nTest of independence or chi\u00b2 with \nchisq.test()\n.\n\n\nYates chi\u00b2, adjustment chi\u00b2 with \nchisq.test()\n.\n\n\nFisher test with \nfisher.test()\n.\n\n\nAdequacy test or Shapiro-Walk test with \nshapiro.test()\n.\n\n\nPositional test or sign test or, median sign test with \nprop.test()\n and \nbinom.test()\n.\n\n\nMedian sign test for two independent samples with \nchisq.test()\n and \nfisher.test()\n.\n\n\nSign test for two matching samples with \nprop.test()\n and \nbinom.test()\n.\n\n\nWilcoxon rank test or Mann-Whitney test for two independent samples with \nwilcox.test()\n.\n\n\nWilcoxon test for two matching samples with \nwilcox.test()\n.\n\n\n\n\nREF: p.459-488\n\n\nChapter 12, Simple and Multiple Linear Regression\n\n\nRegression\n\n\n\n\nlm(y \nsub\nx\n.\n\n\nlm(y \nsub\n0 + x)\n; no intercept.\n\n\nmodel \n- lm(y \nsub\nx)\n; run the regression.\n\n\nsummary(model)\n; extract the results.\n\n\nplot(y \nsub\nx)\n; plot the results.\n\n\nabline(model)\n; add a line on the observations.\n\n\nconfint(model)\n; confidence intervals; 95% or 2.5% on both sides.\n\n\ncoefficients(model)\n; extract one or several coefficient.\n\n\nmodel$coefficients\n.\n\n\nmodel$call\n.\n\n\nmodel$residuals\n.\n\n\nanova(model)\n.\n\n\npredict(model, data.frame(LWT = prediction), interval = \"prediction\")\n\n\nand many more.\n\n\n\n\nprediction, result, extraction, residual\n\n\nREF: p.498-499\n\n\nNormality\n\n\nhistogram, test, residual, quantile-quantile, quantile, qq\n \n\n\npar(mfrow=c(1,2))\nhist(residuals(model), main = \nHistogram\n)\n\n\n\n\n\n\nqqnorm(resid(model), datax = TRUE)\n; quantile-quantile.\n\n\nqqplot()\n.\n\n\nqqline()\n.\n\n\nplot(model, 1:6, col.smooth = \"red\")\n; 6 graphics.\n\n\njarque.bera.test(residuals(model))\n; from the \ntseries\n package.\n\n\ndwtest()\n; Durbin-Watson test from the \nlmtest\n package.\n\n\n\n\nREF: p.502-503\n\n\nCorrelation\n\n\ntest, explanatory variable interaction, colinearity,  best subset\n\n\n\n\npairs(newdata, lower.panel = panel.smooth, upper.panel = add.cor)\n.\n\n\n\n\nREF: p.506\n\n\nModel improvement\n\n\n\n\nVariables selection.\n\n\nBest subset, leaps and bounds.\n\n\nForward selection.\n\n\nBackward selection.\n\n\nStepwise selection.\n\n\nResidual analysis.\n\n\nand many more.\n\n\n\n\nREF: p.511-535\n\n\nPolynomial regression\n\n\nREF: p.535-540\n\n\nChapter 13, Elementary Variance Analysis\n\n\nanova, repeated measure, between, within, inspection, hypothesis, comparison, factor, table, parameter, repeated\n\n\nREF: p.542-571\n\n\nAppendix, Installing the R Software and Packages\n\n\ninstallation, package\n\n\ninstall.packages('package')\n.\n\n\n\n\nBe sure to launch RStudio as an administrator or a superuser to install a package in R (not in RStudio); the package is accessible to all users. Then load the package in R or RStudio.\n\n\nThen attach the package to the work session with \nlibrary('package')\n or \nrequire('packages')\n.\n\n\n\n\nAnswers to the Exercises\n\n\nREF:  p.625-674", 
            "title": "Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#a-presentation", 
            "text": "GUI    Rcommander;  Rcmdr  pachage.    The R Commander: A Basic-Statistics GUI for R    REF: p.3-5", 
            "title": "A, Presentation"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#b-datasets", 
            "text": "Datasets.", 
            "title": "B, Datasets"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#part-1-basics", 
            "text": "", 
            "title": "Part 1, Basics"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-1-basic-concept-organizing-data", 
            "text": "Editors   RStudio.  Tinn-R.  JGR.  Emacs/ESS.   Data entry   x  - 2 .  2 -  2 .   Exponent   exp(1) .   Logarithm   log(3) .  log(x = 3) .  log(x = 3, base(exp(1)) .  log(3, exp(1)) .   Factorial   factorial(2) .   Find out about an object, a variable ( is. )   is.character() .  is.vector() .  is.character() .  is.character()  and many more.   Convert ( as. )   as.character() .  as.raw() .  as.date() .  and many more.   Array and matrix  matrix(1:12, nrow = 4, ncol = 3, byrow = FALSE)\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\nmatrix(1:12, nrow = 4, ncol = 3, byrow = TRUE)\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n\narray(1:12, dim = c(2, 2, 3))\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n, , 3\n\n     [,1] [,2]\n[1,]    9   11\n[2,]   10   12  Vector   vec  - c(1.1, 2.2, 3.5) .  vec  - 1:3 .   Sequence   seq(1:3) .  1:3 .   List  c(1:3) # vector\n[1] 1 2 3\n\n# vs\n\nlist(1:3) # list\n[[1]]\n[1] 1 2 3  Data Frame  table, tabular   data.frame(name = c(), name = c(), name = c(), etc) ; each column is a vector with a name.   Time series   ts(1:10, frequency=4, start=c(1959,2))\n     Qtr1 Qtr2 Qtr3 Qtr4\n1959         1    2    3\n1960    4    5    6    7\n1961    8    9   10       Class and mode  type, data, variable, object   mode() .  class() ; mode != class.  typeof() ; type of storage.", 
            "title": "Chapter 1, Basic Concept, Organizing Data"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-2-import-export-and-producing-data", 
            "text": "import, export, i/o  Input data from files  read.table(file = path/file.txt, header = TRUE, sep=  \\t , dec= . , row.names = 1)   attach(data) ; dataset is attached to .GlobalEnv.  search() ; search objects in .GlobalEnv, including attached dataset,  detach(data) .   More about .GlobalEnv in  Chapter 7, Session Management .  Read .csv and .tsv   read.csv() .  read.csv2() .  read.delim() .  read.delim2() .  and many more.   Read text files   read.ftable(\"file.txt\", row.var.names = c(...), col.vars = list()) .  scan(\"file.txt\", skip = 9, nlines = 1, what = \"\", dec = \"\") .  and many more.   Read software files   .sav ; SPSS.  read.spss .    .mtp ; Minitab.  read.mtp .    .xpt ; SAS en data.frame.  read.xport .    .mat ; Matlab.  readMat() .    and many other formats and commands.   Ouput data and export   lookup.xport() ; for SAS.  write.table(data, file=\"file.txt\", sep=\"\\t\") .  xlsReadWrite() .  and many more.   Measure computation time  # start the timer\ntmps  - Sys.time()\n\n# run\ndbsnp  - read.table( file )\n\n# stop the timer\nSys.time() - tmps  Produce by repetition  repeat  rep(1:4, reach = 2, len = 10)\n[1] 1 2 3 4 1 2 3 4 1 2  Produce random numbers  # generate random numbers between 0 and 1\nrunif(5)\n[1] 0.2424283 0.6140730 0.4824881 0.7263319 0.1381030\n\nrunif(5, min = 2, max = 7)\n[1] 4.588744 5.522278 4.307162 6.248397 3.854982  More on random number in  Chapter 10, Random Variables, Laws, and Simulation .  Produce random number following a distribution  # generate random numbers following the normal distribution\nrnorm(5)\n[1]  0.8752170  1.3869022 -0.4419174 -0.6129075 -1.6987139  Generate numbers with other distributions.  Produce random number by sampling a population  urne  - 0:9\n\n# 20 draws from 'urne'\nsample(urne, 20, replace = TRUE)\n[1] 5 4 2 2 9 7 4 6 2 2 7 8 3 3 9 6 6 1 1 0  Produce data by manual input (vector-like)   First,   z  - scan() .  Second, input data in the prompt.   Produce data with a mini-spreadsheet (tabular-like)   x  - as.data.frame(de(\"\")) ; open a spreadsheet.  data.entry(\"\") ; alternatively.  Input data; column are variables like in a data frame.    fix(x) ; invokes edit on x, then assigns the new (edited) version of x to the user s workspace.   List of objects   ls() ; list of objects.  rm(list = ls()) ; remove all the objects.   Read from and write to a database   RODBC  package.  odbcConnect() .  sqlQuery() .  odbcClose() .   REF: p.82-84  File management   file.choose() ; open a window.   Read from the clipboard   First, copy from a spreadsheet or a table.  Second,  read.clipboard() .", 
            "title": "Chapter 2, Import-Export and Producing Data"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-3-data-manipulation", 
            "text": "Arithmetics  x  - c(1,2,3)\ny  - c(4,5,6)\n\nx + y\n[1] 5 7 9  Built-in functions   length(vec) ; length.  sort(vec, decreasing = TRUE) ; sort.  rev(vec) ; inverse sorting.  order(vec) ; sort a vector according to the names or a list of strings.  names(vec)  - 1:9 ; attribute names.  rank(vec) ; rank the elements.  unique(vec) ; remove doubles.  duplicated(vec) ; create a TRUE/FALSE vector indicating doubles.  x %% y ;  modulus (x mod y).  x %/% y ; integer division.   Dimension functions  number, row, column, dimension   dim(df) .  nrow(df) .  ncol(df) .   Name functions   dimnames(df) .  names(df) ,  colnames(df) .  rownames(df) .   Merge functions  combine   cbind() .  rbind() .   REF: p.98  y  - array(1:12, dim = c(4, 3))\n\ny\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\ny  - cbind(y, c(100, 101, 102, 103))\n\ny\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9  100\n[2,]    2    6   10  101\n[3,]    3    7   11  102\n[4,]    4    8   12  103\n\nmerge(x, y)\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n# idem with rows  REF: p.96-98   gtools  package.  smartbind(x,y) ; for two data frames, similar to merge.   Apply functions and family  excel, wrangle  Among the most useful function for  wrangling  data. Excel-like power. When and how to use them.   apply() .  lapply() .  sapply() .  mapply() .  by() .  with() .  replicate() .  transform() .  rowSums(df) .  colSums(df) .  rowMeans(df) .  colMeans(df) .  sweep() .  stack() .  unstack() .  aggregate()   REF: p.99  More help online such as this article:  Tutorial on the apply family , and complementary notes below.  Sweep functions  u\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\n# removes pattern '3, 5, 3, 5, etc.'\nsweep(u, MARGIN = 1, STATS = c(3, 5), FUN =  - )\n  V1 V2 V3 V4\n1 -2  2  6 97\n2 -3  1  5 96\n3  0  4  8 99\n4 -1  3  7 98\n\n# divide by a vector\nsweep(u, MARGIN = 2, STATS = c(2, 2, 3, 3), FUN =  / )\n   V1  V2       V3       V4\n1 0.5 2.5 3.000000 33.33333\n2 1.0 3.0 3.333333 33.66667\n3 1.5 3.5 3.666667 34.00000\n4 2.0 4.0 4.000000 34.33333  REF: p.100-101  Stack functions  stack, unstack  u\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\nv - stack(u)\nv\n   values ind\n1       1  V1\n2       2  V1\n3       3  V1\n4       4  V1\n5       5  V2\n6       6  V2\n7       7  V2\n8       8  V2\n9       9  V3\n10     10  V3\n11     11  V3\n12     12  V3\n13    100  V4\n14    101  V4\n15    102  V4\n16    103  V4\n\nw  - unstack(v)\nw\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103  Aggregation functions  aggregate  w\n  V1 V2 V3  V4\n1  1  5  9 100\n2  2  6 10 101\n3  3  7 11 102\n4  4  8 12 103\n\nfac  - c( a ,  b ,  b ,  a )\nx  - cbind(w, fac)\nx\n  V1 V2 V3  V4 fac\n1  1  5  9 100   a\n2  2  6 10 101   b\n3  3  7 11 102   b\n4  4  8 12 103   a\n\naggregate(w, by = list(x$fac), sum)\n  Group.1 V1 V2 V3  V4\n1       a  5 13 21 203\n2       b  5 13 21 203  REF: p.101  Boolean   logical functions   logical(2) ; generate two FALSE in a vector; change the length.  !logical(2) ; generate two TRUE.  as.logical(vec) .  is.logical(vec) .  isTRUE() .  ; AND.  ; sequential AND.  | ; OR.  || ; sequential OR.  Prefer   over  , and  ||  over  | . Assessments go from left to right, and keep on going as long as the conditions are TRUE.  xor() ; exclusive OR.  if ,  else .  any() ; if one or another is TRUE.  all() ; if all are TRUE.  identical() ; if all are identical.  all.equal() .  ==  or  all.equal  (and  != ) can yield a FALSE because decimal are different on large numbers.  all.equal(x, y, tolerance = 10^-6)  fixes the problem.    ifelse(cond, a, b) ; if  cond  is TRUE,  a , else,  b .  x  - c(3:-2); sqrt(ifelse(x  = 0, x, NA) .     REF: p.126-127  Venn functions  A  - 1:3\nB  - 3:6\n\nis.element(1, A)\n[1] TRUE\nis.element(4, A)\n[1] FALSE\nis.element(4, B)\n[1] TRUE\n\nall(A %in% B)\n[1] FALSE\nall(B %in% A)\n[1] FALSE\n\nintersect(A, B)\n[1] 3\nunion(A, B)\n[1] 1 2 3 4 5 6\n\nsetdiff(A, B)\n[1] 1 2\nsetdiff(B, A)\n[1] 4 5 6\nintersect(A, B)\n[1] 3  Vector functions   vec[2] ; extract.  vec[2:5] ; extract.  vec[c(T, F, T)] ; extraction with filter.  vec[vec   4] ; conditional extraction.  vec[vec == 3] .  vec[which.max(z)] ; extract the maximum value.  vec[which.min(z)] .  vec   4 ; yield a vector of TRUE or FALSE.  vec[-2] ; exclude.  vec[-c(1,5)] ; exclude.   Search functions   masque  - c(TRUE, FALSE) .  which(masque) ; return the TRUE indices.  which.min(x) ; return the index with minimum value.  which.max(x) .   Replace functions   z[c(1, 5)]  - 1 ; replace value 1 and 5 by 1.  z[which.max(z)]  - 0 ; replace the maximum value.  z[z == 0]  - 8 ; replace zeros and FALSE.   Extend a vector   vecA .  vecB  - c(vecA, 4, 5) .  vecC  - c(vecA[1:4], 8, 5, vecA[5:9]) .   Matrix and array   mat[r, c] ; extract.  mat[1, 2] .  mat[,2] ; all rows, column 2 only.  mat[1, ] ; all columns, row 1 only.  mat[c(1, 3), c(4:5)] .  mat[, 1, drop = FALSE] ; avoid making a (horizontal) row with a (vertical) column.  mat[ind] ; matrix index.  array[r, c, m] ; extract.   REF: p.110-113  Lists  char  - c( a ,  b ,  c )\nnumb  - c(1, 2, 3)\ngreek  - c( alpha ,  beta ,  gamma )\n\nx  - list(char, numb, greek)\nx\n[[1]]\n[1]  a   b   c \n\n[[2]]\n[1] 1 2 3\n\n[[3]]\n[1]  alpha   beta    gamma \n\nnames(x)  - c( char ,  numb ,  greek )\nx\n$char\n[1]  a   b   c \n\n$numb\n[1] 1 2 3\n\n$greek\n[1]  alpha   beta    gamma \n\nx[2]\n$numb\n[1] 1 2 3\n\nx[[2]][2]\n[1] 2\n\nx$numb[2]\n[1] 2  REF: p.113-115  String  bla bla bla \n[1]  bla bla bla \n\nnoquote( bla bla bla )\n[1] bla bla bla\n\nsQuote( bla bla bla )\n[1]  \u2018bla bla bla\u2019 \n\ndQuote( bla bla bla )\n[1]  \u201cbla bla bla\u201d   Text  wrangle, text, string, character, natural language processing, nlp    format() ; and arguments:   digits .  trim .  digit .  nsmall .  justify .  width .  na.encode .  decimal.mark .  drop0trailing .  and many more.     cat(\"current working dir: \", wd) ; print the objects, concatenate the representations.   printf(\"hello %d\\n\", 56) ; mix text and data; pythonic print.  print(paste0(\"current working dir: \", wd)) .  nchar() ; number of characters.  x[nchar(x)   2] .  x[x %n% c(letters, LETTERS)] ; retrieve letters, patterns or strings in a text object; alike Venn.  paste(ch1, ch2, sep = \"-\") ; concatenate.  paste0(ch1, ch2) ; concatenate.  substring(\"abcdef\", first = 1:3, last = 2:4) ; create subsets  ab ,  bc ,  cd .  strsplit(c(\"\",\"\"), split=\" \") ; break down a string.  grep(\"i\", c()) ; extract an object index.  gsub(\"i\", \"L\", c()) ; substitute.  sub() ; substitute the first occurrence.  tolower() .  toupper() .   Date and time  convert, extract   Sys.time() .  date() .  Sys.setlocale() .  as.numeric() .  strptime() ; extract time.  as.POSIXlt() .   REF: p.120-123  Custom functions (two examples)  a  - 2\nb  - -3\n\n# quadratic\nf  - function (x) { x**2 - 2*x - 2 }\n\nf(a)\n[1] -2\n\nf(b)\n[1] 13\n\n# test\ng  - function (x, y) {\n  if (x  = y) {\n    z  - y - x\n    print( x smaller )\n  } else {\n    z  - x - y\n    print( x larger )\n  }\n} \n\ng(a, b)\n[1]  x larger \n\ng(a, abs(b))\n[1]  x smaller   More about custom functions in  Chapter 6, Initiation to R Programming .  Loops structure   for .  while .  repeat .   # while\nwhile(x + y   7) { x  - x + y }\n\n# for\nfor (i in 1:4) {\n    if (i == 3) break\n    for (j in 6:8) {\n        if (j == 7) next\n        j  - i + j\n    }\n}\n\n# repeat\ni  - 0\nrepeat {\n    i  - i + 1\n    if (i == 4) break\n}  Loop example  IMC  - function (poids, taille) {\n  imc  - poids / taille^2\n  names(imc)  -  IMC \n  return(imc)\n}\n\nIMC(100, 1.90)\n     IMC \n27.70083 \n\np  - c(100, 101, 95, 97)\nt  - c(1.90, 1.75, 1.68, 1.92)\n\nIMC(p, t)\n     IMC      NA       NA       NA  \n27.70083 32.97959 33.65930 26.31293 \n\nfor (i in 1:4) {\n  print(IMC(data[i,1], data[i,2]))\n}\n     IMC \n27.70083 \n     IMC \n32.97959 \n    IMC \n33.6593 \n     IMC \n26.31293   Loops increase computation time  system.time(for (i in 1:1000000) sqrt(i))\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       0.19        0.01        0.22 \n\nsystem.time(sqrt(1:1000000))\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       0.07        0.00        0.07   REF: p.133-136  Binary and decimal  convert   bin2dec() .  dec2bin() .   REF: p.136-144, 147-152", 
            "title": "Chapter 3, Data Manipulation"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-4-r-documentation", 
            "text": "Help   ?functionname .  help(functionname) .  help.start() ; user manual in the browser.  apropos('mean') ; object with the name  mean .   Library   install.packages('package') ; should be done as an administrator or superuser directly in R, not an editor. Once the package is installed, it can be loaded on the PC by any user in R or an editor (such as RStudio).  library(package = 'package') ; load a package.  library(help = package) ; help on a specific package.  library(help = base) ; for example.  library(help = utils) ; for example.  library(help = datasets) ; for example.  library(help = graphics) ; for example.  library(help = grDevices) ; for example.    library(lib.loc = .Library) ; find which package (system packages, but not user packages).  find('function') ; find which package (system).  data() ; datasets available.  demo() ; available demos by package.  demo(graphics) ; give examples.  example(mean) ; give examples.   Resources   nmz ; search the content of the R functions, package vignette, and task views.  R-Project Mailing Lists ; mailing lists.  RSeek ; search engine.  StackOverflow ; forum.  Abcd R ; scripts.  CIRAD ; forum.  Developpez.net ; forum.  ETHZ ; mailing list.  ETHZ Manual .  http://a ; manuals and resources. \nstat.ethz.ch/mailman/listinfo/r-annonce  ETHZ List Info .  R Programming Wikibooks ; wiki.  CRAN-R .  CRAN-R Doc ; PDF and documentation.  CRAN-R Views ; projects.  R Journal ; publication.  Journal of Statistical Software ; publication.", 
            "title": "Chapter 4, R Documentation"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-5-techniques-for-plotting-graphics", 
            "text": "Graphic windows  Of course, these commands are automated in an editor (RStudio).   dev.new() ; graphic window on Windows.  windows() ; graphic window on Windows.  win.graph() ; graphic window on Windows.  X11() ; graphic window on Linux.  Add parameters in the command:  width = , height = .  pointsize = .  xpinch = , ypinch = ; pixels per inch.  xpos = , ypos = ; position of the upper left corner, in pixel.    dev.set(num) ; activate window number  num ; there can be several graphic windows.  dev.off(num) ; close a window.  graphics.off() ; close all windows.  dev.list() ; return window numbers.  dev.cur() ; return the current number, active window.  dev.print(png, file = , width = , height = ) ; print.  savePlot(filename = , type = \"png\") ; save.  png(file=\" \", width = , height = ) ; save directly in .png.  jpeg() .  bitmap() .  postscript() .  pdf() .   # examples\n\ndev.new(width = 200, height = 200, xpos = 100, ypos = 100)\nNULL\n\ndev.list()\nRStudioGD       png   windows \n        2         3         4 \n\ndev.cur()\nwindows \n      4\n\nhist(runif(100)) # create a graphic\n\n# in the current working directory\ndev.print(png, file =  mygraph.png , width = 480, height = 480)\nsavePlot(filename =  mygraph.png , type =  png )\npdf(file =  mygraph.pdf )\n\ndev.off()  Multiple windows   par(mfrow = c(3, 2)) ; create 6 graphic boxes, 3 rows X 2 columns.   REF: p.166-167  dev.new()\nmat  - matrix(c(2, 3, 0, 1, 0, 0, 0, 4, 0, 0, 0, 5), 4, 3, byrow = TRUE)\nlayout(mat)\n\n\ndev.new()\nlayout(mat, widths = c(1, 5, 14), heights = c(1,2,4,1))\nlayout.show(5)  REF: p.168  Draw on a graphic   segments(x0 = 0, y0 = 0, x1 = 1, y1 = 1) ; draw lines on a plot.  lines(x = c(1,0), y = c(0,1)) .  abline(h = 0, v = 0) ; add a line to a plot.  abline(a = 1, b = 1) .   Random numbers  x  - runif(12)\nx\n[1] 0.03344539 0.22711659 0.66696650 0.02840671 0.61067995 0.92957527\n[7] 0.26962190 0.60013387 0.04831111 0.12603905 0.41913598 0.13142315\n\n# ranking\ni  - order(x)\ni\n[1]  4  1  9 10 12  2  7 11  8  5  3  6\n\n# reorder\nx  - x[i]\nx\n[1] 0.02840671 0.03344539 0.04831111 0.12603905 0.13142315 0.22711659\n[7] 0.26962190 0.41913598 0.60013387 0.61067995 0.66696650 0.92957527  More on random number in  Chapter 10, Random Variables, Laws, and Simulation .  Examples   example(polygon) ; see examples.  curve(x**3 - 3*x, from = -2, to = 2) ; trace a curve according to a function.  hist(rnorm(10000), prob = TRUE, breaks = 100) .  plot(runif(7), type = \"h\", axes = F); box(lty = \"1373\")  plot(1:10, runif(10), type = \"l\", col = \"orangered\")   Help   colors()[grep(\"orange\", colors())] ; list of tones.   Colors   rgb(red = 26, green = 204, blue = 76, maxColorValue = 255) ; return the code.  rgb(red = 0.1, green = 0.8, blue = 0.3) .  col2rgb(\"#1AVV4C\") ; inversely.  R generates 256\u00b3 colors, 15M.  A site about colors .  rainbow(#) ; show 1, 8, 17 or more colors; R can generate 256\u00b3 colors (over 15M),  pie(rep(1,200), labels = \"\", col = rainbow(200), border = NA) ; show the colors.  plot(1:40, col = rainbow(n), pch = 19, cex = 2),     grid(col = \"grey50\") ; show.  RColorBrewer  package.   library('RColorBrewer')\ndisplay.brewer.all()  Images   caTools  package to manage images.  read.gif() .  image() .   Write and add marks on a graphic   text(coord x, coord y, 'text') ; write.  demo(plotmath) ; see examples.  mtext('bas', side = 1) ; write  bas  on the graphic box; 4 sides of the box (bottom, left, top, right).  locator() ; point with the mouse on a graphic to record coordinates;  esc  to show the coordinates. \nplot(1,1); locate area to be annotated with  text() .  text(locator(1), label=\"ici\") ; add a label, one or several occurrences, by pointing the location with the mouse.  identify(occurrence, label) ; add one or more labels by pointing the location with the mouse.    Graphic parameters and graphic windows  margin, border, box, square, space, height, width, color, text, title, label, mark, font, police, character, language, size, length, size, tick, coordinate, x, y, log, scale, axis, axes, format, alignment, point  Advanced graphic package   rgl .  lattice .  ggplot2 .   REF: p.190-201", 
            "title": "Chapter 5, Techniques for Plotting Graphics"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-6-initiation-to-r-programming", 
            "text": "function ( parameters ) {\n     body \n}  lance  - function (nom) {\n  cat( Bonjour , nom,  ! )\n}\n\nlance('allo')\nBonjour Alex !\n\nbonjour  - function (nom= Pierre , langue= fr ) {\n  cat(switch(langue, fr= Bonjour , esp= Hola , ang= Hi ), nom,  ! )\n}\n\nbonjour()\nBonjour Pierre !\nbonjour(nom= Ben ) # replace the default value.\nBonjour Ben !\nbonjour(langue= ang )\nHi Pierre !\nbonjour(lang =  ang ) # partial call\nHi Pierre !\nbonjour(l= ang )\nHi Pierre !\nbonjour(l= a )\n Pierre !  REF: p.218-219  Union  %union%   - function (A,B) { union(A,B) }\nA  - c(4,5,2,7)\nB  - c(2,1,7,3)\n\nA %union% B\n[1] 4 5 2 7 1 3  Class  obj  - 1:10\n\nclass(obj)\n[1]  integer \n\nclass(obj)  -  TheClass \n\nclass(obj)\n[1]  TheClass \n\ninherits(obj,  TheClass )\n[1] TRUE  Methods  x  - 1:10\n\nprint.default(x)\n[1] 1 2 3 4 5 6 7 8 9 10  REF: p.227-231  Combine and permute   combinat  package.  combn(5,3) ; combine 3 numbers from 1:5.  choose(200,3) ; choose 3 numbers from 1:200.  permn(n,m) .   Very time consuming!  More power, speed   The core of R in programmed in C. Converting a R function into C is easy. C is faster.  Call it through API C.  With R graphic interface and C computation speed, it is the best of both world.  Easier to set up on Linux or OS X (the OS has default compilers. Use the  Rcpp  package.   On Windows, there is a need for  Rtools .   R can be compiled (byte compiler) with the  RevoScaleR  package (parallel computing).  bigmemory ,  ff , packages; split a matrix into sub-matrices, perform computation and combine the results (parallel computing).   Use a multi-core architecture with the  parallel  package.  HighPerformanceComputing list of packages .  Use the  Rmpi  package and the MPI protocol (OpenMPI or mpich2 software).  Build a cluster or a collection of workstations in parallel; the  snow  package .  snow  or Simple Network of Workstation .  Developing parallel programs using snowfall .     Try parallel computing with a Monte Carlo with the  parallel  package  myfunc  - function(M=1000) {\n    decision  - 0\n    for (i in 1:M) {\n      x  - rnorm(100)\n      if (shapiro.test(x)$p   0.05) decision  - decision +1\n    }\nreturn(decision)\n}\n\nsystem.time({\n    M  - 60000\n    decision  - myfunc(M)\n    print(decision/M)\n})   For a parallel execution.  Start menu.  Type devmgmt.msc  Under Processors in Linux, type  top  in the terminal.  Then, type  1  in R.  Enter  detectCores()  from the  parallel  package.   require( parallel )\nsystem.time({\n    nbcores  - 6 # less than detectCores() - 1\n    M  - 60000\n    cl  - makeCluster(nbcores, type =  PSOCK )\n    out  - clusterCall(cl, myfunc, round(M/nbcores))\n    stopCluster(cl)\n    decision  - 0\n    for (clus in 1:nbcores) {\n        decision  - decision + out[[clus]]\n    }\n    print(decision/(round(M/nbcores)*nbcores))\n})   The process number (PID) of each computation node (core) in the cluster.   require( parallel )\nSys.getpid()\ncl  - makeCluster(4,type= PSOCK )\nout  - clusterCall(cl, Sys.getpid))  Involve the graphical card for more power   Run computations with the graphical card, the GPU.  gputools  package  CUDA Education   Training, Accelerate Your Applications      REF: p.303-308", 
            "title": "Chapter 6, Initiation to R Programming"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-7-session-management", 
            "text": "work, session, save, object, instruction, graphic, create, package  Environment   globalenv() ; .GlobalEnv.  new.env() ; new environment with its own functions, variable, etc.  ls() ; list of objects in the environment.  objects() ; idem.  rm() ; remove one or more objects.  rm(list = ls()) ; remove all.  .RData; workspace file extension.  file.RData; R file.  save.image(\"file.RData\") ; save a R workspace to the current working directory. Several workspace can have their own objects.  load(\"file.RData\") ; load a R workspace from the current working directory.  load(file.choose()) ; open the current working directory.  getwd() ; get the current working directory.  setwd() ; set the current working directory.  .Rhistory; history file extension.  history() ; consult the R log.  savehistory() ; save the log to a file.  loadhistory() ; load the log from a file.  search() ; list of attached packages.  searchpaths() ;  list of paths.  library() ; list of packages in memory.  require(\"packages\") ,  library(\"packages\") ; load a package.  attach(data) ; attach a dataset to .GlobalEnv..  detach(data) .  ls(pos = 1) ,  ls() ; object list in database 1.  ls(pos = 2) ; object list in database 2.  ls(pos = match(\"package:datasets\", search())) ; list datasets.  ls(data) ; list object related to the dataset.  fix(data) ; open a spreadsheet with the data.  sink(file = \"sortie.txt\") ; save a file on the current working directory.  sink() ; stop the recording.   File manipulation   Low-level interface to the computer s file system.  Create a file.  Execute the command.  Check out the result in the text files themselves.  file.create(\"sorty.txt\", showWarnings = TRUE) .  file.exists(\"sorty.txt\") .  file.remove(\"sorty.txt\") .  file.rename(\"sorty.txt\", \"sorti.txt\") .  file.append(\"sorty.txt\", \"sorti2.txt\") .  file.copy(\"sorty.txt\", \"sorti2.txt\") .  file.symlink(\"sorty.txt\", \"sorti2.txt\") .  file.path(\"sorty.txt\") .  file.show() .  list.files() .  unlink() .  basename() .  path.expand() .  list.files() .  file.exists() .  memory.size() .  memory.limit() .   REF: p.320-330  Memory management   The KSysGuard software analyzes memory usage in real time. It is a task manager and performance monitor for UNIX-like OS.   Create a package   How to: build a list of R instruction, load a dataset, create function, and output the results.  Run a series of scripts in batch, run a script at a distance.  Set the PATH to an executable Rgui.exe.  Open a R script without opening R.  Create a runthis script, apply  chmod  to use and execute it. The bash script launch R commands (it must begin with #!/in/bash).  and much more.   REF: p.332-338", 
            "title": "Chapter 7, Session Management"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#part-2-mathematics-and-basic-statistics", 
            "text": "", 
            "title": "Part 2, Mathematics and Basic Statistics"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-8-basic-mathematics-matrix-algebra-integration-optimization", 
            "text": "Math functions  REF: p.342  Matrix calculation  A  - matrix(c(2,3,5,4), nrow = 2, ncol = 2)\nB  - matrix(c(1,2,2,7), nrow = 2, ncol = 2)\n\nA\n     [,1] [,2]\n[1,]    2    5\n[2,]    3    4\n\nB\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    7\n\nA + B\n     [,1] [,2]\n[1,]    3    7\n[2,]    5   11\n\nA - B\n     [,1] [,2]\n[1,]    1    3\n[2,]    1   -3\n\n# scalar multiplication\nA * B\n     [,1] [,2]\n[1,]    2   10\n[2,]    6   28\n\n# matrix multiplication\nA %*% B\n     [,1] [,2]\n[1,]   12   39\n[2,]   11   34\n\n# scalar multiplication\na  - 10\na * A\n     [,1] [,2]\n[1,]   20   50\n[2,]   30   40\n\n# transpose\nt(A)\n     [,1] [,2]\n[1,]    2    3\n[2,]    5    4\n\n# inverse\nsolve(B)\n           [,1]       [,2]\n[1,]  2.3333333 -0.6666667\n[2,] -0.6666667  0.3333333\n\nsolve(A) %*% B\n           [,1]      [,2]\n[1,]  0.8571429  3.857143\n[2,] -0.1428571 -1.142857\n\nt(A) * B\n     [,1] [,2]\n[1,]    2    6\n[2,]   10   28\n\nx  - seq(1,4)\ny  - seq(4,7)\n\nx\n[1] 1 2 3 4\ny\n[1] 4 5 6 7\n\nouter(x, y, FUN =  * )\n     [,1] [,2] [,3] [,4]\n[1,]    4    5    6    7\n[2,]    8   10   12   14\n[3,]   12   15   18   21\n[4,]   16   20   24   28\n\n# Kronecker\nkronecker(A, B)\n     [,1] [,2] [,3] [,4]\n[1,]    2    4    5   10\n[2,]    4   14   10   35\n[3,]    3    6    4    8\n[4,]    6   21    8   28\n\n# triangle\nlower.tri(A)\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,]  TRUE FALSE\n\nlower.tri(A, diag = TRUE)\n     [,1]  [,2]\n[1,] TRUE FALSE\n[2,] TRUE  TRUE\n\nlower.tri(A) * A\n     [,1] [,2]\n[1,]    0    0\n[2,]    3    0\n\n# diagonal\ndiag(A)\n[1] 2 4\n\nsum(diag(A))\n[1] 6\n\n# kappa\nkappa(A, exact = TRUE)\n[1] 7.582401\n\n# matrix reduction\nscale(A, scale = FALSE)\n     [,1] [,2]\n[1,] -0.5  0.5\n[2,]  0.5 -0.5\nattr(, scaled:center )\n[1] 2.5 4.5\n\nscale(A, center = FALSE, scale = apply(A, 2, sd))\n         [,1]     [,2]\n[1,] 2.828427 7.071068\n[2,] 4.242641 5.656854\nattr(, scaled:scale )\n[1] 0.7071068 0.7071068\n\n# eigenvalue\neigen(A)\n$values\n[1]  7 -1\n\n$vectors\n           [,1]       [,2]\n[1,] -0.7071068 -0.8574929\n[2,] -0.7071068  0.5144958\n\n# singular value vector\nsvd(A)\n$d\n[1] 7.285383 0.960828\n\n$u\n           [,1]       [,2]\n[1,] -0.7337222 -0.6794496\n[2,] -0.6794496  0.7337222\n\n$v\n           [,1]       [,2]\n[1,] -0.4812092  0.8766058\n[2,] -0.8766058 -0.4812092\n\n# Cholesky\nchol2inv(A) \n          [,1]     [,2]\n[1,]  0.640625 -0.15625\n[2,] -0.156250  0.06250\n\n# QR\nqr(A)\n$qr\n           [,1]      [,2]\n[1,] -3.6055513 -6.101702\n[2,]  0.8320503 -1.941451\n\n$rank\n[1] 2\n\n$qraux\n[1] 1.554700 1.941451\n\n$pivot\n[1] 1 2\n\nattr(, class )\n[1]  qr   Integral calculus  integration  myf  - function(x) { exp(-x^2 / 2) / sqrt(2  *pi) }\n\nintegrate(myf, lower = -Inf, upper = Inf)$value\n[1] 1  Differential calculus  derivative  D(expression(sin(cos(x + y^2))),  x )\n-(cos(cos(x + y^2)) * sin(x + y^2))\n\nf  - deriv( sub x^2,  x , TRUE)\nf(3)\n[1] 9\nattr(, gradient )\n     x\n[1,] 6   numDeriv  package.  grad() ; first-degree derivative.  hessian() ; second-degree derivative.  and many more.     Optimisation  linear, programming, constraints, min-max  # compute a 1-variable min, max (y  sub x)\n\noptimize(function(x) { cos(x^2) }, lower = 0, upper = 2, maximum = FALSE)\n\nsource(' /sub /.active-rstudio-document', echo = TRUE)   nlm() ; compute a 2-variable min, max (z  x + y).  nlminb() ; add contraints on x and y, add parameters  upper  and  lower .  optim() .  constrOptim() .  and many more.   Unit root  unitroot(f=function(x) { cos(x^2) }, lower = 0,upper = 2,tol = 0.00001)$root\n\npolyroot(x(3, -8, 1)) # for p(x) = 3 - 8x + x\u00b2   cummax() .  cummin() .  cumprod() .  cumsum() .  and many more.", 
            "title": "Chapter 8, Basic Mathematics, matrix algebra, integration, optimization"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-9-descriptive-statistics", 
            "text": "Factor, levels, labels   factor(c())  as.factor() .  is.factor() .  levels(var)  - c() .  labels(var)  - c() .  levels(var) ; output the levels.  labels(var) ; output the labels.  nlevels(var) ; output the number of levels.   mydata  - factor(mydata,\n    levels = c(1,2,3),\n    labels = c( red ,  blue ,  green )\n    ) \n\nmydata  - ordered(mydata,\n    levels = c(1,3, 5),\n    labels = c( Low ,  Medium ,  High )\n    )   Names   names(var)  - c() ; add names to a vector, data frame, list.  colnames(var)  - c() ; idem.  rownames() ; left-most column.  dimnames() ; add names to an array.   Order   sort(vec, decreasing = TRUE) .  rev(vec) ; inverse sorting.  order(vec) ; sort a vector with names or a list of strings.  ordered(vec) .  as.ordered() .  is.ordered() .  and many more.   Consult keywords  arithmetics  and  random numbers , where ordering data is commonly used.  Convert ( as. )   is.integer() .  as.integer() .  as.double() .  is.double() .  is.numeric() .  as.numeric() .  is.character() .  as.character() .  and many more.   Table, proportion table  tabular, comparison, 2-dimensional, 2, two, dimensions   table(var1, var2) ; 2-dimensional view; cross table.  as.table(var) ; convert.  cut() ; divide the range into intervals.  table(cut(x, res$breaks, include.lowest = TRUE)) .    addmargins(x, FUN = sum, quiet = TRUE) ; add a column or row with sums, means, etc.  read.ftable() ; frequencies.  tablefreq  - mytable / sum(mytable) .  margin.table(tablefreq, 1) ; margin, right and bottom.  tablefreq[ ,ncol()] ; extract the total.  tablefreq[nrow(), ] ; extract the total.    prop.table(mytable, 1) ; percentage view; 1, row sum = 100%.  prop.table(mytable, 2) ; percentage view; 2, row sum = 100%.  which.max(table) ; find the max, min, mean, etc.   Descriptive statistics   mean(x) .  median(x) .  quantile(x, probs = c(0.1, 0.9)) .  probs = 1:10 / 10 .    max(x) .  min(x) .  diff(range(x)) .  IQR(x) .  var.pop(x) ,  var(x) .  sd.pop(x) ,  sd(x) .  co.var(x) .  mad(x) ; absolute deviation from the median.  mean(abs(x - mean(x))) .    skew(x) .  kurt(x) .  chisq.test() .  round() .  sum() .  nrow() .  ncol() .  cor(var1, var2) .  method = \"kendall\", \"spearman\"    rank() .  rgrs  package .  cramer.v() .     REF: p.378-379  Graphic descriptive statistics   plot() .  dotchart(table(), col = c(\"\", \"\", \"\" ,\"\") ,pch = , main = , lcolor = ) .  barplot() .  barplot(var, col = , pareto = TRUE) .  barplot(sort(table(var)), TRUE)) .  barplot(xlim = , width = , space = , names.arg = , legend = , density = , ylab = , lwd = ) .  pie() .  points(barplot(), cumsum(var), type=\"l\") .  boxplot() .  stem() .  aplpack  package.  stem.left()    hist() .  segments() .   REF: p.392-394, 397-398, 400-410", 
            "title": "Chapter 9, Descriptive Statistics"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-10-random-variables-laws-and-simulation", 
            "text": "x %% m ; modulo or modulus.   Randomness   runif(1) ; generate a pseudo-random number between 0 and 1.  set.seed() ;  shuffle the dice! .  x  - function() { runif(1) } ; generate random numbers, following the uniform distribution.  rnorm(1) ; generate a random numbers, following the normal distribution.  generate random numbers with a randomness function and  curve() .   x  - function() { rnorm(1, 7, 1) }\n# avg = 7, sd = 1, 1 obs\n\ncurve(rnorm(x, 7, 1), xlim = c(-1,10))\n\nplot(density(rnorm(1000, 7, 1)),xlim = c(-1, 10), main= Density Curve )  REF: p.422-423  mean(runif(1))\n[1] 0.6586903\n\nmean(runif(10))\n[1] 0.5196868\n\nmean(runif(100))\n[1] 0.5345603\n\nmean(runif(1000))\n[1] 0.5042301\n\nmean(runif(10000))\n[1] 0.5021896\n\n# getting closer to 0.5 as the sample increases   ConvergenceConcepts  package to view the law of great numbers.   Dice  REF: p.430-431  Bootstrap  REF: p.436  Laws  REF: p.437-447", 
            "title": "Chapter 10, Random Variables, Laws, and Simulation"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-11-confidence-intervals-and-hypothesis-testing", 
            "text": "Confidence intervals   Use t-values with at least 30 observation in the sample.  With smaller samples (or larger too), use bootstraps to simulate populations from the  boot  package ( boot()  and  boot.ci() ).  For proportion with large samples, use the  epitools  package and  binom.approx() . With smaller samples, go with  binom.test() .  Variance confidence intervals; test for normality with  sigma2.test() .  For non-parametric sample, again, simulate populations with  boot()  and  boot.ci() .  For median, use  qbinom() .  For correlation, use  cor.test() .   REF: p.450-456  Test   In a test,  \\alpha  is the signification threshold,  H_0  is the tested hypothesis.  Average tests;  compare the theoretical average to a reference value with the  t.test() .  Compare two theoretical averages with a  t.test() .  Compare pair samples with  t.test(paired=TRUE) .  Test variance(s) with ANOVA.  Compare a theoretical variance with a reference value with  sigma2.test() .  Compare two theoretical variances with  var.test() .  Compare a theoretical proportion with a reference value with  prop.test() .  Compare two theoretical proportions with  prop.test( ).  Test the theoretical correlation coefficient vs a reference value with  cor.test()  and  cor-.test() .  Test two theoretical correlation coefficients vs a reference value with  cor.test.2.sample() .  Test of independence or chi\u00b2 with  chisq.test() .  Yates chi\u00b2, adjustment chi\u00b2 with  chisq.test() .  Fisher test with  fisher.test() .  Adequacy test or Shapiro-Walk test with  shapiro.test() .  Positional test or sign test or, median sign test with  prop.test()  and  binom.test() .  Median sign test for two independent samples with  chisq.test()  and  fisher.test() .  Sign test for two matching samples with  prop.test()  and  binom.test() .  Wilcoxon rank test or Mann-Whitney test for two independent samples with  wilcox.test() .  Wilcoxon test for two matching samples with  wilcox.test() .   REF: p.459-488", 
            "title": "Chapter 11, Confidence Intervals and Hypothesis Testing"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-12-simple-and-multiple-linear-regression", 
            "text": "Regression   lm(y  sub x .  lm(y  sub 0 + x) ; no intercept.  model  - lm(y  sub x) ; run the regression.  summary(model) ; extract the results.  plot(y  sub x) ; plot the results.  abline(model) ; add a line on the observations.  confint(model) ; confidence intervals; 95% or 2.5% on both sides.  coefficients(model) ; extract one or several coefficient.  model$coefficients .  model$call .  model$residuals .  anova(model) .  predict(model, data.frame(LWT = prediction), interval = \"prediction\")  and many more.   prediction, result, extraction, residual  REF: p.498-499  Normality  histogram, test, residual, quantile-quantile, quantile, qq    par(mfrow=c(1,2))\nhist(residuals(model), main =  Histogram )   qqnorm(resid(model), datax = TRUE) ; quantile-quantile.  qqplot() .  qqline() .  plot(model, 1:6, col.smooth = \"red\") ; 6 graphics.  jarque.bera.test(residuals(model)) ; from the  tseries  package.  dwtest() ; Durbin-Watson test from the  lmtest  package.   REF: p.502-503  Correlation  test, explanatory variable interaction, colinearity,  best subset   pairs(newdata, lower.panel = panel.smooth, upper.panel = add.cor) .   REF: p.506  Model improvement   Variables selection.  Best subset, leaps and bounds.  Forward selection.  Backward selection.  Stepwise selection.  Residual analysis.  and many more.   REF: p.511-535  Polynomial regression  REF: p.535-540", 
            "title": "Chapter 12, Simple and Multiple Linear Regression"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-13-elementary-variance-analysis", 
            "text": "anova, repeated measure, between, within, inspection, hypothesis, comparison, factor, table, parameter, repeated  REF: p.542-571", 
            "title": "Chapter 13, Elementary Variance Analysis"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#appendix-installing-the-r-software-and-packages", 
            "text": "installation, package  install.packages('package') .   Be sure to launch RStudio as an administrator or a superuser to install a package in R (not in RStudio); the package is accessible to all users. Then load the package in R or RStudio.  Then attach the package to the work session with  library('package')  or  require('packages') .", 
            "title": "Appendix, Installing the R Software and Packages"
        }, 
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#answers-to-the-exercises", 
            "text": "REF:  p.625-674", 
            "title": "Answers to the Exercises"
        }, 
        {
            "location": "/Intermediate_R/", 
            "text": "1, Conditionals and Control Flow\n\n\n2, Loops\n\n\n3, Functions\n\n\n4, The \napply\n Family\n\n\n5, Utilities\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, Conditionals and Control Flow\n\n\nEquality\n\n\n# Comparison of logicals\nTRUE == FALSE\n\n\n\n\n## [1] FALSE\n\n\n\n# Comparison of numerics\n(-6 * 14) != (17 - 101)\n\n\n\n\n## [1] FALSE\n\n\n\n# Comparison of character strings\n'useR' == 'user'\n\n\n\n\n## [1] FALSE\n\n\n\n# Compare a logical with a numeric\nTRUE == 1\n\n\n\n\n## [1] TRUE\n\n\n\nGreater and less than\n\n\n# Comparison of numerics\n(-6*5 + 2) \n= (-10 + 1)\n\n\n\n\n## [1] FALSE\n\n\n\n# Comparison of character strings\n'raining' \n= 'raining dogs'\n\n\n\n\n## [1] TRUE\n\n\n\n# Comparison of logicals\nTRUE \n FALSE\n\n\n\n\n## [1] TRUE\n\n\n\nCompare vectors\n\n\n# The linkedin and facebook vectors\nlinkedin \n- c(16, 9, 13, 5, 2, 17, 14)\nfacebook \n- c(17, 7, 5, 16, 8, 13, 14)\n\n# Popular days\nlinkedin \n 15\n\n\n\n\n## [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n# Quiet days\nlinkedin \n= 5\n\n\n\n\n## [1] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\n\n\n# LinkedIn more popular than Facebook\nlinkedin \n facebook\n\n\n\n\n## [1] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n\n\n\nCompare matrices\n\n\nviews \n- matrix(c(linkedin, facebook), nrow = 2, byrow = TRUE)\n\n# When does views equal 13?\nviews == 13\n\n\n\n\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n# When is views less than or equal to 14?\nviews \n= 14\n\n\n\n\n##       [,1] [,2] [,3]  [,4] [,5]  [,6] [,7]\n## [1,] FALSE TRUE TRUE  TRUE TRUE FALSE TRUE\n## [2,] FALSE TRUE TRUE FALSE TRUE  TRUE TRUE\n\n\n\n# How often does facebook equal or exceed linkedin times two?\nsum(facebook \n= linkedin * 2)\n\n\n\n\n## [1] 2\n\n\n\n and \n|\n\n\n# The linkedin and last variable\nlinkedin \n- c(16, 9, 13, 5, 2, 17, 14)\nlast \n- tail(linkedin, 1)\n\n# Is last under 5 or above 10?\nlast \n 5 | last \n 10\n\n\n\n\n## [1] TRUE\n\n\n\n# Is last between 15 (exclusive) and 20 (inclusive)?\nlast \n 15 \n last \n= 20\n\n\n\n\n## [1] FALSE\n\n\n\n# Is last between 0 and 5 or between 10 and 15?\n(last \n 0 \n last \n 5) | (last \n 10 \n last \n 15)\n\n\n\n\n## [1] TRUE\n\n\n\n and \n|\n (2)\n\n\n# linkedin exceeds 10 but facebook below 10\nlinkedin \n 10 \n facebook \n 10\n\n\n\n\n## [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n# When were one or both visited at least 12 times?\nlinkedin \n= 12 | facebook \n= 12\n\n\n\n\n## [1]  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n\n\n\n# When is views between 11 (exclusive) and 14 (inclusive)?\nviews \n 11 \n views \n= 14\n\n\n\n\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6] [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE TRUE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE TRUE\n\n\n\nBlend it all together\n\n\nlibrary(XLConnectJars)\nlibrary(XLConnect)\n\n\n\n\n# Select the second column, named day2, from li_df: second\nsecond \n- li_df$day2\n\n# Build a logical vector, TRUE if value in second is extreme: extremes\nextremes \n- (second \n 25 | second \n 5)\n\n# Count the number of TRUEs in extremes\nsum(extremes)\n\n\n\n\n## [1] 16\n\n\n\nThe \nif\n statement\n\n\n# Variables related to your last day of recordings\nmedium \n- 'LinkedIn'\nnum_views \n- 14\n\n# Examine the if statement for medium\nif (medium == 'LinkedIn') {\n  print('Showing LinkedIn information')\n}\n\n\n\n\n## [1] \"Showing LinkedIn information\"\n\n\n\n# Write the if statement for num_views\nif (num_views \n 15) {\n    print('You\\'re popular!')\n}\n\n\n\n\nAdd an \nelse\n\n\n# Variables related to your last day of recordings\nmedium \n- 'LinkedIn'\nnum_views \n- 14\n\n# Control structure for medium\nif (medium == 'LinkedIn') {\n  print('Showing LinkedIn information')\n} else {\n    print('Unknown medium')\n}\n\n\n\n\n## [1] \"Showing LinkedIn information\"\n\n\n\n# Control structure for num_views\nif (num_views \n 15) {\n  print('You\\'re popular!')\n} else {\n    print('Try to be more visible!')\n}\n\n\n\n\n## [1] \"Try to be more visible!\"\n\n\n\nCustomize further: \nelse if\n\n\n# Variables related to your last day of recordings\nmedium \n- 'LinkedIn'\nnum_views \n- 14\n\n# Control structure for medium\nif (medium == 'LinkedIn') {\n  print('Showing LinkedIn information')\n} else if (medium == 'Facebook') {\n  print('Showing Facebook information')\n} else {\n  print('Unknown medium')\n}\n\n\n\n\n## [1] \"Showing LinkedIn information\"\n\n\n\n# Control structure for num_views\nif (num_views \n 15) {\n  print('You\\'re popular!')\n} else if (num_views \n 10 | num_views \n= 15) {\n  print('Your number of views is average')\n} else {\n  print('Try to be more visible!')\n}\n\n\n\n\n## [1] \"Your number of views is average\"\n\n\n\nTake control!\n\n\n# Variables related to your last day of recordings\nli \n- 15\nfb \n- 9\n\n# Code the control-flow construct\nif (li \n= 15 \n fb \n= 15) {\n    sms \n- 2*(li + fb)\n} else if (li \n 10 \n fb \n 10) {\n    sms \n- (li + fb)/2\n} else {\n    sms \n- li + fb\n}\n\n# Print the resulting sms to the console\nsms\n\n\n\n\n## [1] 24\n\n\n\n2, Loops\n\n\nWrite a \nwhile\n loop\n\n\n# Initialize the speed variable\nspeed \n- 64\n\n# Code the while loop\nwhile (speed \n 30) {\n  print('Slow down!')\n  speed \n- speed - 7\n}\n\n\n\n\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n\n\n\n# Print out the speed variable\nspeed\n\n\n\n\n## [1] 29\n\n\n\nThrow in more conditionals\n\n\n# Initialize the speed variable\nspeed \n- 64\n\n# Extend/adapt the while loop\nwhile (speed \n 30) {\n  print(paste('Your speed is ', speed))\n  if (speed \n 48) {\n    print('Slow down big time!')\n    speed \n- speed - 11\n  } else {\n        print('Slow down!')\n        speed \n- speed - 6\n  }\n}\n\n\n\n\n## [1] \"Your speed is  64\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  53\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  42\"\n## [1] \"Slow down!\"\n## [1] \"Your speed is  36\"\n## [1] \"Slow down!\"\n\n\n\nStop the \nwhile\n loop: \nbreak\n\n\n# Initialize the speed variable\nspeed \n- 88\nwhile (speed \n 30) {\n  print(paste('Your speed is',speed))\n    # Break the while loop when speed exceeds 80\n  if (speed \n 80) {\n    break\n  } else if (speed \n 48) {\n    print('Slow down big time!')\n    speed \n- speed - 11\n  } else {\n    print('Slow down!')\n    speed \n- speed - 6\n  }\n}\n\n\n\n\n## [1] \"Your speed is 88\"\n\n\n\nBuild a \nwhile\n loop from scratch\n\n\nstrsplit\n; split up in a vector that contains separate letters.\n\n\n# Initialize i\ni \n- 1\n\n# Code the while loop\nwhile (i \n= 10) {\n  print(i * 3)\n  if ( (i * 3) %% 8 == 0) {\n    break\n  }\n  i \n- i + 1\n}\n\n\n\n\n## [1] 3\n## [1] 6\n## [1] 9\n## [1] 12\n## [1] 15\n## [1] 18\n## [1] 21\n## [1] 24\n\n\n\nLoop over a vector\n\n\n# The linkedin vector\nlinkedin \n- c(16, 9, 13, 5, 2, 17, 14)\n\n# Loop version 1\nfor (lin in linkedin) {\n    print(lin)\n}\n\n\n\n\n## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14\n\n\n\n# Loop version 2\nfor (i in 1:length(linkedin)) {\n    print(linkedin[i])\n}\n\n\n\n\n## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14\n\n\n\nLoop over a list\n\n\n[[]]\n; list of list.\n\n\n# The nyc list is already specified\nnyc \n- list(pop = 8405837, \n            boroughs = c('Manhattan', 'Bronx', 'Brooklyn', 'Queens', 'Staten Island'), \n            capital = FALSE)\n\n# Loop version 1\nfor (item in nyc) {\n    print(item)\n}\n\n\n\n\n## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE\n\n\n\n# Loop version 2\nfor (i in 1:length(nyc)) {\n    print(nyc[[i]])\n}\n\n\n\n\n## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE\n\n\n\nLoop over a matrix\n\n\n# The tic-tac-toe matrix has already been defined for you\nttt \n- matrix(c('O', NA, 'X', NA, 'O', NA, 'X', 'O', 'X'), nrow = 3, ncol = 3)\n\n# define the double for loop\nfor (i in 1:nrow(ttt)) {\n    for (j in 1:ncol(ttt)) {\n    print(paste('On row', i,'and column', j,'the board contains ', ttt[i,j]))\n    }\n}\n\n\n\n\n## [1] \"On row 1 and column 1 the board contains  O\"\n## [1] \"On row 1 and column 2 the board contains  NA\"\n## [1] \"On row 1 and column 3 the board contains  X\"\n## [1] \"On row 2 and column 1 the board contains  NA\"\n## [1] \"On row 2 and column 2 the board contains  O\"\n## [1] \"On row 2 and column 3 the board contains  O\"\n## [1] \"On row 3 and column 1 the board contains  X\"\n## [1] \"On row 3 and column 2 the board contains  NA\"\n## [1] \"On row 3 and column 3 the board contains  X\"\n\n\n\nMix it up with control flow\n\n\n# The linkedin vector\nlinkedin \n- c(16, 9, 13, 5, 2, 17, 14)\n\n# Code the for loop with conditionals\nfor (i in 1:length(linkedin)) {\n    if (linkedin[i] \n 10) {\n        print('You\\'re popular!')\n    } else {\n        print('Be more visible!')\n    }\n    print(linkedin[i])\n}\n\n\n\n\n## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] 2\n## [1] \"You're popular!\"\n## [1] 17\n## [1] \"You're popular!\"\n## [1] 14\n\n\n\nNext, you break it\n\n\n# The linkedin vector\nlinkedin \n- c(16, 9, 13, 5, 2, 17, 14)\n\n# Extend the for loop\nfor (li in linkedin) {\n  if (li \n 10) {\n    print('You\\'re popular!')\n  } else {\n    print('Be more visible!')\n  }\n    # Add code to conditionally break iteration\n  if (li \n 16) {\n    print('This is ridiculous, I\\'m outta here!')\n    break\n  }\n  # Add code to conditionally skip iteration\n  if (li \n 5) {\n    print('This is too embarrassing!')\n    next\n    }\n  print(li)\n}\n\n\n\n\n## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] \"This is too embarrassing!\"\n## [1] \"You're popular!\"\n## [1] \"This is ridiculous, I'm outta here!\"\n\n\n\nBuild a for loop from scratch\n\n\n# Pre-defined variables\nrquote \n- 'R\\'s internals are irrefutably intriguing'\n\nchars \n- strsplit(rquote, split = '')[[1]]\nrcount \n- 0\n\n# Your solution here\nfor (i in 1:length(chars)) {\n    if (chars[i] == 'u') {\n    break\n    }\n    if (chars[i] == 'r' | chars[i] == 'R') {\n        rcount \n- rcount + 1\n    }\n}\n\n# Print the resulting rcount variable to the console\nprint(rcount)\n\n\n\n\n## [1] 5\n\n\n\n3, Functions\n\n\nFunction documentation\n\n\n# Consult the documentation on the mean() function\n?mean\n\n# Inspect the arguments of the mean() function\nargs(mean)\n\n\n\n\nUse a function\n\n\n# The linkedin and facebook vectors\nlinkedin \n- c(16, 9, 13, 5, 2, 17, 14)\nfacebook \n- c(17, 7, 5, 16, 8, 13, 14)\n\n# Calculate average number of views\navg_li \n- mean(linkedin)\navg_fb \n- mean(facebook)\n\n# Inspect avg_li and avg_fb\nprint(avg_li)\n\n\n\n\n## [1] 10.85714\n\n\n\nprint(avg_fb)\n\n\n\n\n## [1] 11.42857\n\n\n\navg_li\n\n\n\n\n## [1] 10.85714\n\n\n\n# Calculate the mean of linkedin minus facebook\nprint(mean(linkedin - facebook))\n\n\n\n\n## [1] -0.5714286\n\n\n\nUse a function (2)\n\n\n# The linkedin and facebook vectors\nlinkedin \n- c(16, 9, 13, 5, 2, 17, 14)\nfacebook \n- c(17, 7, 5, 16, 8, 13, 14)\n\n# Calculate the mean of the sum\navg_sum \n- mean(linkedin + facebook)\n\n# Calculate the trimmed mean of the sum\navg_sum_trimmed \n- mean((linkedin + facebook), trim = 0.2)\n\n# Inspect both new variables\navg_sum\n\n\n\n\n## [1] 22.28571\n\n\n\navg_sum_trimmed\n\n\n\n\n## [1] 22.6\n\n\n\nUse a function (3)\n\n\n# The linkedin and facebook vectors\nlinkedin \n- c(16, 9, 13, 5, NA, 17, 14)\nfacebook \n- c(17, NA, 5, 16, 8, 13, 14)\n\n# Basic average of linkedin\nprint(mean(linkedin))\n\n\n\n\n## [1] NA\n\n\n\n# Advanced average of facebook\nprint(mean(facebook, na.rm = TRUE))\n\n\n\n\n## [1] 12.16667\n\n\n\nFunctions inside functions\n\n\n# The linkedin and facebook vectors\nlinkedin \n- c(16, 9, 13, 5, NA, 17, 14)\nfacebook \n- c(17, NA, 5, 16, 8, 13, 14)\n\n# Calculate the mean absolute deviation\nmean((abs(linkedin - facebook)), na.rm = TRUE)\n\n\n\n\n## [1] 4.8\n\n\n\nWrite your own function\n\n\n# Create a function pow_two()\npow_two \n- function(arg1) {\n    arg1^2\n}\n\n# Use the function \npow_two(12)\n\n\n\n\n## [1] 144\n\n\n\n# Create a function sum_abs()\nsum_abs \n- function(arg2,arg3) {\n    abs(arg2) + abs(arg3)\n}\n\n# Use the function\nsum_abs(-2,3)\n\n\n\n\n## [1] 5\n\n\n\nWrite your own function (2)\n\n\n# Define the function hello()\nhello \n- function() {\n    print('Hi there!')\n    return(TRUE)\n}\n\n# Call the function hello()\nhello()\n\n\n\n\n## [1] \"Hi there!\"\n\n## [1] TRUE\n\n\n\n# Define the function my_filter()\nmy_filter \n- function(arg1) {\n    if (arg1 \n 0) {\n        return(arg1)\n    } else {\n        return(NULL)\n    }\n}\n\n# Call the function my_filter() twice\nmy_filter(5)\n\n\n\n\n## [1] 5\n\n\n\nmy_filter(-5)\n\n\n\n\n## NULL\n\n\n\nWrite your own function (3)\n\n\nVariables inside a function are not in the Global Environment.\n\n\n# Extend the pow_two() function\npow_two \n- function(x, print_info = TRUE) {\n  y \n- x ^ 2\n  if (print_info) {\n    print(paste(x, 'to the power two equals', y))\n  }\n  return(y)\n}\n\n#pow_two(2)\npow_two(2, FALSE)\n\n\n\n\n## [1] 4\n\n\n\nR you functional?\n\n\n# The linkedin and facebook vectors\nlinkedin \n- c(16, 9, 13, 5, NA, 17, 14)\nfacebook \n- c(17, 7, 5, 16, 8, 13, 14)\n\n# Define the interpret function\ninterpret \n- function(arg) {\n    if (arg \n 15) {\n        print('You\\'re popular!')\n        return(arg)\n    } else {\n        print('Try to be more visible!')\n        return(0)\n    }\n}\n\ninterpret(linkedin[1])\n\n\n\n\n## [1] \"You're popular!\"\n\n## [1] 16\n\n\n\ninterpret(facebook[2])\n\n\n\n\n## [1] \"Try to be more visible!\"\n\n## [1] 0\n\n\n\nR you functional? (2)\n\n\n# The linkedin and facebook vectors\nlinkedin \n- c(16, 9, 13, 5, 2, 17, 14)\nfacebook \n- c(17, 7, 5, 16, 8, 13, 14)\n\n# The interpret() can be used inside interpret_all()\ninterpret \n- function(num_views){\n  if (num_views \n 15) {\n    print('You\\'re popular!')\n    return(num_views)\n  } else {\n    print('Try to be more visible!')\n    return(0)\n  }\n}\n\n# Define the interpret_all() function\ninterpret_all \n- function(data, logi = TRUE){\n  yy \n- 0\n  for (i in data) {\n    yy \n- yy + interpret(i)\n  }\n  if (logi) {\n    return(yy)\n  } else {\n    return(NULL)\n  }\n}\n\n# Call the interpret_all() function on both linkedin and facebook\ninterpret_all(linkedin)\n\n\n\n\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33\n\n\n\ninterpret_all(facebook)\n\n\n\n\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33\n\n\n\nLoad an R package\n\n\n# The mtcars vectors have already been prepared for you\nwt \n- mtcars$wt\nhp \n- mtcars$hp\n\n# Request the currently attached packages\nsearch()\n\n\n\n\n##  [1] \".GlobalEnv\"            \"package:XLConnect\"    \n##  [3] \"package:XLConnectJars\" \"package:stats\"        \n##  [5] \"package:graphics\"      \"package:grDevices\"    \n##  [7] \"package:utils\"         \"package:datasets\"     \n##  [9] \"package:methods\"       \"Autoloads\"            \n## [11] \"package:base\"\n\n\n\n# Try the qplot() function with wt and hp\nplot(wt,hp)\n\n# Load the ggplot2 package\nlibrary('ggplot2')\n\n\n\n\n\n\n# or\nrequire('ggplot2')\n\n# Retry the qplot() function\nqplot(wt,hp)\n\n\n\n\n\n\n# Check out the currently attached packages again\nsearch()\n\n\n\n\n##  [1] \".GlobalEnv\"            \"package:ggplot2\"      \n##  [3] \"package:XLConnect\"     \"package:XLConnectJars\"\n##  [5] \"package:stats\"         \"package:graphics\"     \n##  [7] \"package:grDevices\"     \"package:utils\"        \n##  [9] \"package:datasets\"      \"package:methods\"      \n## [11] \"Autoloads\"             \"package:base\"\n\n\n\n4, The \napply\n Family\n\n\nUse \nlapply\n with a built-in R function\n\n\n# The vector pioneers\npioneers \n- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\n# Split names from birth year: split_math\nsplit_math \n- strsplit(pioneers, ':')\n\n# Convert to lowercase strings: split_low\nsplit_low \n- lapply(split_math,tolower)\n\n# Take a look at the structure of split_low\nstr(split_low)\n\n\n\n\n## List of 4\n##  $ : chr [1:2] \"gauss\" \"1777\"\n##  $ : chr [1:2] \"bayes\" \"1702\"\n##  $ : chr [1:2] \"pascal\" \"1623\"\n##  $ : chr [1:2] \"pearson\" \"1857\"\n\n\n\nUse \nlapply\n with your own function\n\n\n# Code from previous exercise\npioneers \n- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\nsplit \n- strsplit(pioneers, split = ':')\nsplit_low \n- lapply(split, tolower)\n\n# Write function select_first()\nselect_first \n- function(x) {\n    return(x[1])\n}\n\n# Apply select_first() over split_low: names\nnames \n- lapply(split_low, select_first)\nprint(names)\n\n\n\n\n## [[1]]\n## [1] \"gauss\"\n## \n## [[2]]\n## [1] \"bayes\"\n## \n## [[3]]\n## [1] \"pascal\"\n## \n## [[4]]\n## [1] \"pearson\"\n\n\n\n# Write function select_second()\nselect_second \n- function(x) {\n    return(x[2])\n}\n\n# Apply select_second() over split_low: years\nyears \n- lapply(split_low, select_second)\nprint(years)\n\n\n\n\n## [[1]]\n## [1] \"1777\"\n## \n## [[2]]\n## [1] \"1702\"\n## \n## [[3]]\n## [1] \"1623\"\n## \n## [[4]]\n## [1] \"1857\"\n\n\n\nlapply\n and anonymous functions\n\n\nAnonymous function == lambda function.\n\n\n# Definition of split_low\npioneers \n- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\nsplit \n- strsplit(pioneers, split = ':')\nsplit_low \n- lapply(split, tolower)\n\n#select_first \n- function(x) {\n#  x[1]\n#}\n\nnames \n- lapply(split_low, function(x) { x[1] })\n\n#select_second \n- function(x) {\n#  x[2]\n#}\n\nyears \n- lapply(split_low, function(x) { x[2] })\n\n\n\n\nUse \nlapply\n with additional arguments\n\n\n# Definition of split_low\npioneers \n- c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\nsplit \n- strsplit(pioneers, split = ':')\nsplit_low \n- lapply(split, tolower)\n\n# Replace the select_*() functions by a single function: select_el\nselect_el \n- function(x, i) { \n  x[i] \n}\n\n#select_second \n- function(x) { \n#  x[2] \n#}\n\n# Call the select_el() function twice on split_low: names and years\nnames \n- lapply(split_low, select_el, i=1)\nyears \n- lapply(split_low, select_el, 2)\n\n\n\n\nHow to use \nsapply\n\n\ntemp1 \n- c(3, 7, 9, 6, -1)\ntemp2 \n- c(6, 9, 12, 13, 5)\ntemp3 \n- c(4, 8, 3, -1, -3)\ntemp4 \n- c(1, 4, 7, 2, -2)\ntemp5 \n- c(5, 7, 9, 4, 2)\ntemp6 \n- c(-3, 5, 8, 9, 4)\ntemp7 \n- c(3, 6, 9, 4, 1)\n\ntemp \n- list(temp1, temp2, temp3, temp4, temp5, temp6, temp7)\n\n# Use lapply() to find each day's minimum temperature\nlapply(temp, min)\n\n\n\n\n## [[1]]\n## [1] -1\n## \n## [[2]]\n## [1] 5\n## \n## [[3]]\n## [1] -3\n## \n## [[4]]\n## [1] -2\n## \n## [[5]]\n## [1] 2\n## \n## [[6]]\n## [1] -3\n## \n## [[7]]\n## [1] 1\n\n\n\n# Use sapply() to find each day's minimum temperature\nsapply(temp, min)\n\n\n\n\n## [1] -1  5 -3 -2  2 -3  1\n\n\n\n# Use lapply() to find each day's maximum temperature\nlapply(temp, max)\n\n\n\n\n## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 13\n## \n## [[3]]\n## [1] 8\n## \n## [[4]]\n## [1] 7\n## \n## [[5]]\n## [1] 9\n## \n## [[6]]\n## [1] 9\n## \n## [[7]]\n## [1] 9\n\n\n\n# Use sapply() to find each day's maximum temperature\nsapply(temp, max)\n\n\n\n\n## [1]  9 13  8  7  9  9  9\n\n\n\nsapply\n with your own function\n\n\n# temp is already defined in the workspace\n\n# Define a function calculates the average of the min and max of a vector: extremes_avg\nextremes_avg \n- function(x) {\n    return((min(x) + max(x))/2)\n}\n\n# Apply extremes_avg() over temp using sapply()\nsapply(temp, extremes_avg)\n\n\n\n\n## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0\n\n\n\n# Apply extremes_avg() over temp using lapply()\nlapply(temp, extremes_avg)\n\n\n\n\n## [[1]]\n## [1] 4\n## \n## [[2]]\n## [1] 9\n## \n## [[3]]\n## [1] 2.5\n## \n## [[4]]\n## [1] 2.5\n## \n## [[5]]\n## [1] 5.5\n## \n## [[6]]\n## [1] 3\n## \n## [[7]]\n## [1] 5\n\n\n\nsapply\n with function returning vector\n\n\n# temp is already available in the workspace\n\n# Create a function that returns min and max of a vector: extremes\nextremes \n- function(x) {\n    c(min(x), max(x))\n}\n\n# Apply extremes() over temp with sapply()\nsapply(temp, extremes)\n\n\n\n\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [1,]   -1    5   -3   -2    2   -3    1\n## [2,]    9   13    8    7    9    9    9\n\n\n\n# Apply extremes() over temp with lapply()\nlapply(temp, extremes)\n\n\n\n\n## [[1]]\n## [1] -1  9\n## \n## [[2]]\n## [1]  5 13\n## \n## [[3]]\n## [1] -3  8\n## \n## [[4]]\n## [1] -2  7\n## \n## [[5]]\n## [1] 2 9\n## \n## [[6]]\n## [1] -3  9\n## \n## [[7]]\n## [1] 1 9\n\n\n\nsapply\n can\nt simplify, now what?\n\n\n# temp is already prepared for you in the workspace\n\n# Create a function that returns all values below zero: below_zero\nbelow_zero \n- function(x) {\n    x[x\n0]\n}\n\n#below_zero(temp) alone won't work!!!\n\n# Apply below_zero over temp using sapply(): freezing_s\nfreezing_s \n- sapply(temp, below_zero)\n\n# Apply below_zero over temp using lapply(): freezing_l\nfreezing_l \n- lapply(temp, below_zero)\n\n# Compare freezing_s to freezing_l using identical()\nidentical(freezing_s, freezing_l)\n\n\n\n\n## [1] TRUE\n\n\n\nsapply\n with functions that return NULL\n\n\n# temp is already available in the workspace\n\n# Write a function that 'cat()s' out the average temperatures: print_info\nprint_info \n- function(x) {\n    cat('The average temperature is', mean(x), '\\n')\n}\n\n# Apply print_info() over temp using lapply()\nlapply(temp, print_info)\n\n\n\n\n## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n\n\n\n# Apply print_info() over temp using sapply()\nsapply(temp, print_info)\n\n\n\n\n## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n\n\n\nUse \nvapply\n\n\n# temp is already available in the workspace\n\n# Code the basics() function\nbasics \n- function(x) {\n    c(minimum = min(x), average = mean(x), maximum = max(x))\n}\n\n# Apply basics() over temp using vapply()\nvapply(temp, basics, numeric(3))\n\n\n\n\n##         [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## minimum -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## average  4.8    9  2.2  2.4  5.4  4.6  4.6\n## maximum  9.0   13  8.0  7.0  9.0  9.0  9.0\n\n\n\nUse \nvapply\n (2)\n\n\n# temp is already available in the workspace\n\n# Definition of the basics() function\nbasics \n- function(x) {\n  c(min = min(x), mean = mean(x), median = median(x), max = max(x))\n}\n\n# Fix the error:\nvapply(temp, basics, numeric(4))\n\n\n\n\n##        [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## min    -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## mean    4.8    9  2.2  2.4  5.4  4.6  4.6\n## median  6.0    9  3.0  2.0  5.0  5.0  4.0\n## max     9.0   13  8.0  7.0  9.0  9.0  9.0\n\n\n\nFrom \nsapply\n to \nvapply\n\n\n# temp is already defined in the workspace\n\n# Convert to vapply() expression\nvapply(temp, max, numeric(1))\n\n\n\n\n## [1]  9 13  8  7  9  9  9\n\n\n\n# Convert to vapply() expression\nvapply(temp, function(x, y) { mean(x) \n y }, y = 5, logical(1))\n\n\n\n\n## [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n\n\n\n# Definition of get_info (don't change)\nget_info \n- function(x, y) { \n  if (mean(x) \n y) {\n    return('Not too cold!')\n  } else {\n    return('Pretty cold!')\n  }\n}\n\n# Convert to vapply() expression\nvapply(temp, get_info, y = 5, character(1))\n\n\n\n\n## [1] \"Pretty cold!\"  \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\" \n## [5] \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\"\n\n\n\nApply your knowledge. Or better yet: \nsapply\n it?\n\n\n# work_todos and fun_todos have already been defined\nwork_todos \n- c('Schedule call with team', \n                'Fix error in Recommendation System', \n                'Respond to Marc from IT')\n\nfun_todos \n- c('Sleep', 'Make arrangements for summer trip')\n\n# Create a list: todos\ntodos \n- list(work_todos, fun_todos)\ntodos\n\n\n\n\n## [[1]]\n## [1] \"Schedule call with team\"           \n## [2] \"Fix error in Recommendation System\"\n## [3] \"Respond to Marc from IT\"           \n## \n## [[2]]\n## [1] \"Sleep\"                             \"Make arrangements for summer trip\"\n\n\n\n# Sort the vectors inside todos alphabetically\nlapply(todos, sort)\n\n\n\n\n## [[1]]\n## [1] \"Fix error in Recommendation System\"\n## [2] \"Respond to Marc from IT\"           \n## [3] \"Schedule call with team\"           \n## \n## [[2]]\n## [1] \"Make arrangements for summer trip\" \"Sleep\"\n\n\n\n5, Utilities\n\n\nMathematical utilities\n\n\n\n\nabs\n; calculate the absolute value.\n\n\nsum\n; calculate the sum of all the values in a data structure.\n\n\nmean\n; calculate the arithmetic mean.\n\n\nround\n; round the values to 0 decimal places by default. Try out\n\n\n?round\n in the console for variations of \nround\n and ways to change\n\n    the number of digits to round to.\n\n\n\n\n\n\n\n# The errors vector\nerrors \n- c(1.9, -2.6, 4.0, -9.5, -3.4, 7.3)\n\n# Sum of absolute rounded values of errors\nsum(abs(round(errors)))\n\n\n\n\n## [1] 29\n\n\n\nFind the error\n\n\n# Vectors\nvec1 \n- c(1.5, 2.5, 8.4, 3.7, 6.3)\nvec2 \n- rev(vec1)\n\n# Fix the error\nmean(abs(append(vec1, vec2)))\n\n\n\n\n## [1] 4.48\n\n\n\nData utilities\n\n\n\n\nseq\n; generate sequences, by specifying the from, to and\n\n    by arguments.\n\n\nrep\n; replicate elements of vectors and lists.\n\n\nsort\n; sort a vector in ascending order. Works on numerics, but\n\n    also on character strings and logicals.\n\n\nrev\n; reverse the elements in a data structures for which reversal\n\n    is defined.\n\n\nstr\n; display the structure of any R object. append; Merge vectors\n\n    or lists.\n\n\nis.*\n; check for the class of an R object.\n\n\nas.*\n; convert an R object from one class to another.\n\n\nunlist\n; flatten (possibly embedded) lists to produce a vector.\n\n\n\n\n\n\n\n# The linkedin and facebook vectors\nlinkedin \n- list(16, 9, 13, 5, 2, 17, 14)\nfacebook \n- list(17, 7, 5, 16, 8, 13, 14)\n\n# Convert linkedin and facebook to a vector: li_vec and fb_vec\nli_vec \n- unlist(as.vector(linkedin))\nfb_vec \n- unlist(as.vector(facebook))\n\n# Append fb_vec to li_vec: social_vec\nsocial_vec \n- append(li_vec, fb_vec)\n\n# Sort social_vec\nsort(social_vec, decreasing = TRUE)\n\n\n\n\n##  [1] 17 17 16 16 14 14 13 13  9  8  7  5  5  2\n\n\n\nFind the error (2)\n\n\n# Fix me\nround(sum(unlist(list(1.1, 3, 5))))\n\n\n\n\n## [1] 9\n\n\n\n# Fix me\nrep(seq(1, 7, by = 2), times = 7)\n\n\n\n\n##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7\n\n\n\nprint(rep(seq(1, 7, by = 2), times = 7))\n\n\n\n\n##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7\n\n\n\nBeat Gauss using R\n\n\n# Create first sequence: seq1\nseq1 \n- seq(1,500, by = 3)\nprint(seq1)\n\n\n\n\n##   [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49\n##  [18]  52  55  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\n##  [35] 103 106 109 112 115 118 121 124 127 130 133 136 139 142 145 148 151\n##  [52] 154 157 160 163 166 169 172 175 178 181 184 187 190 193 196 199 202\n##  [69] 205 208 211 214 217 220 223 226 229 232 235 238 241 244 247 250 253\n##  [86] 256 259 262 265 268 271 274 277 280 283 286 289 292 295 298 301 304\n## [103] 307 310 313 316 319 322 325 328 331 334 337 340 343 346 349 352 355\n## [120] 358 361 364 367 370 373 376 379 382 385 388 391 394 397 400 403 406\n## [137] 409 412 415 418 421 424 427 430 433 436 439 442 445 448 451 454 457\n## [154] 460 463 466 469 472 475 478 481 484 487 490 493 496 499\n\n\n\n# Create second sequence: seq2\nseq2 \n- seq(1200, 900, by = -7)\nprint(seq2)\n\n\n\n\n##  [1] 1200 1193 1186 1179 1172 1165 1158 1151 1144 1137 1130 1123 1116 1109\n## [15] 1102 1095 1088 1081 1074 1067 1060 1053 1046 1039 1032 1025 1018 1011\n## [29] 1004  997  990  983  976  969  962  955  948  941  934  927  920  913\n## [43]  906\n\n\n\n# Calculate total sum of the sequences\nprint(sum(append(seq1, seq2)))\n\n\n\n\n## [1] 87029\n\n\n\ngrepl\n \n \ngrep\n\n\n\n\ngrepl\n; return TRUE when a pattern is found in the corresponding\n\n    character string.\n\n\ngrep\n; return a vector of indices of the character strings that\n\n    contains the pattern.\n\n\n\n\n\n\n\n# The emails vector has\nemails \n- c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use grepl() to match for 'edu'\nprint(grepl(pattern = 'edu', x = emails))\n\n\n\n\n## [1]  TRUE  TRUE FALSE  TRUE  TRUE FALSE\n\n\n\n# Use grep() to match for 'edu', save result to hits\nhits \n- grep(pattern = 'edu', x = emails)\nhits\n\n\n\n\n## [1] 1 2 4 5\n\n\n\n# Subset emails using hits\nprint(emails[hits])\n\n\n\n\n## [1] \"john.doe@ivyleague.edu\"   \"education@world.gov\"     \n## [3] \"invalid.edu\"              \"quant@bigdatacollege.edu\"\n\n\n\ngrepl\n \n \ngrep\n (2)\n\n\nConsult a regex character chart for more.\n\n\n# The emails vector\nemails \n- c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use grep() to match for .edu addresses more robustly\nprint(grep(pattern = '@.*\\\\.edu$',x = emails))\n\n\n\n\n## [1] 1 5\n\n\n\n# Use grepl() to match for .edu addresses more robustly, save result to hits\nhits \n- grepl(pattern = '@.*\\\\.edu$',x = emails)\nhits\n\n\n\n\n## [1]  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n# Subset emails using hits\nprint(emails[hits])\n\n\n\n\n## [1] \"john.doe@ivyleague.edu\"   \"quant@bigdatacollege.edu\"\n\n\n\nsub\n \n \ngsub\n\n\n# The emails vector\nemails \n- c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use sub() to convert the email domains to datacamp.edu (attempt 1)\nprint(sub(pattern = '@.*\\\\.edu$', replacement = 'datacamp.edu', x = emails))\n\n\n\n\n## [1] \"john.doedatacamp.edu\"     \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quantdatacamp.edu\"        \"cookie.monster@sesame.tv\"\n\n\n\n# Use sub() to convert the email domains to datacamp.edu (attempt 2)\nprint(sub(pattern = '@.*\\\\.edu$', replacement = '@datacamp.edu', x = emails))\n\n\n\n\n## [1] \"john.doe@datacamp.edu\"    \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quant@datacamp.edu\"       \"cookie.monster@sesame.tv\"\n\n\n\nRight here, right now\n\n\n# Get the current date: today\ntoday \n- Sys.Date()\ntoday\n\n\n\n\n## [1] \"2017-02-15\"\n\n\n\n# See what today looks like under the hood\nprint(unclass(today))\n\n\n\n\n## [1] 17212\n\n\n\n# Get the current time: now\nnow \n- Sys.time()\nnow\n\n\n\n\n## [1] \"2017-02-15 11:11:32 EST\"\n\n\n\n# See what now looks like under the hood\nprint(unclass(now))\n\n\n\n\n## [1] 1487175093\n\n\n\nCreate and format dates\n\n\n\n\n\n\n\n\nSymbol\n\n\nMeaning\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n%d\n\n\nday as a number (0-31)\n\n\n31-janv\n\n\n\n\n\n\n%a\n\n\nabbreviated weekday\n\n\nMon\n\n\n\n\n\n\n%A\n\n\nunabbreviated weekday\n\n\nMonday\n\n\n\n\n\n\n%m\n\n\nmonth (00-12)\n\n\n00-12\n\n\n\n\n\n\n%b\n\n\nabbreviated month\n\n\nJan\n\n\n\n\n\n\n%B\n\n\nunabbreviated month\n\n\nJanuary\n\n\n\n\n\n\n%y\n\n\n2-digit year\n\n\n07\n\n\n\n\n\n\n%Y\n\n\n4-digit year\n\n\n2007\n\n\n\n\n\n\n%H\n\n\nhours as a decimal number\n\n\n23\n\n\n\n\n\n\n%M\n\n\nminutes as a decimal number\n\n\n10\n\n\n\n\n\n\n%S\n\n\nseconds as a decimal number\n\n\n53\n\n\n\n\n\n\n%T\n\n\nshorthand notation for the typical format %H:%M:%S\n\n\n23:10:53\n\n\n\n\n\n\n\n\n\nFind out more with \n?strptime\n.\n\n\nR offer default functions for dealing with time and dates. There are\n\nbetter packages: \ndate\n and \nlubridate\n.\n\n\nlubridate\n enhances time-series packages such as \nzoo\n and \nxts\n, and\n\nworks well with \ndplyr\n for data wrangling.\n\n\nlibrary(date)\n\n# Definition of character strings representing dates\nstr1 \n- \nMay 23, 96\n\nstr2 \n- \n2012-3-15\n\nstr3 \n- \n30/January/2006\n\n\n# Convert the strings to dates: date1, date2, date3\ndate1 \n- as.date(str1, order = \nmdy\n)\ndate1\n\n\n\n\n## [1] 23May96\n\n\n\ndate1 \n- as.POSIXct(date1, format = \n%d %m %y\n)\ndate1\n\n\n\n\n## [1] \"1996-05-22 20:00:00 EDT\"\n\n\n\ndate2 \n- as.date(str2, order = \nymd\n)\ndate2\n\n\n\n\n## [1] 15Mar2012\n\n\n\ndate2 \n- as.POSIXct(date2, format = \n%d %m %y\n)\ndate2\n\n\n\n\n## [1] \"2012-03-14 20:00:00 EDT\"\n\n\n\ndate3 \n- as.date(str3, order = \ndmy\n)\ndate3\n\n\n\n\n## [1] 30Jan2006\n\n\n\ndate3 \n- as.POSIXct(date3, format = \n%d %m %y\n)\ndate3\n\n\n\n\n## [1] \"2006-01-29 19:00:00 EST\"\n\n\n\n# Convert dates to formatted strings\nformat(date1, \n%A\n)\n\n\n\n\n## [1] \"mercredi\"\n\n\n\nformat(date2, \n%d\n)\n\n\n\n\n## [1] \"14\"\n\n\n\nformat(date3, \n%b %Y\n)\n\n\n\n\n## [1] \"janv. 2006\"\n\n\n\n# convert dates to character data\nstrDate2 \n- as.character(date2)\nstrDate2\n\n\n\n\n## [1] \"2012-03-14 20:00:00\"\n\n\n\nCreate and format times\n\n\n# Definition of character strings representing times\nstr1 \n- \n2012-3-12 14:23:08\n\n\n# Convert the strings to POSIXct objects: time1, time2\ntime1 \n- as.POSIXct(str2, format = \n%Y-%m-%d %H:%M:%S\n)\n\n# Convert times to formatted strings\n\n# Definition of character strings representing dates\nformat(time1, \n%M\n)\n\n\n\n\n## [1] NA\n\n\n\nformat(time1, \n%I:%M %p\n)\n\n\n\n\n## [1] NA\n\n\n\nCalculations with dates\n\n\n# day1, day2, day3, day4 and day5\nday1 \n- as.Date(\n2016-11-21\n)\nday2 \n- as.Date(\n2016-11-16\n)\nday3 \n- as.Date(\n2016-11-27\n)\nday4 \n- as.Date(\n2016-11-14\n)\nday5 \n- as.Date(\n2016-12-02\n)\n\n# Difference between last and first pizza day\nprint(day5 - day1)\n\n\n\n\n## Time difference of 11 days\n\n\n\n# Create vector pizza\npizza \n- c(day1, day2, day3, day4, day5)\n\n# Create differences between consecutive pizza days: day_diff\nday_diff \n- diff(pizza, lag = 1, differences = 1)\nday_diff\n\n\n\n\n## Time differences in days\n## [1]  -5  11 -13  18\n\n\n\n# Average period between two consecutive pizza days\nprint(mean(day_diff))\n\n\n\n\n## Time difference of 2.75 days\n\n\n\nCalculus with times\n\n\n# login and logout\nlogin \n- as.POSIXct(c(\n2016-11-18 10:18:04 UTC\n, \n2016-11-23 09:14:18 UTC\n, \n2016-11-23 12:21:51 UTC\n, \n2016-11-23 12:37:24 UTC\n, \n2016-11-25 21:37:55 UTC\n))\n\nlogout \n- as.POSIXct(c(\n2016-11-18 10:56:29 UTC\n, \n2016-11-23 09:14:52 UTC\n, \n2016-11-23 12:35:48 UTC\n, \n2016-11-23 13:17:22 UTC\n, \n2016-11-25 22:08:47 UTC\n))\n\n# Calculate the difference between login and logout: time_online\ntime_online \n- logout - login\n\n# Inspect the variable time_online\n#class(time_online)\ntime_online\n\n\n\n\n## Time differences in secs\n## [1] 2305   34  837 2398 1852\n\n\n\n# Calculate the total time online\nprint(sum(time_online))\n\n\n\n\n## Time difference of 7426 secs\n\n\n\n# Calculate the average time online\nprint(mean(time_online))\n\n\n\n\n## Time difference of 1485.2 secs", 
            "title": "Intermediate R"
        }, 
        {
            "location": "/Intermediate_R/#2-loops", 
            "text": "Write a  while  loop  # Initialize the speed variable\nspeed  - 64\n\n# Code the while loop\nwhile (speed   30) {\n  print('Slow down!')\n  speed  - speed - 7\n}  ## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"  # Print out the speed variable\nspeed  ## [1] 29  Throw in more conditionals  # Initialize the speed variable\nspeed  - 64\n\n# Extend/adapt the while loop\nwhile (speed   30) {\n  print(paste('Your speed is ', speed))\n  if (speed   48) {\n    print('Slow down big time!')\n    speed  - speed - 11\n  } else {\n        print('Slow down!')\n        speed  - speed - 6\n  }\n}  ## [1] \"Your speed is  64\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  53\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  42\"\n## [1] \"Slow down!\"\n## [1] \"Your speed is  36\"\n## [1] \"Slow down!\"  Stop the  while  loop:  break  # Initialize the speed variable\nspeed  - 88\nwhile (speed   30) {\n  print(paste('Your speed is',speed))\n    # Break the while loop when speed exceeds 80\n  if (speed   80) {\n    break\n  } else if (speed   48) {\n    print('Slow down big time!')\n    speed  - speed - 11\n  } else {\n    print('Slow down!')\n    speed  - speed - 6\n  }\n}  ## [1] \"Your speed is 88\"  Build a  while  loop from scratch  strsplit ; split up in a vector that contains separate letters.  # Initialize i\ni  - 1\n\n# Code the while loop\nwhile (i  = 10) {\n  print(i * 3)\n  if ( (i * 3) %% 8 == 0) {\n    break\n  }\n  i  - i + 1\n}  ## [1] 3\n## [1] 6\n## [1] 9\n## [1] 12\n## [1] 15\n## [1] 18\n## [1] 21\n## [1] 24  Loop over a vector  # The linkedin vector\nlinkedin  - c(16, 9, 13, 5, 2, 17, 14)\n\n# Loop version 1\nfor (lin in linkedin) {\n    print(lin)\n}  ## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14  # Loop version 2\nfor (i in 1:length(linkedin)) {\n    print(linkedin[i])\n}  ## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14  Loop over a list  [[]] ; list of list.  # The nyc list is already specified\nnyc  - list(pop = 8405837, \n            boroughs = c('Manhattan', 'Bronx', 'Brooklyn', 'Queens', 'Staten Island'), \n            capital = FALSE)\n\n# Loop version 1\nfor (item in nyc) {\n    print(item)\n}  ## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE  # Loop version 2\nfor (i in 1:length(nyc)) {\n    print(nyc[[i]])\n}  ## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE  Loop over a matrix  # The tic-tac-toe matrix has already been defined for you\nttt  - matrix(c('O', NA, 'X', NA, 'O', NA, 'X', 'O', 'X'), nrow = 3, ncol = 3)\n\n# define the double for loop\nfor (i in 1:nrow(ttt)) {\n    for (j in 1:ncol(ttt)) {\n    print(paste('On row', i,'and column', j,'the board contains ', ttt[i,j]))\n    }\n}  ## [1] \"On row 1 and column 1 the board contains  O\"\n## [1] \"On row 1 and column 2 the board contains  NA\"\n## [1] \"On row 1 and column 3 the board contains  X\"\n## [1] \"On row 2 and column 1 the board contains  NA\"\n## [1] \"On row 2 and column 2 the board contains  O\"\n## [1] \"On row 2 and column 3 the board contains  O\"\n## [1] \"On row 3 and column 1 the board contains  X\"\n## [1] \"On row 3 and column 2 the board contains  NA\"\n## [1] \"On row 3 and column 3 the board contains  X\"  Mix it up with control flow  # The linkedin vector\nlinkedin  - c(16, 9, 13, 5, 2, 17, 14)\n\n# Code the for loop with conditionals\nfor (i in 1:length(linkedin)) {\n    if (linkedin[i]   10) {\n        print('You\\'re popular!')\n    } else {\n        print('Be more visible!')\n    }\n    print(linkedin[i])\n}  ## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] 2\n## [1] \"You're popular!\"\n## [1] 17\n## [1] \"You're popular!\"\n## [1] 14  Next, you break it  # The linkedin vector\nlinkedin  - c(16, 9, 13, 5, 2, 17, 14)\n\n# Extend the for loop\nfor (li in linkedin) {\n  if (li   10) {\n    print('You\\'re popular!')\n  } else {\n    print('Be more visible!')\n  }\n    # Add code to conditionally break iteration\n  if (li   16) {\n    print('This is ridiculous, I\\'m outta here!')\n    break\n  }\n  # Add code to conditionally skip iteration\n  if (li   5) {\n    print('This is too embarrassing!')\n    next\n    }\n  print(li)\n}  ## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] \"This is too embarrassing!\"\n## [1] \"You're popular!\"\n## [1] \"This is ridiculous, I'm outta here!\"  Build a for loop from scratch  # Pre-defined variables\nrquote  - 'R\\'s internals are irrefutably intriguing'\n\nchars  - strsplit(rquote, split = '')[[1]]\nrcount  - 0\n\n# Your solution here\nfor (i in 1:length(chars)) {\n    if (chars[i] == 'u') {\n    break\n    }\n    if (chars[i] == 'r' | chars[i] == 'R') {\n        rcount  - rcount + 1\n    }\n}\n\n# Print the resulting rcount variable to the console\nprint(rcount)  ## [1] 5", 
            "title": "2, Loops"
        }, 
        {
            "location": "/Intermediate_R/#3-functions", 
            "text": "Function documentation  # Consult the documentation on the mean() function\n?mean\n\n# Inspect the arguments of the mean() function\nargs(mean)  Use a function  # The linkedin and facebook vectors\nlinkedin  - c(16, 9, 13, 5, 2, 17, 14)\nfacebook  - c(17, 7, 5, 16, 8, 13, 14)\n\n# Calculate average number of views\navg_li  - mean(linkedin)\navg_fb  - mean(facebook)\n\n# Inspect avg_li and avg_fb\nprint(avg_li)  ## [1] 10.85714  print(avg_fb)  ## [1] 11.42857  avg_li  ## [1] 10.85714  # Calculate the mean of linkedin minus facebook\nprint(mean(linkedin - facebook))  ## [1] -0.5714286  Use a function (2)  # The linkedin and facebook vectors\nlinkedin  - c(16, 9, 13, 5, 2, 17, 14)\nfacebook  - c(17, 7, 5, 16, 8, 13, 14)\n\n# Calculate the mean of the sum\navg_sum  - mean(linkedin + facebook)\n\n# Calculate the trimmed mean of the sum\navg_sum_trimmed  - mean((linkedin + facebook), trim = 0.2)\n\n# Inspect both new variables\navg_sum  ## [1] 22.28571  avg_sum_trimmed  ## [1] 22.6  Use a function (3)  # The linkedin and facebook vectors\nlinkedin  - c(16, 9, 13, 5, NA, 17, 14)\nfacebook  - c(17, NA, 5, 16, 8, 13, 14)\n\n# Basic average of linkedin\nprint(mean(linkedin))  ## [1] NA  # Advanced average of facebook\nprint(mean(facebook, na.rm = TRUE))  ## [1] 12.16667  Functions inside functions  # The linkedin and facebook vectors\nlinkedin  - c(16, 9, 13, 5, NA, 17, 14)\nfacebook  - c(17, NA, 5, 16, 8, 13, 14)\n\n# Calculate the mean absolute deviation\nmean((abs(linkedin - facebook)), na.rm = TRUE)  ## [1] 4.8  Write your own function  # Create a function pow_two()\npow_two  - function(arg1) {\n    arg1^2\n}\n\n# Use the function \npow_two(12)  ## [1] 144  # Create a function sum_abs()\nsum_abs  - function(arg2,arg3) {\n    abs(arg2) + abs(arg3)\n}\n\n# Use the function\nsum_abs(-2,3)  ## [1] 5  Write your own function (2)  # Define the function hello()\nhello  - function() {\n    print('Hi there!')\n    return(TRUE)\n}\n\n# Call the function hello()\nhello()  ## [1] \"Hi there!\"\n\n## [1] TRUE  # Define the function my_filter()\nmy_filter  - function(arg1) {\n    if (arg1   0) {\n        return(arg1)\n    } else {\n        return(NULL)\n    }\n}\n\n# Call the function my_filter() twice\nmy_filter(5)  ## [1] 5  my_filter(-5)  ## NULL  Write your own function (3)  Variables inside a function are not in the Global Environment.  # Extend the pow_two() function\npow_two  - function(x, print_info = TRUE) {\n  y  - x ^ 2\n  if (print_info) {\n    print(paste(x, 'to the power two equals', y))\n  }\n  return(y)\n}\n\n#pow_two(2)\npow_two(2, FALSE)  ## [1] 4  R you functional?  # The linkedin and facebook vectors\nlinkedin  - c(16, 9, 13, 5, NA, 17, 14)\nfacebook  - c(17, 7, 5, 16, 8, 13, 14)\n\n# Define the interpret function\ninterpret  - function(arg) {\n    if (arg   15) {\n        print('You\\'re popular!')\n        return(arg)\n    } else {\n        print('Try to be more visible!')\n        return(0)\n    }\n}\n\ninterpret(linkedin[1])  ## [1] \"You're popular!\"\n\n## [1] 16  interpret(facebook[2])  ## [1] \"Try to be more visible!\"\n\n## [1] 0  R you functional? (2)  # The linkedin and facebook vectors\nlinkedin  - c(16, 9, 13, 5, 2, 17, 14)\nfacebook  - c(17, 7, 5, 16, 8, 13, 14)\n\n# The interpret() can be used inside interpret_all()\ninterpret  - function(num_views){\n  if (num_views   15) {\n    print('You\\'re popular!')\n    return(num_views)\n  } else {\n    print('Try to be more visible!')\n    return(0)\n  }\n}\n\n# Define the interpret_all() function\ninterpret_all  - function(data, logi = TRUE){\n  yy  - 0\n  for (i in data) {\n    yy  - yy + interpret(i)\n  }\n  if (logi) {\n    return(yy)\n  } else {\n    return(NULL)\n  }\n}\n\n# Call the interpret_all() function on both linkedin and facebook\ninterpret_all(linkedin)  ## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33  interpret_all(facebook)  ## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33  Load an R package  # The mtcars vectors have already been prepared for you\nwt  - mtcars$wt\nhp  - mtcars$hp\n\n# Request the currently attached packages\nsearch()  ##  [1] \".GlobalEnv\"            \"package:XLConnect\"    \n##  [3] \"package:XLConnectJars\" \"package:stats\"        \n##  [5] \"package:graphics\"      \"package:grDevices\"    \n##  [7] \"package:utils\"         \"package:datasets\"     \n##  [9] \"package:methods\"       \"Autoloads\"            \n## [11] \"package:base\"  # Try the qplot() function with wt and hp\nplot(wt,hp)\n\n# Load the ggplot2 package\nlibrary('ggplot2')   # or\nrequire('ggplot2')\n\n# Retry the qplot() function\nqplot(wt,hp)   # Check out the currently attached packages again\nsearch()  ##  [1] \".GlobalEnv\"            \"package:ggplot2\"      \n##  [3] \"package:XLConnect\"     \"package:XLConnectJars\"\n##  [5] \"package:stats\"         \"package:graphics\"     \n##  [7] \"package:grDevices\"     \"package:utils\"        \n##  [9] \"package:datasets\"      \"package:methods\"      \n## [11] \"Autoloads\"             \"package:base\"", 
            "title": "3, Functions"
        }, 
        {
            "location": "/Intermediate_R/#4-the-apply-family", 
            "text": "Use  lapply  with a built-in R function  # The vector pioneers\npioneers  - c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\n# Split names from birth year: split_math\nsplit_math  - strsplit(pioneers, ':')\n\n# Convert to lowercase strings: split_low\nsplit_low  - lapply(split_math,tolower)\n\n# Take a look at the structure of split_low\nstr(split_low)  ## List of 4\n##  $ : chr [1:2] \"gauss\" \"1777\"\n##  $ : chr [1:2] \"bayes\" \"1702\"\n##  $ : chr [1:2] \"pascal\" \"1623\"\n##  $ : chr [1:2] \"pearson\" \"1857\"  Use  lapply  with your own function  # Code from previous exercise\npioneers  - c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\nsplit  - strsplit(pioneers, split = ':')\nsplit_low  - lapply(split, tolower)\n\n# Write function select_first()\nselect_first  - function(x) {\n    return(x[1])\n}\n\n# Apply select_first() over split_low: names\nnames  - lapply(split_low, select_first)\nprint(names)  ## [[1]]\n## [1] \"gauss\"\n## \n## [[2]]\n## [1] \"bayes\"\n## \n## [[3]]\n## [1] \"pascal\"\n## \n## [[4]]\n## [1] \"pearson\"  # Write function select_second()\nselect_second  - function(x) {\n    return(x[2])\n}\n\n# Apply select_second() over split_low: years\nyears  - lapply(split_low, select_second)\nprint(years)  ## [[1]]\n## [1] \"1777\"\n## \n## [[2]]\n## [1] \"1702\"\n## \n## [[3]]\n## [1] \"1623\"\n## \n## [[4]]\n## [1] \"1857\"  lapply  and anonymous functions  Anonymous function == lambda function.  # Definition of split_low\npioneers  - c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\nsplit  - strsplit(pioneers, split = ':')\nsplit_low  - lapply(split, tolower)\n\n#select_first  - function(x) {\n#  x[1]\n#}\n\nnames  - lapply(split_low, function(x) { x[1] })\n\n#select_second  - function(x) {\n#  x[2]\n#}\n\nyears  - lapply(split_low, function(x) { x[2] })  Use  lapply  with additional arguments  # Definition of split_low\npioneers  - c('GAUSS:1777', 'BAYES:1702', 'PASCAL:1623', 'PEARSON:1857')\n\nsplit  - strsplit(pioneers, split = ':')\nsplit_low  - lapply(split, tolower)\n\n# Replace the select_*() functions by a single function: select_el\nselect_el  - function(x, i) { \n  x[i] \n}\n\n#select_second  - function(x) { \n#  x[2] \n#}\n\n# Call the select_el() function twice on split_low: names and years\nnames  - lapply(split_low, select_el, i=1)\nyears  - lapply(split_low, select_el, 2)  How to use  sapply  temp1  - c(3, 7, 9, 6, -1)\ntemp2  - c(6, 9, 12, 13, 5)\ntemp3  - c(4, 8, 3, -1, -3)\ntemp4  - c(1, 4, 7, 2, -2)\ntemp5  - c(5, 7, 9, 4, 2)\ntemp6  - c(-3, 5, 8, 9, 4)\ntemp7  - c(3, 6, 9, 4, 1)\n\ntemp  - list(temp1, temp2, temp3, temp4, temp5, temp6, temp7)\n\n# Use lapply() to find each day's minimum temperature\nlapply(temp, min)  ## [[1]]\n## [1] -1\n## \n## [[2]]\n## [1] 5\n## \n## [[3]]\n## [1] -3\n## \n## [[4]]\n## [1] -2\n## \n## [[5]]\n## [1] 2\n## \n## [[6]]\n## [1] -3\n## \n## [[7]]\n## [1] 1  # Use sapply() to find each day's minimum temperature\nsapply(temp, min)  ## [1] -1  5 -3 -2  2 -3  1  # Use lapply() to find each day's maximum temperature\nlapply(temp, max)  ## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 13\n## \n## [[3]]\n## [1] 8\n## \n## [[4]]\n## [1] 7\n## \n## [[5]]\n## [1] 9\n## \n## [[6]]\n## [1] 9\n## \n## [[7]]\n## [1] 9  # Use sapply() to find each day's maximum temperature\nsapply(temp, max)  ## [1]  9 13  8  7  9  9  9  sapply  with your own function  # temp is already defined in the workspace\n\n# Define a function calculates the average of the min and max of a vector: extremes_avg\nextremes_avg  - function(x) {\n    return((min(x) + max(x))/2)\n}\n\n# Apply extremes_avg() over temp using sapply()\nsapply(temp, extremes_avg)  ## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0  # Apply extremes_avg() over temp using lapply()\nlapply(temp, extremes_avg)  ## [[1]]\n## [1] 4\n## \n## [[2]]\n## [1] 9\n## \n## [[3]]\n## [1] 2.5\n## \n## [[4]]\n## [1] 2.5\n## \n## [[5]]\n## [1] 5.5\n## \n## [[6]]\n## [1] 3\n## \n## [[7]]\n## [1] 5  sapply  with function returning vector  # temp is already available in the workspace\n\n# Create a function that returns min and max of a vector: extremes\nextremes  - function(x) {\n    c(min(x), max(x))\n}\n\n# Apply extremes() over temp with sapply()\nsapply(temp, extremes)  ##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [1,]   -1    5   -3   -2    2   -3    1\n## [2,]    9   13    8    7    9    9    9  # Apply extremes() over temp with lapply()\nlapply(temp, extremes)  ## [[1]]\n## [1] -1  9\n## \n## [[2]]\n## [1]  5 13\n## \n## [[3]]\n## [1] -3  8\n## \n## [[4]]\n## [1] -2  7\n## \n## [[5]]\n## [1] 2 9\n## \n## [[6]]\n## [1] -3  9\n## \n## [[7]]\n## [1] 1 9  sapply  can t simplify, now what?  # temp is already prepared for you in the workspace\n\n# Create a function that returns all values below zero: below_zero\nbelow_zero  - function(x) {\n    x[x 0]\n}\n\n#below_zero(temp) alone won't work!!!\n\n# Apply below_zero over temp using sapply(): freezing_s\nfreezing_s  - sapply(temp, below_zero)\n\n# Apply below_zero over temp using lapply(): freezing_l\nfreezing_l  - lapply(temp, below_zero)\n\n# Compare freezing_s to freezing_l using identical()\nidentical(freezing_s, freezing_l)  ## [1] TRUE  sapply  with functions that return NULL  # temp is already available in the workspace\n\n# Write a function that 'cat()s' out the average temperatures: print_info\nprint_info  - function(x) {\n    cat('The average temperature is', mean(x), '\\n')\n}\n\n# Apply print_info() over temp using lapply()\nlapply(temp, print_info)  ## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL  # Apply print_info() over temp using sapply()\nsapply(temp, print_info)  ## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL  Use  vapply  # temp is already available in the workspace\n\n# Code the basics() function\nbasics  - function(x) {\n    c(minimum = min(x), average = mean(x), maximum = max(x))\n}\n\n# Apply basics() over temp using vapply()\nvapply(temp, basics, numeric(3))  ##         [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## minimum -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## average  4.8    9  2.2  2.4  5.4  4.6  4.6\n## maximum  9.0   13  8.0  7.0  9.0  9.0  9.0  Use  vapply  (2)  # temp is already available in the workspace\n\n# Definition of the basics() function\nbasics  - function(x) {\n  c(min = min(x), mean = mean(x), median = median(x), max = max(x))\n}\n\n# Fix the error:\nvapply(temp, basics, numeric(4))  ##        [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## min    -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## mean    4.8    9  2.2  2.4  5.4  4.6  4.6\n## median  6.0    9  3.0  2.0  5.0  5.0  4.0\n## max     9.0   13  8.0  7.0  9.0  9.0  9.0  From  sapply  to  vapply  # temp is already defined in the workspace\n\n# Convert to vapply() expression\nvapply(temp, max, numeric(1))  ## [1]  9 13  8  7  9  9  9  # Convert to vapply() expression\nvapply(temp, function(x, y) { mean(x)   y }, y = 5, logical(1))  ## [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  # Definition of get_info (don't change)\nget_info  - function(x, y) { \n  if (mean(x)   y) {\n    return('Not too cold!')\n  } else {\n    return('Pretty cold!')\n  }\n}\n\n# Convert to vapply() expression\nvapply(temp, get_info, y = 5, character(1))  ## [1] \"Pretty cold!\"  \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\" \n## [5] \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\"  Apply your knowledge. Or better yet:  sapply  it?  # work_todos and fun_todos have already been defined\nwork_todos  - c('Schedule call with team', \n                'Fix error in Recommendation System', \n                'Respond to Marc from IT')\n\nfun_todos  - c('Sleep', 'Make arrangements for summer trip')\n\n# Create a list: todos\ntodos  - list(work_todos, fun_todos)\ntodos  ## [[1]]\n## [1] \"Schedule call with team\"           \n## [2] \"Fix error in Recommendation System\"\n## [3] \"Respond to Marc from IT\"           \n## \n## [[2]]\n## [1] \"Sleep\"                             \"Make arrangements for summer trip\"  # Sort the vectors inside todos alphabetically\nlapply(todos, sort)  ## [[1]]\n## [1] \"Fix error in Recommendation System\"\n## [2] \"Respond to Marc from IT\"           \n## [3] \"Schedule call with team\"           \n## \n## [[2]]\n## [1] \"Make arrangements for summer trip\" \"Sleep\"", 
            "title": "4, The apply Family"
        }, 
        {
            "location": "/Intermediate_R/#5-utilities", 
            "text": "Mathematical utilities   abs ; calculate the absolute value.  sum ; calculate the sum of all the values in a data structure.  mean ; calculate the arithmetic mean.  round ; round the values to 0 decimal places by default. Try out  ?round  in the console for variations of  round  and ways to change \n    the number of digits to round to.    # The errors vector\nerrors  - c(1.9, -2.6, 4.0, -9.5, -3.4, 7.3)\n\n# Sum of absolute rounded values of errors\nsum(abs(round(errors)))  ## [1] 29  Find the error  # Vectors\nvec1  - c(1.5, 2.5, 8.4, 3.7, 6.3)\nvec2  - rev(vec1)\n\n# Fix the error\nmean(abs(append(vec1, vec2)))  ## [1] 4.48  Data utilities   seq ; generate sequences, by specifying the from, to and \n    by arguments.  rep ; replicate elements of vectors and lists.  sort ; sort a vector in ascending order. Works on numerics, but \n    also on character strings and logicals.  rev ; reverse the elements in a data structures for which reversal \n    is defined.  str ; display the structure of any R object. append; Merge vectors \n    or lists.  is.* ; check for the class of an R object.  as.* ; convert an R object from one class to another.  unlist ; flatten (possibly embedded) lists to produce a vector.    # The linkedin and facebook vectors\nlinkedin  - list(16, 9, 13, 5, 2, 17, 14)\nfacebook  - list(17, 7, 5, 16, 8, 13, 14)\n\n# Convert linkedin and facebook to a vector: li_vec and fb_vec\nli_vec  - unlist(as.vector(linkedin))\nfb_vec  - unlist(as.vector(facebook))\n\n# Append fb_vec to li_vec: social_vec\nsocial_vec  - append(li_vec, fb_vec)\n\n# Sort social_vec\nsort(social_vec, decreasing = TRUE)  ##  [1] 17 17 16 16 14 14 13 13  9  8  7  5  5  2  Find the error (2)  # Fix me\nround(sum(unlist(list(1.1, 3, 5))))  ## [1] 9  # Fix me\nrep(seq(1, 7, by = 2), times = 7)  ##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7  print(rep(seq(1, 7, by = 2), times = 7))  ##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7  Beat Gauss using R  # Create first sequence: seq1\nseq1  - seq(1,500, by = 3)\nprint(seq1)  ##   [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49\n##  [18]  52  55  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\n##  [35] 103 106 109 112 115 118 121 124 127 130 133 136 139 142 145 148 151\n##  [52] 154 157 160 163 166 169 172 175 178 181 184 187 190 193 196 199 202\n##  [69] 205 208 211 214 217 220 223 226 229 232 235 238 241 244 247 250 253\n##  [86] 256 259 262 265 268 271 274 277 280 283 286 289 292 295 298 301 304\n## [103] 307 310 313 316 319 322 325 328 331 334 337 340 343 346 349 352 355\n## [120] 358 361 364 367 370 373 376 379 382 385 388 391 394 397 400 403 406\n## [137] 409 412 415 418 421 424 427 430 433 436 439 442 445 448 451 454 457\n## [154] 460 463 466 469 472 475 478 481 484 487 490 493 496 499  # Create second sequence: seq2\nseq2  - seq(1200, 900, by = -7)\nprint(seq2)  ##  [1] 1200 1193 1186 1179 1172 1165 1158 1151 1144 1137 1130 1123 1116 1109\n## [15] 1102 1095 1088 1081 1074 1067 1060 1053 1046 1039 1032 1025 1018 1011\n## [29] 1004  997  990  983  976  969  962  955  948  941  934  927  920  913\n## [43]  906  # Calculate total sum of the sequences\nprint(sum(append(seq1, seq2)))  ## [1] 87029  grepl     grep   grepl ; return TRUE when a pattern is found in the corresponding \n    character string.  grep ; return a vector of indices of the character strings that \n    contains the pattern.    # The emails vector has\nemails  - c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use grepl() to match for 'edu'\nprint(grepl(pattern = 'edu', x = emails))  ## [1]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  # Use grep() to match for 'edu', save result to hits\nhits  - grep(pattern = 'edu', x = emails)\nhits  ## [1] 1 2 4 5  # Subset emails using hits\nprint(emails[hits])  ## [1] \"john.doe@ivyleague.edu\"   \"education@world.gov\"     \n## [3] \"invalid.edu\"              \"quant@bigdatacollege.edu\"  grepl     grep  (2)  Consult a regex character chart for more.  # The emails vector\nemails  - c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use grep() to match for .edu addresses more robustly\nprint(grep(pattern = '@.*\\\\.edu$',x = emails))  ## [1] 1 5  # Use grepl() to match for .edu addresses more robustly, save result to hits\nhits  - grepl(pattern = '@.*\\\\.edu$',x = emails)\nhits  ## [1]  TRUE FALSE FALSE FALSE  TRUE FALSE  # Subset emails using hits\nprint(emails[hits])  ## [1] \"john.doe@ivyleague.edu\"   \"quant@bigdatacollege.edu\"  sub     gsub  # The emails vector\nemails  - c('john.doe@ivyleague.edu', 'education@world.gov', 'dalai.lama@peace.org', \n            'invalid.edu', 'quant@bigdatacollege.edu', 'cookie.monster@sesame.tv')\n\n# Use sub() to convert the email domains to datacamp.edu (attempt 1)\nprint(sub(pattern = '@.*\\\\.edu$', replacement = 'datacamp.edu', x = emails))  ## [1] \"john.doedatacamp.edu\"     \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quantdatacamp.edu\"        \"cookie.monster@sesame.tv\"  # Use sub() to convert the email domains to datacamp.edu (attempt 2)\nprint(sub(pattern = '@.*\\\\.edu$', replacement = '@datacamp.edu', x = emails))  ## [1] \"john.doe@datacamp.edu\"    \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quant@datacamp.edu\"       \"cookie.monster@sesame.tv\"  Right here, right now  # Get the current date: today\ntoday  - Sys.Date()\ntoday  ## [1] \"2017-02-15\"  # See what today looks like under the hood\nprint(unclass(today))  ## [1] 17212  # Get the current time: now\nnow  - Sys.time()\nnow  ## [1] \"2017-02-15 11:11:32 EST\"  # See what now looks like under the hood\nprint(unclass(now))  ## [1] 1487175093  Create and format dates     Symbol  Meaning  Example      %d  day as a number (0-31)  31-janv    %a  abbreviated weekday  Mon    %A  unabbreviated weekday  Monday    %m  month (00-12)  00-12    %b  abbreviated month  Jan    %B  unabbreviated month  January    %y  2-digit year  07    %Y  4-digit year  2007    %H  hours as a decimal number  23    %M  minutes as a decimal number  10    %S  seconds as a decimal number  53    %T  shorthand notation for the typical format %H:%M:%S  23:10:53     Find out more with  ?strptime .  R offer default functions for dealing with time and dates. There are \nbetter packages:  date  and  lubridate .  lubridate  enhances time-series packages such as  zoo  and  xts , and \nworks well with  dplyr  for data wrangling.  library(date)\n\n# Definition of character strings representing dates\nstr1  -  May 23, 96 \nstr2  -  2012-3-15 \nstr3  -  30/January/2006 \n\n# Convert the strings to dates: date1, date2, date3\ndate1  - as.date(str1, order =  mdy )\ndate1  ## [1] 23May96  date1  - as.POSIXct(date1, format =  %d %m %y )\ndate1  ## [1] \"1996-05-22 20:00:00 EDT\"  date2  - as.date(str2, order =  ymd )\ndate2  ## [1] 15Mar2012  date2  - as.POSIXct(date2, format =  %d %m %y )\ndate2  ## [1] \"2012-03-14 20:00:00 EDT\"  date3  - as.date(str3, order =  dmy )\ndate3  ## [1] 30Jan2006  date3  - as.POSIXct(date3, format =  %d %m %y )\ndate3  ## [1] \"2006-01-29 19:00:00 EST\"  # Convert dates to formatted strings\nformat(date1,  %A )  ## [1] \"mercredi\"  format(date2,  %d )  ## [1] \"14\"  format(date3,  %b %Y )  ## [1] \"janv. 2006\"  # convert dates to character data\nstrDate2  - as.character(date2)\nstrDate2  ## [1] \"2012-03-14 20:00:00\"  Create and format times  # Definition of character strings representing times\nstr1  -  2012-3-12 14:23:08 \n\n# Convert the strings to POSIXct objects: time1, time2\ntime1  - as.POSIXct(str2, format =  %Y-%m-%d %H:%M:%S )\n\n# Convert times to formatted strings\n\n# Definition of character strings representing dates\nformat(time1,  %M )  ## [1] NA  format(time1,  %I:%M %p )  ## [1] NA  Calculations with dates  # day1, day2, day3, day4 and day5\nday1  - as.Date( 2016-11-21 )\nday2  - as.Date( 2016-11-16 )\nday3  - as.Date( 2016-11-27 )\nday4  - as.Date( 2016-11-14 )\nday5  - as.Date( 2016-12-02 )\n\n# Difference between last and first pizza day\nprint(day5 - day1)  ## Time difference of 11 days  # Create vector pizza\npizza  - c(day1, day2, day3, day4, day5)\n\n# Create differences between consecutive pizza days: day_diff\nday_diff  - diff(pizza, lag = 1, differences = 1)\nday_diff  ## Time differences in days\n## [1]  -5  11 -13  18  # Average period between two consecutive pizza days\nprint(mean(day_diff))  ## Time difference of 2.75 days  Calculus with times  # login and logout\nlogin  - as.POSIXct(c( 2016-11-18 10:18:04 UTC ,  2016-11-23 09:14:18 UTC ,  2016-11-23 12:21:51 UTC ,  2016-11-23 12:37:24 UTC ,  2016-11-25 21:37:55 UTC ))\n\nlogout  - as.POSIXct(c( 2016-11-18 10:56:29 UTC ,  2016-11-23 09:14:52 UTC ,  2016-11-23 12:35:48 UTC ,  2016-11-23 13:17:22 UTC ,  2016-11-25 22:08:47 UTC ))\n\n# Calculate the difference between login and logout: time_online\ntime_online  - logout - login\n\n# Inspect the variable time_online\n#class(time_online)\ntime_online  ## Time differences in secs\n## [1] 2305   34  837 2398 1852  # Calculate the total time online\nprint(sum(time_online))  ## Time difference of 7426 secs  # Calculate the average time online\nprint(mean(time_online))  ## Time difference of 1485.2 secs", 
            "title": "5, Utilities"
        }, 
        {
            "location": "/Intermediate_R_-_apply_Functions/", 
            "text": "Foreword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\napply\n functions\n\n\n# Apply Functions Over Array Margins\napply; sweep; scale\n\n# Apply a Function over a List or Vector\nlapply; sapply; vapply; replicate; rapply\n\n# Apply a Function to Multiple List or Vector Arguments\nmapply; Vectorize\n\n# Apply a Function Over a Ragged Array\ntapply\n\n# Apply a Function to a Data Frame Split by Factors\nby; aggregate; split\n\n# Apply a Function Over Values in an Environment\neapply\n\n\n\n\napply\n, \nsweep\n, \nscale\n\n\n# \napply\n returns a vector or array or list of values obtained by applying a function to margins of an array or matrix.\n(m = matrix(1:6, nrow = 2))\n\n\n\n\n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\n\n\n\napply(m, 1, sum)\n\n\n\n\n## [1]  9 12\n\n\n\napply(m, 1:2, sqrt)\n\n\n\n\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 1.732051 2.236068\n## [2,] 1.414214 2.000000 2.449490\n\n\n\n# \nsweep\n returns an array obtained from an input array by sweeping out a summary statistic.\n(X = array(1:24, dim = 4:2))\n\n\n\n\n## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]   13   17   21\n## [2,]   14   18   22\n## [3,]   15   19   23\n## [4,]   16   20   24\n\n\n\nsweep(X, 1, apply(X, 1, mean))\n\n\n\n\n## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]  -10   -6   -2\n## [2,]  -10   -6   -2\n## [3,]  -10   -6   -2\n## [4,]  -10   -6   -2\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]    2    6   10\n## [2,]    2    6   10\n## [3,]    2    6   10\n## [4,]    2    6   10\n\n\n\n# \nscale\n is generic function whose default method centers and/or scales the columns of a numeric matrix.\n(mat = matrix(round(rnorm(10)*10), nrow = 5))\n\n\n\n\n##      [,1] [,2]\n## [1,]  -25   -9\n## [2,]   -1    6\n## [3,]   -2   21\n## [4,]  -18  -10\n## [5,]   13    2\n\n\n\nscale(mat)\n\n\n\n\n##            [,1]       [,2]\n## [1,] -1.2231382 -0.8682707\n## [2,]  0.3722595  0.3157348\n## [3,]  0.3057846  1.4997404\n## [4,] -0.7578139 -0.9472044\n## [5,]  1.3029081  0.0000000\n## attr(,\"scaled:center\")\n## [1] -6.6  2.0\n## attr(,\"scaled:scale\")\n## [1] 15.04327 12.66886\n\n\n\napply(mat, 2, mean)\n\n\n\n\n## [1] -6.6  2.0\n\n\n\napply(mat, 2, sd)\n\n\n\n\n## [1] 15.04327 12.66886\n\n\n\ncov(scale(mat))\n\n\n\n\n##           [,1]      [,2]\n## [1,] 1.0000000 0.5889881\n## [2,] 0.5889881 1.0000000\n\n\n\nlapply\n, \nsapply\n, \nvapply\n, \nreplicate\n, \nrapply\n\n\n# \nlapply\n returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.\nlapply(1:5, sqrt)\n\n\n\n\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1.414214\n## \n## [[3]]\n## [1] 1.732051\n## \n## [[4]]\n## [1] 2\n## \n## [[5]]\n## [1] 2.236068\n\n\n\n# \nsapply\n is a user-friendly version of lapply by default returning a vector or matrix if appropriate.\nsapply(1:5, sqrt)\n\n\n\n\n## [1] 1.000000 1.414214 1.732051 2.000000 2.236068\n\n\n\n# \nvapply\n is similar to sapply, but has a pre-specified type of return value, so it can be safer (and sometimes faster) to use.\nvapply(1:5, sqrt, 1i)\n\n\n\n\n## [1] 1.000000+0i 1.414214+0i 1.732051+0i 2.000000+0i 2.236068+0i\n\n\n\n# \nreplicate\n is a wrapper for the common use of sapply for repeated evaluation of an expression (which will usually involve random number generation).\nreplicate(3, rnorm(5))\n\n\n\n\n##             [,1]       [,2]       [,3]\n## [1,]  1.27143977  1.2557048 -1.8773681\n## [2,]  1.39467927 -0.4677682  1.0071086\n## [3,] -1.40587412 -0.4815548  1.6167008\n## [4,] -0.69712942  0.6934136 -0.4936395\n## [5,]  0.06646416 -0.4202453  0.3566716\n\n\n\n# \nrapply\n is a recursive version of lapply.\n(x = list(A = list(a = pi, b = list(b1 = 1)), B = \na character string\n))\n\n\n\n\n## $A\n## $A$a\n## [1] 3.141593\n## \n## $A$b\n## $A$b$b1\n## [1] 1\n## \n## \n## \n## $B\n## [1] \"a character string\"\n\n\n\nrapply(x, sqrt, classes = \nnumeric\n, how = \nunlist\n)\n\n\n\n\n##      A.a   A.b.b1 \n## 1.772454 1.000000\n\n\n\nrapply(x, sqrt, classes = \nnumeric\n, how = \nreplace\n)\n\n\n\n\n## $A\n## $A$a\n## [1] 1.772454\n## \n## $A$b\n## $A$b$b1\n## [1] 1\n## \n## \n## \n## $B\n## [1] \"a character string\"\n\n\n\nrapply(x, sqrt, classes = \nnumeric\n, how = \nlist\n)\n\n\n\n\n## $A\n## $A$a\n## [1] 1.772454\n## \n## $A$b\n## $A$b$b1\n## [1] 1\n## \n## \n## \n## $B\n## NULL\n\n\n\nrapply(x, sqrt, classes = \nnumeric\n, deflt = NA, how = \nunlist\n)\n\n\n\n\n##      A.a   A.b.b1        B \n## 1.772454 1.000000       NA\n\n\n\nrapply(x, sqrt, classes = \nnumeric\n, deflt = NA, how = \nreplace\n)\n\n\n\n\n## $A\n## $A$a\n## [1] 1.772454\n## \n## $A$b\n## $A$b$b1\n## [1] 1\n## \n## \n## \n## $B\n## [1] \"a character string\"\n\n\n\nrapply(x, sqrt, classes = \nnumeric\n, deflt = NA, how = \nlist\n)\n\n\n\n\n## $A\n## $A$a\n## [1] 1.772454\n## \n## $A$b\n## $A$b$b1\n## [1] 1\n## \n## \n## \n## $B\n## [1] NA\n\n\n\nrapply(x, round, classes = \nnumeric\n, how = \nreplace\n, digits = 2)\n\n\n\n\n## $A\n## $A$a\n## [1] 3.14\n## \n## $A$b\n## $A$b$b1\n## [1] 1\n## \n## \n## \n## $B\n## [1] \"a character string\"\n\n\n\nmapply\n, \nVectorize\n\n\n# \nmapply\n is a multivariate version of sapply. mapply applies FUN to the first elements of each argument, the second elements, the third elements, and so on. Arguments are recycled if necessary.\nmapply(rep, LETTERS[1:3], 1:3)\n\n\n\n\n## $A\n## [1] \"A\"\n## \n## $B\n## [1] \"B\" \"B\"\n## \n## $C\n## [1] \"C\" \"C\" \"C\"\n\n\n\n# \nVectorize\n returns a new function that acts as if mapply was called.\nvrep = Vectorize(rep)\nvrep(LETTERS[1:3], 1:3)\n\n\n\n\n## [1] \"A\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\n\n\nvrep = Vectorize(rep.int)\nvrep(LETTERS[1:3], 1:3)\n\n\n\n\n## $A\n## [1] \"A\"\n## \n## $B\n## [1] \"B\" \"B\"\n## \n## $C\n## [1] \"C\" \"C\" \"C\"\n\n\n\ntapply\n\n\n# \ntapply\n applies a function to each cell of a ragged array, that is to each (non-empty) group of values given by a unique combination of the levels of certain factors.\n(m = matrix(1:6, 2, 3))\n\n\n\n\n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\n\n\n\n(fac = matrix(c(1,3,1,2,2,2), 2, 3))\n\n\n\n\n##      [,1] [,2] [,3]\n## [1,]    1    1    2\n## [2,]    3    2    2\n\n\n\ntapply(m, fac, max)\n\n\n\n\n## 1 2 3 \n## 3 6 2\n\n\n\nby\n, \naggregate\n, \nsplit\n\n\n# \nby\n is an object-oriented wrapper for tapply applied to data frames.\nby(mtcars$mpg, mtcars$cyl, max)\n\n\n\n\n## mtcars$cyl: 4\n## [1] 33.9\n## -------------------------------------------------------- \n## mtcars$cyl: 6\n## [1] 21.4\n## -------------------------------------------------------- \n## mtcars$cyl: 8\n## [1] 19.2\n\n\n\n# \naggregate\n splits the data into subsets, computes summary statistics for each, and returns the result in a convenient form.\naggregate(mpg ~ cyl, data = mtcars, max)\n\n\n\n\n##   cyl  mpg\n## 1   4 33.9\n## 2   6 21.4\n## 3   8 19.2\n\n\n\n# See help for usage of NA, formula, dot notation, xtabs, nfrequency.\n\n# \nsplit\n divides the data in the vector x into the groups defined by f. The replacement forms replace values corresponding to such a division.\nsplit(mtcars[1], mtcars[2])\n\n\n\n\n## $`4`\n##                 mpg\n## Datsun 710     22.8\n## Merc 240D      24.4\n## Merc 230       22.8\n## Fiat 128       32.4\n## Honda Civic    30.4\n## Toyota Corolla 33.9\n## Toyota Corona  21.5\n## Fiat X1-9      27.3\n## Porsche 914-2  26.0\n## Lotus Europa   30.4\n## Volvo 142E     21.4\n## \n## $`6`\n##                 mpg\n## Mazda RX4      21.0\n## Mazda RX4 Wag  21.0\n## Hornet 4 Drive 21.4\n## Valiant        18.1\n## Merc 280       19.2\n## Merc 280C      17.8\n## Ferrari Dino   19.7\n## \n## $`8`\n##                      mpg\n## Hornet Sportabout   18.7\n## Duster 360          14.3\n## Merc 450SE          16.4\n## Merc 450SL          17.3\n## Merc 450SLC         15.2\n## Cadillac Fleetwood  10.4\n## Lincoln Continental 10.4\n## Chrysler Imperial   14.7\n## Dodge Challenger    15.5\n## AMC Javelin         15.2\n## Camaro Z28          13.3\n## Pontiac Firebird    19.2\n## Ford Pantera L      15.8\n## Maserati Bora       15.0\n\n\n\neapply\n\n\n# \neapply\n applies FUN to the named values from an environment and returns the results as a list. The user can request that all named objects are used (normally names that begin with a dot are not). The output is not sorted and no parent environments are searched.\nenv = new.env()\nenv$x = 1:3\nenv$y = 4:6\nenv$x\n\n\n\n\n## [1] 1 2 3\n\n\n\nenv$y\n\n\n\n\n## [1] 4 5 6\n\n\n\neapply(env, sum)\n\n\n\n\n## $x\n## [1] 6\n## \n## $y\n## [1] 15", 
            "title": "Intermediate R - apply Functions"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/", 
            "text": "Preliminary Options\n\n\nChunks\n\n\nInputting Data\n\n\nFormatting Code Chunks\n\n\nFormatting Plot Chunk\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nCode snippets and results.\n\n\n\n\n\n\nPreliminary Options\n\n\nUsually, the following code is set to FALSE (not showing in a report).\n\nThese are the general options. Code and plot chunks overrule the general\n\noptions.\n\n\nknitr::opts_chunk$set(echo=TRUE, eval=TRUE, fig.height=3, fig.width=3)\n\n\n\n\nChunks\n\n\nNaming a chunk is including it in the document outline. The outline is a\n\nnavigation tool to jump though the document.\n\n\nInputting Data\n\n\nThe dataset comes from the \nUS Census\n\nBureau\n.\n\nOn their website, open Excel file \nNST-EST2011-02\n about the annual\n\nestimates of the resident population.\n\n\nThe data have become an object: a data frame. Check it out, and add\n\ncolumn names:\n\n\nhead(USstatePops,3)\n\n\n\n\n##        V1      V2\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013\n\n\n\ncolnames(USstatePops) \n- c('State', 'Pop')\n\nhead(USstatePops, 3)\n\n\n\n\n##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013\n\n\n\nCheck out the data frame:\n\n\nstr(USstatePops)\n\n\n\n\n## 'data.frame':    51 obs. of  2 variables:\n##  $ State: Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...\n\n\n\nThe states should be strings, not factors.\n\n\nExtract numbers or strings without any loss from a factor structure:\n\n\n# make a copy for safety\nUSstatePops2 \n- USstatePops\n\nUSstatePops2$State \n- as.character(levels(USstatePops2$State))\n\n\n\n\nCheck out the new data frame:\n\n\nstr(USstatePops2)\n\n\n\n\n## 'data.frame':    51 obs. of  2 variables:\n##  $ State: chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...\n\n\n\nhead(USstatePops2, 3)\n\n\n\n\n##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013\n\n\n\nFormatting Code Chunks\n\n\neval=TRUE\n; show the results (default).\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\neval=FALSE\n; or no results.\n\n\nmean(USstatePops2$Pop)\n\n\n\n\nresults='markup'\n; show split code/results/code/results (default).\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nmedian(USstatePops2$Pop)\n\n\n\n\n## [1] 4339362\n\n\n\neval='asis'\n; show \nunboxed\n results.\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n\n\n1\n 6053834\n\n\nmedian(USstatePops2$Pop)\n\n\n\n\n\n\n1\n 4339362\n\n\neval='hide'\n; show code only.\n\n\nmean(USstatePops2$Pop)\nmedian(USstatePops2$Pop)\n\n\n\n\neval='hold'\n; show code block/results block.\n\n\nmean(USstatePops2$Pop)\nmedian(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n## [1] 4339362\n\n\n\n\n\necho=TRUE\n; show the code (default).\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nmedian(USstatePops2$Pop)\n\n\n\n\n## [1] 4339362\n\n\n\necho=FALSE\n; or no code.\n\n\n## [1] 6053834\n\n## [1] 4339362\n\n\n\n\n\nwarning\n, \nerror\n, \nmessage\n are set to TRUE by default. They can be set of FALSE when running a library() code to avoid polluting the report.\n\n\n{r, warning=TRUE, error=TRUE, message=TRUE}\n\n\n\n\n\n\ntidy=TRUE/FALSE\n; with the \nformatR\n and \nshiny\n packages (you manage spaces and indents) (FALSE by default).\n\n\n{r, tidy=TRUE}\n\n\n\n\n\n\ncache=TRUE/FALSE; cache the results (FALSE by default).\n\n\nCan be resused in future knits since it \ncreates a subdir (the\n\n\ncache\n)\n with a R workspace, .rdb and .rdx files.\n\n\n{r, cache=TRUE}\n\n\n\n\nThe \ncache.path='cache/'\n can be changed. See \ncache-comments\n,\n\n\ncache.lazy\n, \ncache.vars\n, \nautodep\n, \ndependson\n.\n\n\n\n\ncomment='##'\n; the comments in results (by default).\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\ncomment='#'\n; the new comments.\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n# [1] 6053834\n\n\n\n\n\ncode chunk \n{r}\n.\n\n\nlist \n- c(1, 2, 3)\nlist\n\n\n\n\n## [1] 1 2 3\n\n\n\ncode chunk \n{code=NULL}\n.\n\n\nlist \n- c(1, 2, 3)\nlist\n\n\n\n\ncode chunk \n{text}\n.\n\n\nlist \n- c(1, 2, 3)\nlist\n\n\n\n\ncode chunk \n{python}\n.\n\n\nlist = [1, 2, 3]\nprint(list)\n\n\n\n\nSet up the new language first.\n\n\n\n\nhightlight=TRUE\n; hightlight the code (default).\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nhightlight=FALSE\n; or not.\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nprompt=TRUE\n; add \n before the code.\n\n\n mean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nprompt=FALSE\n; or not (default).\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nstrip.white=TRUE\n; remove white space from the code (default).\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nstrip.white=FALSE\n; or not.\n\n\nmean(USstatePops2$Pop)\n\n\n\n\n## [1] 6053834\n\n\n\nFormatting Plot Chunk\n\n\nPrints the plots in the .html report and and creates a subdir with the\n\nplot files (the references).\n\n\nfig.path='figure/'\n; new file path for this chunk.\n\n\nOtherwise, the path is set in the general options.\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\nThe device prints .png files by default.\n\n\nIt can be changed to other formats.\n\n\ndev='png'\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\ndev='jpeg'\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\ndev='pdf'\n; \n'pdf'\n cannot be printed in the .html report, but only\n\nincluded in the subdir.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nfig.width= , fig.height=\n; change the box size (\n=7\n by default).\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.width=5, fig.height=5\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.height=3\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.width=3\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nout.height=100, out.width=100\n; in pixels.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nresize.height=200, resize.width=200\n; resize tike graphics for latex, in pixels.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nsanitize=TRUE\n; sanitize \ntike\n graphics for latex.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nSet the device arguments:\n\n\ndev.args=list(bg='yellow', pointsize=10)\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\ndev.args=list(pointsize=8), fig.height=3\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\ndots per inch.\n\n\ndpi=72\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\ndpi=90\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\ndpi multiplier for .html output on retina screens:\n\n\nfig.retina=1\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.retina=2\n; double dpi.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nfig.align='left'\n or \nfig.align='default'\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.align='right'\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nfig.align='center'\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\n\n\nFigure captions at the bottom of the plot; figure caption in latex:\n\n\nfig.cap='CAPTION 14'\n.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nSee:\n\n\n\n\nfig.lp=''\n; figure caption prefix\n\n\nfig.scap=''\n; short figure caption prefix.\n\n\nfig.subcap=''\n; subcaption.\n\n\nfig.env=''\n; the latex environment for figures.\n\n\n\n\n\n\nVersions\n\n\nfig.keep='high'\n; merge low-level changes into high-level plots.\n\n\nhist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')\n\n\n\n\n\n\nSee:\n\n\n\n\nfig.keep='all'\n; keep all plots (low-level changes may produce\n\n    new plots).\n\n\nfig.keep='first'/'last'\n; keep the first/last plot only.\n\n\nfig.keep='none'\n; discard all plots.\n\n\n\n\n\n\nfig.pos='test'\n; string to be used as the figure position arrangement in latex.\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\n\n\nShow\n\n\nfig.show='asis'\n.\n\n\nhist(USstatePops2$Pop, breaks = 10, main = '', xlab = 'Pop')\n\n\n\n\n\n\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\n\n\nfig.show='hold'\n; display the plots at the very end of the chunk.\n\n\n\n\n\n\n\nhist(USstatePops2$Pop, breaks = 10, main = '', xlab = 'Pop')\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')\n\n\n\n\n\n\nSee:\n\n\n\n\nfig.show='hide'\n; generate the plots, but not in the\n\n    final document.\n\n\nfig.show='animate'\n; combine all of the plots created into\n\n    an animation. Additional packages and settings are required.", 
            "title": "Code & Plot Chunk Options"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#chunks", 
            "text": "Naming a chunk is including it in the document outline. The outline is a \nnavigation tool to jump though the document.", 
            "title": "Chunks"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#inputting-data", 
            "text": "The dataset comes from the  US Census \nBureau . \nOn their website, open Excel file  NST-EST2011-02  about the annual \nestimates of the resident population.  The data have become an object: a data frame. Check it out, and add \ncolumn names:  head(USstatePops,3)  ##        V1      V2\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013  colnames(USstatePops)  - c('State', 'Pop')\n\nhead(USstatePops, 3)  ##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013  Check out the data frame:  str(USstatePops)  ## 'data.frame':    51 obs. of  2 variables:\n##  $ State: Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...  The states should be strings, not factors.  Extract numbers or strings without any loss from a factor structure:  # make a copy for safety\nUSstatePops2  - USstatePops\n\nUSstatePops2$State  - as.character(levels(USstatePops2$State))  Check out the new data frame:  str(USstatePops2)  ## 'data.frame':    51 obs. of  2 variables:\n##  $ State: chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...  head(USstatePops2, 3)  ##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013", 
            "title": "Inputting Data"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#formatting-code-chunks", 
            "text": "", 
            "title": "Formatting Code Chunks"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#evaltrue-show-the-results-default", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834", 
            "title": "eval=TRUE; show the results (default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#evalfalse-or-no-results", 
            "text": "mean(USstatePops2$Pop)", 
            "title": "eval=FALSE; or no results."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#resultsmarkup-show-split-coderesultscoderesults-default", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834  median(USstatePops2$Pop)  ## [1] 4339362", 
            "title": "results='markup'; show split code/results/code/results (default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#evalasis-show-unboxed-results", 
            "text": "mean(USstatePops2$Pop)   1  6053834  median(USstatePops2$Pop)   1  4339362", 
            "title": "eval='asis'; show 'unboxed' results."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#evalhide-show-code-only", 
            "text": "mean(USstatePops2$Pop)\nmedian(USstatePops2$Pop)", 
            "title": "eval='hide'; show code only."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#evalhold-show-code-blockresults-block", 
            "text": "mean(USstatePops2$Pop)\nmedian(USstatePops2$Pop)  ## [1] 6053834\n## [1] 4339362", 
            "title": "eval='hold'; show code block/results block."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#echotrue-show-the-code-default", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834  median(USstatePops2$Pop)  ## [1] 4339362", 
            "title": "echo=TRUE; show the code (default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#echofalse-or-no-code", 
            "text": "## [1] 6053834\n\n## [1] 4339362", 
            "title": "echo=FALSE; or no code."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#warning-error-message-are-set-to-true-by-default-they-can-be-set-of-false-when-running-a-library-code-to-avoid-polluting-the-report", 
            "text": "{r, warning=TRUE, error=TRUE, message=TRUE}", 
            "title": "warning, error, message are set to TRUE by default. They can be set of FALSE when running a library() code to avoid polluting the report."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#tidytruefalse-with-the-formatr-and-shiny-packages-you-manage-spaces-and-indents-false-by-default", 
            "text": "{r, tidy=TRUE}", 
            "title": "tidy=TRUE/FALSE; with the formatR and shiny packages (you manage spaces and indents) (FALSE by default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#cachetruefalse-cache-the-results-false-by-default", 
            "text": "Can be resused in future knits since it  creates a subdir (the  cache )  with a R workspace, .rdb and .rdx files.  {r, cache=TRUE}  The  cache.path='cache/'  can be changed. See  cache-comments ,  cache.lazy ,  cache.vars ,  autodep ,  dependson .", 
            "title": "cache=TRUE/FALSE; cache the results (FALSE by default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#comment-the-comments-in-results-by-default", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834", 
            "title": "comment='##'; the comments in results (by default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#comment-the-new-comments", 
            "text": "mean(USstatePops2$Pop)  # [1] 6053834", 
            "title": "comment='#'; the new comments."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-r", 
            "text": "list  - c(1, 2, 3)\nlist  ## [1] 1 2 3", 
            "title": "code chunk {r}."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-codenull", 
            "text": "list  - c(1, 2, 3)\nlist", 
            "title": "code chunk {code=NULL}."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-text", 
            "text": "list  - c(1, 2, 3)\nlist", 
            "title": "code chunk {text}."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-python", 
            "text": "list = [1, 2, 3]\nprint(list)  Set up the new language first.", 
            "title": "code chunk {python}."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#hightlighttrue-hightlight-the-code-default", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834", 
            "title": "hightlight=TRUE; hightlight the code (default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#hightlightfalse-or-not", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834", 
            "title": "hightlight=FALSE; or not."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#prompttrue-add-before-the-code", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834", 
            "title": "prompt=TRUE; add &gt; before the code."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#promptfalse-or-not-default", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834", 
            "title": "prompt=FALSE; or not (default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#stripwhitetrue-remove-white-space-from-the-code-default", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834", 
            "title": "strip.white=TRUE; remove white space from the code (default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#stripwhitefalse-or-not", 
            "text": "mean(USstatePops2$Pop)  ## [1] 6053834", 
            "title": "strip.white=FALSE; or not."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#formatting-plot-chunk", 
            "text": "Prints the plots in the .html report and and creates a subdir with the \nplot files (the references).", 
            "title": "Formatting Plot Chunk"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figpathfigure-new-file-path-for-this-chunk", 
            "text": "Otherwise, the path is set in the general options.  hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')", 
            "title": "fig.path='figure/'; new file path for this chunk."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#the-device-prints-png-files-by-default", 
            "text": "It can be changed to other formats.  dev='png' .  hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')   dev='jpeg' .  hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')   dev='pdf' ;  'pdf'  cannot be printed in the .html report, but only \nincluded in the subdir.  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "The device prints .png files by default."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figwidth-figheight-change-the-box-size-7-by-default", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "fig.width= , fig.height=; change the box size (=7 by default)."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figwidth5-figheight5", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "fig.width=5, fig.height=5."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figheight3", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "fig.height=3."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figwidth3", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "fig.width=3."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#outheight100-outwidth100-in-pixels", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "out.height=100, out.width=100; in pixels."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#resizeheight200-resizewidth200-resize-tike-graphics-for-latex-in-pixels", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "resize.height=200, resize.width=200; resize tike graphics for latex, in pixels."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#sanitizetrue-sanitize-tike-graphics-for-latex", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "sanitize=TRUE; sanitize 'tike' graphics for latex."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#set-the-device-arguments", 
            "text": "dev.args=list(bg='yellow', pointsize=10) .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   dev.args=list(pointsize=8), fig.height=3 .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "Set the device arguments:"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#dots-per-inch", 
            "text": "dpi=72 .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   dpi=90 .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "dots per inch."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#dpi-multiplier-for-html-output-on-retina-screens", 
            "text": "fig.retina=1  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   fig.retina=2 ; double dpi.  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "dpi multiplier for .html output on retina screens:"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figalignleft-or-figaligndefault", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "fig.align='left' or fig.align='default'."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figalignright", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "fig.align='right'."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figaligncenter", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')", 
            "title": "fig.align='center'."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figure-captions-at-the-bottom-of-the-plot-figure-caption-in-latex", 
            "text": "fig.cap='CAPTION 14' .  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   See:   fig.lp='' ; figure caption prefix  fig.scap='' ; short figure caption prefix.  fig.subcap='' ; subcaption.  fig.env='' ; the latex environment for figures.", 
            "title": "Figure captions at the bottom of the plot; figure caption in latex:"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#versions", 
            "text": "fig.keep='high' ; merge low-level changes into high-level plots.  hist(USstatePops2$Pop, breaks = 20, main ='', xlab = 'Pop')   See:   fig.keep='all' ; keep all plots (low-level changes may produce \n    new plots).  fig.keep='first'/'last' ; keep the first/last plot only.  fig.keep='none' ; discard all plots.", 
            "title": "Versions"
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#figpostest-string-to-be-used-as-the-figure-position-arrangement-in-latex", 
            "text": "hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')", 
            "title": "fig.pos='test'; string to be used as the figure position arrangement in latex."
        }, 
        {
            "location": "/Code___Plot_Chunk_Options/#show", 
            "text": "fig.show='asis' .  hist(USstatePops2$Pop, breaks = 10, main = '', xlab = 'Pop')   hist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')    fig.show='hold' ; display the plots at the very end of the chunk.    hist(USstatePops2$Pop, breaks = 10, main = '', xlab = 'Pop')\nhist(USstatePops2$Pop, breaks = 20, main = '', xlab = 'Pop')   See:   fig.show='hide' ; generate the plots, but not in the \n    final document.  fig.show='animate' ; combine all of the plots created into \n    an animation. Additional packages and settings are required.", 
            "title": "Show"
        }, 
        {
            "location": "/Working_with_RStudio_IDE/", 
            "text": "Working with RStudio IDE\n\n    (Part 1)\n\n\n1, Orientation\n\n\n2, Programming\n\n\n3, Project\n\n\n\n\n\n\nWorking with RStudio IDE\n\n    (Part 2)\n\n\n1, Packages\n\n\n2, Version Control\n\n\n3, Reporting\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\n\n\n\n\nWorking with RStudio IDE (Part 1)\n\n\nHELP\n\n\n\n\nAlt+Shift+k or Tools/Keyboard Shortcuts Help.\n\n\n\n\n1, Orientation\n\n\nCommands\n\n\n\n\nCtrl+Up; command history.\n\n\nTab; completion for all.\n\n\nCtrl+L; clear the console.\n\n\nTools/Global Options\n set Up RStudio.\n\n\nview(dataframe)\n; open a spreadsheet, show in new window, sort the\n\n    rows, search, filter the rows.\n\n\n\n\n\n\n\ndf \n- data.frame(colA = c(1, 2), colB = c(3, 6))\ndf\n\n\n\n\n##   colA colB\n## 1    1    3\n## 2    2    6\n\n\n\nView(df) # in a new window\n\n\n\n\n\n\nIDE panes\n\n\n\n\nEnvironment pane; load, save, remove objects, read a dataset,\n\n    import dataset.\n\n\nHistory pane; idem, clear all or one item at the time, copy the\n\n    command in the console pane, reload a command with Shift+Enter and\n\n    run it.\n\n\nFile pane; working directories, files, add a new folder, rename\n\n\ngetwd()\n; get working directory.\n\n\nsetwd()\n; set working directory.\n\n\nPlot pane; save (extension, size, resolution).\n\n\nPackage pane; update.\n\n\nHelp pane; pages.\n\n\nViewer pane; more than plots!\n\n\n\n\n2, Programming\n\n\nScripting\n\n\n\n\nCtrl+Shift+m; pipe or \n%\n%\n.\n\n\nAlt+-; \n-\n.\n\n\nCtrl+Shift+C; add/delete a \n#\n for commenting.\n\n\n\n\n\n\n\n%\n% \n\n\n- \n\n# comment\n\n\n\n\nCode\n\n\n\n\n\n\nCtrl+Alt+i; new chunk.\n\n\n\n\n\n\nConvert into a function.\n\n\n\n\n\n\nType code,\n\n\n\n\nHighlight it,\n\n\nCode/Extract Function to create a function:\n\n\n\n\nThis:\n\n\nrnorm(10, 0, 1)\n\n\n\n\nBecomes:\n\n\nrnorm \n- function() {\n  rnorm(10, 0, 1)\n}\n\n\n\n\n\n\nCtrl+Alt+click; multiple cursors for typing!\n\n\nSwitch between Default, Vim and Emacs modes with\n\n    Tools/Global Options/Code/Keybindings.\n\n\nShift+Alt+g; go to line.\n\n\nCtrl+f: find/replace in the current document.\n\n\nAlt+o, Alt+Shift+o; fold/unfold the code.\n\n\nCtrl+p; jump between symbols like (), {}, [].\n\n\nCtrl+Shift+Enter; run and source the code.\n\n\nCtrl+Enter; run and source the code.\n\n\nCtrl+Shift+F10; restart, refresh the R session.\n\n\n\n\nError handling\n\n\nRStudio traces back the error origin.\n\n\nToggle the show/hide traceback in the console when there is an error or\n\nrerun with the bug, watch the right pane for traceback.\n\n\nInvestigate, highlight the next line of code, click in the traceback\n\nwindow, click the dropdown menu in the Global Environment (upper-right).\n\nStop, continue in the debugger mode, press c, press q.\n\n\nAdd/remove breakpoints, where the line numbers are.\n\n\n\n\ndebugonce\n; automatically call the debugger when the function is\n\n    called, but only once.\n\n\ndebug\n, \nundebug\n; automatically call the debugger when the\n\n    function is called.\n\n\n\n\nAdd \noptions(error=browser)\n or \noptions(error=NULL)\n at the beginning\n\nof the script; R automatically open the debugger mode.\n\n\n\n\nn; next line.\n\n\nstep-into icon.\n\n\nShift+F4; step into.\n\n\nShift+F6; execute the remainder of the bug.\n\n\n\n\n3, Project\n\n\n\n\nCreate a project with a folder and all the files (Global\n\n    Environment, History, etc.): New Directory, Existing Directory,\n\n    Version Control\n Empty Project, R Package (project), Shiny\n\n    Web Application.\n\n\nCreate a Git repository with the new project\n\n\n\n\nCommands\n\n\n\n\nCtrl+Shift+f; find in files.\n\n\nCtrl+F9, F10; go backwards/forwards.\n\n\n\n\nPackrat\n\n\nPackrat is a dependency management system for R. Use one version of a\n\npackage for one project, another version of a package for another\n\nproject. Associate a project with its own set of packages.\n\n\n\n\nrstudio.github.io/packrat/\n\n\nActivate Packrat when creating a project or\n\n    Tools/Project Options/Packrat.\n\n\nThe library is virtually separate from R library.\n\n\nPerfect for collaborating with GitHub\n\n\n\n\n\n\nWorking with RStudio IDE (Part 2)\n\n\n1, Packages\n\n\nIntroduction to R packages\n\n\nAnything that can be automated, should be automated. Do as little as\n\npossible by hand. Do as much as possible with functions.\n\n\nProcess:\n\n\n\n\nWriting R functions.\n\n\nDocumenting functions.\n\n\nWriting tests.\n\n\nChecking compatibility.\n\n\nBuilding the package for sharing and using.\n\n\n\n\nSee the book: \nR Packages\n.\n\n\nCreate a new R package\n\n\nCreate a new directory with new files, folders, and meta-information.\n\n\nTo update or not to update\n\n\nRStudio generates the NAMESPACE content for you automatically.\n\n\nRead r-pkgs.had.co.nz/description.html\n\n\nImport \n load source files\n\n\nMove existing functions from an existing project into the new package.\n\nThe function are moves to a new folder in the project.\n\n\nTest created functions all in once with the Load All command in the\n\nBuild Tab.\n\n\nSimulations:\n\n\n\n\nCtrl+Shift+L\n\n\nCtrl+Shift+F10; restart a R session.\n\n\n\n\nPackages documentation\n\n\nCreate a help page (.Rd file). Written in HTML.\n\n\nUse the \nroxigen\n \npackage\n to document.\n\n\nTools/Project Optons/Build Tools/Generate documentation. Generate the\n\ndoc, add comments.\n\n\nFirst, create a doc skeleton above the function:\n\n\n\n\nCtrl+Alt+Shift+R\n\n\n\n\n\n\n\n#' Title\n#'\n#' @param x \n#'\n#' @return\n#' @export\n#'\n#' @examples\ncEnter \n- function(x) {\n  x - mean(x)\n}\n\n\n\n\nFill in the blanks.\n\n\nPackage documentation (2)\n\n\nTitle of the help page. Text on how to use.\n\n\nThe 4 tags (\n@\n) help organizing the doc: \n@export\n (tells R that this\n\nfunction should be made available to people who load your package. ),\n\n\n@params\n, \n@return\n, and \n@examples\n. There are many more advanced\n\ntags.\n\n\nHighlight a section and test it, run it.\n\n\nPackage documentation (3)\n\n\nBuild Tab/build the package.\n\n\n\n\nCtrl+Shift+d\n\n\n\n\nCompile all. Load all. Open the help page with \n?function\n.\n\n\nLearn the \nroxigen\n workflow to ease the work.\n\n\nTest your package\n\n\nWith the \ntestthat\n (and \ndevtools\n) package.\n\n\nInstall both packages. Run \ndevtools::use_testthat()\n and a now\n\ndirectory with subdir appears in the package. This is where we save\n\ntests.\n\n\nTest your package (2)\n\n\nReady. Write tests. Open a new R script and save it to the tests\n\ndirectory. Create a \ncontext\n function. Add \ntest_that\n functions with\n\narguments.\n\n\ncontext(\ncEnter\n)\n\ntest_that(\ncEnter handles integers\n, {\n  expect_equal(cEnter(1:3), -1:1)\n  expect_equal(cEnter(-(1:3)), 1:-1)\n})\n\ncontext(\nscale\n)\n\ntest_that(\nscale handles integers\n, {\n  expect_equal(scale(1:3), 1:3)\n  expect_equal(scale(-(1:3)), -(1:3))\n})\n\ncontext(\nstandardize\n)\n\ntest_that(\nstandardize handles integers\n, {\n  expect_equal(standardize(1:3), -1:1)\n  expect_equal(standardize(-(1:3)), 1:-1)\n})\n\n\n\n\nTest your package (3)\n\n\nRun the tests.\n\n\n\n\nCtl+Shift+T\n\n\n\n\nGet a summary (pass or not pass). Test and retest the package.\n\n\nRead r-pkgs.had.co.nz/tests.html\n\n\nTime to test your package\n\n\nRun the test with Build Tab/Test package.\n\n\nCheck your package\n\n\nUpload the package to GitHub. Recreate the package structure in the\n\nrepo. Download and install the package with \ninstall_github()\n from the\n\n\ndevtools\n package. Test the package on GitHub to complete the tests.\n\nType \nR CMD check\n in the terminal. Or Build Tab/Check icon.\n\n\n\n\nCtrl+Shift+E\n\n\n\n\nBuild your package\n\n\nA package is a tarball or package bundle.\n\n\nBuild Tab/Build \n Reload. Install and load the package.\n\n\n\n\nCtrl+Shift+B\n\n\n\n\nBuild and reload will overwrite the existing package. Run\n\n\ndevtools::dev_mode()\n creates a separate library for development.\n\nRunning the command again cancels it.\n\n\nTwo formats: source package or binary package (more compresses and\n\noptimized).\n\n\nRead r-pkgs.had.co.nz/package.html#package\n\n\nWrap-up\n\n\nGet www.rstudio.com/resources/cheatsheets/\n\n\n2, Version Control\n\n\nIntroduction to Git\n\n\nUse Git to work in team. Even on R scripts, reports and packages.\n\n\nInstall Git.\n\n\nIn RStudio, Tools/Global Options/Git/SVN + Tools/Project Options/Select\n\nVCS, restart RStudio. RStudio has now a Git pane and a Git icon.\n\n\nStage \n commit\n\n\nThe Git Tab is a directory. The real life (local) version of the\n\nproject. The official version of the project as recorded with Git.\n\n\nThe two versions are different. View the differences. When you commit,\n\nyou add thing from the real to the official version.\n\n\nGreen highlighting indicates something you\nve added to the official\n\nversion, while red highlighting indicates lines you have removed.\n\n\n.gitignore\n\n\nInside the project, ther is a .gignore file. Add file that are excluded\n\nfrom the offcial version. The file can be accessed from the Git pane.\n\n\nGit icons\n\n\nAdd, (cancel), commit, (cancel). The icons will changed.\n\n\nCommit history\n\n\nHistory viewer: each commitment. HEAD commit and parent commit. Master\n\nbranch or another.\n\n\nUndo commited changes: checkout\n\n\nCheckout command. Go back to a previous commit. The commit stays in the\n\nhistory (not deleted).\n\n\nRun it in the shell: Tools/shell.\n\n\ngit checkout sha# nameofthefile\n and Git will reverse the commit. The\n\nfile is in the stage area as before the commit.\n\n\nUndo uncommited changes\n\n\nThe file is in the stage area, ready to commit. Cancel the addition.\n\n\nIn the Change window, click on the Revert button.\n\n\nOr, click on Discard chunk.\n\n\nOr, Ctlr+Z to undo.\n\n\nThen, Save the file.\n\n\nIntroduction to GitHub\n\n\nCentralize, host, track issues, track metrics.\n\n\nInstall a R package From GitHub with \ninstall_github\n from the the\n\n\ndevtools\n packages.\n\n\nPull \n Push\n\n\nLocal and GitHub\n\n\nWrap-up\n\n\nGo to help.github.com\n\n\nGo to stackoverflow.com\n\n\n3, Reporting\n\n\nTools for reporting\n\n\nR Markdown and Shiny (over the web).\n\n\nIntroduction to R Markdown\n\n\nText and code.\n\n\nWeb link: \nhttp://www\n\n\nR Markdown in RStudio\n\n\nReport (HTML, PDF, Word) or presentation(slide).\n\n\nCreate a template. Load in new templates. The \nrticles\n package has\n\ntemplates for academic journals.\n\n\nThe outline (name the chunks).\n\n\nHelp/Markdown Quick Reference\n\n\nAdd a code chunk wih Ctl+Alt+I\n\n\nRendering R Markdown\n\n\nKnit or preview. For pdf, need LaTeX (install).\n\n\nAdding \nruntime: shiny\n generates a Shiny report. Launch the interactive\n\napp.\n\n\nPublish reports online.\n\n\nCompile notebook\n\n\nFile/Compile Notebook for R script. The script becomes a report (Word,\n\nPDF, HTML).\n\n\nRStudio\ns LaTex editor\n\n\nInstall LaTeX from www.latex-project.org\n\n\nOpen a .tex file in RStudio. RStudio has limited options to edit LaTeX;\n\nenough to write and compile.\n\n\nThe preview window is linked to the source window. If we click on a\n\ncharacter in the source window, press Ctrl+click: the corresponding\n\ncharacter is highlighted in the preview window. Vice-versa.\n\n\nTools/Global options/Sweave to change the LaTeX options.\n\n\nShiny\n\n\nThe server is online.\n\n\nshiny.rstudio.com\n\n\nWhen we create a Shiny app, we create two files: ui.R and server.R\n\n\nRun App (the app) or Ctrl+Shift+Enter\n\n\nPublish Shiny apps\n\n\nNeed an account and the \nshinyapps\n package.\n\n\ndevtools::install_github(\"rstudio/shinyapps\")\n\n\nDeploy the local app online. Unique URL. Monitor usage, view logs,\n\narchive or delete the app. Max 5 apps at a time for free. Paid account.", 
            "title": "Working with the RStudio IDE"
        }, 
        {
            "location": "/Working_with_RStudio_IDE/#1-orientation", 
            "text": "Commands   Ctrl+Up; command history.  Tab; completion for all.  Ctrl+L; clear the console.  Tools/Global Options  set Up RStudio.  view(dataframe) ; open a spreadsheet, show in new window, sort the \n    rows, search, filter the rows.    df  - data.frame(colA = c(1, 2), colB = c(3, 6))\ndf  ##   colA colB\n## 1    1    3\n## 2    2    6  View(df) # in a new window   IDE panes   Environment pane; load, save, remove objects, read a dataset, \n    import dataset.  History pane; idem, clear all or one item at the time, copy the \n    command in the console pane, reload a command with Shift+Enter and \n    run it.  File pane; working directories, files, add a new folder, rename  getwd() ; get working directory.  setwd() ; set working directory.  Plot pane; save (extension, size, resolution).  Package pane; update.  Help pane; pages.  Viewer pane; more than plots!", 
            "title": "1, Orientation"
        }, 
        {
            "location": "/Working_with_RStudio_IDE/#2-programming", 
            "text": "Scripting   Ctrl+Shift+m; pipe or  % % .  Alt+-;  - .  Ctrl+Shift+C; add/delete a  #  for commenting.    % %  - \n\n# comment  Code    Ctrl+Alt+i; new chunk.    Convert into a function.    Type code,   Highlight it,  Code/Extract Function to create a function:   This:  rnorm(10, 0, 1)  Becomes:  rnorm  - function() {\n  rnorm(10, 0, 1)\n}   Ctrl+Alt+click; multiple cursors for typing!  Switch between Default, Vim and Emacs modes with \n    Tools/Global Options/Code/Keybindings.  Shift+Alt+g; go to line.  Ctrl+f: find/replace in the current document.  Alt+o, Alt+Shift+o; fold/unfold the code.  Ctrl+p; jump between symbols like (), {}, [].  Ctrl+Shift+Enter; run and source the code.  Ctrl+Enter; run and source the code.  Ctrl+Shift+F10; restart, refresh the R session.   Error handling  RStudio traces back the error origin.  Toggle the show/hide traceback in the console when there is an error or \nrerun with the bug, watch the right pane for traceback.  Investigate, highlight the next line of code, click in the traceback \nwindow, click the dropdown menu in the Global Environment (upper-right). \nStop, continue in the debugger mode, press c, press q.  Add/remove breakpoints, where the line numbers are.   debugonce ; automatically call the debugger when the function is \n    called, but only once.  debug ,  undebug ; automatically call the debugger when the \n    function is called.   Add  options(error=browser)  or  options(error=NULL)  at the beginning \nof the script; R automatically open the debugger mode.   n; next line.  step-into icon.  Shift+F4; step into.  Shift+F6; execute the remainder of the bug.", 
            "title": "2, Programming"
        }, 
        {
            "location": "/Working_with_RStudio_IDE/#3-project", 
            "text": "Create a project with a folder and all the files (Global \n    Environment, History, etc.): New Directory, Existing Directory, \n    Version Control  Empty Project, R Package (project), Shiny \n    Web Application.  Create a Git repository with the new project   Commands   Ctrl+Shift+f; find in files.  Ctrl+F9, F10; go backwards/forwards.   Packrat  Packrat is a dependency management system for R. Use one version of a \npackage for one project, another version of a package for another \nproject. Associate a project with its own set of packages.   rstudio.github.io/packrat/  Activate Packrat when creating a project or \n    Tools/Project Options/Packrat.  The library is virtually separate from R library.  Perfect for collaborating with GitHub", 
            "title": "3, Project"
        }, 
        {
            "location": "/Working_with_RStudio_IDE/#working-with-rstudio-ide-part-2", 
            "text": "", 
            "title": "Working with RStudio IDE (Part 2)"
        }, 
        {
            "location": "/Working_with_RStudio_IDE/#1-packages", 
            "text": "Introduction to R packages  Anything that can be automated, should be automated. Do as little as \npossible by hand. Do as much as possible with functions.  Process:   Writing R functions.  Documenting functions.  Writing tests.  Checking compatibility.  Building the package for sharing and using.   See the book:  R Packages .  Create a new R package  Create a new directory with new files, folders, and meta-information.  To update or not to update  RStudio generates the NAMESPACE content for you automatically.  Read r-pkgs.had.co.nz/description.html  Import   load source files  Move existing functions from an existing project into the new package. \nThe function are moves to a new folder in the project.  Test created functions all in once with the Load All command in the \nBuild Tab.  Simulations:   Ctrl+Shift+L  Ctrl+Shift+F10; restart a R session.   Packages documentation  Create a help page (.Rd file). Written in HTML.  Use the  roxigen   package  to document.  Tools/Project Optons/Build Tools/Generate documentation. Generate the \ndoc, add comments.  First, create a doc skeleton above the function:   Ctrl+Alt+Shift+R    #' Title\n#'\n#' @param x \n#'\n#' @return\n#' @export\n#'\n#' @examples\ncEnter  - function(x) {\n  x - mean(x)\n}  Fill in the blanks.  Package documentation (2)  Title of the help page. Text on how to use.  The 4 tags ( @ ) help organizing the doc:  @export  (tells R that this \nfunction should be made available to people who load your package. ),  @params ,  @return , and  @examples . There are many more advanced \ntags.  Highlight a section and test it, run it.  Package documentation (3)  Build Tab/build the package.   Ctrl+Shift+d   Compile all. Load all. Open the help page with  ?function .  Learn the  roxigen  workflow to ease the work.  Test your package  With the  testthat  (and  devtools ) package.  Install both packages. Run  devtools::use_testthat()  and a now \ndirectory with subdir appears in the package. This is where we save \ntests.  Test your package (2)  Ready. Write tests. Open a new R script and save it to the tests \ndirectory. Create a  context  function. Add  test_that  functions with \narguments.  context( cEnter )\n\ntest_that( cEnter handles integers , {\n  expect_equal(cEnter(1:3), -1:1)\n  expect_equal(cEnter(-(1:3)), 1:-1)\n})\n\ncontext( scale )\n\ntest_that( scale handles integers , {\n  expect_equal(scale(1:3), 1:3)\n  expect_equal(scale(-(1:3)), -(1:3))\n})\n\ncontext( standardize )\n\ntest_that( standardize handles integers , {\n  expect_equal(standardize(1:3), -1:1)\n  expect_equal(standardize(-(1:3)), 1:-1)\n})  Test your package (3)  Run the tests.   Ctl+Shift+T   Get a summary (pass or not pass). Test and retest the package.  Read r-pkgs.had.co.nz/tests.html  Time to test your package  Run the test with Build Tab/Test package.  Check your package  Upload the package to GitHub. Recreate the package structure in the \nrepo. Download and install the package with  install_github()  from the  devtools  package. Test the package on GitHub to complete the tests. \nType  R CMD check  in the terminal. Or Build Tab/Check icon.   Ctrl+Shift+E   Build your package  A package is a tarball or package bundle.  Build Tab/Build   Reload. Install and load the package.   Ctrl+Shift+B   Build and reload will overwrite the existing package. Run  devtools::dev_mode()  creates a separate library for development. \nRunning the command again cancels it.  Two formats: source package or binary package (more compresses and \noptimized).  Read r-pkgs.had.co.nz/package.html#package  Wrap-up  Get www.rstudio.com/resources/cheatsheets/", 
            "title": "1, Packages"
        }, 
        {
            "location": "/Working_with_RStudio_IDE/#2-version-control", 
            "text": "Introduction to Git  Use Git to work in team. Even on R scripts, reports and packages.  Install Git.  In RStudio, Tools/Global Options/Git/SVN + Tools/Project Options/Select \nVCS, restart RStudio. RStudio has now a Git pane and a Git icon.  Stage   commit  The Git Tab is a directory. The real life (local) version of the \nproject. The official version of the project as recorded with Git.  The two versions are different. View the differences. When you commit, \nyou add thing from the real to the official version.  Green highlighting indicates something you ve added to the official \nversion, while red highlighting indicates lines you have removed.  .gitignore  Inside the project, ther is a .gignore file. Add file that are excluded \nfrom the offcial version. The file can be accessed from the Git pane.  Git icons  Add, (cancel), commit, (cancel). The icons will changed.  Commit history  History viewer: each commitment. HEAD commit and parent commit. Master \nbranch or another.  Undo commited changes: checkout  Checkout command. Go back to a previous commit. The commit stays in the \nhistory (not deleted).  Run it in the shell: Tools/shell.  git checkout sha# nameofthefile  and Git will reverse the commit. The \nfile is in the stage area as before the commit.  Undo uncommited changes  The file is in the stage area, ready to commit. Cancel the addition.  In the Change window, click on the Revert button.  Or, click on Discard chunk.  Or, Ctlr+Z to undo.  Then, Save the file.  Introduction to GitHub  Centralize, host, track issues, track metrics.  Install a R package From GitHub with  install_github  from the the  devtools  packages.  Pull   Push  Local and GitHub  Wrap-up  Go to help.github.com  Go to stackoverflow.com", 
            "title": "2, Version Control"
        }, 
        {
            "location": "/Working_with_RStudio_IDE/#3-reporting", 
            "text": "Tools for reporting  R Markdown and Shiny (over the web).  Introduction to R Markdown  Text and code.  Web link:  http://www  R Markdown in RStudio  Report (HTML, PDF, Word) or presentation(slide).  Create a template. Load in new templates. The  rticles  package has \ntemplates for academic journals.  The outline (name the chunks).  Help/Markdown Quick Reference  Add a code chunk wih Ctl+Alt+I  Rendering R Markdown  Knit or preview. For pdf, need LaTeX (install).  Adding  runtime: shiny  generates a Shiny report. Launch the interactive \napp.  Publish reports online.  Compile notebook  File/Compile Notebook for R script. The script becomes a report (Word, \nPDF, HTML).  RStudio s LaTex editor  Install LaTeX from www.latex-project.org  Open a .tex file in RStudio. RStudio has limited options to edit LaTeX; \nenough to write and compile.  The preview window is linked to the source window. If we click on a \ncharacter in the source window, press Ctrl+click: the corresponding \ncharacter is highlighted in the preview window. Vice-versa.  Tools/Global options/Sweave to change the LaTeX options.  Shiny  The server is online.  shiny.rstudio.com  When we create a Shiny app, we create two files: ui.R and server.R  Run App (the app) or Ctrl+Shift+Enter  Publish Shiny apps  Need an account and the  shinyapps  package.  devtools::install_github(\"rstudio/shinyapps\")  Deploy the local app online. Unique URL. Monitor usage, view logs, \narchive or delete the app. Max 5 apps at a time for free. Paid account.", 
            "title": "3, Reporting"
        }, 
        {
            "location": "/IO_snippets___Cleaning/", 
            "text": "Datasets\n\n\nImporting Data Into R\n\n\n1, Importing Data from Flat\n\n    Files\n\n\n2, Importing Data from Excel\n\n\n3, Importing Data from Other Statistical\n\n    Software\n\n\n4, Importing Data from Relational\n\n    Data\n\n\n4b, Importing Data from Relational Data \n\n    More\n\n\n5, Importing Data from the Web\n\n\n6, Keyboard Inputting\n\n\n7, Exporting Data\n\n\n8, Inspecting Data - Missing\n\n    Data\n\n\n9, Labels \n Levels\n\n\n\n\n\n\nHow to work with Quandl in R\n\n\n1, Importing Quandl Datasets\n\n\n2, Manipulating Quandl Datasets\n\n\n\n\n\n\nCleaning Data in R\n\n\n1, Introduction and Exploring Raw\n\n    Data\n\n\n2, Tidying Data\n\n\n3, Preparing Data for Analysis\n\n\n4, Putting it All Together\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nThe project stands in a directory (the \nproject\n) where the we find\n\n    the .rmd file, the data and the output.\n\n\n\n\n\n\nDatasets\n\n\n\n\nR Dataset\n\n    Packages\n;\n\n    by default in R. -Other dataset can be imported with\n\n\ndata(Cars93, package = 'MASS')\n for example.\n\n\ncsv/doc\n\n    Datasets\n.\n\n\nFree Datasets\n from the\n\n    World Bank, Gapminder, Kaggle, Quandl, Reddit, and many\n\n    more websites.\n\n\nDatasets\n\n    to Practice Your Data Mining.\n\n\nHoughton Mifflin\n\n    Data\n\n    for linear regressions.\n\n\nRegression Datasets\n\n    for Generalized Linear Models (linear, logistic, poisson,\n\n    multinomial, survival).\n\n\nPublic Datasets on\n\n    GitHub\n\n\n\n\nAwesome Public\n    Datasets\n\n\nhttps://github.com/caesar0301/awesome-public-datasets\n)\n\n\n\n\n\n\nImporting Data Into R\n\n\nThe packages:\n\n\n\n\nutils\n.\n\n\nreadr\n.\n\n\ndata.table\n.\n\n\nreadxl\n.\n\n\ngdata\n.\n\n\nXLConnect\n.\n\n\nhaven\n.\n\n\nforeign\n.\n\n\nDBI\n.\n\n\nhttr\n.\n\n\njsonlite\n.\n\n\n\n\n1, Importing Data from Flat Files\n\n\nR functions, by default.\n\n\n\n\nread.csv\n; \nsep = ','\n, \ndec = '.'\n.\n\n\nread.delim\n; .txt, \ndec = '.'\n.\n\n\nread.csv2\n; \nsep = ';'\n, \ndec = ','\n.\n\n\nread.delim2\n; .txt, \ndec = ','\n.\n\n\nNeeds arguments.\n\n\n\n\nread.csv\n for .csv files\n\n\n# List the files in your working directory\ndir()\n\n# Import swimming_pools.csv: pools\n# stringAsFactors = FALSE does not import strings as categorical variables\npools \n- read.csv('swimming_pools.csv', stringsAsFactors = FALSE)\n\n\n\n\nstringsAsFactors\n\n\n# Import swimming_pools.csv correctly: pools\npools \n- read.csv('swimming_pools.csv', stringsAsFactor = FALSE, header = TRUE, sep = ',')\n\n# Import swimming_pools.csv with factors: pools_factor\npools_factor \n- read.csv('swimming_pools.csv', header = TRUE, sep = ',')\n\n\n\n\nread.delim\n for .txt files\n\n\n# Import hotdogs.txt: hotdogs\nhotdogs \n- read.delim('hotdogs.txt', header = FALSE)\n\n# Name the columns of hotdogs appropriately\nnames(hotdogs) \n- c('type', 'calories', 'sodium')\n\n\n\n\nArguments.\n\n\n# Load in the hotdogs data set: hotdogs\nhotdogs \n- read.delim('hotdogs.txt', header = FALSE, sep = '\\t', col.names = c('type', 'calories', 'sodium'))\n\n# Select the hot dog with the least calories: lily\nlily \n- hotdogs[which.min(hotdogs$calories), ]\n# Select the observation with the most sodium: tom\n\ntom \n- hotdogs[which.max(hotdogs$sodium), ]\n\n\n\n\n# Previous call to import hotdogs.txt\nhotdogs \n- read.delim('hotdogs.txt', header = FALSE, col.names = c('type', 'calories', 'sodium'))\n\n# Print a vector representing the classes of the columns\nsapply(hotdogs, class)\n\n# Edit the colClasses argument to import the data correctly: hotdogs2\nhotdogs2 \n- read.delim('hotdogs.txt', header = FALSE, col.names = c('type', 'calories', 'sodium'), colClasses = c('factor', 'NULL', 'numeric'))\n\n\n\n\nThe \nutils\n package\n\n\n\n\nread.table\n; \nsep = '\\t'\n, \n= ','\n, \n= ';'\n.\n\n\nRead any tabular as a d.f.\n\n\nNeeds arguments; lots of argument for precision.\n\n\nSlow.\n\n\n\n\n\n\n\nlibrary(utils)\n\n\n\n\nread.table\n .txt files\n\n\n# Create a path to the hotdogs.txt file\npath \n- file.path('hotdogs', 'hotdogs.txt')\n\n# Import the hotdogs.txt file: hotdogs\nhotdogs \n- read.table(path, header = FALSE, sep = '\\t', col.names = c('type', 'calories', 'sodium'))\n\n\n\n\n\n\n(from 5, Importing Data from the Web)\n\n\n# https URL to the swimming_pools csv file.\nurl_csv \n- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\n\n# Import the file using read.csv(): pools1\npools1 \n- read.csv(url_csv)\n\n\n\n\n\n\nThe \nreadr\n package\n\n\n\n\nread_delim\n; \ndelim = '\\t'\n, \n= ','\n.\n\n\nread_csv\n; read \n100.000, 200.000\n\n\nread_tsv\n; idem.\n\n\nread_csv2\n; read \n100,000; 200,000\n or European files..\n\n\nread_tsv2\n; idem.\n\n\nread_lines\n.\n\n\nread_file\n.\n\n\nwrite_csv\n.\n\n\nwrite_rds\n.\n\n\ntype_convert\n.\n\n\nparse_factor\n.\n\n\nparse_date\n.\n\n\nparse_number\n.\n\n\nspec_csv\n.\n\n\nspec_delim\n.\n\n\nFast, few arguments.\n\n\nDetect data type.\n\n\n\n\n\n\n\nlibrary(readr)\n\n\n\n\nread_delim\n .txt files\n\n\n# Import potatoes.txt using read_delim(): potatoes\npotatoes \n- read_delim('potatoes.txt', delim = '\\t')\n\n\n\n\nread_csv\n .csv files\n\n\n# Column names\nproperties \n- c('area', 'temp', 'size', 'storage', 'method', 'texture', 'flavor', 'moistness')\n\n# Import potatoes.csv with read_csv(): potatoes\npotatoes \n- read_csv('potatoes.csv', col_names = properties)\n\n# Create a copy of potatoes: potatoes2\npotatoes2 \n- potatoes\n\n# Convert the method column of potatoes2 to a factor\npotatoes2$method = factor(potatoes2$method)\n\n# or\n\npotatoes2$method = as.factor(potatoes2$method)\n\n\n\n\ncol_types\n, \nskip\n and \nn_max\n in .tsv files\n\n\n# Column names\nproperties \n- c('area', 'temp', 'size', 'storage', 'method', 'texture', 'flavor', 'moistness')\n\n# Import 5 observations from potatoes.txt: potatoes_fragment\n# read_tsv or tab-separated values\npotatoes_fragment \n- read_tsv('potatoes.txt', col_names = properties, skip = 7, n_max = 5)\n\n# Import all data, but force all columns to be character: potatoes_char\npotatoes_char \n- read_tsv('potatoes.txt', col_types = 'cccccccc')\n\n\n\n\nSetting column types\n\n\ncols(\n  weight = col_integer(),\n  feed = col_character()\n)\n\n\n\n\nRemoving NA\n\n\nna = c('NA', 'null')\n\n\n\n\ncol_types\n with collectors .tsv files\n\n\n# Import without col_types\nhotdogs \n- read_tsv('hotdogs.txt', col_names = c('type', 'calories', 'sodium'))\n\n# The collectors you will need to import the data\nfac \n- col_factor(levels = c('Beef', 'Meat', 'Poultry'))\nint \n- col_integer()\n\n# Edit the col_types argument to import the data correctly: hotdogs_factor\n# Change col_types to the correct vector of collectors; coerce the vector into a list\nhotdogs_factor \n- read_tsv('hotdogs.txt', col_names = c('type', 'calories', 'sodium'), col_types = list(fac, int, int))\n\n\n\n\nSkiping columns\n\n\nsalaries \n- read_tsv('Salaries.txt', col_names = FALSE, col_types = cols(\n  X2 = col_skip(),\n  X3 = col_skip(), \n  X4 = col_skip()\n))\n\n\n\n\nReading an ordinary text file\n\n\n# vector of character strings. \n# Import as a character vector, one item per line: tweets\ntweets \n- read_lines('tweets.txt')\ntweets\n\n# returns a length 1 vector of the entire file, with line breaks represented as \\n\n# Import as a length 1 vector: tweets_all\ntweets_all \n- read_file('tweets.txt')\ntweets_all\n\n\n\n\nWriting .csv and .tsv files\n\n\n# Save cwts as chickwts.csv\nwrite_csv(cwts, \nchickwts.csv\n)\n\n# Append cwts2 to chickwts.csv\nwrite_csv(cwts2, \nchickwts.csv\n, append = TRUE)\n\n\n\n\nWriting .rds files\n\n\n# Save trees as trees.rds\nwrite_rds(trees, 'trees.rds')\n\n# Import trees.rds: trees2\ntrees2 \n- read_rds('trees.rds')\n\n# Check whether trees and trees2 are the same\nidentical(trees, trees2)\n\n\n\n\nCoercing columns to different data types\n\n\n# Convert all columns to double\ntrees2 \n- type_convert(trees, col_types = cols(Girth = 'd', Height = 'd', Volume = 'd'))\n\n\n\n\nCoercing character columns into factors\n\n\n# Parse the title column\nsalaries$title \n- parse_factor(salaries$title, levels = c('Prof', 'AsstProf', 'AssocProf'))\n\n# Parse the gender column\nsalaries$gender \n- parse_factor(salaries$gender, levels = c('Male', 'Female'))\n\n\n\n\nCreating Date objects\n\n\n# Change type of date column\nweather$date \n- parse_date(weather$date, format = '%m/%d/%Y')\n\n\n\n\nParsing number formats\n\n\n# Parse amount column as a number\ndebt$amount \n- parse_number(debt$amount)\n\n\n\n\nViewing metadata before importing\n\n\n\n\nspec_csv\n for .csv and .tsv files.\n\n\nspec_delim\n for .txt files (among others).\n\n\n\n\n\n\n\n# Specifications of chickwts\nspec_csv('chickwts.csv')\n\n\n\n\n\n\n(from 5, Importing Data from the Web)\n\n\nImport Flat files from the web\n\n\n# Import the csv file: pools\nurl_csv \n- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\npools \n- read_csv(url_csv)\n\npools\n\n# Import the txt file: potatoes\nurl_delim \n- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/potatoes.txt'\npotatoes \n- read_tsv(url_delim)\n\npotatoes\n\n\n\n\nSecure importing\n\n\n# Import the file using read_csv(): pools2\npools2 \n- read_csv(url_csv)\n\n\n\n\n\n\nThe \ndata.table\n package\n\n\n\n\nfread\n == \nread.table\n.\n\n\n.txt files only.\n\n\nFast.\n\n\n\n\n\n\n\nlibrary(data.table)\n\n\n\n\nfread\n for .txt files\n\n\n# Import potatoes.txt with fread(): potatoes\npotatoes \n- fread('potatoes.txt')\n\n# Print out arranged version of potatoes\npotatoes[order(moistness),] \n\n# Import 20 rows of potatoes.txt with fread(): potatoes_part\npotatoes_part \n- fread('potatoes.txt', nrows = 20)\n\n\n\n\nfread\n: more advanced use\n\n\n# Import columns 6, 7 and 8 of potatoes.txt: potatoes\npotatoes \n- fread('potatoes.txt', select = c(6:8))\n\n# Keep only tasty potatoes (flavor \n 3): tasty_potatoes\ntasty_potatoes \n- subset(potatoes, potatoes$flavor \n 3)\n\n\n\n\n2, Importing Data from Excel\n\n\nThe \nreadxl\n package\n\n\n\n\nexcel_sheets\n; list.\n\n\nread_excel\n; import.\n\n\n.xlsx files only.\n\n\n\n\n\n\n\nlibrary(readxl)\n\n\n\n\nList the sheets of an Excel file\n\n\n# Find the names of both spreadsheets: sheets\n# Before, find out what is in the directory with 'dir()'\nsheets \n- excel_sheets('latitude.xlsx')\n\nsheets\n\n\n\n\nImporting an Excel sheet\n\n\n# Read the first sheet of latitude.xlsx: latitude_1\nlatitude_1 \n- read_excel('latitude.xlsx', sheet = 1)\n\n# Read the second sheet of latitude.xlsx: latitude_2\nlatitude_2 \n- read_excel('latitude.xlsx', sheet = 2)\n\n# Put latitude_1 and latitude_2 in a list: lat_list\nlat_list \n- list(latitude_1, latitude_2)\n\n\n\n\nReading a workbook\n\n\n# Read all Excel sheets with lapply(): lat_list\nlat_list \n- lapply(excel_sheets('latitude.xlsx'), read_excel, path = 'latitude.xlsx')\n\n\n\n\nThe \ncol_names\n argument\n\n\n# Import the the first Excel sheet of latitude_nonames.xlsx (R gives names): latitude_3\nlatitude_3 \n- read_excel('latitude_nonames.xlsx', sheet = 1, col_names = FALSE)\n\n# Import the the second Excel sheet of latitude_nonames.xlsx (specify col_names): latitude_4 \nlatitude_4 \n- read_excel('latitude_nonames.xlsx', sheet = 1, col_names = c('country', 'latitude'))\n\n\n\n\nThe \nskip\n argument\n\n\n# Import the second sheet of latitude.xlsx, skipping the first 21 rows: latitude_sel\nlatitude_sel \n- read_excel('latitude.xlsx', skip = 21, col_names = FALSE)\n\n\n\n\n\n\n(from 5, Importing Data from the Web)\n\n\nImport Excel files from the web\n\n\n# Download file behind URL, name it local_latitude.xls\ndownload.file(url_xls, 'local_latitude.xls')\n\n# Import the local .xls file with readxl: excel_readxl\nexcel_readxl \n- read_excel('local_latitude.xls')\n\n\n\n\nDownloading any file, secure or not\n\n\n# https URL to the wine RData file.\nurl_rdata \n- 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/wine.RData'\n\n# Download the wine file to your working directory\ndownload.file(url_rdata, 'wine_local.RData')\n\n\n\n\n\n\nThe \nXLConnect\n package\n\n\n\n\nloadWorkbook\n.\n\n\ngetSheets\n.\n\n\nreadWorksheet\n.\n\n\nreadWorksheetFromFile\n\n\nreadNamedRegion\n\n\n\n\nreadNamedRegionFromFile\n\n\n\n\n\n\n.xls \n .xlsx files.\n\n\n\n\nLike reading a database.\n\n\n\n\n\n\n\nlibrary(XLConnectJars)\nlibrary(XLConnect)\n\n\n\n\nImport a workbook\n\n\n# Build connection to latitude.xlsx: my_book\nmy_book \n- loadWorkbook('latitude.xlsx')\n\n\n\n\nList and read Excel sheets\n\n\n# Build connection to latitude.xlsx\nmy_book \n- loadWorkbook('latitude.xlsx')\n\n# List the sheets in latitude.xlsx\ngetSheets(my_book)\n\n# Import the second sheet in latitude.xlsx\nreadWorksheet(my_book, sheet = 2)\n\n# Import the second column of the first sheet in latitude.xlsx\nreadWorksheet(my_book, sheet = 2, startCol = 2)\n\n\n\n\nAdd and populate worksheets\n\n\n# Build connection to latitude.xlsx\nmy_book \n- loadWorkbook('latitude.xlsx')\n\n# Create data frame: summ\ndims1 \n- dim(readWorksheet(my_book, 1))\ndims2 \n- dim(readWorksheet(my_book, 2))\nsumm \n- data.frame(sheets = getSheets(my_book), \n                   nrows = c(dims1[1], dims2[1]), \n                   ncols = c(dims1[2], dims2[2]))\n\n# Add a worksheet to my_book, named 'data_summary'\ncreateSheet(my_book, name = 'data_summary')\n\n# Populate 'data_summary' with summ data frame\nwriteWorksheet(my_book, summ, sheet = 'data_summary')\n# Save workbook as latitude_with_summ.xlsx\n\nsaveWorkbook(my_book, 'latitude_with_summ.xlsx')\n\n\n\n\nOne unique function\n\n\n# Read in the data set and assign to the object\nimpact \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'impact', header = TRUE, startCol = 1, startRow = 1)\n\n# more arguments\n# endCol = 1\n# endRow = 1\n# autofitRow = \n# autofitCol = \n# region =\n# rownames =\n# colTypes =\n# forceConversion =\n# dateTimeFormat =\n# check.names =\n# useCachedValues =\n# keep =\n# drop =\n# simplify =\n# readStrategy =\n\n\n\n\n3, Importing Data from Other Statistical Software\n\n\nThe \nhaven\n package\n\n\n\n\nread_sas\n; sas7bdat \n sas7bcat files.\n\n\nread_stata\n; version; dta files.\n\n\nread_dta\n; idem.\n\n\nread_spss\n; sav \n por files (and see below).\n\n\nread_por\n.\n\n\nread_sav\n.\n\n\nSimple, few arguments.\n\n\nCreate a d.f.\n\n\n\n\n\n\n\nlibrary(haven)\n\n\n\n\nImport SAS data with haven\n\n\n# Import sales.sas7bdat: sales\nsales \n- read_sas('sales.sas7bdat')\n\n\n\n\nImport STATA data with haven\n\n\n# Import the data from the URL: sugar\nsugar \n- read_dta('http://assets.datacamp.com/course/importing_data_into_r/trade.dta')\n\n\n\n\nImport SPSS data with haven\n\n\n# Specify the file path using file.path(): path\npath \n- file.path('datasets', 'person.sav')\n\n# Import person.sav, which is in the datasets folder: traits\ntraits \n- read_sav(path)\n\n\n\n\nFactorize, round two\n\n\n# Import SPSS data from the URL: work\nwork \n- read_sav('http://assets.datacamp.com/course/importing_data_into_r/employee.sav')\n\n\n\n\nThe\nforeign\n package\n\n\n\n\nCannot import SAS, see the \nsas7bdat\n package.\n\n\nread.dta\n; dta files.\n\n\nread.spss\n; sav \n por files.\n\n\nComprehensive.\n\n\n\n\n\n\n\nlibrary(foreign)\n\n\n\n\nImport STATA data with foreign (1)\n\n\n# Import florida.dta and name the resulting data frame florida\nflorida \n- read.dta('florida.dta')\n\n\n\n\nImport STATA data with foreign (2)\n\n\n# Specify the file path using file.path(): path\npath \n- file.path('worldbank', 'edequality.dta')\n\n# Create and print structure of edu_equal_1\nedu_equal_1 \n- read.dta(path)\n\n# Create and print structure of edu_equal_2\nedu_equal_2 \n- read.dta(path, convert.factors = FALSE)\n\n# Create and print structure of edu_equal_3\nedu_equal_3 \n- read.dta(path, convert.underscore = TRUE) \n\n\n\n\nImport SPSS data with foreign (1)\n\n\n# Import international.sav as a data frame: demo\ndemo \n- read.spss('international.sav', to.data.frame = TRUE)\n\n\n\n\nImport SPSS data with foreign (2)\n\n\n# Import international.sav as demo_1\ndemo_1 \n- read.spss('international.sav', to.data.frame = TRUE)\n\n# Import international.sav as demo_2\ndemo_2 \n- read.spss('international.sav', to.data.frame = TRUE, use.value.labels = FALSE)\n\n\n\n\n4, Importing Data from Relational Data\n\n\nThe \nDBI\n package\n\n\n\n\ndbConnect\n.\n\n\ndbReadTable\n.\n\n\ndbGetQuery\n.\n\n\ndbFetch\n.\n\n\ndbDisconnect\n.\n\n\n\n\n\n\n\nlibrary(DBI)\n\n\n\n\nStep 1: Establish a connection\n\n\n# Connect to the MySQL database: con\ncon \n- dbConnect(RMySQL::MySQL(), dbname = 'tweater', host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', port = 3306, user = 'student', password = 'datacamp') \n\ncon\n\n\n\n\nStep 2: List the database tables\n\n\n# Connect to the MySQL database: con\ncon \n- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Build a vector of table names: tables\ntables \n- dbListTables(con)\n\n# Display structure of tables\nstr(tables)\n\n\n\n\nStep 3: Import data from a table\n\n\n# Connect to the MySQL database: con\ncon \n- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Import the users table from tweater: users\nusers \n- dbReadTable(con, 'users')\n\nusers\n\n# Import and print the tweats table from tweater: tweats\ntweats \n- dbReadTable(con, 'tweats')\n\ntweats\n\n# Import and print the comments table from tweater: comments\ncomments \n- dbReadTable(con, 'comments')\n\ncomments\n\n\n\n\nYour very first SQL query\n\n\ncon \n- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Import post column of tweats where date is higher than '2015-09-21': latest\nlatest \n- dbGetQuery(con, 'SELECT post FROM tweats WHERE date \n \\'2015-09-21\\'')\n\nlatest\n\n# Import tweat_id column of comments where user_id is 1: elisabeth\nelisabeth \n- dbGetQuery(con, 'SELECT tweat_id FROM comments WHERE user_id = 1')\n\nelisabeth\n\n\n\n\nMore advanced SQL queries\n\n\n# Connect to the database\ncon \n- dbConnect(RMySQL::MySQL(),\n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Create data frame specific\nspecific \n- dbGetQuery(con, 'SELECT message FROM comments WHERE tweat_id = 77 AND user_id \n 4')\n\nspecific\n\n# Create data frame short\nshort \n- dbGetQuery(con, 'SELECT id, name FROM users WHERE CHAR_LENGTH(name) \n 5')\n\nshort\n\n\n\n\nSend - Fetch - Clear\n\n\n# Connect to the database\ncon \n- dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Send query to the database with dbSendQuery(): res\nres \n- dbSendQuery(con, 'SELECT * FROM comments WHERE user_id \n 4')\n\n# Display information contained in res\ndbGetInfo(res)\n\n# Use dbFetch() twice\nwhile (!dbHasCompleted(res)) {\n    chunk \n- dbFetch(res, n = 2)\n    chunk2 \n- dbFetch(res)\n    print(chunk)\n}\n\n# Clear res\ndbClearResult(res)\n\n\n\n\nBe polite and \n\n\n# Database specifics\ndbname \n- 'tweater'\nhost \n- 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\nport \n- 3306\nuser \n- 'student'\npassword \n- 'datacamp'\n\n# Connect to the database\ncon \n- dbConnect(RMySQL::MySQL(), dbname = 'tweater', host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', port = 3306 , user = 'student', password = 'datacamp')\n\n# Create the data frame  long_tweats\nlong_tweats \n- dbGetQuery(con, 'SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) \n 40')\n\n# Print long_tweats\nprint(long_tweats)\n\n# Disconnect from the database\ndbDisconnect(con)\n\n\n\n\nOther general packages\n\n\nThe \nRODBC\n package provides access to databases (including Microsoft\n\nAccess and Microsoft SQL Server) through an ODBC interface.\n\n\nThe \nRJDBC\n package provides access to databases through a JDBC\n\ninterface.\n\n\nSpecialized packages\n\n\n\n\nROracle\n provides an interface for Oracle.\n\n\nRMySQL\n provides access to MySQL.\n\n\nRpostgreSQL\n to PostgreSQL.\n\n\nRSQLite\n to SQLite.\n\n\nAnd there are manu more packages for NoSQL databases such\n\n    as MongoDB.\n\n\n\n\n4b, Importing Data from Relational Data \n More\n\n\nDBI\n\n\nFirst, change the working directory with \nsetwd\n. Install the \nDBI\n\nlibrary.\n\n\nConnect and read preliminary results\n\n\nlibrary(DBI)\nlibrary(sqliter)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nsqlite = dbDriver('SQLite')\n\n# Assign the connection string to a connection object\nsqlitedb \n- dbConnect(RSQLite::SQLite(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(sqlitedb)\n\n\n\n\nExtract some data\n\n\n# Assign the results of a SQL query to an object\nresults = dbSendQuery(sqlitedb, \nSELECT * FROM albums\n)\n\n# Return results from a custom object to a data.frame\ndata = fetch(results)\n\n# Print data frame to console\nhead(data)\n\n\n\n\n# Clear the results and close the connection\ndbClearResult(results)\n\n# Disconnect from the database\ndbDisconnect(sqlitedb)\n\n\n\n\nRSQLite\n\n\nFirst, change the working directory with \nsetwd\n. Install the \nRSQLite\n\nlibrary.\n\n\nConnect and read preliminary results\n\n\nlibrary(RSQLite)\nlibrary(sqliter)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nsqlite = dbDriver('SQLite')\n\n# Assign the connection string to a connection object\nmysqldb = dbConnect(sqlite, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(sqlitedb)\n\n\n\n\nExtract some data\n\n\n# Assign the results of a SQL query to an object\nresults = dbSendQuery(sqlitedb, \nSELECT * FROM albums\n)\n\n# Check the object\nresults\ndbGetInfo(results)\n\n# Return results from a custom object to a data.frame\ndata = fetch(results)\n\n# Print data frame to console\nhead(data)\n\n\n\n\n# Clear the results and close the connection\ndbClearResult(results)\n\n# Disconnect from the database\ndbDisconnect(sqlitedb)\n\n\n\n\nMySQL with \nDBI\n or \nRMySQL\n\n\nlibrary(DBI)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nmysql = dbDriver('MySQL')\n\n# Assign the connection string to a connection object\nmysqldb \n- dbConnect(RMySQL::MySQL(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Disconnect from the database\ndbDisconnect(mysqldb)\n\n\n\n\nlibrary(RMySQL)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nmysql = dbDriver('MySQL')\n\n# Assign the connection string to a connection object\nmysqldb = dbConnect(mysql, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Disconnect from the database\ndbDisconnect(mysqldb)\n\n\n\n\nPosgreSQL with \nDBI\n or \nRPostgreSQL\n\n\nlibrary(DBI)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\npostgresql = dbDriver('PostgreSQL')\n\n# Assign the connection string to a connection object\npostgresqldb \n- dbConnect(RPostgreSQL::PostgreSQL(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Disconnect from the database\ndbDisconnect(postgresqldb)\n\n\n\n\nlibrary(RPostgreSQL)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\npostgresql = dbDriver('PostgreSQL')\n\n# Assign the connection string to a connection object\npostgresqldb = dbConnect(postgresql, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Disconnect from the database\ndbDisconnect(postgresqldb)\n\n\n\n\n5, Importing Data from the Web\n\n\nThe other package above can download files from the web. The next\n\npackages are web-oriented.\n\n\nThe \nhttr\n package\n\n\n\n\nGET\n pages and files from the web.\n\n\nConcise.\n\n\nParse JSON files.\n\n\nCommunicate with APIs.\n\n\n\n\n\n\n\nlibrary(httr)\n\n\n\n\nHTTP? \nhttr\n!\n\n\n# Get the url, save response to resp\nurl \n- 'http://docs.datacamp.com/teach/'\nresp \n- GET(url)\n\nresp\n\n# Get the raw content of resp\nraw_content \n- content(resp, as = 'raw')\n\n# Print the head of content\nhead(raw_content)\n\n\n\n\n# Get the url\nurl \n- 'https://www.omdbapi.com/?t=Annie+Hall\ny=\nplot=short\nr=json'\n\nresp \n- GET(url)\n\n# Print resp\nresp\n\n# Print content of resp as text\ncontent(resp, as = 'text')\n\n# Print content of resp\ncontent(resp)\n\n\n\n\nThe \njsonlite\n package\n\n\n\n\nRobust.\n\n\nImprove the imported data.\n\n\nfromJSON\n.\n\n\nfrom an R object to \ntoJSON\n\n\nprettify\n.\n\n\nminify\n.\n\n\n\n\n\n\n\nlibrary(jsonlite)\n\n\n\n\nFrom \nJSON\n to R\n\n\n# Convert wine_json to a list: wine\nwine_json \n- '{'name':'Chateau Migraine', 'year':1997, 'alcohol_pct':12.4, 'color':'red', 'awarded':false}'\nwine \n- fromJSON(wine_json)\n\nstr(wine)\n\n# Import Quandl data: quandl_data\nquandl_url \n- 'http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz'\nquandl_data \n- fromJSON(quandl_url)\n\nstr(quandl_data)\n\n\n\n\n# Experiment 1\njson1 \n- '[1, 2, 3, 4, 5, 6]'\nfromJSON(json1)\n\n# Experiment 2\njson2 \n- '{'a': [1, 2, 3], 'b': [4, 5, 6]}'\nfromJSON(json2)\n\n# Experiment 3\njson3 \n- '[[1, 2], [3, 4]]'\nfromJSON(json3)\n\n# Experiment 4\njson4 \n- '[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]'\nfromJSON(json4)\n\n\n\n\nAsk OMDb\n\n\n# Definition of the URLs\nurl_sw4 \n- 'http://www.omdbapi.com/?i=tt0076759\nr=json'\nurl_sw3 \n- 'http://www.omdbapi.com/?i=tt0121766\nr=json'\n\n# Import two URLs with fromJSON(): sw4 and sw3\nsw4 \n- fromJSON(url_sw4)\nsw3 \n- fromJSON(url_sw3)\n\n# Print out the Title element of both lists\nsw4$Title\nsw3$Title\n\n# Is the release year of sw4 later than sw3\nsw4$Year \n sw3$Year\n\n\n\n\nFrom R to \nJSON\n\n\n# URL pointing to the .csv file\nurl_csv \n- 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/water.csv'\n\n# Import the .csv file located at url_csv\nwater \n- read.csv(url_csv, stringsAsFactors = FALSE)\n\n# Generate a summary of water\nsummary(water)\n\n# Convert the data file according to the requirements\nwater_json \n- toJSON(water)\n\nwater_json\n\n\n\n\nMinify\n and \nprettify\n\n\n# Convert mtcars to a pretty JSON: pretty_json\npretty_json \n- toJSON(mtcars, pretty = TRUE)\n\n# Print pretty_json\npretty_json\n\n# Minify pretty_json: mini_json\nmini_json \n- minify(pretty_json)\n\n# Print mini_json\nmini_json\n\n\n\n\n6, Keyboard Inputting\n\n\nCoding\n\n\n# create a data frame from scratch\nage \n- c(25, 30, 56)\ngender \n- c(\nmale\n, \nfemale\n, \nmale\n)\nweight \n- c(160, 110, 220)\nmydata \n- data.frame(age,gender,weight)\n\n\n\n\nSpreadsheet-like\n\n\n# enter data using editor\nmydata \n- data.frame(age = numeric(0), gender = character(0), weight = numeric(0))\n\nmydata \n- edit(mydata)\n# note that without the assignment in the line above, the edits are not saved! \n\n\n\n\n7, Exporting Data\n\n\nTo a Tab-Delimited Text File\n\n\nwrite.table(mydata, 'c:/mydata.txt', sep = \n\\t\n)\n\n\n\n\nTo an Excel Spreadsheet\n\n\nlibrary(xlsx)\n\nwrite.xlsx(mydata, \nc:/mydata.xlsx\n)\n\n\n\n\nWorksheet\n\n\nlibrary(XLConnect)\n# xls or xlsx\n\n# write a worksheet in steps\nwb \n- loadWorkbook('XLConnectExample1.xls', create = TRUE)\ncreateSheet(wb, name = 'chickSheet')\nwriteWorksheet(wb, ChickWeight, sheet = 'chickSheet', startRow = 3, startCol = 4)\nsaveWorkbook(wb)\n\n# write a worksheet all in one step\nChickWeight \n- 1\n\nwriteWorksheetToFile('XLConnectExample2.xlsx', data = ChickWeight, sheet = 'chickSheet', startRow = 3, startCol = 4)\n\n\n\n\nField\n\n\n# write a field in steps\nwb = loadWorkbook('XLConnectExample3.xlsx', create = TRUE)\ncreateSheet(wb, name = 'womenData')\ncreateName(wb, name = 'womenName', formula = 'womenData!$C$5', overwrite = TRUE)\nwriteNamedRegion(wb, women, name = \nwomenName\n)\nsaveWorkbook(wb)\n\n# write a field all in one step\nwriteNamedRegionToFile(\nXLConnectExample4.xlsx\n, women, name = \nwomenName\n, formula = \nwomenData!$C$5\n)\n\n\n\n\nI/O\n\n\n# Build connection to latitude.xlsx\nmy_book \n- loadWorkbook('latitude.xlsx')\n\n# Create data frame: summ\ndims1 \n- dim(readWorksheet(my_book, 1))\ndims2 \n- dim(readWorksheet(my_book, 2))\nsumm \n- data.frame(sheets = getSheets(my_book), \n                   nrows = c(dims1[1], dims2[1]), \n                   ncols = c(dims1[2], dims2[2]))\n\n# Add a worksheet to my_book, named 'data_summary'\ncreateSheet(my_book, name = 'data_summary')\n\n# Populate 'data_summary' with summ data frame\nwriteWorksheet(my_book, summ, sheet = 'data_summary')\n# Save workbook as latitude_with_summ.xlsx\n\nsaveWorkbook(my_book, 'latitude_with_summ.xlsx')\n\n\n\n\nTo SPSS\n\n\nlibrary(foreign)\n\nwrite.foreign(mydata, \nc:/mydata.txt\n, \nc:/mydata.sps\n, package = \nSPSS\n)\n\n\n\n\nTo SAS\n\n\nlibrary(foreign)\n\nwrite.foreign(mydata, \nc:/mydata.txt\n, \nc:/mydata.sas\n, package = \nSAS\n) \n\n\n\n\nTo Stata\n\n\nlibrary(foreign)\n\nwrite.dta(mydata, \nc:/mydata.dta\n) \n\n\n\n\n8, Inspecting Data - Missing Data\n\n\nInspecting\n\n\n\n\nls(object)\n\n\nnames(object)\n\n\nstr(object)\n\n\nlevels(object$v1)\n\n\ndim(object)\n\n\nclass(object)\n\n\nprint(object)\n\n\nhead(object, 10)\n\n\ntail(object, 20)\n\n\n\n\nTesting for Missing Values\n\n\ny \n- c(1, 2, 3, NA) # returns TRUE of x is missing\nis.na(y) # returns a vector (F F F T) \n\n\n\n\nRecoding Values to Missing\n\n\n# recode 99 to missing for variable v1\n# select rows where v1 is 99 and recode column v1\nmydata$v1[mydata$v1 == 99] \n- NA \n\n\n\n\nExcluding Missing Values from Analyses\n\n\nx \n- c(1, 2, NA, 3)\n\nmean(x) # returns NA\nmean(x, na.rm = TRUE) # returns 2 \n\n\n\n\n# list rows of data that have missing values\nmydata[!complete.cases(mydata),]\n\n\n\n\n# create new dataset without missing data\nnewdata \n- na.omit(mydata) \n\n\n\n\nThe \ndplyr\n package\n\n\nlibrary(dplyr)\n\ntbl_df(iris) # almost like head/tail\nglimpse(iris) # almost like str\nView(iris) # open a spreadsheet\n\n\n\n\nFor thorough cleaning\n\n\n\n\nThe \nAmelia II\n software.\n\n\nThe \nmitools\n package.\n\n\n\n\n9, Labels \n Levels\n\n\nBasic\n\n\n# variable v1 is coded 1, 2 or 3\n# we want to attach value labels 1=red, 2=blue, 3=green\nmydata$v1 \n- factor(mydata$v1, \n                    levels = c(1,2,3),\n                    labels = c(\nred\n, \nblue\n, \ngreen\n))\n\n\n\n\n# variable y is coded 1, 3 or 5\n# we want to attach value labels 1=Low, 3=Medium, 5=High\nmydata$v1 \n- ordered(mydata$y,\n                     levels = c(1,3, 5),\n                     labels = c(\nLow\n, \nMedium\n, \nHigh\n)) \n\n\n\n\nOrder\n\n\n# Create a vector of temperature observations\ntemperature_vector \n- c('High', 'Low', 'High', 'Low', 'Medium')\n\n# Specify that they are ordinal variables with the given levels\nfactor_temperature_vector \n- factor(temperature_vector, order = TRUE, levels = c('Low', 'Medium', 'High'))\n\n\n\n\nAdd comments to an object\n\n\nnames(iris)[5] \n- \nThis is the label for variable 5\n\n\nnames(iris)[5] # the comment\niris[5] # the data\n\n\n\n\n# labeling the variables\nlibrary(Hmisc)\n\nlabel(iris$Species) \n- \nVariable label for variable myvar\n\n\ndescribe(iris$Species) # commented\n#vs\ndescribe(iris$Sepal.Length) # not commented\n\n\n\n\n\n\nHow to work with Quandl in R\n\n\n1, Importing Quandl Datasets\n\n\nQuandl\n delivers financial, economic and\n\nalternative data to the world\ns top hedge funds, asset managers and\n\ninvestment banks in several formats:\n\n\n\n\nExcel.\n\n\nR.\n\n\nPython.\n\n\nAPI.\n\n\nDB.\n\n\n\n\nThe packages used:\n\n\n\n\nQuandl\n.\n\n\nquantmod\n for plotting.\n\n\n\n\nQuandl - A first date\n\n\n# Load in the Quandl package\nlibrary(Quandl)\n\n# Assign your first dataset to the variable:\nmydata \n- Quandl('NSE/OIL')\n\n\n\n\nIdentifying a dataset with its ID\n\n\n# Assign the Prague Stock Exchange to:\nPragueStockExchange \n- Quandl('PRAGUESE/PX')\n\n\n\n\nPlotting a stock chart\n\n\n# The quantmod package\nlibrary(quantmod)\n\n# Load the Facebook data with the help of Quandl\nFacebook \n- Quandl('GOOG/NASDAQ_FB', type = 'xts')\n\n# Plot the chart with the help of candleChart()\ncandleChart(Facebook)\n\n\n\n\nSearching a Quandl dataset in R\n\n\n# Look up the first 3 results for 'Bitcoin' within the Quandl database:\nresults \n- Quandl.search(query = 'Bitcoin', silent = FALSE)\n\n# Print out the results\nstr(results)\n\n# Assign the data set with code BCHAIN/TOTBC\nBitCoin \n- Quandl('BCHAIN/TOTBC')\n\n\n\n\n2, Manipulating Quandl Datasets\n\n\nManipulating data\n\n\n# Assign to the variable Exchange\nExchange \n- Quandl('BNP/USDEUR', start_date = '2013-01-01', end_date = '2013-12-01')\n\n\n\n\nTransforming your Quandl dataset\n\n\n# API transformation\n# The result:\nGDP_Change \n- Quandl('FRED/CANRGDPR', transformation = 'rdiff')\nhead(GDP_Change)\nGDP_Chang \n- Quandl('FRED/CANRGDPR')\nhead(GDP_Chang)\n\n\n\n\nThe magic of frequency collapsing\n\n\n# The result:\neiaQuarterly \n- Quandl('DOE/RWTC', collapse = 'quarterly')\n\n\n\n\nTruncation and sort\n\n\n# Assign to TruSo the first 5 observations of the crude oil prices\nTruSo \n- Quandl('DOE/RWTC', sort = 'asc', rows = 5)\n\n# Print the result\nTruSo\n\n\n\n\nA complex example\n\n\n# Here you should place the return:\nFinal \n- Quandl('DOE/RWTC', collapse = 'daily', transformation = 'rdiff', start_date = '2005-01-01', end_date = '2010-03-01', sort = 'asc')\n\n\n\n\n\n\nCleaning Data in R\n\n\nThe packages used:\n\n\n\n\ndplyr\n \n \ntidyr\n for data wrangling.\n\n\nstringr\n for regex.\n\n\nlubridate\n for time and date.\n\n\n\n\n1, Introduction and Exploring Raw Data\n\n\nHere\ns what messy data look like\n\n\n# View the first 6 rows of data\nhead(weather)\n\n# View the last 6 rows of data\ntail(weather)\n\n# View a condensed summary of the data\nstr(weather)\n\n\n\n\nGetting a feel for your data\n\n\n# Check the class of bmi\nclass(bmi)\n\n# Check the dimensions of bmi\ndim(bmi)\n\n# View the column names of bmi\nnames(bmi)\n\n\n\n\nViewing the structure of your data\n\n\n# Check the structure of bmi\nstr(bmi)\n\n# Load dplyr\nlibrary(dplyr)\n\n# Check the structure of bmi, the dplyr way\nglimpse(bmi)\n\n# View a summary of bmi\nsummary(bmi)\n\n\n\n\nLooking at your data\n\n\n# Print bmi to the console\nprint(bmi)\n\n# View the first 6 rows\nhead(bmi, 6)\n\n# View the first 15 rows\nhead(bmi, 15)\n\n# View the last 6 rows\ntail(bmi, 6)\n\n# View the last 10 rows\ntail(bmi, 10)\n\n\n\n\nVisualizing your data\n\n\n# Histogram of BMIs from 2008\nhist(bmi$Y2008)\n\n# Scatter plot comparing BMIs from 1980 to those from 2008\nplot(bmi$Y1980, bmi$Y2008)\n\n\n\n\n2, Tidying Data\n\n\nGathering columns into key-value pairs\n\n\n# Load tidyr\nlibrary(tidyr)\n\n# Apply gather() to bmi and save the result as bmi_long\nbmi_long \n- gather(bmi, year, bmi_val, -Country)\n\n# View the first 20 rows of the result\nhead(bmi_long, 20)\n\n\n\n\nSpreading key-value pairs into columns\n\n\n# Apply spread() to bmi_long\nbmi_wide \n- spread(bmi_long, year, bmi_val)\n\n# View the head of bmi_wide\nhead(bmi_wide)\n\n\n\n\nSeparating columns\n\n\n# Apply separate() to bmi_cc\nbmi_cc_clean \n- separate(bmi_cc, col = Country_ISO, into = c('Country', 'ISO'), sep = '/')\n\n# Print the head of the result\nhead(bmi_cc_clean)\n\n\n\n\nUniting columns\n\n\n# Apply unite() to bmi_cc_clean\nbmi_cc \n- unite(bmi_cc_clean, Country_ISO, Country, ISO, sep = '-')\n\n# View the head of the result\nhead(bmi_cc)\n\n\n\n\nColumn headers are values, not variable names\n\n\n# View the head of census\nhead(census)\n\n# Gather the month columns\ncensus2 \n- gather(census, month, amount, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC)\n\n# Arrange rows by YEAR using dplyr's arrange\ncensus2 \n- arrange(census2, YEAR)\n\n# View first 20 rows of census2\nhead(census2, 20)\n\n\n\n\nVariables are stored in both rows and columns\n\n\n# View first 50 rows of census_long\nhead(census_long, 50)\n\n# Spread the type column\ncensus_long2 \n- spread(census_long, type, amount)\n\n# View first 20 rows of census_long2\nhead(census_long2, 20)\n\n\n\n\nMultiple values are stored in one column\n\n\n# View the head of census_long3\nhead(census_long3)\n\n# Separate the yr_month column into two\ncensus_long4 \n- separate(census_long3, yr_month, c('year', 'month'), '_')\n\n# View the first 6 rows of the result\nhead(census_long4, 6)\n\n\n\n\n3, Preparing Data for Analysis\n\n\nTypes of variables in R\n\n\n# Make this evaluate to character\nclass('true')\n\n# Make this evaluate to numeric\nclass(8484.00)\n\n# Make this evaluate to integer\nclass(99L)\n\n# Make this evaluate to factor\nclass(factor('factor'))\n\n# Make this evaluate to logical\nclass(FALSE)\n\n\n\n\nCommon type conversions\n\n\n# Preview students with str()\nstr(students)\n\n# Coerce Grades to character\nstudents$Grades \n- as.character(students$Grades)\n\n# Coerce Medu to factor\nstudents$Medu \n- as.factor(students$Medu)\n\n# Coerce Fedu to factor\nstudents$Fedu \n- as.factor(students$Fedu)\n\n # Look at students once more with str()\nstr(students)\n\n\n\n\nWorking with dates\n\n\n# Preview students2 with str()\nstr(students2)\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Parse as date\nymd('2015-Sep-17')\n\n# Parse as date and time (with no seconds!)\nymd_hm('2012-July-15, 12.56')\n\n# Coerce dob to a date (with no time)\nstudents2$dob \n- ymd(students2$dob)\n\n# Coerce nurse_visit to a date and time\nstudents2$nurse_visit \n- ymd_hms(students2$nurse_visit)\n\n# Look at students2 once more with str()\nstr(students2)\n\n\n\n\nTrimming and padding strings\n\n\n# Load the stringr package\nlibrary(stringr)\n\n# Trim all leading and trailing whitespace\nstr_trim(c('   Filip ', 'Nick  ', ' Jonathan'))\n\n# Pad these strings with leading zeros\nstr_pad(c('23485W', '8823453Q', '994Z'), width = 9, side = 'left', pad = '0')\n\n\n\n\nUpper and lower case\n\n\n# Print state abbreviations\nstates\n\n# Make states all uppercase and save result to states_upper\nstates_upper \n- toupper(states)\nstates_upper\n\n# Make states_upper all lowercase again\ntolower(states_upper)\n\n\n\n\nFinding and replacing strings\n\n\n# stringr has been loaded for you\n# Look at the head of students2\nhead(students2)\n\n# Detect all dates of birth (dob) in 1997\nstr_detect(students2$dob, '1997')\n\n# In the sex column, replace 'F' with 'Female'...\nstudents2$sex \n- str_replace(students2$sex, 'F', 'Female')\n\n# ...And 'M' with 'Male'\nstudents2$sex \n- str_replace(students2$sex, 'M', 'Male')\n\n# View the head of students2\nhead(students2)\n\n\n\n\nFinding missing values\n\n\n# Call is.na() on the full social_df to spot all NAs\nis.na(social_df)\n\n# Use the any() function to ask whether there are any NAs in the data\nany(is.na(social_df))\nsum(is.na(social_df))\n\n# View a summary() of the dataset\nsummary(social_df)\n\n# Call table() on the status column\ntable(social_df$status)\n\n\n\n\nDealing with missing values\n\n\n# Use str_replace() to replace all missing strings in status with NA\nsocial_df$status \n- str_replace(social_df$status, '^$', NA)\n\n# Print social_df to the console\nsocial_df\n\n# Use complete.cases() to see which rows have no missing values\ncomplete.cases(social_df)\n\n# Use na.omit() to remove all rows with any missing values\nna.omit(social_df)\n\n\n\n\nDealing with outliers and obvious errors\n\n\n# Look at a summary() of students3\nsummary(students3)\n\n# View a histogram of the age variable\nhist(students3$age, breaks = 20)\n\n# View a histogram of the absences variable\nhist(students3$absences, breaks = 20)\n\n# View a histogram of absences, but force zeros to be bucketed to the right of zero\nhist(students3$absences, breaks = 20, right = FALSE)\n\n\n\n\nAnother look at strange values\n\n\n# View a boxplot of age\nboxplot(students3$age)\n\n# View a boxplot of absences\nboxplot(students3$absences)\n\n\n\n\n4, Putting it All Together\n\n\nGet a feel for the data\n\n\n# Verify that weather is a data.frame\nclass(weather)\n\n# Check the dimensions\ndim(weather)\n\n# View the column names\nnames(weather)\n\n\n\n\nSummarize the data\n\n\n# View the structure of the data\nstr(weather)\n\n# Load dplyr package\nlibrary(dplyr)\n\n# Look at the structure using dplyr's glimpse()\nglimpse(weather)\n\n# View a summary of the data\nsummary(weather)\n\n\n\n\nTake a closer look\n\n\n# View first 6 rows\nhead(weather, 6)\n\n# View first 15 rows\nhead(weather, 15)\n\n# View the last 6 rows\ntail(weather, 6)\n\n# View the last 10 rows\ntail(weather, 10)\n\n\n\n\nColumn names are values\n\n\n# Load the tidyr package\nlibrary(tidyr)\n\n# Gather the columns\nweather2 \n- gather(weather, day, value, X1:X31, na.rm = TRUE)\n\n# View the head\nhead(weather2)\n\n\n\n\nValues are variable names\n\n\n# First remove column of row names\nweather2 \n- weather2[, -1]\n\n# Spread the data\nweather3 \n- spread(weather2, measure, value)\n\n# View the head\nhead(weather3)\n\n\n\n\nClean up dates\n\n\n# Load the stringr and lubridate packages\nlibrary(stringr)\nlibrary(lubridate)\n\n# Remove X's from day column\n# weather3$day \n- str_pad(str_replace(weather3$day, 'X', ''), width = 2, side = 'left', pad = '0')\nweather3$day \n- str_replace(weather3$day, 'X', '')\n\n# Unite the year, month, and day columns\nweather4 \n- unite(weather3, date, year, month, day, sep = '-')\n\n# Convert date column to proper date format using stringr's ymd()\nweather4$date \n- ymd(weather4$date)\n\n# Rearrange columns using dplyr's select()\nweather5 \n- select(weather4, date, Events, CloudCover:WindDirDegrees)\n\n# View the head\nhead(weather5)\n\n\n\n\nA closer look at column types\n\n\n# View the structure of weather5\nstr(weather5)\n\n# Examine the first 20 rows of weather5. Are most of the characters numeric?\nhead(weather5, 20)\n\n# See what happens if we try to convert PrecipitationIn to numeric\nas.numeric(weather5$PrecipitationIn)\n\n\n\n\nColumn type conversions\n\n\n# The dplyr package is already loaded\n# Replace T with 0 (T = trace)\nweather5$PrecipitationIn \n- str_replace(weather5$PrecipitationIn, 'T', '0')\n\n# Convert characters to numerics\nweather6 \n- mutate_each(weather5, funs(as.numeric), CloudCover:WindDirDegrees)\n\n# Look at result\nstr(weather6)\n\n\n\n\nFind missing values\n\n\n# Count missing values\nsum(is.na(weather6))\n\n# Find missing values\nsummary(weather6)\n\n# Find indices of NAs in Max.Gust.SpeedMPH\nind \n- which(is.na(weather6$Max.Gust.SpeedMPH))\nind\n\n# Look at the full rows for records missing Max.Gust.SpeedMPH\nweather6[ind, ]\n\n\n\n\nAn obvious error\n\n\n# Review distibutions for all variables\nsummary(weather6)\n\n# Find row with Max.Humidity of 1000\nind \n- which(weather6$Max.Humidity == 1000)\n\n# Look at the data for that day\nweather6[ind, ]\n\n# Change 1000 to 100\nweather6$Max.Humidity[ind] \n- 100\n\n\n\n\nAnother obvious error\n\n\n# Look at summary of Mean.VisibilityMiles\nsummary(weather6$Mean.VisibilityMiles)\n\n# Get index of row with -1 value\nind \n- which(weather6$Mean.VisibilityMiles == -1)\n\n# Look at full row\nweather6[ind, ]\n\n# Set Mean.VisibilityMiles to the appropriate value\nweather6$Mean.VisibilityMiles[ind] \n- 10\n\n\n\n\nCheck other extreme values\n\n\n# Review summary of full data once more\nsummary(weather6)\n\n# Look at histogram for MeanDew.PointF\nhist(weather6$MeanDew.PointF)\n\n# Look at histogram for Min.TemperatureF\nhist(weather6$Min.TemperatureF)\n\n# Compare to histogram for Mean.TemperatureF\nhist(weather6$Mean.TemperatureF)\n\n\n\n\nFinishing touches\n\n\n# Clean up column names\nnames(weather6) \n- new_colnames\n\n# Replace empty cells in Events column\nweather6$events[weather6$events == ''] \n- 'None'\n\n# Print the first 6 rows of weather6\nhead(weather6, 6)", 
            "title": "I/O snippets & Cleaning"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#importing-data-into-r", 
            "text": "The packages:   utils .  readr .  data.table .  readxl .  gdata .  XLConnect .  haven .  foreign .  DBI .  httr .  jsonlite .", 
            "title": "Importing Data Into R"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#1-importing-data-from-flat-files", 
            "text": "", 
            "title": "1, Importing Data from Flat Files"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#r-functions-by-default", 
            "text": "read.csv ;  sep = ',' ,  dec = '.' .  read.delim ; .txt,  dec = '.' .  read.csv2 ;  sep = ';' ,  dec = ',' .  read.delim2 ; .txt,  dec = ',' .  Needs arguments.   read.csv  for .csv files  # List the files in your working directory\ndir()\n\n# Import swimming_pools.csv: pools\n# stringAsFactors = FALSE does not import strings as categorical variables\npools  - read.csv('swimming_pools.csv', stringsAsFactors = FALSE)  stringsAsFactors  # Import swimming_pools.csv correctly: pools\npools  - read.csv('swimming_pools.csv', stringsAsFactor = FALSE, header = TRUE, sep = ',')\n\n# Import swimming_pools.csv with factors: pools_factor\npools_factor  - read.csv('swimming_pools.csv', header = TRUE, sep = ',')  read.delim  for .txt files  # Import hotdogs.txt: hotdogs\nhotdogs  - read.delim('hotdogs.txt', header = FALSE)\n\n# Name the columns of hotdogs appropriately\nnames(hotdogs)  - c('type', 'calories', 'sodium')  Arguments.  # Load in the hotdogs data set: hotdogs\nhotdogs  - read.delim('hotdogs.txt', header = FALSE, sep = '\\t', col.names = c('type', 'calories', 'sodium'))\n\n# Select the hot dog with the least calories: lily\nlily  - hotdogs[which.min(hotdogs$calories), ]\n# Select the observation with the most sodium: tom\n\ntom  - hotdogs[which.max(hotdogs$sodium), ]  # Previous call to import hotdogs.txt\nhotdogs  - read.delim('hotdogs.txt', header = FALSE, col.names = c('type', 'calories', 'sodium'))\n\n# Print a vector representing the classes of the columns\nsapply(hotdogs, class)\n\n# Edit the colClasses argument to import the data correctly: hotdogs2\nhotdogs2  - read.delim('hotdogs.txt', header = FALSE, col.names = c('type', 'calories', 'sodium'), colClasses = c('factor', 'NULL', 'numeric'))", 
            "title": "R functions, by default."
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-utils-package", 
            "text": "read.table ;  sep = '\\t' ,  = ',' ,  = ';' .  Read any tabular as a d.f.  Needs arguments; lots of argument for precision.  Slow.    library(utils)  read.table  .txt files  # Create a path to the hotdogs.txt file\npath  - file.path('hotdogs', 'hotdogs.txt')\n\n# Import the hotdogs.txt file: hotdogs\nhotdogs  - read.table(path, header = FALSE, sep = '\\t', col.names = c('type', 'calories', 'sodium'))", 
            "title": "The utils package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#from-5-importing-data-from-the-web", 
            "text": "# https URL to the swimming_pools csv file.\nurl_csv  - 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\n\n# Import the file using read.csv(): pools1\npools1  - read.csv(url_csv)", 
            "title": "(from 5, Importing Data from the Web)"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-readr-package", 
            "text": "read_delim ;  delim = '\\t' ,  = ',' .  read_csv ; read  100.000, 200.000  read_tsv ; idem.  read_csv2 ; read  100,000; 200,000  or European files..  read_tsv2 ; idem.  read_lines .  read_file .  write_csv .  write_rds .  type_convert .  parse_factor .  parse_date .  parse_number .  spec_csv .  spec_delim .  Fast, few arguments.  Detect data type.    library(readr)  read_delim  .txt files  # Import potatoes.txt using read_delim(): potatoes\npotatoes  - read_delim('potatoes.txt', delim = '\\t')  read_csv  .csv files  # Column names\nproperties  - c('area', 'temp', 'size', 'storage', 'method', 'texture', 'flavor', 'moistness')\n\n# Import potatoes.csv with read_csv(): potatoes\npotatoes  - read_csv('potatoes.csv', col_names = properties)\n\n# Create a copy of potatoes: potatoes2\npotatoes2  - potatoes\n\n# Convert the method column of potatoes2 to a factor\npotatoes2$method = factor(potatoes2$method)\n\n# or\n\npotatoes2$method = as.factor(potatoes2$method)  col_types ,  skip  and  n_max  in .tsv files  # Column names\nproperties  - c('area', 'temp', 'size', 'storage', 'method', 'texture', 'flavor', 'moistness')\n\n# Import 5 observations from potatoes.txt: potatoes_fragment\n# read_tsv or tab-separated values\npotatoes_fragment  - read_tsv('potatoes.txt', col_names = properties, skip = 7, n_max = 5)\n\n# Import all data, but force all columns to be character: potatoes_char\npotatoes_char  - read_tsv('potatoes.txt', col_types = 'cccccccc')  Setting column types  cols(\n  weight = col_integer(),\n  feed = col_character()\n)  Removing NA  na = c('NA', 'null')  col_types  with collectors .tsv files  # Import without col_types\nhotdogs  - read_tsv('hotdogs.txt', col_names = c('type', 'calories', 'sodium'))\n\n# The collectors you will need to import the data\nfac  - col_factor(levels = c('Beef', 'Meat', 'Poultry'))\nint  - col_integer()\n\n# Edit the col_types argument to import the data correctly: hotdogs_factor\n# Change col_types to the correct vector of collectors; coerce the vector into a list\nhotdogs_factor  - read_tsv('hotdogs.txt', col_names = c('type', 'calories', 'sodium'), col_types = list(fac, int, int))  Skiping columns  salaries  - read_tsv('Salaries.txt', col_names = FALSE, col_types = cols(\n  X2 = col_skip(),\n  X3 = col_skip(), \n  X4 = col_skip()\n))  Reading an ordinary text file  # vector of character strings. \n# Import as a character vector, one item per line: tweets\ntweets  - read_lines('tweets.txt')\ntweets\n\n# returns a length 1 vector of the entire file, with line breaks represented as \\n\n# Import as a length 1 vector: tweets_all\ntweets_all  - read_file('tweets.txt')\ntweets_all  Writing .csv and .tsv files  # Save cwts as chickwts.csv\nwrite_csv(cwts,  chickwts.csv )\n\n# Append cwts2 to chickwts.csv\nwrite_csv(cwts2,  chickwts.csv , append = TRUE)  Writing .rds files  # Save trees as trees.rds\nwrite_rds(trees, 'trees.rds')\n\n# Import trees.rds: trees2\ntrees2  - read_rds('trees.rds')\n\n# Check whether trees and trees2 are the same\nidentical(trees, trees2)  Coercing columns to different data types  # Convert all columns to double\ntrees2  - type_convert(trees, col_types = cols(Girth = 'd', Height = 'd', Volume = 'd'))  Coercing character columns into factors  # Parse the title column\nsalaries$title  - parse_factor(salaries$title, levels = c('Prof', 'AsstProf', 'AssocProf'))\n\n# Parse the gender column\nsalaries$gender  - parse_factor(salaries$gender, levels = c('Male', 'Female'))  Creating Date objects  # Change type of date column\nweather$date  - parse_date(weather$date, format = '%m/%d/%Y')  Parsing number formats  # Parse amount column as a number\ndebt$amount  - parse_number(debt$amount)  Viewing metadata before importing   spec_csv  for .csv and .tsv files.  spec_delim  for .txt files (among others).    # Specifications of chickwts\nspec_csv('chickwts.csv')", 
            "title": "The readr package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#from-5-importing-data-from-the-web_1", 
            "text": "Import Flat files from the web  # Import the csv file: pools\nurl_csv  - 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\npools  - read_csv(url_csv)\n\npools\n\n# Import the txt file: potatoes\nurl_delim  - 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/potatoes.txt'\npotatoes  - read_tsv(url_delim)\n\npotatoes  Secure importing  # Import the file using read_csv(): pools2\npools2  - read_csv(url_csv)", 
            "title": "(from 5, Importing Data from the Web)"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-datatable-package", 
            "text": "fread  ==  read.table .  .txt files only.  Fast.    library(data.table)  fread  for .txt files  # Import potatoes.txt with fread(): potatoes\npotatoes  - fread('potatoes.txt')\n\n# Print out arranged version of potatoes\npotatoes[order(moistness),] \n\n# Import 20 rows of potatoes.txt with fread(): potatoes_part\npotatoes_part  - fread('potatoes.txt', nrows = 20)  fread : more advanced use  # Import columns 6, 7 and 8 of potatoes.txt: potatoes\npotatoes  - fread('potatoes.txt', select = c(6:8))\n\n# Keep only tasty potatoes (flavor   3): tasty_potatoes\ntasty_potatoes  - subset(potatoes, potatoes$flavor   3)", 
            "title": "The data.table package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#2-importing-data-from-excel", 
            "text": "", 
            "title": "2, Importing Data from Excel"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-readxl-package", 
            "text": "excel_sheets ; list.  read_excel ; import.  .xlsx files only.    library(readxl)  List the sheets of an Excel file  # Find the names of both spreadsheets: sheets\n# Before, find out what is in the directory with 'dir()'\nsheets  - excel_sheets('latitude.xlsx')\n\nsheets  Importing an Excel sheet  # Read the first sheet of latitude.xlsx: latitude_1\nlatitude_1  - read_excel('latitude.xlsx', sheet = 1)\n\n# Read the second sheet of latitude.xlsx: latitude_2\nlatitude_2  - read_excel('latitude.xlsx', sheet = 2)\n\n# Put latitude_1 and latitude_2 in a list: lat_list\nlat_list  - list(latitude_1, latitude_2)  Reading a workbook  # Read all Excel sheets with lapply(): lat_list\nlat_list  - lapply(excel_sheets('latitude.xlsx'), read_excel, path = 'latitude.xlsx')  The  col_names  argument  # Import the the first Excel sheet of latitude_nonames.xlsx (R gives names): latitude_3\nlatitude_3  - read_excel('latitude_nonames.xlsx', sheet = 1, col_names = FALSE)\n\n# Import the the second Excel sheet of latitude_nonames.xlsx (specify col_names): latitude_4 \nlatitude_4  - read_excel('latitude_nonames.xlsx', sheet = 1, col_names = c('country', 'latitude'))  The  skip  argument  # Import the second sheet of latitude.xlsx, skipping the first 21 rows: latitude_sel\nlatitude_sel  - read_excel('latitude.xlsx', skip = 21, col_names = FALSE)", 
            "title": "The readxl package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#from-5-importing-data-from-the-web_2", 
            "text": "Import Excel files from the web  # Download file behind URL, name it local_latitude.xls\ndownload.file(url_xls, 'local_latitude.xls')\n\n# Import the local .xls file with readxl: excel_readxl\nexcel_readxl  - read_excel('local_latitude.xls')  Downloading any file, secure or not  # https URL to the wine RData file.\nurl_rdata  - 'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/wine.RData'\n\n# Download the wine file to your working directory\ndownload.file(url_rdata, 'wine_local.RData')", 
            "title": "(from 5, Importing Data from the Web)"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-xlconnect-package", 
            "text": "loadWorkbook .  getSheets .  readWorksheet .  readWorksheetFromFile  readNamedRegion   readNamedRegionFromFile    .xls   .xlsx files.   Like reading a database.    library(XLConnectJars)\nlibrary(XLConnect)  Import a workbook  # Build connection to latitude.xlsx: my_book\nmy_book  - loadWorkbook('latitude.xlsx')  List and read Excel sheets  # Build connection to latitude.xlsx\nmy_book  - loadWorkbook('latitude.xlsx')\n\n# List the sheets in latitude.xlsx\ngetSheets(my_book)\n\n# Import the second sheet in latitude.xlsx\nreadWorksheet(my_book, sheet = 2)\n\n# Import the second column of the first sheet in latitude.xlsx\nreadWorksheet(my_book, sheet = 2, startCol = 2)  Add and populate worksheets  # Build connection to latitude.xlsx\nmy_book  - loadWorkbook('latitude.xlsx')\n\n# Create data frame: summ\ndims1  - dim(readWorksheet(my_book, 1))\ndims2  - dim(readWorksheet(my_book, 2))\nsumm  - data.frame(sheets = getSheets(my_book), \n                   nrows = c(dims1[1], dims2[1]), \n                   ncols = c(dims1[2], dims2[2]))\n\n# Add a worksheet to my_book, named 'data_summary'\ncreateSheet(my_book, name = 'data_summary')\n\n# Populate 'data_summary' with summ data frame\nwriteWorksheet(my_book, summ, sheet = 'data_summary')\n# Save workbook as latitude_with_summ.xlsx\n\nsaveWorkbook(my_book, 'latitude_with_summ.xlsx')  One unique function  # Read in the data set and assign to the object\nimpact  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'impact', header = TRUE, startCol = 1, startRow = 1)\n\n# more arguments\n# endCol = 1\n# endRow = 1\n# autofitRow = \n# autofitCol = \n# region =\n# rownames =\n# colTypes =\n# forceConversion =\n# dateTimeFormat =\n# check.names =\n# useCachedValues =\n# keep =\n# drop =\n# simplify =\n# readStrategy =", 
            "title": "The XLConnect package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#3-importing-data-from-other-statistical-software", 
            "text": "", 
            "title": "3, Importing Data from Other Statistical Software"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-haven-package", 
            "text": "read_sas ; sas7bdat   sas7bcat files.  read_stata ; version; dta files.  read_dta ; idem.  read_spss ; sav   por files (and see below).  read_por .  read_sav .  Simple, few arguments.  Create a d.f.    library(haven)  Import SAS data with haven  # Import sales.sas7bdat: sales\nsales  - read_sas('sales.sas7bdat')  Import STATA data with haven  # Import the data from the URL: sugar\nsugar  - read_dta('http://assets.datacamp.com/course/importing_data_into_r/trade.dta')  Import SPSS data with haven  # Specify the file path using file.path(): path\npath  - file.path('datasets', 'person.sav')\n\n# Import person.sav, which is in the datasets folder: traits\ntraits  - read_sav(path)  Factorize, round two  # Import SPSS data from the URL: work\nwork  - read_sav('http://assets.datacamp.com/course/importing_data_into_r/employee.sav')", 
            "title": "The haven package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#theforeign-package", 
            "text": "Cannot import SAS, see the  sas7bdat  package.  read.dta ; dta files.  read.spss ; sav   por files.  Comprehensive.    library(foreign)  Import STATA data with foreign (1)  # Import florida.dta and name the resulting data frame florida\nflorida  - read.dta('florida.dta')  Import STATA data with foreign (2)  # Specify the file path using file.path(): path\npath  - file.path('worldbank', 'edequality.dta')\n\n# Create and print structure of edu_equal_1\nedu_equal_1  - read.dta(path)\n\n# Create and print structure of edu_equal_2\nedu_equal_2  - read.dta(path, convert.factors = FALSE)\n\n# Create and print structure of edu_equal_3\nedu_equal_3  - read.dta(path, convert.underscore = TRUE)   Import SPSS data with foreign (1)  # Import international.sav as a data frame: demo\ndemo  - read.spss('international.sav', to.data.frame = TRUE)  Import SPSS data with foreign (2)  # Import international.sav as demo_1\ndemo_1  - read.spss('international.sav', to.data.frame = TRUE)\n\n# Import international.sav as demo_2\ndemo_2  - read.spss('international.sav', to.data.frame = TRUE, use.value.labels = FALSE)", 
            "title": "Theforeign package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#4-importing-data-from-relational-data", 
            "text": "", 
            "title": "4, Importing Data from Relational Data"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-dbi-package", 
            "text": "dbConnect .  dbReadTable .  dbGetQuery .  dbFetch .  dbDisconnect .    library(DBI)  Step 1: Establish a connection  # Connect to the MySQL database: con\ncon  - dbConnect(RMySQL::MySQL(), dbname = 'tweater', host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', port = 3306, user = 'student', password = 'datacamp') \n\ncon  Step 2: List the database tables  # Connect to the MySQL database: con\ncon  - dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Build a vector of table names: tables\ntables  - dbListTables(con)\n\n# Display structure of tables\nstr(tables)  Step 3: Import data from a table  # Connect to the MySQL database: con\ncon  - dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Import the users table from tweater: users\nusers  - dbReadTable(con, 'users')\n\nusers\n\n# Import and print the tweats table from tweater: tweats\ntweats  - dbReadTable(con, 'tweats')\n\ntweats\n\n# Import and print the comments table from tweater: comments\ncomments  - dbReadTable(con, 'comments')\n\ncomments  Your very first SQL query  con  - dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Import post column of tweats where date is higher than '2015-09-21': latest\nlatest  - dbGetQuery(con, 'SELECT post FROM tweats WHERE date   \\'2015-09-21\\'')\n\nlatest\n\n# Import tweat_id column of comments where user_id is 1: elisabeth\nelisabeth  - dbGetQuery(con, 'SELECT tweat_id FROM comments WHERE user_id = 1')\n\nelisabeth  More advanced SQL queries  # Connect to the database\ncon  - dbConnect(RMySQL::MySQL(),\n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Create data frame specific\nspecific  - dbGetQuery(con, 'SELECT message FROM comments WHERE tweat_id = 77 AND user_id   4')\n\nspecific\n\n# Create data frame short\nshort  - dbGetQuery(con, 'SELECT id, name FROM users WHERE CHAR_LENGTH(name)   5')\n\nshort  Send - Fetch - Clear  # Connect to the database\ncon  - dbConnect(RMySQL::MySQL(), \n                 dbname = 'tweater', \n                 host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                 port = 3306,\n                 user = 'student',\n                 password = 'datacamp')\n\n# Send query to the database with dbSendQuery(): res\nres  - dbSendQuery(con, 'SELECT * FROM comments WHERE user_id   4')\n\n# Display information contained in res\ndbGetInfo(res)\n\n# Use dbFetch() twice\nwhile (!dbHasCompleted(res)) {\n    chunk  - dbFetch(res, n = 2)\n    chunk2  - dbFetch(res)\n    print(chunk)\n}\n\n# Clear res\ndbClearResult(res)  Be polite and   # Database specifics\ndbname  - 'tweater'\nhost  - 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\nport  - 3306\nuser  - 'student'\npassword  - 'datacamp'\n\n# Connect to the database\ncon  - dbConnect(RMySQL::MySQL(), dbname = 'tweater', host = 'courses.csrrinzqubik.us-east-1.rds.amazonaws.com', port = 3306 , user = 'student', password = 'datacamp')\n\n# Create the data frame  long_tweats\nlong_tweats  - dbGetQuery(con, 'SELECT post, date FROM tweats WHERE CHAR_LENGTH(post)   40')\n\n# Print long_tweats\nprint(long_tweats)\n\n# Disconnect from the database\ndbDisconnect(con)  Other general packages  The  RODBC  package provides access to databases (including Microsoft \nAccess and Microsoft SQL Server) through an ODBC interface.  The  RJDBC  package provides access to databases through a JDBC \ninterface.  Specialized packages   ROracle  provides an interface for Oracle.  RMySQL  provides access to MySQL.  RpostgreSQL  to PostgreSQL.  RSQLite  to SQLite.  And there are manu more packages for NoSQL databases such \n    as MongoDB.", 
            "title": "The DBI package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#4b-importing-data-from-relational-data-more", 
            "text": "", 
            "title": "4b, Importing Data from Relational Data -- More"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#dbi", 
            "text": "First, change the working directory with  setwd . Install the  DBI \nlibrary.  Connect and read preliminary results  library(DBI)\nlibrary(sqliter)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nsqlite = dbDriver('SQLite')\n\n# Assign the connection string to a connection object\nsqlitedb  - dbConnect(RSQLite::SQLite(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(sqlitedb)  Extract some data  # Assign the results of a SQL query to an object\nresults = dbSendQuery(sqlitedb,  SELECT * FROM albums )\n\n# Return results from a custom object to a data.frame\ndata = fetch(results)\n\n# Print data frame to console\nhead(data)  # Clear the results and close the connection\ndbClearResult(results)\n\n# Disconnect from the database\ndbDisconnect(sqlitedb)", 
            "title": "DBI"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#rsqlite", 
            "text": "First, change the working directory with  setwd . Install the  RSQLite \nlibrary.  Connect and read preliminary results  library(RSQLite)\nlibrary(sqliter)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nsqlite = dbDriver('SQLite')\n\n# Assign the connection string to a connection object\nmysqldb = dbConnect(sqlite, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(sqlitedb)  Extract some data  # Assign the results of a SQL query to an object\nresults = dbSendQuery(sqlitedb,  SELECT * FROM albums )\n\n# Check the object\nresults\ndbGetInfo(results)\n\n# Return results from a custom object to a data.frame\ndata = fetch(results)\n\n# Print data frame to console\nhead(data)  # Clear the results and close the connection\ndbClearResult(results)\n\n# Disconnect from the database\ndbDisconnect(sqlitedb)", 
            "title": "RSQLite"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#mysql-with-dbi-or-rmysql", 
            "text": "library(DBI)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nmysql = dbDriver('MySQL')\n\n# Assign the connection string to a connection object\nmysqldb  - dbConnect(RMySQL::MySQL(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Disconnect from the database\ndbDisconnect(mysqldb)  library(RMySQL)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\nmysql = dbDriver('MySQL')\n\n# Assign the connection string to a connection object\nmysqldb = dbConnect(mysql, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Request a list of tables using the connection object\ndbListTables(mysqldb)\n\n# Disconnect from the database\ndbDisconnect(mysqldb)", 
            "title": "MySQL with DBI or RMySQL"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#posgresql-with-dbi-or-rpostgresql", 
            "text": "library(DBI)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\npostgresql = dbDriver('PostgreSQL')\n\n# Assign the connection string to a connection object\npostgresqldb  - dbConnect(RPostgreSQL::PostgreSQL(), \n                 dbname = dbfile, \n                 host = '', \n                 port = 3306,\n                 user = '',\n                 password = '')\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Disconnect from the database\ndbDisconnect(postgresqldb)  library(RPostgreSQL)\n\n# Assign the sqlite database and full path to a variable\ndbfile = 'chinook.db'\n\n# Instantiate the dbDriver to a convenient object\npostgresql = dbDriver('PostgreSQL')\n\n# Assign the connection string to a connection object\npostgresqldb = dbConnect(postgresql, dbfile)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Request a list of tables using the connection object\ndbListTables(postgresqldb)\n\n# Disconnect from the database\ndbDisconnect(postgresqldb)", 
            "title": "PosgreSQL with DBI or RPostgreSQL"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#5-importing-data-from-the-web", 
            "text": "The other package above can download files from the web. The next \npackages are web-oriented.", 
            "title": "5, Importing Data from the Web"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-httr-package", 
            "text": "GET  pages and files from the web.  Concise.  Parse JSON files.  Communicate with APIs.    library(httr)  HTTP?  httr !  # Get the url, save response to resp\nurl  - 'http://docs.datacamp.com/teach/'\nresp  - GET(url)\n\nresp\n\n# Get the raw content of resp\nraw_content  - content(resp, as = 'raw')\n\n# Print the head of content\nhead(raw_content)  # Get the url\nurl  - 'https://www.omdbapi.com/?t=Annie+Hall y= plot=short r=json'\n\nresp  - GET(url)\n\n# Print resp\nresp\n\n# Print content of resp as text\ncontent(resp, as = 'text')\n\n# Print content of resp\ncontent(resp)", 
            "title": "The httr package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-jsonlite-package", 
            "text": "Robust.  Improve the imported data.  fromJSON .  from an R object to  toJSON  prettify .  minify .    library(jsonlite)  From  JSON  to R  # Convert wine_json to a list: wine\nwine_json  - '{'name':'Chateau Migraine', 'year':1997, 'alcohol_pct':12.4, 'color':'red', 'awarded':false}'\nwine  - fromJSON(wine_json)\n\nstr(wine)\n\n# Import Quandl data: quandl_data\nquandl_url  - 'http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz'\nquandl_data  - fromJSON(quandl_url)\n\nstr(quandl_data)  # Experiment 1\njson1  - '[1, 2, 3, 4, 5, 6]'\nfromJSON(json1)\n\n# Experiment 2\njson2  - '{'a': [1, 2, 3], 'b': [4, 5, 6]}'\nfromJSON(json2)\n\n# Experiment 3\njson3  - '[[1, 2], [3, 4]]'\nfromJSON(json3)\n\n# Experiment 4\njson4  - '[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]'\nfromJSON(json4)  Ask OMDb  # Definition of the URLs\nurl_sw4  - 'http://www.omdbapi.com/?i=tt0076759 r=json'\nurl_sw3  - 'http://www.omdbapi.com/?i=tt0121766 r=json'\n\n# Import two URLs with fromJSON(): sw4 and sw3\nsw4  - fromJSON(url_sw4)\nsw3  - fromJSON(url_sw3)\n\n# Print out the Title element of both lists\nsw4$Title\nsw3$Title\n\n# Is the release year of sw4 later than sw3\nsw4$Year   sw3$Year  From R to  JSON  # URL pointing to the .csv file\nurl_csv  - 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/water.csv'\n\n# Import the .csv file located at url_csv\nwater  - read.csv(url_csv, stringsAsFactors = FALSE)\n\n# Generate a summary of water\nsummary(water)\n\n# Convert the data file according to the requirements\nwater_json  - toJSON(water)\n\nwater_json  Minify  and  prettify  # Convert mtcars to a pretty JSON: pretty_json\npretty_json  - toJSON(mtcars, pretty = TRUE)\n\n# Print pretty_json\npretty_json\n\n# Minify pretty_json: mini_json\nmini_json  - minify(pretty_json)\n\n# Print mini_json\nmini_json", 
            "title": "The jsonlite package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#6-keyboard-inputting", 
            "text": "Coding  # create a data frame from scratch\nage  - c(25, 30, 56)\ngender  - c( male ,  female ,  male )\nweight  - c(160, 110, 220)\nmydata  - data.frame(age,gender,weight)  Spreadsheet-like  # enter data using editor\nmydata  - data.frame(age = numeric(0), gender = character(0), weight = numeric(0))\n\nmydata  - edit(mydata)\n# note that without the assignment in the line above, the edits are not saved!", 
            "title": "6, Keyboard Inputting"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#7-exporting-data", 
            "text": "", 
            "title": "7, Exporting Data"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#to-a-tab-delimited-text-file", 
            "text": "write.table(mydata, 'c:/mydata.txt', sep =  \\t )", 
            "title": "To a Tab-Delimited Text File"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#to-an-excel-spreadsheet", 
            "text": "library(xlsx)\n\nwrite.xlsx(mydata,  c:/mydata.xlsx )  Worksheet  library(XLConnect)\n# xls or xlsx\n\n# write a worksheet in steps\nwb  - loadWorkbook('XLConnectExample1.xls', create = TRUE)\ncreateSheet(wb, name = 'chickSheet')\nwriteWorksheet(wb, ChickWeight, sheet = 'chickSheet', startRow = 3, startCol = 4)\nsaveWorkbook(wb)\n\n# write a worksheet all in one step\nChickWeight  - 1\n\nwriteWorksheetToFile('XLConnectExample2.xlsx', data = ChickWeight, sheet = 'chickSheet', startRow = 3, startCol = 4)  Field  # write a field in steps\nwb = loadWorkbook('XLConnectExample3.xlsx', create = TRUE)\ncreateSheet(wb, name = 'womenData')\ncreateName(wb, name = 'womenName', formula = 'womenData!$C$5', overwrite = TRUE)\nwriteNamedRegion(wb, women, name =  womenName )\nsaveWorkbook(wb)\n\n# write a field all in one step\nwriteNamedRegionToFile( XLConnectExample4.xlsx , women, name =  womenName , formula =  womenData!$C$5 )  I/O  # Build connection to latitude.xlsx\nmy_book  - loadWorkbook('latitude.xlsx')\n\n# Create data frame: summ\ndims1  - dim(readWorksheet(my_book, 1))\ndims2  - dim(readWorksheet(my_book, 2))\nsumm  - data.frame(sheets = getSheets(my_book), \n                   nrows = c(dims1[1], dims2[1]), \n                   ncols = c(dims1[2], dims2[2]))\n\n# Add a worksheet to my_book, named 'data_summary'\ncreateSheet(my_book, name = 'data_summary')\n\n# Populate 'data_summary' with summ data frame\nwriteWorksheet(my_book, summ, sheet = 'data_summary')\n# Save workbook as latitude_with_summ.xlsx\n\nsaveWorkbook(my_book, 'latitude_with_summ.xlsx')", 
            "title": "To an Excel Spreadsheet"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#to-spss", 
            "text": "library(foreign)\n\nwrite.foreign(mydata,  c:/mydata.txt ,  c:/mydata.sps , package =  SPSS )", 
            "title": "To SPSS"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#to-sas", 
            "text": "library(foreign)\n\nwrite.foreign(mydata,  c:/mydata.txt ,  c:/mydata.sas , package =  SAS )", 
            "title": "To SAS"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#to-stata", 
            "text": "library(foreign)\n\nwrite.dta(mydata,  c:/mydata.dta )", 
            "title": "To Stata"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#8-inspecting-data-missing-data", 
            "text": "", 
            "title": "8, Inspecting Data - Missing Data"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#inspecting", 
            "text": "ls(object)  names(object)  str(object)  levels(object$v1)  dim(object)  class(object)  print(object)  head(object, 10)  tail(object, 20)   Testing for Missing Values  y  - c(1, 2, 3, NA) # returns TRUE of x is missing\nis.na(y) # returns a vector (F F F T)   Recoding Values to Missing  # recode 99 to missing for variable v1\n# select rows where v1 is 99 and recode column v1\nmydata$v1[mydata$v1 == 99]  - NA   Excluding Missing Values from Analyses  x  - c(1, 2, NA, 3)\n\nmean(x) # returns NA\nmean(x, na.rm = TRUE) # returns 2   # list rows of data that have missing values\nmydata[!complete.cases(mydata),]  # create new dataset without missing data\nnewdata  - na.omit(mydata)", 
            "title": "Inspecting"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#the-dplyr-package", 
            "text": "library(dplyr)\n\ntbl_df(iris) # almost like head/tail\nglimpse(iris) # almost like str\nView(iris) # open a spreadsheet", 
            "title": "The dplyr package"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#for-thorough-cleaning", 
            "text": "The  Amelia II  software.  The  mitools  package.", 
            "title": "For thorough cleaning"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#9-labels-levels", 
            "text": "Basic  # variable v1 is coded 1, 2 or 3\n# we want to attach value labels 1=red, 2=blue, 3=green\nmydata$v1  - factor(mydata$v1, \n                    levels = c(1,2,3),\n                    labels = c( red ,  blue ,  green ))  # variable y is coded 1, 3 or 5\n# we want to attach value labels 1=Low, 3=Medium, 5=High\nmydata$v1  - ordered(mydata$y,\n                     levels = c(1,3, 5),\n                     labels = c( Low ,  Medium ,  High ))   Order  # Create a vector of temperature observations\ntemperature_vector  - c('High', 'Low', 'High', 'Low', 'Medium')\n\n# Specify that they are ordinal variables with the given levels\nfactor_temperature_vector  - factor(temperature_vector, order = TRUE, levels = c('Low', 'Medium', 'High'))  Add comments to an object  names(iris)[5]  -  This is the label for variable 5 \n\nnames(iris)[5] # the comment\niris[5] # the data  # labeling the variables\nlibrary(Hmisc)\n\nlabel(iris$Species)  -  Variable label for variable myvar \n\ndescribe(iris$Species) # commented\n#vs\ndescribe(iris$Sepal.Length) # not commented", 
            "title": "9, Labels &amp; Levels"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#how-to-work-with-quandl-in-r", 
            "text": "", 
            "title": "How to work with Quandl in R"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#1-importing-quandl-datasets", 
            "text": "Quandl  delivers financial, economic and \nalternative data to the world s top hedge funds, asset managers and \ninvestment banks in several formats:   Excel.  R.  Python.  API.  DB.   The packages used:   Quandl .  quantmod  for plotting.   Quandl - A first date  # Load in the Quandl package\nlibrary(Quandl)\n\n# Assign your first dataset to the variable:\nmydata  - Quandl('NSE/OIL')  Identifying a dataset with its ID  # Assign the Prague Stock Exchange to:\nPragueStockExchange  - Quandl('PRAGUESE/PX')  Plotting a stock chart  # The quantmod package\nlibrary(quantmod)\n\n# Load the Facebook data with the help of Quandl\nFacebook  - Quandl('GOOG/NASDAQ_FB', type = 'xts')\n\n# Plot the chart with the help of candleChart()\ncandleChart(Facebook)  Searching a Quandl dataset in R  # Look up the first 3 results for 'Bitcoin' within the Quandl database:\nresults  - Quandl.search(query = 'Bitcoin', silent = FALSE)\n\n# Print out the results\nstr(results)\n\n# Assign the data set with code BCHAIN/TOTBC\nBitCoin  - Quandl('BCHAIN/TOTBC')", 
            "title": "1, Importing Quandl Datasets"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#2-manipulating-quandl-datasets", 
            "text": "Manipulating data  # Assign to the variable Exchange\nExchange  - Quandl('BNP/USDEUR', start_date = '2013-01-01', end_date = '2013-12-01')  Transforming your Quandl dataset  # API transformation\n# The result:\nGDP_Change  - Quandl('FRED/CANRGDPR', transformation = 'rdiff')\nhead(GDP_Change)\nGDP_Chang  - Quandl('FRED/CANRGDPR')\nhead(GDP_Chang)  The magic of frequency collapsing  # The result:\neiaQuarterly  - Quandl('DOE/RWTC', collapse = 'quarterly')  Truncation and sort  # Assign to TruSo the first 5 observations of the crude oil prices\nTruSo  - Quandl('DOE/RWTC', sort = 'asc', rows = 5)\n\n# Print the result\nTruSo  A complex example  # Here you should place the return:\nFinal  - Quandl('DOE/RWTC', collapse = 'daily', transformation = 'rdiff', start_date = '2005-01-01', end_date = '2010-03-01', sort = 'asc')", 
            "title": "2, Manipulating Quandl Datasets"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#cleaning-data-in-r", 
            "text": "The packages used:   dplyr     tidyr  for data wrangling.  stringr  for regex.  lubridate  for time and date.", 
            "title": "Cleaning Data in R"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#1-introduction-and-exploring-raw-data", 
            "text": "Here s what messy data look like  # View the first 6 rows of data\nhead(weather)\n\n# View the last 6 rows of data\ntail(weather)\n\n# View a condensed summary of the data\nstr(weather)  Getting a feel for your data  # Check the class of bmi\nclass(bmi)\n\n# Check the dimensions of bmi\ndim(bmi)\n\n# View the column names of bmi\nnames(bmi)  Viewing the structure of your data  # Check the structure of bmi\nstr(bmi)\n\n# Load dplyr\nlibrary(dplyr)\n\n# Check the structure of bmi, the dplyr way\nglimpse(bmi)\n\n# View a summary of bmi\nsummary(bmi)  Looking at your data  # Print bmi to the console\nprint(bmi)\n\n# View the first 6 rows\nhead(bmi, 6)\n\n# View the first 15 rows\nhead(bmi, 15)\n\n# View the last 6 rows\ntail(bmi, 6)\n\n# View the last 10 rows\ntail(bmi, 10)  Visualizing your data  # Histogram of BMIs from 2008\nhist(bmi$Y2008)\n\n# Scatter plot comparing BMIs from 1980 to those from 2008\nplot(bmi$Y1980, bmi$Y2008)", 
            "title": "1, Introduction and Exploring Raw Data"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#2-tidying-data", 
            "text": "Gathering columns into key-value pairs  # Load tidyr\nlibrary(tidyr)\n\n# Apply gather() to bmi and save the result as bmi_long\nbmi_long  - gather(bmi, year, bmi_val, -Country)\n\n# View the first 20 rows of the result\nhead(bmi_long, 20)  Spreading key-value pairs into columns  # Apply spread() to bmi_long\nbmi_wide  - spread(bmi_long, year, bmi_val)\n\n# View the head of bmi_wide\nhead(bmi_wide)  Separating columns  # Apply separate() to bmi_cc\nbmi_cc_clean  - separate(bmi_cc, col = Country_ISO, into = c('Country', 'ISO'), sep = '/')\n\n# Print the head of the result\nhead(bmi_cc_clean)  Uniting columns  # Apply unite() to bmi_cc_clean\nbmi_cc  - unite(bmi_cc_clean, Country_ISO, Country, ISO, sep = '-')\n\n# View the head of the result\nhead(bmi_cc)  Column headers are values, not variable names  # View the head of census\nhead(census)\n\n# Gather the month columns\ncensus2  - gather(census, month, amount, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC)\n\n# Arrange rows by YEAR using dplyr's arrange\ncensus2  - arrange(census2, YEAR)\n\n# View first 20 rows of census2\nhead(census2, 20)  Variables are stored in both rows and columns  # View first 50 rows of census_long\nhead(census_long, 50)\n\n# Spread the type column\ncensus_long2  - spread(census_long, type, amount)\n\n# View first 20 rows of census_long2\nhead(census_long2, 20)  Multiple values are stored in one column  # View the head of census_long3\nhead(census_long3)\n\n# Separate the yr_month column into two\ncensus_long4  - separate(census_long3, yr_month, c('year', 'month'), '_')\n\n# View the first 6 rows of the result\nhead(census_long4, 6)", 
            "title": "2, Tidying Data"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#3-preparing-data-for-analysis", 
            "text": "Types of variables in R  # Make this evaluate to character\nclass('true')\n\n# Make this evaluate to numeric\nclass(8484.00)\n\n# Make this evaluate to integer\nclass(99L)\n\n# Make this evaluate to factor\nclass(factor('factor'))\n\n# Make this evaluate to logical\nclass(FALSE)  Common type conversions  # Preview students with str()\nstr(students)\n\n# Coerce Grades to character\nstudents$Grades  - as.character(students$Grades)\n\n# Coerce Medu to factor\nstudents$Medu  - as.factor(students$Medu)\n\n# Coerce Fedu to factor\nstudents$Fedu  - as.factor(students$Fedu)\n\n # Look at students once more with str()\nstr(students)  Working with dates  # Preview students2 with str()\nstr(students2)\n\n# Load the lubridate package\nlibrary(lubridate)\n\n# Parse as date\nymd('2015-Sep-17')\n\n# Parse as date and time (with no seconds!)\nymd_hm('2012-July-15, 12.56')\n\n# Coerce dob to a date (with no time)\nstudents2$dob  - ymd(students2$dob)\n\n# Coerce nurse_visit to a date and time\nstudents2$nurse_visit  - ymd_hms(students2$nurse_visit)\n\n# Look at students2 once more with str()\nstr(students2)  Trimming and padding strings  # Load the stringr package\nlibrary(stringr)\n\n# Trim all leading and trailing whitespace\nstr_trim(c('   Filip ', 'Nick  ', ' Jonathan'))\n\n# Pad these strings with leading zeros\nstr_pad(c('23485W', '8823453Q', '994Z'), width = 9, side = 'left', pad = '0')  Upper and lower case  # Print state abbreviations\nstates\n\n# Make states all uppercase and save result to states_upper\nstates_upper  - toupper(states)\nstates_upper\n\n# Make states_upper all lowercase again\ntolower(states_upper)  Finding and replacing strings  # stringr has been loaded for you\n# Look at the head of students2\nhead(students2)\n\n# Detect all dates of birth (dob) in 1997\nstr_detect(students2$dob, '1997')\n\n# In the sex column, replace 'F' with 'Female'...\nstudents2$sex  - str_replace(students2$sex, 'F', 'Female')\n\n# ...And 'M' with 'Male'\nstudents2$sex  - str_replace(students2$sex, 'M', 'Male')\n\n# View the head of students2\nhead(students2)  Finding missing values  # Call is.na() on the full social_df to spot all NAs\nis.na(social_df)\n\n# Use the any() function to ask whether there are any NAs in the data\nany(is.na(social_df))\nsum(is.na(social_df))\n\n# View a summary() of the dataset\nsummary(social_df)\n\n# Call table() on the status column\ntable(social_df$status)  Dealing with missing values  # Use str_replace() to replace all missing strings in status with NA\nsocial_df$status  - str_replace(social_df$status, '^$', NA)\n\n# Print social_df to the console\nsocial_df\n\n# Use complete.cases() to see which rows have no missing values\ncomplete.cases(social_df)\n\n# Use na.omit() to remove all rows with any missing values\nna.omit(social_df)  Dealing with outliers and obvious errors  # Look at a summary() of students3\nsummary(students3)\n\n# View a histogram of the age variable\nhist(students3$age, breaks = 20)\n\n# View a histogram of the absences variable\nhist(students3$absences, breaks = 20)\n\n# View a histogram of absences, but force zeros to be bucketed to the right of zero\nhist(students3$absences, breaks = 20, right = FALSE)  Another look at strange values  # View a boxplot of age\nboxplot(students3$age)\n\n# View a boxplot of absences\nboxplot(students3$absences)", 
            "title": "3, Preparing Data for Analysis"
        }, 
        {
            "location": "/IO_snippets___Cleaning/#4-putting-it-all-together", 
            "text": "Get a feel for the data  # Verify that weather is a data.frame\nclass(weather)\n\n# Check the dimensions\ndim(weather)\n\n# View the column names\nnames(weather)  Summarize the data  # View the structure of the data\nstr(weather)\n\n# Load dplyr package\nlibrary(dplyr)\n\n# Look at the structure using dplyr's glimpse()\nglimpse(weather)\n\n# View a summary of the data\nsummary(weather)  Take a closer look  # View first 6 rows\nhead(weather, 6)\n\n# View first 15 rows\nhead(weather, 15)\n\n# View the last 6 rows\ntail(weather, 6)\n\n# View the last 10 rows\ntail(weather, 10)  Column names are values  # Load the tidyr package\nlibrary(tidyr)\n\n# Gather the columns\nweather2  - gather(weather, day, value, X1:X31, na.rm = TRUE)\n\n# View the head\nhead(weather2)  Values are variable names  # First remove column of row names\nweather2  - weather2[, -1]\n\n# Spread the data\nweather3  - spread(weather2, measure, value)\n\n# View the head\nhead(weather3)  Clean up dates  # Load the stringr and lubridate packages\nlibrary(stringr)\nlibrary(lubridate)\n\n# Remove X's from day column\n# weather3$day  - str_pad(str_replace(weather3$day, 'X', ''), width = 2, side = 'left', pad = '0')\nweather3$day  - str_replace(weather3$day, 'X', '')\n\n# Unite the year, month, and day columns\nweather4  - unite(weather3, date, year, month, day, sep = '-')\n\n# Convert date column to proper date format using stringr's ymd()\nweather4$date  - ymd(weather4$date)\n\n# Rearrange columns using dplyr's select()\nweather5  - select(weather4, date, Events, CloudCover:WindDirDegrees)\n\n# View the head\nhead(weather5)  A closer look at column types  # View the structure of weather5\nstr(weather5)\n\n# Examine the first 20 rows of weather5. Are most of the characters numeric?\nhead(weather5, 20)\n\n# See what happens if we try to convert PrecipitationIn to numeric\nas.numeric(weather5$PrecipitationIn)  Column type conversions  # The dplyr package is already loaded\n# Replace T with 0 (T = trace)\nweather5$PrecipitationIn  - str_replace(weather5$PrecipitationIn, 'T', '0')\n\n# Convert characters to numerics\nweather6  - mutate_each(weather5, funs(as.numeric), CloudCover:WindDirDegrees)\n\n# Look at result\nstr(weather6)  Find missing values  # Count missing values\nsum(is.na(weather6))\n\n# Find missing values\nsummary(weather6)\n\n# Find indices of NAs in Max.Gust.SpeedMPH\nind  - which(is.na(weather6$Max.Gust.SpeedMPH))\nind\n\n# Look at the full rows for records missing Max.Gust.SpeedMPH\nweather6[ind, ]  An obvious error  # Review distibutions for all variables\nsummary(weather6)\n\n# Find row with Max.Humidity of 1000\nind  - which(weather6$Max.Humidity == 1000)\n\n# Look at the data for that day\nweather6[ind, ]\n\n# Change 1000 to 100\nweather6$Max.Humidity[ind]  - 100  Another obvious error  # Look at summary of Mean.VisibilityMiles\nsummary(weather6$Mean.VisibilityMiles)\n\n# Get index of row with -1 value\nind  - which(weather6$Mean.VisibilityMiles == -1)\n\n# Look at full row\nweather6[ind, ]\n\n# Set Mean.VisibilityMiles to the appropriate value\nweather6$Mean.VisibilityMiles[ind]  - 10  Check other extreme values  # Review summary of full data once more\nsummary(weather6)\n\n# Look at histogram for MeanDew.PointF\nhist(weather6$MeanDew.PointF)\n\n# Look at histogram for Min.TemperatureF\nhist(weather6$Min.TemperatureF)\n\n# Compare to histogram for Mean.TemperatureF\nhist(weather6$Mean.TemperatureF)  Finishing touches  # Clean up column names\nnames(weather6)  - new_colnames\n\n# Replace empty cells in Events column\nweather6$events[weather6$events == '']  - 'None'\n\n# Print the first 6 rows of weather6\nhead(weather6, 6)", 
            "title": "4, Putting it All Together"
        }, 
        {
            "location": "/Reading_Data_into_R_with_readr/", 
            "text": "1, Importing data with \nreadr\n\n\n2, Parsing Data with \nreadr\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, Importing data with \nreadr\n\n\nReading a .csv file\n\n\nYour first task will be to master the use of the \nread_csv()\n function.\n\nThere are many arguments available, but the only required argument is\n\n\nfile\n, a path to a CSV file on your computer (or the web).\n\n\nOne big advantage that \nread_csv()\n has over \nread.csv()\n is that it\n\ndoesn\nt convert strings into factors by default.\n\n\nread_csv()\n recognizes 8 different data types (integer, logical, etc.)\n\nand leaves anything else as characters. That means you don\nt have to set\n\n\nstringsAsFactors = FALSE\n every time you import a CSV file with\n\ncharacter strings!\n\n\n#install.packages('readr')\nlibrary(readr)\n\ngetwd()\n\n\n\n\n## [1] \"D:/Admin_Hugues/Documents/Rprojects/Data Wrangling\"\n\n\n\nsetwd(\nD:/Admin_Hugues/Documents/Rprojects/Data Wrangling\n)\n\n\n\n\nImport .csv (only \n,\n).\n\n\n# Import chickwts.csv: cwts\ncwts \n- read_csv('chickwts.csv')\n\n# View the head of cwts\nhead(cwts)\n\n\n\n\n## # A tibble: 6 \u00d7 2\n##   weight      feed\n##    \nint\n     \nchr\n\n## 1    179 horsebean\n## 2    160 horsebean\n## 3    136 horsebean\n## 4    227 horsebean\n## 5    217 horsebean\n## 6    168 horsebean\n\n\n\nReading a (.txt) .tsv file\n\n\nSkipping columns with \ncol_skip()\n.\n\n\nCode only:\n\n\n\n\nSetting the column type.\n\n\n\n\n\n\n\ncols(\n  weight = col_integer(),\n  feed = col_character()\n)\n\n\n\n\n\n\nSetting the column names.\n\n\n\n\n\n\n\ncol_names = c('name', 'state', 'phone')\n\n\n\n\n\n\nRemoving NA.\n\n\n\n\n\n\n\nna = c('NA', 'null')\n\n\n\n\nIn practice.\n\n\n# Import data\nsalaries \n- read_tsv('Salaries.txt', col_names = FALSE, col_types = cols(\n  X2 = col_skip(),\n  X3 = col_skip(), \n  X4 = col_skip()\n))\n\n# View first six rows of salaries\nhead(salaries)\n\n\n\n\n## # A tibble: 6 \u00d7 3\n##          X1    X5     X6\n##       \nchr\n \nchr\n  \nint\n\n## 1      Prof  Male 139750\n## 2      Prof  Male 173200\n## 3  AsstProf  Male  79750\n## 4      Prof  Male 115000\n## 5      Prof  Male 141500\n## 6 AssocProf  Male  97000\n\n\n\nReading a European .csv\n\n\nIn most of Europe, commas (rather than periods) are used as decimal\n\npoints.\n\n\n# Import data with read_csv2(): trees\ntrees \n- read_csv2('trees.csv')\n\n# View dimensions and head of trees\ndim(trees)\n\n\n\n\n## [1] 9 3\n\n\n\nhead(trees)\n\n\n\n\n## # A tibble: 6 \u00d7 3\n##   Girth Height Volume\n##   \ndbl\n  \nint\n  \ndbl\n\n## 1    83     70    103\n## 2    86     65    103\n## 3    88     63    102\n## 4   105     72    164\n## 5   107     81    188\n## 6   108     83    197\n\n\n\nRead a fixed-width file\n\n\nFiles containing columns of data that are separated by whitespace and\n\nall line up on one side.\n\n\nCode only:\n\n\n# Import names.txt: names\nnames \n- read_table('names.txt', col_names = c('name', 'state', 'phone'), na = c('NA', 'null'))\n\n\n\n\nReading a text file\n\n\nImport ordinary text files.\n\n\n# vector of character strings. \n# Import as a character vector, one item per line: tweets\ntweets \n- read_lines('tweets.txt')\ntweets\n\n\n\n\n## [1] \"carrots  can be eat by most people\"                                                                                          \n## [2] \"On predisents day we honor the big US man himself: Aberham Liclon.   Tall, skinny, dry, and cruncy - he was america's carrot\"\n## [3] \"knock knoc who is there? yup: carosot   ( joke )\"                                                                            \n## [4] \"it is 2016 time for a carot emoji   please!\"                                                                                 \n## [5] \"when life give you lemnos ,  have a carrot\"                                                                                  \n## [6] \"If you squent your eyes real hard a football  look like  a dry brown carrot   Honestly\"\n\n\n\n# returns a length 1 vector of the entire file, with line breaks represented as \\n\n# Import as a length 1 vector: tweets_all\ntweets_all \n- read_file('tweets.txt')\ntweets_all\n\n\n\n\n## [1] \"carrots  can be eat by most people\\r\\nOn predisents day we honor the big US man himself: Aberham Liclon.   Tall, skinny, dry, and cruncy - he was america's carrot\\r\\nknock knoc who is there? yup: carosot   ( joke )\\r\\nit is 2016 time for a carot emoji   please!\\r\\nwhen life give you lemnos ,  have a carrot\\r\\nIf you squent your eyes real hard a football  look like  a dry brown carrot   Honestly\"\n\n\n\nWriting .csv and .tsv files\n\n\nCode only:\n\n\n# Save cwts as chickwts.csv\nwrite_csv(cwts, \nchickwts.csv\n)\n\n# Append cwts2 to chickwts.csv\nwrite_csv(cwts2, \nchickwts.csv\n, append = TRUE)\n\n\n\n\nWriting .rds files\n\n\nIf the R object you\nre working with has metadata associated with it,\n\nsaving to a CSV will cause that information to be lost.\n\n\nExports an entire R object (metadata and all).\n\n\nCode only:\n\n\n# Save trees as trees.rds\nwrite_rds(trees, 'trees.rds')\n\n# Import trees.rds: trees2\ntrees2 \n- read_rds('trees.rds')\n\n# Check whether trees and trees2 are the same\nidentical(trees, trees2)\n\n\n\n\n2, Parsing Data with \nreadr\n\n\nCoercing columns to different data types\n\n\nreadr\n functions are quite good at guessing the correct data type for\n\neach column in a dataset. Of course, they aren\nt perfect, so sometimes\n\nyou will need to change the type of a column after importing.\n\n\nCode only:\n\n\n# Convert all columns to double\ntrees2 \n- type_convert(trees, col_types = cols(Girth = 'd', Height = 'd', Volume = 'd'))\n\n\n\n\nCoercing character columns into factors\n\n\nreadr\n import functions is that they don\nt automatically convert\n\nstrings into factors like \nread.csv\n does.\n\n\nCode only:\n\n\n# Parse the title column\nsalaries$title \n- parse_factor(salaries$title, levels = c('Prof', 'AsstProf', 'AssocProf'))\n\n# Parse the gender column\nsalaries$gender \n- parse_factor(salaries$gender, levels = c('Male', 'Female'))\n\n\n\n\nCreating Date objects\n\n\nThe \nreadr\n import functions can automatically recognize dates in\n\nstandard ISO 8601 format (YYYY-MM-DD) and parse columns accordingly. If\n\nyou want to import a dataset with dates in other formats, you can use\n\n\nparse_date\n.\n\n\nCode only:\n\n\n# Change type of date column\nweather$date \n- parse_date(weather$date, format = '%m/%d/%Y')\n\n\n\n\nParsing number formats\n\n\nThe \nreadr\n importing functions can sometimes run into trouble parsing a\n\ncolumn as numbers when it contains non-numeric symbols in addition to\n\nnumerals.\n\n\nCode only:\n\n\n# Parse amount column as a number\ndebt$amount \n- parse_number(debt$amount)\n\n\n\n\nViewing metadata before importing\n\n\nIn some cases, it may be easier to get an idea of how \nreadr\n plans to\n\nparse a dataset before you actually import it. When you see the planned\n\ncolumn specification, you might decide to change the type of one or more\n\ncolumns, for example.\n\n\n\n\nspec_csv\n for .csv and .tsv files.\n\n\nspec_delim\n for .txt files (among others).\n\n\n\n\n\n\n\n# Specifications of chickwts\nspec_csv('chickwts.csv')\n\n\n\n\n## cols(\n##   weight = col_integer(),\n##   feed = col_character()\n## )", 
            "title": "Reading Data into R with readr"
        }, 
        {
            "location": "/Reading_Data_into_R_with_readr/#2-parsing-data-with-readr", 
            "text": "Coercing columns to different data types  readr  functions are quite good at guessing the correct data type for \neach column in a dataset. Of course, they aren t perfect, so sometimes \nyou will need to change the type of a column after importing.  Code only:  # Convert all columns to double\ntrees2  - type_convert(trees, col_types = cols(Girth = 'd', Height = 'd', Volume = 'd'))  Coercing character columns into factors  readr  import functions is that they don t automatically convert \nstrings into factors like  read.csv  does.  Code only:  # Parse the title column\nsalaries$title  - parse_factor(salaries$title, levels = c('Prof', 'AsstProf', 'AssocProf'))\n\n# Parse the gender column\nsalaries$gender  - parse_factor(salaries$gender, levels = c('Male', 'Female'))  Creating Date objects  The  readr  import functions can automatically recognize dates in \nstandard ISO 8601 format (YYYY-MM-DD) and parse columns accordingly. If \nyou want to import a dataset with dates in other formats, you can use  parse_date .  Code only:  # Change type of date column\nweather$date  - parse_date(weather$date, format = '%m/%d/%Y')  Parsing number formats  The  readr  importing functions can sometimes run into trouble parsing a \ncolumn as numbers when it contains non-numeric symbols in addition to \nnumerals.  Code only:  # Parse amount column as a number\ndebt$amount  - parse_number(debt$amount)  Viewing metadata before importing  In some cases, it may be easier to get an idea of how  readr  plans to \nparse a dataset before you actually import it. When you see the planned \ncolumn specification, you might decide to change the type of one or more \ncolumns, for example.   spec_csv  for .csv and .tsv files.  spec_delim  for .txt files (among others).    # Specifications of chickwts\nspec_csv('chickwts.csv')  ## cols(\n##   weight = col_integer(),\n##   feed = col_character()\n## )", 
            "title": "2, Parsing Data with readr"
        }, 
        {
            "location": "/Data_Wrangling/", 
            "text": "Documentation\n\n\nData Analysis in R, the \ndata.table\n Way\n\n\n1, \ndata.table\n novice\n\n\n2, \ndata.table\n yeoman\n\n\n3, \ndata.table\n expert\n\n\n\n\n\n\nData Manipulation in R with \ndplyr\n\n\n1, Introduction to \ndplyr\n\n\n2, \nselect\n and \nmutate\n\n\n3, \nfilter\n and \narrange\n\n\n4, \nsummarise\n and the Pipe Operator\n\n\n5, \ngroup_by\n and working with data\n\n\n\n\n\n\nAdding \ntidyr\n Functions\n\n\nExtension: \nJoining Data in R with \ndplyr\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDocumentation\n\n\ndata.table\n\n\n\n\nextension of \ndata.frame\n.\n\n\nFast aggregation of large data (e.g. 100GB in RAM), fast ordered\n\n    joins, fast add/modify/delete of columns by group using no copies at\n\n    all, list columns, a fast friendly file reader and parallel\n\n    file writer. Offers a natural and flexible syntax, for\n\n    faster development.\n\n\n\n\ndplyr\n\n\n\n\nA fast, consistent tool for working with data frame like objects,\n\n    both in memory and out of memory.\n\n\nPipelines.\n\n\n\n\ntidyr\n\n\n\n\nAn evolution of \nreshape2\n. It\ns designed specifically for data\n\n    tidying (not general reshaping or aggregating) and works well with\n\n    dplyr data pipelines.\n\n\n\n\n\n\n\n\n\n\npackage\n\n\n'narrower'\n\n\n'wider'\n\n\n\n\n\n\n\n\n\n\ntidyr\n\n\ngather\n\n\nspread\n\n\n\n\n\n\nreshape2\n\n\nmelt\n\n\ncast\n\n\n\n\n\n\nspreadsheets\n\n\nunpivot\n\n\npivot\n\n\n\n\n\n\ndatabases\n\n\nfold\n\n\nunfold\n\n\n\n\n\n\n\n\n\nData Analysis in R, the \ndata.table\n Way\n\n\n1, \ndata.table\n novice\n\n\nFind out more with \n?data.table\n.\n\n\nCreate and subset a \ndata.table\n\n\n# The data.table package\nlibrary(data.table)\n\n# Create my_first_data_table\nmy_first_data_table \n- data.table(x = c('a', 'b', 'c', 'd', 'e'), y = c(1, 2, 3, 4, 5))\n\n# Create a data.table using recycling\nDT \n- data.table(a = 1:2, b = c('A', 'B', 'C', 'D'))\n\n# Print the third row to the console\nDT[3,]\n\n\n\n\n##    a b\n## 1: 1 C\n\n\n\n# Print the second and third row to the console, but do not commas\nDT[2:3]\n\n\n\n\n##    a b\n## 1: 2 B\n## 2: 1 C\n\n\n\nGetting to know a \ndata.table\n\n\nLike \nhead\n, \ntail\n.\n\n\n# Print the penultimate row of DT using .N\nDT[.N - 1]\n\n\n\n\n##    a b\n## 1: 1 C\n\n\n\n# Print the column names of DT, and number of rows and number of columns\ncolnames(DT)\n\n\n\n\n## [1] \"a\" \"b\"\n\n\n\ndim(DT)\n\n\n\n\n## [1] 4 2\n\n\n\n# Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1.\nDT[c(2, 2, 3)]\n\n\n\n\n##    a b\n## 1: 2 B\n## 2: 2 B\n## 3: 1 C\n\n\n\nDT\n is a data.table/data.frame, but \nDT[ , B]\n is a vector;\n\n\nDT[ , .(B)]\n is a subsetted data.table.\n\n\nSubsetting data tables\n\n\nDT[i, j, by]\n means take \nDT\n, subset rows using \ni\n, then calculate\n\n\nj\n grouped by \nby\n. You can wrap \nj\n with \n.()\n.\n\n\nA \n- c(1, 2, 3, 4, 5)\nB \n- c('a', 'b', 'c', 'd', 'e')\nC \n- c(6, 7, 8, 9, 10)\nDT \n- data.table(A, B, C)\n\n# Subset rows 1 and 3, and columns B and C\nDT[c(1,3) ,.(B, C)]\n\n\n\n\n##    B C\n## 1: a 6\n## 2: c 8\n\n\n\n# Assign to ans the correct value\nans \n- data.table(DT[, .(B, val = A * C)])\n\n# Fill in the blanks such that ans2 equals target\n#target \n- data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'),  val = as.integer(c(6:10, 1:5)))\nans2 \n- data.table(DT[, .(B, val = as.integer(c(6:10, 1:5)))])\n\n\n\n\nThe \nby\n basics\n\n\n# iris and iris3 are already available in the workspace\n\n# Convert iris to a data.table: DT\nDT \n- as.data.table(iris)\n\n# For each Species, print the mean Sepal.Length\nDT[, .(mean(Sepal.Length)), by = .(Species)]\n\n\n\n\n##       Species    V1\n## 1:     setosa 5.006\n## 2: versicolor 5.936\n## 3:  virginica 6.588\n\n\n\n# Print mean Sepal.Length, grouping by first letter of Species\nDT[, .(mean(Sepal.Length)), by = .(substr(Species, 1,1))]\n\n\n\n\n##    substr    V1\n## 1:      s 5.006\n## 2:      v 6.262\n\n\n\nUsing \n.N\n and \nby\n\n\n.N\n, number, in row or column.\n\n\n# data.table version of iris: DT\nDT \n- as.data.table(iris)\n\n# Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group.\nDT[, .N, by = 10 * round(Sepal.Length * Sepal.Width / 10)]\n\n\n\n\n##    round   N\n## 1:    20 117\n## 2:    10  29\n## 3:    30   4\n\n\n\n# Now name the output columns `Area` and `Count`\nDT[, .(Count = .N), by = .(Area = 10 * round(Sepal.Length * Sepal.Width / 10))]\n\n\n\n\n##    Area Count\n## 1:   20   117\n## 2:   10    29\n## 3:   30     4\n\n\n\nReturn multiple numbers in \nj\n\n\n# Create the data.table DT\nset.seed(1L)\nDT \n- data.table(A = rep(letters[2:1], each = 4L), B = rep(1:4, each = 2L), C = sample(8))\n\n# Create the new data.table, DT2\nDT2 \n- DT[, .(C = cumsum(C)), by=.(A,B)]\n\n# Select from DT2 the last two values from C while you group by A\nDT2[, .(C = tail(C,2)), by=A]\n\n\n\n\n##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8\n\n\n\n2, \ndata.table\n yeoman\n\n\nChaining, the basics\n\n\n# Build DT\nset.seed(1L)\nDT \n- data.table(A = rep(letters[2:1], each = 4L), B = rep(1:4, each = 2L), C = sample(8)) \nDT\n\n\n\n\n##    A B C\n## 1: b 1 3\n## 2: b 1 8\n## 3: b 2 4\n## 4: b 2 5\n## 5: a 3 1\n## 6: a 3 7\n## 7: a 4 2\n## 8: a 4 6\n\n\n\n# Use chaining\n# Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A\nDT[, .(C = cumsum(C)), by = .(A,B)][, .(C = tail(C,2)), by = .(A)]\n\n\n\n\n##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8\n\n\n\nChaining your \niris\n dataset\n\n\nDT \n- data.table(iris)\n\n# Perform chained operations on DT\nDT[, .(Sepal.Length = median(Sepal.Length), Sepal.Width = median(Sepal.Width), Petal.Length = median(Petal.Length), Petal.Width = median(Petal.Width)), by = .(Species)][order(Species, decreasing = TRUE)]\n\n\n\n\n##       Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1:  virginica          6.5         3.0         5.55         2.0\n## 2: versicolor          5.9         2.8         4.35         1.3\n## 3:     setosa          5.0         3.4         1.50         0.2\n\n\n\nDT\n\n\n\n\n##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica\n\n\n\nProgramming time vs readability\n\n\nx \n- c(2, 1, 2, 1, 2, 2, 1)\ny \n- c(1, 3, 5, 7, 9, 11, 13)\nz \n- c(2, 4, 6, 8, 10, 12, 14)\nDT \n- data.table(x, y, z)\n\n# Mean of columns\nDT[, lapply(.SD, mean), by = .(x)] \n\n\n\n\n##    x        y        z\n## 1: 2 6.500000 7.500000\n## 2: 1 7.666667 8.666667\n\n\n\n# Median of columns\nDT[, lapply(.SD, median), by = .(x)]\n\n\n\n\n##    x y z\n## 1: 2 7 8\n## 2: 1 7 8\n\n\n\nIntroducing \n.SDcols\n\n\n.SDcols\n specifies the columns of \nDT\n that are included in \n.SD\n.\n\n\ngrp \n- c(6, 6, 8, 8, 8)\nQ1 \n- c(4, 3, 3, 5, 3)\nQ2 \n- c(1, 4, 1, 4, 4)\nQ3 \n- c(3, 1, 5, 5, 2)\nH1 \n- c(1, 2, 3, 2, 4)\nH2 \n- c(1, 4, 3, 4, 3)\nDT \n- data.table(grp, Q1, Q2, Q3, H1, H2)\n\n# Calculate the sum of the Q columns\nDT[, lapply(.SD, sum), .SDcols = 2:4]\n\n\n\n\n##    Q1 Q2 Q3\n## 1: 18 14 16\n\n\n\n# Calculate the sum of columns H1 and H2 \nDT[, lapply(.SD, sum), .SDcols = 5:6]\n\n\n\n\n##    H1 H2\n## 1: 12 15\n\n\n\n# Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns. \nDT[, .SD[-1], .SDcols = 2:4, by = .(grp)]\n\n\n\n\n##    grp Q1 Q2 Q3\n## 1:   6  3  4  1\n## 2:   8  5  4  5\n## 3:   8  3  4  2\n\n\n\nMixing it together: \nlapply\n, \n.SD\n, \nSDcols\n and \n.N\n\n\nx \n- c(2, 1, 2, 1, 2, 2, 1)\ny \n- c(1, 3, 5, 7, 9, 11, 13)\nz \n- c(2, 4, 6, 8, 10, 12, 14)\nDT \n- data.table(x, y, z)\n\n# Sum of all columns and the number of rows\n# For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together.\nDT\n\n\n\n\n##    x  y  z\n## 1: 2  1  2\n## 2: 1  3  4\n## 3: 2  5  6\n## 4: 1  7  8\n## 5: 2  9 10\n## 6: 2 11 12\n## 7: 1 13 14\n\n\n\nDT[,c(lapply(.SD, sum), .N), .SDcols = 1:3, by = x]\n\n\n\n\n##    x x  y  z N\n## 1: 2 8 26 30 4\n## 2: 1 3 23 26 3\n\n\n\n# Cumulative sum of column x and y while grouping by x and z \n 8\nDT[,lapply(.SD, cumsum), .SDcols = 1:2, by = .(by1 = x, by2 = z \n 8)]\n\n\n\n\n##    by1   by2 x  y\n## 1:   2 FALSE 2  1\n## 2:   2 FALSE 4  6\n## 3:   1 FALSE 1  3\n## 4:   1 FALSE 2 10\n## 5:   2  TRUE 2  9\n## 6:   2  TRUE 4 20\n## 7:   1  TRUE 1 13\n\n\n\n# Chaining\nDT[,lapply(.SD, cumsum), .SDcols = 1:2, by = .(by1 = x, by2 = z \n 8)][,lapply(.SD, max), .SDcols = 3:4, by = by1]\n\n\n\n\n##    by1 x  y\n## 1:   2 4 20\n## 2:   1 2 13\n\n\n\nAdding, updating and removing columns\n\n\n:=\n is defined for use in \nj\n only.\n\n\n# The data.table DT\nDT \n- data.table(A = letters[c(1, 1, 1, 2, 2)], B = 1:5)\nDT\n\n\n\n\n##    A B\n## 1: a 1\n## 2: a 2\n## 3: a 3\n## 4: b 4\n## 5: b 5\n\n\n\n# Add column by reference: Total\nDT[, ('Total') := sum(B), by = .(A)]\n\n\n\n\n##    A B Total\n## 1: a 1     6\n## 2: a 2     6\n## 3: a 3     6\n## 4: b 4     9\n## 5: b 5     9\n\n\n\n# Add 1 to column B\nDT[c(2,4), ('B') := as.integer(1 + B)]\n\n\n\n\n##    A B Total\n## 1: a 1     6\n## 2: a 3     6\n## 3: a 3     6\n## 4: b 5     9\n## 5: b 5     9\n\n\n\n# Add a new column Total2\nDT[2:4, ':='(Total2 = sum(B)), by = .(A)]\n\n\n\n\n##    A B Total Total2\n## 1: a 1     6     NA\n## 2: a 3     6      6\n## 3: a 3     6      6\n## 4: b 5     9      5\n## 5: b 5     9     NA\n\n\n\n# Remove the Total column\nDT[, Total := NULL]\n\n\n\n\n##    A B Total2\n## 1: a 1     NA\n## 2: a 3      6\n## 3: a 3      6\n## 4: b 5      5\n## 5: b 5     NA\n\n\n\n# Select the third column using `[[`\nDT[[3]]\n\n\n\n\n## [1] NA  6  6  5 NA\n\n\n\nThe functional form\n\n\n# A data.table DT\nDT \n- data.table(A = c(1, 1, 1, 2, 2), B = 1:5)\n\n# Update B, add C and D\nDT[, `:=`(B = B + 1,  C = A + B, D = 2)]\n\n\n\n\n##    A B C D\n## 1: 1 2 2 2\n## 2: 1 3 3 2\n## 3: 1 4 4 2\n## 4: 2 5 6 2\n## 5: 2 6 7 2\n\n\n\n# Delete my_cols\nmy_cols \n- c('B', 'C')\nDT[, (my_cols) := NULL]\n\n\n\n\n##    A D\n## 1: 1 2\n## 2: 1 2\n## 3: 1 2\n## 4: 2 2\n## 5: 2 2\n\n\n\n# Delete column 2 by number\nDT[, 2 := NULL]\n\n\n\n\n##    A\n## 1: 1\n## 2: 1\n## 3: 1\n## 4: 2\n## 5: 2\n\n\n\nReady, \nset\n, go!\n\n\nThe \nset\n function is used to repeatedly update a data.table by\n\nreference. You can think of the \nset\n function as a loopable.\n\n\nA \n- c(2, 2, 3, 5, 2, 5, 5, 4, 4, 1)\nB \n- c(2, 1, 4, 2, 4, 3, 4, 5, 2, 4)\nC \n- c(5, 2, 4, 1, 2, 2, 1, 2, 5, 2)\nD \n- c(3, 3, 3, 1, 5, 4, 4, 1, 4, 3)\nDT \n- data.table(A, B, C, D)\n\n# Set the seed\nset.seed(1)\n\n# Check the DT\nDT\n\n\n\n\n##     A B C D\n##  1: 2 2 5 3\n##  2: 2 1 2 3\n##  3: 3 4 4 3\n##  4: 5 2 1 1\n##  5: 2 4 2 5\n##  6: 5 3 2 4\n##  7: 5 4 1 4\n##  8: 4 5 2 1\n##  9: 4 2 5 4\n## 10: 1 4 2 3\n\n\n\n# For loop with set\nfor (l in 2:4) set(DT, sample(10,3), l, NA)\n\n# Change the column names to lowercase\nsetnames(DT,c('A','B','C','D'), c('a','b','c','d'))\n\n# Print the resulting DT to the console\nDT\n\n\n\n\n##     a  b  c  d\n##  1: 2  2  5  3\n##  2: 2  1 NA  3\n##  3: 3 NA  4  3\n##  4: 5 NA  1  1\n##  5: 2 NA  2  5\n##  6: 5  3  2 NA\n##  7: 5  4  1  4\n##  8: 4  5 NA  1\n##  9: 4  2  5 NA\n## 10: 1  4 NA NA\n\n\n\nThe \nset\n family\n\n\n# Define DT\nDT \n- data.table(a = letters[c(1, 1, 1, 2, 2)], b = 1)\nDT\n\n\n\n\n##    a b\n## 1: a 1\n## 2: a 1\n## 3: a 1\n## 4: b 1\n## 5: b 1\n\n\n\n# Add a postfix '_2' to all column names\nsetnames(DT, c(1:2), paste0(c('a','b'), '_2'))\nDT\n\n\n\n\n##    a_2 b_2\n## 1:   a   1\n## 2:   a   1\n## 3:   a   1\n## 4:   b   1\n## 5:   b   1\n\n\n\n# Change column name 'a_2' to 'A2'\nsetnames(DT, 'a_2', 'A2')\nDT\n\n\n\n\n##    A2 b_2\n## 1:  a   1\n## 2:  a   1\n## 3:  a   1\n## 4:  b   1\n## 5:  b   1\n\n\n\n# Reverse the order of the columns\nsetcolorder(DT, c('b_2','A2'))\n\n\n\n\n3, \ndata.table\n expert\n\n\nSelecting rows the \ndata.table\n way\n\n\n# Convert iris to a data.table\niris \n- data.table('Sepal.Length' = iris$Sepal.Length, 'Sepal.Width' = iris$Sepal.Width, 'Petal.Length' = iris$Petal.Length, 'Petal.Width' = iris$Petal.Width, 'Species' = iris$Species)\niris\n\n\n\n\n##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica\n\n\n\n# Species is 'virginica'\nhead(iris[Species == 'virginica'], 20)\n\n\n\n\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##  1:          6.3         3.3          6.0         2.5 virginica\n##  2:          5.8         2.7          5.1         1.9 virginica\n##  3:          7.1         3.0          5.9         2.1 virginica\n##  4:          6.3         2.9          5.6         1.8 virginica\n##  5:          6.5         3.0          5.8         2.2 virginica\n##  6:          7.6         3.0          6.6         2.1 virginica\n##  7:          4.9         2.5          4.5         1.7 virginica\n##  8:          7.3         2.9          6.3         1.8 virginica\n##  9:          6.7         2.5          5.8         1.8 virginica\n## 10:          7.2         3.6          6.1         2.5 virginica\n## 11:          6.5         3.2          5.1         2.0 virginica\n## 12:          6.4         2.7          5.3         1.9 virginica\n## 13:          6.8         3.0          5.5         2.1 virginica\n## 14:          5.7         2.5          5.0         2.0 virginica\n## 15:          5.8         2.8          5.1         2.4 virginica\n## 16:          6.4         3.2          5.3         2.3 virginica\n## 17:          6.5         3.0          5.5         1.8 virginica\n## 18:          7.7         3.8          6.7         2.2 virginica\n## 19:          7.7         2.6          6.9         2.3 virginica\n## 20:          6.0         2.2          5.0         1.5 virginica\n\n\n\n# Species is either 'virginica' or 'versicolor'\nhead(iris[Species %in% c('virginica', 'versicolor')], 20)\n\n\n\n\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n##  1:          7.0         3.2          4.7         1.4 versicolor\n##  2:          6.4         3.2          4.5         1.5 versicolor\n##  3:          6.9         3.1          4.9         1.5 versicolor\n##  4:          5.5         2.3          4.0         1.3 versicolor\n##  5:          6.5         2.8          4.6         1.5 versicolor\n##  6:          5.7         2.8          4.5         1.3 versicolor\n##  7:          6.3         3.3          4.7         1.6 versicolor\n##  8:          4.9         2.4          3.3         1.0 versicolor\n##  9:          6.6         2.9          4.6         1.3 versicolor\n## 10:          5.2         2.7          3.9         1.4 versicolor\n## 11:          5.0         2.0          3.5         1.0 versicolor\n## 12:          5.9         3.0          4.2         1.5 versicolor\n## 13:          6.0         2.2          4.0         1.0 versicolor\n## 14:          6.1         2.9          4.7         1.4 versicolor\n## 15:          5.6         2.9          3.6         1.3 versicolor\n## 16:          6.7         3.1          4.4         1.4 versicolor\n## 17:          5.6         3.0          4.5         1.5 versicolor\n## 18:          5.8         2.7          4.1         1.0 versicolor\n## 19:          6.2         2.2          4.5         1.5 versicolor\n## 20:          5.6         2.5          3.9         1.1 versicolor\n\n\n\nRemoving columns and adapting your column names\n\n\nRefer to a regex cheat sheet for metacharacter.\n\n\n# iris as a data.table\niris \n- as.data.table(iris)\niris\n\n\n\n\n##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica\n\n\n\n# Remove the 'Sepal.' prefix\n#gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x\nsetnames(iris, c('Sepal.Length', 'Sepal.Width'), c('Length','Width')) \n#gsub('^Sepal\\\\.','', iris)\n\n# Remove the two columns starting with 'Petal'\niris[, c('Petal.Length', 'Petal.Width') := NULL]\n\n\n\n\n##      Length Width   Species\n##   1:    5.1   3.5    setosa\n##   2:    4.9   3.0    setosa\n##   3:    4.7   3.2    setosa\n##   4:    4.6   3.1    setosa\n##   5:    5.0   3.6    setosa\n##  ---                       \n## 146:    6.7   3.0 virginica\n## 147:    6.3   2.5 virginica\n## 148:    6.5   3.0 virginica\n## 149:    6.2   3.4 virginica\n## 150:    5.9   3.0 virginica\n\n\n\nUnderstanding automatic indexing\n\n\n# Cleaned up iris data.table\niris2 \n- data.frame(Length = iris$Sepal.Length, Width = iris$Sepal.Width, Species = iris$Species)\niris2 \n- as.data.table(iris2)\n\n\n\n\n# Area is greater than 20 square centimeters\niris2[ Width * Length \n 20 ], 20\n\n\n\n\n   Length Width    Species is_large\n 1:    5.4   3.9     setosa    FALSE\n 2:    5.8   4.0     setosa    FALSE\n 3:    5.7   4.4     setosa     TRUE\n 4:    5.4   3.9     setosa    FALSE\n 5:    5.7   3.8     setosa    FALSE\n 6:    5.2   4.1     setosa    FALSE\n 7:    5.5   4.2     setosa    FALSE\n 8:    7.0   3.2 versicolor    FALSE\n 9:    6.4   3.2 versicolor    FALSE\n10:    6.9   3.1 versicolor    FALSE\n11:    6.3   3.3 versicolor    FALSE\n12:    6.7   3.1 versicolor    FALSE\n13:    6.7   3.0 versicolor    FALSE\n14:    6.0   3.4 versicolor    FALSE\n15:    6.7   3.1 versicolor    FALSE\n16:    6.3   3.3  virginica    FALSE\n17:    7.1   3.0  virginica    FALSE\n18:    7.6   3.0  virginica    FALSE\n19:    7.3   2.9  virginica    FALSE\n20:    7.2   3.6  virginica     TRUE\n...\n\n\n\n\n# Add new boolean column\niris2[, is_large := Width * Length \n 25]\n\n\n\n\n# Now large observations with is_large\niris2[is_large == TRUE]\n\n\n\n\n   Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE\n\n\n\n\niris2[(is_large)] # Also OK\n\n\n\n\n   Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE\n\n\n\n\nSelecting groups or parts of groups\n\n\n# The 'keyed' data.table DT\nDT \n- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],\n                 B = c(5, 4, 1, 9,8 ,8, 6), \n                 C = 6:12)\nsetkey(DT, A, B)\n\n# Select the 'b' group\nDT['b']\n\n\n\n\n##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11\n\n\n\n# 'b' and 'c' groups\nDT[c('b', 'c')]\n\n\n\n\n##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11\n## 4: c 6 12\n## 5: c 9  9\n\n\n\n# The first row of the 'b' and 'c' groups\nDT[c('b', 'c'), mult = 'first']\n\n\n\n\n##    A B  C\n## 1: b 1  8\n## 2: c 6 12\n\n\n\n# First and last row of the 'b' and 'c' groups\nDT[c('b', 'c'), .SD[c(1, .N)], by = .EACHI]\n\n\n\n\n##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9\n\n\n\n# Copy and extend code for instruction 4: add printout\nDT[c('b', 'c'), { print(.SD); .SD[c(1, .N)] }, by = .EACHI]\n\n\n\n\n##    B  C\n## 1: 1  8\n## 2: 5  6\n## 3: 8 11\n##    B  C\n## 1: 6 12\n## 2: 9  9\n\n##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9\n\n\n\nRolling joins - part one\n\n\n# Keyed data.table DT\nDT \n- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],                  B = c(5, 4, 1, 9, 8, 8, 6), \n                 C = 6:12, \n                 key = 'A,B')\n\n# Get the key of DT\nkey(DT)\n\n\n\n\n## [1] \"A\" \"B\"\n\n\n\n# Row where A == 'b' \n B == 6\nsetkey(DT, A, B)\nDT[.('b', 6)]\n\n\n\n\n##    A B  C\n## 1: b 6 NA\n\n\n\n# Return the prevailing row\nDT[.('b',6), roll = TRUE]\n\n\n\n\n##    A B C\n## 1: b 6 6\n\n\n\n# Return the nearest row\nDT[.('b',6), roll =+ Inf]\n\n\n\n\n##    A B C\n## 1: b 6 6\n\n\n\nRolling joins - part two\n\n\n# Keyed data.table DT\nDT \n- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],                  B = c(5, 4, 1, 9, 8, 8, 6), \n                 C = 6:12, \n                 key = 'A,B')\n\n# Look at the sequence (-2):10 for the 'b' group\nDT[.('b', (-2):10)]\n\n\n\n\n##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2 NA\n##  6: b  3 NA\n##  7: b  4 NA\n##  8: b  5  6\n##  9: b  6 NA\n## 10: b  7 NA\n## 11: b  8 11\n## 12: b  9 NA\n## 13: b 10 NA\n\n\n\n# Add code: carry the prevailing values forwards\nDT[.('b', (-2):10), roll = TRUE]\n\n\n\n\n##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11\n\n\n\n# Add code: carry the first observation backwards\nDT[.('b', (-2):10), roll = TRUE, rollends = TRUE]\n\n\n\n\n##     A  B  C\n##  1: b -2  8\n##  2: b -1  8\n##  3: b  0  8\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11\n\n\n\n\n\nData Manipulation in R with \ndplyr\n\n\n1, Introduction to \ndplyr\n\n\nLoad the \ndplyr\n and \nhflights\n package\n\n\n# Load the dplyr package\nlibrary(dplyr)\nlibrary(dtplyr)\n\n# Load the hflights package\n# A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011\nlibrary(hflights)\n\n# Call both head() and summary() on hflights\nhead(hflights)\n\n\n\n\n##      Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## 5424 2011     1          1         6    1400    1500            AA\n## 5425 2011     1          2         7    1401    1501            AA\n## 5426 2011     1          3         1    1352    1502            AA\n## 5427 2011     1          4         2    1403    1513            AA\n## 5428 2011     1          5         3    1405    1507            AA\n## 5429 2011     1          6         4    1359    1503            AA\n##      FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin\n## 5424       428  N576AA                60      40      -10        0    IAH\n## 5425       428  N557AA                60      45       -9        1    IAH\n## 5426       428  N541AA                70      48       -8       -8    IAH\n## 5427       428  N403AA                70      39        3        3    IAH\n## 5428       428  N492AA                62      44       -3        5    IAH\n## 5429       428  N262AA                64      45       -7       -1    IAH\n##      Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted\n## 5424  DFW      224      7      13         0                         0\n## 5425  DFW      224      6       9         0                         0\n## 5426  DFW      224      5      17         0                         0\n## 5427  DFW      224      9      22         0                         0\n## 5428  DFW      224      9       9         0                         0\n## 5429  DFW      224      6      13         0                         0\n\n\n\nsummary(hflights)\n\n\n\n\n##       Year          Month          DayofMonth      DayOfWeek    \n##  Min.   :2011   Min.   : 1.000   Min.   : 1.00   Min.   :1.000  \n##  1st Qu.:2011   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:2.000  \n##  Median :2011   Median : 7.000   Median :16.00   Median :4.000  \n##  Mean   :2011   Mean   : 6.514   Mean   :15.74   Mean   :3.948  \n##  3rd Qu.:2011   3rd Qu.: 9.000   3rd Qu.:23.00   3rd Qu.:6.000  \n##  Max.   :2011   Max.   :12.000   Max.   :31.00   Max.   :7.000  \n##                                                                 \n##     DepTime        ArrTime     UniqueCarrier        FlightNum   \n##  Min.   :   1   Min.   :   1   Length:227496      Min.   :   1  \n##  1st Qu.:1021   1st Qu.:1215   Class :character   1st Qu.: 855  \n##  Median :1416   Median :1617   Mode  :character   Median :1696  \n##  Mean   :1396   Mean   :1578                      Mean   :1962  \n##  3rd Qu.:1801   3rd Qu.:1953                      3rd Qu.:2755  \n##  Max.   :2400   Max.   :2400                      Max.   :7290  \n##  NA's   :2905   NA's   :3066                                    \n##    TailNum          ActualElapsedTime    AirTime         ArrDelay      \n##  Length:227496      Min.   : 34.0     Min.   : 11.0   Min.   :-70.000  \n##  Class :character   1st Qu.: 77.0     1st Qu.: 58.0   1st Qu.: -8.000  \n##  Mode  :character   Median :128.0     Median :107.0   Median :  0.000  \n##                     Mean   :129.3     Mean   :108.1   Mean   :  7.094  \n##                     3rd Qu.:165.0     3rd Qu.:141.0   3rd Qu.: 11.000  \n##                     Max.   :575.0     Max.   :549.0   Max.   :978.000  \n##                     NA's   :3622      NA's   :3622    NA's   :3622     \n##     DepDelay          Origin              Dest              Distance     \n##  Min.   :-33.000   Length:227496      Length:227496      Min.   :  79.0  \n##  1st Qu.: -3.000   Class :character   Class :character   1st Qu.: 376.0  \n##  Median :  0.000   Mode  :character   Mode  :character   Median : 809.0  \n##  Mean   :  9.445                                         Mean   : 787.8  \n##  3rd Qu.:  9.000                                         3rd Qu.:1042.0  \n##  Max.   :981.000                                         Max.   :3904.0  \n##  NA's   :2905                                                            \n##      TaxiIn           TaxiOut         Cancelled       CancellationCode  \n##  Min.   :  1.000   Min.   :  1.00   Min.   :0.00000   Length:227496     \n##  1st Qu.:  4.000   1st Qu.: 10.00   1st Qu.:0.00000   Class :character  \n##  Median :  5.000   Median : 14.00   Median :0.00000   Mode  :character  \n##  Mean   :  6.099   Mean   : 15.09   Mean   :0.01307                     \n##  3rd Qu.:  7.000   3rd Qu.: 18.00   3rd Qu.:0.00000                     \n##  Max.   :165.000   Max.   :163.00   Max.   :1.00000                     \n##  NA's   :3066      NA's   :2947                                         \n##     Diverted       \n##  Min.   :0.000000  \n##  1st Qu.:0.000000  \n##  Median :0.000000  \n##  Mean   :0.002853  \n##  3rd Qu.:0.000000  \n##  Max.   :1.000000  \n##\n\n\n\nConvert \ndata.frame\n to table\n\n\n# Convert the hflights data.frame into a hflights tbl\nhflights \n- tbl_df(hflights)\n\n# Display the hflights tbl\nhflights\n\n\n\n\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     1          1         6    1400    1500            AA\n## 2   2011     1          2         7    1401    1501            AA\n## 3   2011     1          3         1    1352    1502            AA\n## 4   2011     1          4         2    1403    1513            AA\n## 5   2011     1          5         3    1405    1507            AA\n## 6   2011     1          6         4    1359    1503            AA\n## 7   2011     1          7         5    1359    1509            AA\n## 8   2011     1          8         6    1355    1454            AA\n## 9   2011     1          9         7    1443    1554            AA\n## 10  2011     1         10         1    1443    1553            AA\n## # ... with 227,486 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# Create the object carriers, containing only the UniqueCarrier variable of hflights\ncarriers \n- hflights$UniqueCarrier\n\n\n\n\nChanging labels of \nhflights\n, part 1 of 2\n\n\n# add\nlut \n- c('AA' = 'American', 'AS' = 'Alaska', 'B6' = 'JetBlue', 'CO' = 'Continental', \n         'DL' = 'Delta', 'OO' = 'SkyWest', 'UA' = 'United', 'US' = 'US_Airways', \n         'WN' = 'Southwest', 'EV' = 'Atlantic_Southeast', 'F9' = 'Frontier', \n         'FL' = 'AirTran', 'MQ' = 'American_Eagle', 'XE' = 'ExpressJet', 'YV' = 'Mesa')\n\n# Use lut to translate the UniqueCarrier column of hflights\nhflights$UniqueCarrier \n- lut[hflights$UniqueCarrier]\n\n# Inspect the resulting raw values of your variables\nglimpse(hflights)\n\n\n\n\n## Observations: 227,496\n## Variables: 21\n## $ Year              \nint\n 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             \nint\n 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        \nint\n 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         \nint\n 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           \nint\n 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           \nint\n 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     \nchr\n \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         \nint\n 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           \nchr\n \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime \nint\n 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           \nint\n 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          \nint\n -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          \nint\n 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            \nchr\n \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              \nchr\n \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          \nint\n 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            \nint\n 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           \nint\n 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  \nchr\n \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\nChanging labels of \nhflights\n, part 2 of 2\n\n\n# Build the lookup table: lut\nlut \n- c(\nA\n = \ncarrier\n, \nB\n = \nweather\n ,\nC\n = \nFFA\n ,\nD\n = \nsecurity\n, \nE\n = \nnot cancelled\n)\n\n# Add the Code column\nhflights$Code \n- lut[hflights$CancellationCode]\n\n# Glimpse at hflights\nglimpse(hflights)\n\n\n\n\nResult.\n\n\nObservations: 227,496\nVariables: 22\n$ Year              \nint\n 2011, 2011, 2011, 2011, 2011, 2011,...\n$ Month             \nint\n 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n$ DayofMonth        \nint\n 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...\n$ DayOfWeek         \nint\n 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,...\n$ DepTime           \nint\n 1400, 1401, 1352, 1403, 1405, 1359,...\n$ ArrTime           \nint\n 1500, 1501, 1502, 1513, 1507, 1503,...\n$ UniqueCarrier     \nchr\n \nAmerican\n, \nAmerican\n, \nAmerican\n,...\n$ FlightNum         \nint\n 428, 428, 428, 428, 428, 428, 428, ...\n$ TailNum           \nchr\n \nN576AA\n, \nN557AA\n, \nN541AA\n, \nN403...\n$ ActualElapsedTime \nint\n 60, 60, 70, 70, 62, 64, 70, 59, 71,...\n$ AirTime           \nint\n 40, 45, 48, 39, 44, 45, 43, 40, 41,...\n$ ArrDelay          \nint\n -10, -9, -8, 3, -3, -7, -1, -16, 44...\n$ DepDelay          \nint\n 0, 1, -8, 3, 5, -1, -1, -5, 43, 43,...\n$ Origin            \nchr\n \nIAH\n, \nIAH\n, \nIAH\n, \nIAH\n, \nIAH\n, ...\n$ Dest              \nchr\n \nDFW\n, \nDFW\n, \nDFW\n, \nDFW\n, \nDFW\n, ...\n$ Distance          \nint\n 224, 224, 224, 224, 224, 224, 224, ...\n$ TaxiIn            \nint\n 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4...\n$ TaxiOut           \nint\n 13, 9, 17, 22, 9, 13, 15, 12, 22, 1...\n$ Cancelled         \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ CancellationCode  \nchr\n \n, \n, \n, \n, \n, \n, \n, \n, \n,...\n$ Diverted          \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ Code              \nchr\n NA, NA, NA, NA, NA, NA, NA, NA, NA,...\n\n\n\n\n2, \nselect\n and \nmutate\n\n\nThe five verbs and their meaning\n\n\n\n\nselect\n; which returns a subset of the columns.\n\n\nfilter\n; that is able to return a subset of the rows.\n\n\narrange\n; that reorders the rows according to single or\n\n    multiple variables.\n\n\nmutate\n; used to add columns from existing data.\n\n\nsummarise\n; which reduces each group to a single row by calculating\n\n    aggregate measures.\n\n\n\n\nChoosing is not losing! The \nselect\n verb\n\n\n# Print out a tbl with the four columns of hflights related to delay\nselect(hflights, ActualElapsedTime, AirTime, ArrDelay, DepDelay)\n\n\n\n\n## # A tibble: 227,496 \u00d7 4\n##    ActualElapsedTime AirTime ArrDelay DepDelay\n## *              \nint\n   \nint\n    \nint\n    \nint\n\n## 1                 60      40      -10        0\n## 2                 60      45       -9        1\n## 3                 70      48       -8       -8\n## 4                 70      39        3        3\n## 5                 62      44       -3        5\n## 6                 64      45       -7       -1\n## 7                 70      43       -1       -1\n## 8                 59      40      -16       -5\n## 9                 71      41       44       43\n## 10                70      45       43       43\n## # ... with 227,486 more rows\n\n\n\n# Print out hflights, nothing has changed!\nhflights\n\n\n\n\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# Print out the columns Origin up to Cancelled of hflights\nselect(hflights, 14:19)\n\n\n\n\n## # A tibble: 227,496 \u00d7 6\n##    Origin  Dest Distance TaxiIn TaxiOut Cancelled\n## *   \nchr\n \nchr\n    \nint\n  \nint\n   \nint\n     \nint\n\n## 1     IAH   DFW      224      7      13         0\n## 2     IAH   DFW      224      6       9         0\n## 3     IAH   DFW      224      5      17         0\n## 4     IAH   DFW      224      9      22         0\n## 5     IAH   DFW      224      9       9         0\n## 6     IAH   DFW      224      6      13         0\n## 7     IAH   DFW      224     12      15         0\n## 8     IAH   DFW      224      7      12         0\n## 9     IAH   DFW      224      8      22         0\n## 10    IAH   DFW      224      6      19         0\n## # ... with 227,486 more rows\n\n\n\n# Answer to last question: be concise!\nselect(hflights, 1:4, 12:21)\n\n\n\n\n## # A tibble: 227,496 \u00d7 14\n##     Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin  Dest\n## *  \nint\n \nint\n      \nint\n     \nint\n    \nint\n    \nint\n  \nchr\n \nchr\n\n## 1   2011     1          1         6      -10        0    IAH   DFW\n## 2   2011     1          2         7       -9        1    IAH   DFW\n## 3   2011     1          3         1       -8       -8    IAH   DFW\n## 4   2011     1          4         2        3        3    IAH   DFW\n## 5   2011     1          5         3       -3        5    IAH   DFW\n## 6   2011     1          6         4       -7       -1    IAH   DFW\n## 7   2011     1          7         5       -1       -1    IAH   DFW\n## 8   2011     1          8         6      -16       -5    IAH   DFW\n## 9   2011     1          9         7       44       43    IAH   DFW\n## 10  2011     1         10         1       43       43    IAH   DFW\n## # ... with 227,486 more rows, and 6 more variables: Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\nHelper functions for variable selection\n\n\nselect\n:\n\n\n\n\nstarts_with(\"X\")\n; every name that starts with \n\"X\"\n,\n\n\nends_with(\"X\")\n; every name that ends with \n\"X\"\n,\n\n\ncontains(\"X\")\n; every name that contains \n\"X\"\n,\n\n\nmatches(\"X\")\n; every name that matches \n\"X\"\n, where \n\"X\"\n can be a\n\n    regular expression,\n\n\nnum_range(\"x\", 1:5)\n; the variables named \nx01\n, \nx02\n, \nx03\n, -\n\n\nx04\n and \nx05\n,\n\n\none_of(x)\n; every name that appears in \nx\n, which should be a\n\n    character vector.\n\n\n\n\n\n\n\n# Print out a tbl containing just ArrDelay and DepDelay\nselect(hflights, ArrDelay, DepDelay)\n\n\n\n\n## # A tibble: 227,496 \u00d7 2\n##    ArrDelay DepDelay\n## *     \nint\n    \nint\n\n## 1       -10        0\n## 2        -9        1\n## 3        -8       -8\n## 4         3        3\n## 5        -3        5\n## 6        -7       -1\n## 7        -1       -1\n## 8       -16       -5\n## 9        44       43\n## 10       43       43\n## # ... with 227,486 more rows\n\n\n\n# Print out a tbl as described in the second instruction, using both helper functions and variable names\nselect(hflights, UniqueCarrier, ends_with('Num'), starts_with('Cancel'))\n\n\n\n\n## # A tibble: 227,496 \u00d7 5\n##    UniqueCarrier FlightNum TailNum Cancelled CancellationCode\n## *          \nchr\n     \nint\n   \nchr\n     \nint\n            \nchr\n\n## 1       American       428  N576AA         0                 \n## 2       American       428  N557AA         0                 \n## 3       American       428  N541AA         0                 \n## 4       American       428  N403AA         0                 \n## 5       American       428  N492AA         0                 \n## 6       American       428  N262AA         0                 \n## 7       American       428  N493AA         0                 \n## 8       American       428  N477AA         0                 \n## 9       American       428  N476AA         0                 \n## 10      American       428  N504AA         0                 \n## # ... with 227,486 more rows\n\n\n\n# Print out a tbl as described in the third instruction, using only helper functions.\nselect(hflights, ends_with('Time'), ends_with('Delay'))\n\n\n\n\n## # A tibble: 227,496 \u00d7 6\n##    DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay\n## *    \nint\n   \nint\n             \nint\n   \nint\n    \nint\n    \nint\n\n## 1     1400    1500                60      40      -10        0\n## 2     1401    1501                60      45       -9        1\n## 3     1352    1502                70      48       -8       -8\n## 4     1403    1513                70      39        3        3\n## 5     1405    1507                62      44       -3        5\n## 6     1359    1503                64      45       -7       -1\n## 7     1359    1509                70      43       -1       -1\n## 8     1355    1454                59      40      -16       -5\n## 9     1443    1554                71      41       44       43\n## 10    1443    1553                70      45       43       43\n## # ... with 227,486 more rows\n\n\n\nComparison to basic R\n\n\n# add\nex1r \n- hflights[c('TaxiIn','TaxiOut','Distance')]\n\nex1d \n- select(hflights, starts_with('Taxi'), Distance)\n\nex2r \n- hflights[c('Year','Month','DayOfWeek','DepTime','ArrTime')]\n\nex2d \n- select(hflights, Year, Month, DayOfWeek, DepTime, ArrTime)\n\nex3r \n- hflights[c('TailNum','TaxiIn','TaxiOut')]\n\nex3d \n- select(hflights, TailNum, starts_with('Taxi'))\n\n\n\n\nMutating is creating\n\n\n# Add the new variable ActualGroundTime to a copy of hflights and save the result as g1\ng1 \n- mutate(hflights, ActualGroundTime = ActualElapsedTime - AirTime)\nglimpse(hflights)\n\n\n\n\n## Observations: 227,496\n## Variables: 21\n## $ Year              \nint\n 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             \nint\n 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        \nint\n 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         \nint\n 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           \nint\n 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           \nint\n 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     \nchr\n \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         \nint\n 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           \nchr\n \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime \nint\n 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           \nint\n 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          \nint\n -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          \nint\n 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            \nchr\n \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              \nchr\n \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          \nint\n 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            \nint\n 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           \nint\n 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  \nchr\n \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\nglimpse(g1)\n\n\n\n\n## Observations: 227,496\n## Variables: 22\n## $ Year              \nint\n 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             \nint\n 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        \nint\n 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         \nint\n 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           \nint\n 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           \nint\n 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     \nchr\n \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         \nint\n 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           \nchr\n \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime \nint\n 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           \nint\n 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          \nint\n -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          \nint\n 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            \nchr\n \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              \nchr\n \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          \nint\n 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            \nint\n 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           \nint\n 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  \nchr\n \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ ActualGroundTime  \nint\n 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n\n\n\n# Add the new variable GroundTime to a g1; save the result as g2\ng2 \n- mutate(g1, GroundTime = TaxiIn + TaxiOut)\n\nhead(g1$ActualGroundTime == g2$GroundTime, 20)\n\n\n\n\n##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n## [15] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\n# Add the new variable AverageSpeed to g2; save the result as g3\ng3 \n- mutate(g2, AverageSpeed = Distance / AirTime * 60)\ng3\n\n\n\n\n## # A tibble: 227,496 \u00d7 24\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 17 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n, ActualGroundTime \nint\n, GroundTime \nint\n,\n## #   AverageSpeed \ndbl\n\n\n\n\nAdd multiple variables using mutate\n\n\n# Add a second variable loss_percent to the dataset: m1\nm1 \n- mutate(hflights, loss = ArrDelay - DepDelay, loss_percent = (ArrDelay - DepDelay)/DepDelay*100)\n\n# Copy and adapt the previous command to reduce redendancy: m2\nm2 \n- mutate(hflights, loss = ArrDelay - DepDelay, loss_percent = loss/DepDelay * 100)\n\n# Add the three variables as described in the third instruction: m3\nm3 \n- mutate(hflights, TotalTaxi = TaxiIn + TaxiOut, ActualGroundTime = ActualElapsedTime - AirTime, Diff = TotalTaxi - ActualGroundTime)\nglimpse(m3)\n\n\n\n\n## Observations: 227,496\n## Variables: 24\n## $ Year              \nint\n 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             \nint\n 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        \nint\n 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         \nint\n 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           \nint\n 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           \nint\n 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     \nchr\n \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         \nint\n 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           \nchr\n \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime \nint\n 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           \nint\n 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          \nint\n -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          \nint\n 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            \nchr\n \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              \nchr\n \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          \nint\n 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            \nint\n 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           \nint\n 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  \nchr\n \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ TotalTaxi         \nint\n 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ ActualGroundTime  \nint\n 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ Diff              \nint\n 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\n3, \nfilter\n and \narrange\n\n\nLogical operators\n\n\nfilter\n:\n\n\n\n\nx \n y\n; \nTRUE\n if \nx\n is less than \ny\n.\n\n\nx \n= y\n; \nTRUE\n if \nx\n is less than or equal to \ny\n.\n\n\nx == y\n; \nTRUE\n if \nx\n equals \ny\n.\n\n\nx != y\n; \nTRUE\n if \nx\n does not equal \ny\n.\n\n\nx \n= y\n; \nTRUE\n if \nx\n is greater than or equal to \ny\n.\n\n\nx \n y\n; \nTRUE\n if \nx\n is greater than \ny\n.\n\n\nx %in% c(a, b, c)\n; \nTRUE\n if \nx\n is in the vector \nc(a, b, c)\n.\n\n\n\n\n\n\n\n# All flights that traveled 3000 miles or more\nfilter(hflights, Distance \n= 3000)\n\n\n\n\n## # A tibble: 527 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     1         31         1     924    1413   Continental\n## 2   2011     1         30         7     925    1410   Continental\n## 3   2011     1         29         6    1045    1445   Continental\n## 4   2011     1         28         5    1516    1916   Continental\n## 5   2011     1         27         4     950    1344   Continental\n## 6   2011     1         26         3     944    1350   Continental\n## 7   2011     1         25         2     924    1337   Continental\n## 8   2011     1         24         1    1144    1605   Continental\n## 9   2011     1         23         7     926    1335   Continental\n## 10  2011     1         22         6     942    1340   Continental\n## # ... with 517 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# All flights flown by one of JetBlue, Southwest, or Delta\nfilter(hflights, UniqueCarrier %in% c('JetBlue','Southwest','Delta'))\n\n\n\n\n## # A tibble: 48,679 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     1          1         6     654    1124       JetBlue\n## 2   2011     1          1         6    1639    2110       JetBlue\n## 3   2011     1          2         7     703    1113       JetBlue\n## 4   2011     1          2         7    1604    2040       JetBlue\n## 5   2011     1          3         1     659    1100       JetBlue\n## 6   2011     1          3         1    1801    2200       JetBlue\n## 7   2011     1          4         2     654    1103       JetBlue\n## 8   2011     1          4         2    1608    2034       JetBlue\n## 9   2011     1          5         3     700    1103       JetBlue\n## 10  2011     1          5         3    1544    1954       JetBlue\n## # ... with 48,669 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# All flights where taxiing took longer than flying\nfilter(hflights, (TaxiIn + TaxiOut) \n AirTime)\n\n\n\n\n## # A tibble: 1,389 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     1         24         1     731     904      American\n## 2   2011     1         30         7    1959    2132      American\n## 3   2011     1         24         1    1621    1749      American\n## 4   2011     1         10         1     941    1113      American\n## 5   2011     1         31         1    1301    1356   Continental\n## 6   2011     1         31         1    2113    2215   Continental\n## 7   2011     1         31         1    1434    1539   Continental\n## 8   2011     1         31         1     900    1006   Continental\n## 9   2011     1         30         7    1304    1408   Continental\n## 10  2011     1         30         7    2004    2128   Continental\n## # ... with 1,379 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\nCombining tests using boolean operators\n\n\n# All flights that departed before 5am or arrived after 10pm\nfilter(hflights, DepTime \n 500 | ArrTime \n 2200)\n\n\n\n\n## # A tibble: 27,799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     1          4         2    2100    2207      American\n## 2   2011     1         14         5    2119    2229      American\n## 3   2011     1         10         1    1934    2235      American\n## 4   2011     1         26         3    1905    2211      American\n## 5   2011     1         30         7    1856    2209      American\n## 6   2011     1          9         7    1938    2228        Alaska\n## 7   2011     1         31         1    1919    2231   Continental\n## 8   2011     1         31         1    2116    2344   Continental\n## 9   2011     1         31         1    1850    2211   Continental\n## 10  2011     1         31         1    2102    2216   Continental\n## # ... with 27,789 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# All flights that departed late but arrived ahead of schedule\nfilter(hflights, DepDelay \n 0 \n ArrDelay \n 0)\n\n\n\n\n## # A tibble: 27,712 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     1          2         7    1401    1501      American\n## 2   2011     1          5         3    1405    1507      American\n## 3   2011     1         18         2    1408    1508      American\n## 4   2011     1         18         2     721     827      American\n## 5   2011     1         12         3    2015    2113      American\n## 6   2011     1         13         4    2020    2116      American\n## 7   2011     1         26         3    2009    2103      American\n## 8   2011     1          1         6    1631    1736      American\n## 9   2011     1         10         1    1639    1740      American\n## 10  2011     1         12         3    1631    1739      American\n## # ... with 27,702 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# All cancelled weekend flights\nfilter(hflights, DayOfWeek %in% c(6,7) \n Cancelled == 1)\n\n\n\n\n## # A tibble: 585 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n              \nchr\n\n## 1   2011     1          9         7      NA      NA           American\n## 2   2011     1         29         6      NA      NA        Continental\n## 3   2011     1          9         7      NA      NA        Continental\n## 4   2011     1          9         7      NA      NA              Delta\n## 5   2011     1          9         7      NA      NA            SkyWest\n## 6   2011     1          2         7      NA      NA          Southwest\n## 7   2011     1         29         6      NA      NA              Delta\n## 8   2011     1          9         7      NA      NA Atlantic_Southeast\n## 9   2011     1          1         6      NA      NA            AirTran\n## 10  2011     1          9         7      NA      NA            AirTran\n## # ... with 575 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# All flights that were cancelled after being delayed\nfilter(hflights, DepDelay \n 0 \n Cancelled == 1)\n\n\n\n\n## # A tibble: 40 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     1         26         3    1926      NA   Continental\n## 2   2011     1         11         2    1100      NA    US_Airways\n## 3   2011     1         19         3    1811      NA    ExpressJet\n## 4   2011     1          7         5    2028      NA    ExpressJet\n## 5   2011     2          4         5    1638      NA      American\n## 6   2011     2          8         2    1057      NA   Continental\n## 7   2011     2          2         3     802      NA    ExpressJet\n## 8   2011     2          9         3     904      NA    ExpressJet\n## 9   2011     2          1         2    1508      NA       SkyWest\n## 10  2011     3         31         4    1016      NA   Continental\n## # ... with 30 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\nBlend together what you\nve learned!\n\n\n# Select the flights that had JFK as their destination: c1\nc1 \n- filter(hflights, Dest == 'JFK')\n\n# Combine the Year, Month and DayofMonth variables to create a Date column: c2\nc2 \n- mutate(c1, Date = paste(Year, Month, DayofMonth, sep = '-'))\n\n# Print out a selection of columns of c2\nselect(c2, Date, DepTime, ArrTime, TailNum)\n\n\n\n\n## # A tibble: 695 \u00d7 4\n##        Date DepTime ArrTime TailNum\n##       \nchr\n   \nint\n   \nint\n   \nchr\n\n## 1  2011-1-1     654    1124  N324JB\n## 2  2011-1-1    1639    2110  N324JB\n## 3  2011-1-2     703    1113  N324JB\n## 4  2011-1-2    1604    2040  N324JB\n## 5  2011-1-3     659    1100  N229JB\n## 6  2011-1-3    1801    2200  N206JB\n## 7  2011-1-4     654    1103  N267JB\n## 8  2011-1-4    1608    2034  N267JB\n## 9  2011-1-5     700    1103  N708JB\n## 10 2011-1-5    1544    1954  N644JB\n## # ... with 685 more rows\n\n\n\nArranging your data\n\n\n# Definition of dtc\ndtc \n- filter(hflights, Cancelled == 1, !is.na(DepDelay))\n\n# Arrange dtc by departure delays\narrange(dtc, DepDelay)\n\n\n\n\n## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n          \nchr\n\n## 1   2011     7         23         6     605      NA       Frontier\n## 2   2011     1         17         1     916      NA     ExpressJet\n## 3   2011    12          1         4     541      NA     US_Airways\n## 4   2011    10         12         3    2022      NA American_Eagle\n## 5   2011     7         29         5    1424      NA    Continental\n## 6   2011     9         29         4    1639      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     5          9         1     715      NA        SkyWest\n## 9   2011     1         20         4    1413      NA         United\n## 10  2011     1         17         1     831      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# Arrange dtc so that cancellation reasons are grouped\narrange(dtc, CancellationCode)\n\n\n\n\n## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n          \nchr\n\n## 1   2011     1         20         4    1413      NA         United\n## 2   2011     1          7         5    2028      NA     ExpressJet\n## 3   2011     2          4         5    1638      NA       American\n## 4   2011     2          8         2    1057      NA    Continental\n## 5   2011     2          1         2    1508      NA        SkyWest\n## 6   2011     2         21         1    2257      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     3         18         5     727      NA         United\n## 9   2011     4          4         1    1632      NA          Delta\n## 10  2011     4          8         5    1608      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# Arrange dtc according to carrier and departure delays\narrange(dtc, UniqueCarrier, DepDelay)\n\n\n\n\n## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n              \nchr\n\n## 1   2011     6         11         6    1649      NA            AirTran\n## 2   2011     8         18         4    1808      NA           American\n## 3   2011     2          4         5    1638      NA           American\n## 4   2011    10         12         3    2022      NA     American_Eagle\n## 5   2011     2          9         3     555      NA     American_Eagle\n## 6   2011     7         17         7    1917      NA     American_Eagle\n## 7   2011     4         30         6     612      NA Atlantic_Southeast\n## 8   2011     4         10         7    1147      NA Atlantic_Southeast\n## 9   2011     5         23         1     657      NA Atlantic_Southeast\n## 10  2011     9         29         4     723      NA Atlantic_Southeast\n## # ... with 58 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\nReverse the order of arranging\n\n\n# Arrange according to carrier and decreasing departure delays\narrange(hflights, UniqueCarrier, desc(DepDelay))\n\n\n\n\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     2         19         6    1902    2143       AirTran\n## 2   2011     3         14         1    2024    2309       AirTran\n## 3   2011     2         16         3    2349     227       AirTran\n## 4   2011    11         13         7    2312     213       AirTran\n## 5   2011     5         26         4    2353     305       AirTran\n## 6   2011     5         26         4    1922    2229       AirTran\n## 7   2011     4         28         4    1045    1328       AirTran\n## 8   2011     6          5         7    2207      52       AirTran\n## 9   2011     5          7         6    1009    1256       AirTran\n## 10  2011     7         25         1    2107      14       AirTran\n## # ... with 227,486 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# Arrange flights by total delay (normal order).\narrange(hflights, (ArrDelay+DepDelay))\n\n\n\n\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n         \nchr\n\n## 1   2011     7          3         7    1914    2039    ExpressJet\n## 2   2011     8         31         3     934    1039       SkyWest\n## 3   2011     8         21         7     935    1039       SkyWest\n## 4   2011     8         28         7    2059    2206       SkyWest\n## 5   2011     8         29         1     935    1041       SkyWest\n## 6   2011    12         25         7     741     926       SkyWest\n## 7   2011     1         30         7     620     812       SkyWest\n## 8   2011     8          3         3    1741    1810    ExpressJet\n## 9   2011     8          4         4     930    1041       SkyWest\n## 10  2011     8         18         4     939    1043       SkyWest\n## # ... with 227,486 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n# Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime \narrange(filter(hflights, Dest == 'DFW' \n DepTime\n800), desc(AirTime))\n\n\n\n\n## # A tibble: 799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    \nint\n \nint\n      \nint\n     \nint\n   \nint\n   \nint\n          \nchr\n\n## 1   2011    11         22         2     635     825       American\n## 2   2011     8         25         4     602     758 American_Eagle\n## 3   2011    10         12         3     559     738 American_Eagle\n## 4   2011     5          2         1     716     854       American\n## 5   2011     4          4         1     741     949       American\n## 6   2011     4          4         1     627     742 American_Eagle\n## 7   2011     6         21         2     726     848     ExpressJet\n## 8   2011     9          1         4     715     844       American\n## 9   2011     3         14         1     729     917    Continental\n## 10  2011    12          5         1     724     847    Continental\n## # ... with 789 more rows, and 14 more variables: FlightNum \nint\n,\n## #   TailNum \nchr\n, ActualElapsedTime \nint\n, AirTime \nint\n, ArrDelay \nint\n,\n## #   DepDelay \nint\n, Origin \nchr\n, Dest \nchr\n, Distance \nint\n,\n## #   TaxiIn \nint\n, TaxiOut \nint\n, Cancelled \nint\n, CancellationCode \nchr\n,\n## #   Diverted \nint\n\n\n\n\n4, \nsummarise\n and the Pipe Operator\n\n\nThe syntax of summarise\n\n\n# Print out a summary with variables min_dist and max_dist\nsummarise(hflights, min_dist = min(Distance), max_dist = max(Distance))\n\n\n\n\n## # A tibble: 1 \u00d7 2\n##   min_dist max_dist\n##      \nint\n    \nint\n\n## 1       79     3904\n\n\n\n# Print out a summary with variable max_div\nsummarise(filter(hflights, Diverted == 1), max_div = max(Distance))\n\n\n\n\n## # A tibble: 1 \u00d7 1\n##   max_div\n##     \nint\n\n## 1    3904\n\n\n\nAggregate functions\n\n\nsummarise\n:\n\n\n\n\nmin(x)\n; minimum value of vector x.\n\n\nmax(x)\n; maximum value of vector x.\n\n\nmean(x)\n; mean value of vector x.\n\n\nmedian(x)\n; median value of vector x.\n\n\nquantile(x, p)\n; pth quantile of vector x.\n\n\nsd(x)\n; standard deviation of vector x.\n\n\nvar(x)\n; variance of vector x.\n\n\nIQR(x)\n; Inter Quartile Range (IQR) of vector x.\n\n\ndiff(range(x))\n; total range of vector x.\n\n\n\n\n\n\n\n# Remove rows that have NA ArrDelay: temp1\ntemp1 \n- filter(hflights, !is.na(ArrDelay))\n\n# Generate summary about ArrDelay column of temp1\nsummarise(temp1, earliest = min(ArrDelay), average = mean(ArrDelay), latest = max(ArrDelay), sd = sd(ArrDelay))\n\n\n\n\n## # A tibble: 1 \u00d7 4\n##   earliest  average latest       sd\n##      \nint\n    \ndbl\n  \nint\n    \ndbl\n\n## 1      -70 7.094334    978 30.70852\n\n\n\n# Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2\ntemp2 \n- filter(hflights, !is.na(TaxiIn), !is.na(TaxiOut))\n\n# Print the maximum taxiing difference of temp2 with summarise()\nsummarise(temp2, max_taxi_diff = max(abs(TaxiIn - TaxiOut)))\n\n\n\n\n## # A tibble: 1 \u00d7 1\n##   max_taxi_diff\n##           \nint\n\n## 1           160\n\n\n\ndplyr\n aggregate functions\n\n\n\n\nfirst(x)\n; the first element of vector \nx\n.\n\n\nlast(x)\n; the last element of vector \nx\n.\n\n\nnth(x, n)\n; The \nn\nth element of vector \nx\n.\n\n\nn()\n; The number of rows in the data.frame or group of observations\n\n    that \nsummarise()\n describes.\n\n\nn_distinct(x)\n; The number of unique values in vector \nx\n.\n\n\n\n\n\n\n\n# Generate summarizing statistics for hflights\nsummarise(hflights, n_obs = n(), n_carrier = n_distinct(UniqueCarrier), n_dest = n_distinct(Dest), dest100 = nth(Dest, 100))\n\n\n\n\n## # A tibble: 1 \u00d7 4\n##    n_obs n_carrier n_dest dest100\n##    \nint\n     \nint\n  \nint\n   \nchr\n\n## 1 227496        15    116     DFW\n\n\n\n# Filter hflights to keep all American Airline flights: aa\naa \n- filter(hflights, UniqueCarrier == 'American')\n\n# Generate summarizing statistics for aa \nsummarise(aa, n_flights = n(), n_canc = sum(Cancelled), p_canc = n_canc/n_flights*100, avg_delay = mean(ArrDelay, na.rm = TRUE))\n\n\n\n\n## # A tibble: 1 \u00d7 4\n##   n_flights n_canc   p_canc avg_delay\n##       \nint\n  \nint\n    \ndbl\n     \ndbl\n\n## 1      3244     60 1.849568 0.8917558\n\n\n\nOverview of syntax\n\n\n# Write the 'piped' version of the English sentences\nhflights %\n%\n    mutate(diff = TaxiOut - TaxiIn) %\n%\n    filter(!is.na(diff)) %\n%\n    summarise( avg = mean(diff))\n\n\n\n\n## # A tibble: 1 \u00d7 1\n##        avg\n##      \ndbl\n\n## 1 8.992064\n\n\n\nDrive or fly? Part 1 of 2\n\n\n# Part 1, concerning the selection and creation of columns\nd \n- hflights %\n%\n  select(Dest, UniqueCarrier, Distance, ActualElapsedTime) %\n%  \n  mutate(RealTime = ActualElapsedTime + 100, mph = Distance / RealTime * 60)\n\n# Part 2, concerning flights that had an actual average speed of \n 70 mph.\nd %\n%\n  filter(!is.na(mph), mph \n 70) %\n%\n  summarise( n_less = n(), \n             n_dest = n_distinct(Dest), \n             min_dist = min(Distance), \n             max_dist = max(Distance))\n\n\n\n\n## # A tibble: 1 \u00d7 4\n##   n_less n_dest min_dist max_dist\n##    \nint\n  \nint\n    \nint\n    \nint\n\n## 1   6726     13       79      305\n\n\n\nDrive or fly? Part 2 of 2\n\n\n# Solve the exercise using a combination of dplyr verbs and %\n%\nhflights %\n%\n    #summarise(all_flights = n()) %\n%\n    filter(((Distance / (ActualElapsedTime + 100) * 60) \n 105) | Cancelled == 1 | Diverted == 1) %\n%\n    summarise(n_non = n(), p_non = n_non / 22751 *100, n_dest = n_distinct(Dest), min_dist = min(Distance), max_dist = max(Distance))\n\n\n\n\n## # A tibble: 1 \u00d7 5\n##   n_non    p_non n_dest min_dist max_dist\n##   \nint\n    \ndbl\n  \nint\n    \nint\n    \nint\n\n## 1 42400 186.3654    113       79     3904\n\n\n\nAdvanced piping exercise\n\n\n# Count the number of overnight flights\nhflights %\n%\n    filter(ArrTime \n DepTime \n !is.na(DepTime) \n !is.na(ArrTime)) %\n%\n    summarise(n = n())\n\n\n\n\n## # A tibble: 1 \u00d7 1\n##       n\n##   \nint\n\n## 1  2718\n\n\n\n5, \ngroup_by\n and working with data\n\n\nUnite and conquer using \ngroup_by\n\n\n# Make an ordered per-carrier summary of hflights\nhflights %\n%\n   group_by(UniqueCarrier) %\n%\n   summarise(n_flights = n(), \n             n_canc = sum(Cancelled == 1), \n             p_canc = mean(Cancelled == 1) * 100, \n             avg_delay = mean(ArrDelay, na.rm = TRUE)) %\n%\n   arrange(avg_delay, p_canc)\n\n\n\n\n## # A tibble: 15 \u00d7 5\n##         UniqueCarrier n_flights n_canc    p_canc  avg_delay\n##                 \nchr\n     \nint\n  \nint\n     \ndbl\n      \ndbl\n\n## 1          US_Airways      4082     46 1.1268986 -0.6307692\n## 2            American      3244     60 1.8495684  0.8917558\n## 3             AirTran      2139     21 0.9817672  1.8536239\n## 4              Alaska       365      0 0.0000000  3.1923077\n## 5                Mesa        79      1 1.2658228  4.0128205\n## 6               Delta      2641     42 1.5903067  6.0841374\n## 7         Continental     70032    475 0.6782614  6.0986983\n## 8      American_Eagle      4648    135 2.9044750  7.1529751\n## 9  Atlantic_Southeast      2204     76 3.4482759  7.2569543\n## 10          Southwest     45343    703 1.5504047  7.5871430\n## 11           Frontier       838      6 0.7159905  7.6682692\n## 12         ExpressJet     73053   1132 1.5495599  8.1865242\n## 13            SkyWest     16061    224 1.3946828  8.6934922\n## 14            JetBlue       695     18 2.5899281  9.8588410\n## 15             United      2072     34 1.6409266 10.4628628\n\n\n\n# Make an ordered per-day summary of hflights\nhflights %\n% \n   group_by(DayOfWeek) %\n%\n   summarise(avg_taxi = mean(TaxiIn + TaxiOut, na.rm=TRUE)) %\n%\n   arrange(desc(avg_taxi))\n\n\n\n\n## # A tibble: 7 \u00d7 2\n##   DayOfWeek avg_taxi\n##       \nint\n    \ndbl\n\n## 1         1 21.77027\n## 2         2 21.43505\n## 3         4 21.26076\n## 4         3 21.19055\n## 5         5 21.15805\n## 6         7 20.93726\n## 7         6 20.43061\n\n\n\nCombine \ngroup_by\n with \nmutate\n\n\n# Solution to first instruction\nhflights %\n%\n    filter(!is.na(ArrDelay)) %\n%\n    group_by(UniqueCarrier) %\n%\n    summarise(p_delay = sum(ArrDelay \n 0) / n()) %\n%\n    mutate(rank = rank(p_delay)) %\n%\n    arrange(rank)\n\n\n\n\n## # A tibble: 15 \u00d7 3\n##         UniqueCarrier   p_delay  rank\n##                 \nchr\n     \ndbl\n \ndbl\n\n## 1            American 0.3030208     1\n## 2             AirTran 0.3112269     2\n## 3          US_Airways 0.3267990     3\n## 4  Atlantic_Southeast 0.3677511     4\n## 5      American_Eagle 0.3696714     5\n## 6               Delta 0.3871092     6\n## 7             JetBlue 0.3952452     7\n## 8              Alaska 0.4368132     8\n## 9           Southwest 0.4644557     9\n## 10               Mesa 0.4743590    10\n## 11        Continental 0.4907385    11\n## 12         ExpressJet 0.4943420    12\n## 13             United 0.4963109    13\n## 14            SkyWest 0.5350105    14\n## 15           Frontier 0.5564904    15\n\n\n\n# Solution to second instruction\nhflights %\n%\n    filter(!is.na(ArrDelay), ArrDelay \n 0) %\n%\n    group_by(UniqueCarrier) %\n%\n    summarise(avg = mean(ArrDelay)) %\n%\n    mutate(rank = rank(avg)) %\n%\n    arrange(rank)\n\n\n\n\n## # A tibble: 15 \u00d7 3\n##         UniqueCarrier      avg  rank\n##                 \nchr\n    \ndbl\n \ndbl\n\n## 1                Mesa 18.67568     1\n## 2            Frontier 18.68683     2\n## 3          US_Airways 20.70235     3\n## 4         Continental 22.13374     4\n## 5              Alaska 22.91195     5\n## 6             SkyWest 24.14663     6\n## 7          ExpressJet 24.19337     7\n## 8           Southwest 25.27750     8\n## 9             AirTran 27.85693     9\n## 10           American 28.49740    10\n## 11              Delta 32.12463    11\n## 12             United 32.48067    12\n## 13     American_Eagle 38.75135    13\n## 14 Atlantic_Southeast 40.24231    14\n## 15            JetBlue 45.47744    15\n\n\n\nAdvanced \ngroup_by\n exercises\n\n\n# Which plane (by tail number) flew out of Houston the most times? How many times? adv1\nadv1 \n- hflights %\n%\n          group_by(TailNum) %\n%\n          summarise(n = n()) %\n%\n          filter(n == max(n))\n\n# How many airplanes only flew to one destination from Houston? adv2\nadv2 \n- hflights %\n%\n          group_by(TailNum) %\n%\n          summarise(ndest = n_distinct(Dest)) %\n%\n          filter(ndest == 1) %\n%\n          summarise(nplanes = n())\n\n# Find the most visited destination for each carrier: adv3\nadv3 \n- hflights %\n% \n          group_by(UniqueCarrier, Dest) %\n%\n          summarise(n = n()) %\n%\n          mutate(rank = rank(desc(n))) %\n%\n          filter(rank == 1)\n\n# Find the carrier that travels to each destination the most: adv4\nadv4 \n- hflights %\n% \n          group_by(Dest, UniqueCarrier) %\n%\n          summarise(n = n()) %\n%\n          mutate(rank = rank(desc(n))) %\n%\n          filter(rank == 1)\n\n\n\n\ndplyr\n deals with different types\n\n\n# Use summarise to calculate n_carrier\ns2 \n- hflights %\n%\n    summarise(n_carrier = n_distinct(UniqueCarrier))\n\n\n\n\ndplyr\n and mySQL databases\n\n\nCode only.\n\n\n# set up a src that connects to the mysql database (src_mysql is provided by dplyr)\nmy_db \n- src_mysql(dbname = 'dplyr', \n                  host = 'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                  port = 3306,\n                  user = 'dplyr',\n                  password = 'dplyr')\n\n# and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table\nnycflights \n- tbl(my_db, 'dplyr')\n\n# glimpse at nycflights\nglimpse(nycflights)\n\n# Calculate the grouped summaries detailed in the instructions\nnycflights %\n%\n   group_by(carrier) %\n%\n   summarise(n_flights = n(), avg_delay = mean(arr_delay)) %\n%\n   arrange(avg_delay)\n\n\n\n\nAdding \ntidyr\n Functions\n\n\n\n\ncomplete\n\n\ndrop_na\n\n\nexpand\n\n\nextract\n\n\nextract_numeric\n\n\ncomplete\n\n\nfill\n\n\nfull_seq\n\n\ngather\n\n\nnest\n\n\nreplace_na\n\n\nseparate\n\n\nseparate_rows\n\n\nseparate_rows_\n\n\nsmiths\n\n\nspread\n\n\ntable1\n\n\nunite\n\n\nunnest\n\n\nwho\n\n\n\n\n\n\nExtension: \nJoining Data in R with \ndplyr\n\n\n## 1, Mutating Joins\n\n## 2, Filtering Joins and Set Operations\n\n## 3, Assembling Data\n\n## 4, Advanced Joining\n\n## 5, Case Study", 
            "title": "Data Wrangling"
        }, 
        {
            "location": "/Data_Wrangling/#data-analysis-in-r-the-datatable-way", 
            "text": "", 
            "title": "Data Analysis in R, the data.table Way"
        }, 
        {
            "location": "/Data_Wrangling/#1-datatable-novice", 
            "text": "Find out more with  ?data.table .  Create and subset a  data.table  # The data.table package\nlibrary(data.table)\n\n# Create my_first_data_table\nmy_first_data_table  - data.table(x = c('a', 'b', 'c', 'd', 'e'), y = c(1, 2, 3, 4, 5))\n\n# Create a data.table using recycling\nDT  - data.table(a = 1:2, b = c('A', 'B', 'C', 'D'))\n\n# Print the third row to the console\nDT[3,]  ##    a b\n## 1: 1 C  # Print the second and third row to the console, but do not commas\nDT[2:3]  ##    a b\n## 1: 2 B\n## 2: 1 C  Getting to know a  data.table  Like  head ,  tail .  # Print the penultimate row of DT using .N\nDT[.N - 1]  ##    a b\n## 1: 1 C  # Print the column names of DT, and number of rows and number of columns\ncolnames(DT)  ## [1] \"a\" \"b\"  dim(DT)  ## [1] 4 2  # Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1.\nDT[c(2, 2, 3)]  ##    a b\n## 1: 2 B\n## 2: 2 B\n## 3: 1 C  DT  is a data.table/data.frame, but  DT[ , B]  is a vector;  DT[ , .(B)]  is a subsetted data.table.  Subsetting data tables  DT[i, j, by]  means take  DT , subset rows using  i , then calculate  j  grouped by  by . You can wrap  j  with  .() .  A  - c(1, 2, 3, 4, 5)\nB  - c('a', 'b', 'c', 'd', 'e')\nC  - c(6, 7, 8, 9, 10)\nDT  - data.table(A, B, C)\n\n# Subset rows 1 and 3, and columns B and C\nDT[c(1,3) ,.(B, C)]  ##    B C\n## 1: a 6\n## 2: c 8  # Assign to ans the correct value\nans  - data.table(DT[, .(B, val = A * C)])\n\n# Fill in the blanks such that ans2 equals target\n#target  - data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'),  val = as.integer(c(6:10, 1:5)))\nans2  - data.table(DT[, .(B, val = as.integer(c(6:10, 1:5)))])  The  by  basics  # iris and iris3 are already available in the workspace\n\n# Convert iris to a data.table: DT\nDT  - as.data.table(iris)\n\n# For each Species, print the mean Sepal.Length\nDT[, .(mean(Sepal.Length)), by = .(Species)]  ##       Species    V1\n## 1:     setosa 5.006\n## 2: versicolor 5.936\n## 3:  virginica 6.588  # Print mean Sepal.Length, grouping by first letter of Species\nDT[, .(mean(Sepal.Length)), by = .(substr(Species, 1,1))]  ##    substr    V1\n## 1:      s 5.006\n## 2:      v 6.262  Using  .N  and  by  .N , number, in row or column.  # data.table version of iris: DT\nDT  - as.data.table(iris)\n\n# Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group.\nDT[, .N, by = 10 * round(Sepal.Length * Sepal.Width / 10)]  ##    round   N\n## 1:    20 117\n## 2:    10  29\n## 3:    30   4  # Now name the output columns `Area` and `Count`\nDT[, .(Count = .N), by = .(Area = 10 * round(Sepal.Length * Sepal.Width / 10))]  ##    Area Count\n## 1:   20   117\n## 2:   10    29\n## 3:   30     4  Return multiple numbers in  j  # Create the data.table DT\nset.seed(1L)\nDT  - data.table(A = rep(letters[2:1], each = 4L), B = rep(1:4, each = 2L), C = sample(8))\n\n# Create the new data.table, DT2\nDT2  - DT[, .(C = cumsum(C)), by=.(A,B)]\n\n# Select from DT2 the last two values from C while you group by A\nDT2[, .(C = tail(C,2)), by=A]  ##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8", 
            "title": "1, data.table novice"
        }, 
        {
            "location": "/Data_Wrangling/#2-datatable-yeoman", 
            "text": "Chaining, the basics  # Build DT\nset.seed(1L)\nDT  - data.table(A = rep(letters[2:1], each = 4L), B = rep(1:4, each = 2L), C = sample(8)) \nDT  ##    A B C\n## 1: b 1 3\n## 2: b 1 8\n## 3: b 2 4\n## 4: b 2 5\n## 5: a 3 1\n## 6: a 3 7\n## 7: a 4 2\n## 8: a 4 6  # Use chaining\n# Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A\nDT[, .(C = cumsum(C)), by = .(A,B)][, .(C = tail(C,2)), by = .(A)]  ##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8  Chaining your  iris  dataset  DT  - data.table(iris)\n\n# Perform chained operations on DT\nDT[, .(Sepal.Length = median(Sepal.Length), Sepal.Width = median(Sepal.Width), Petal.Length = median(Petal.Length), Petal.Width = median(Petal.Width)), by = .(Species)][order(Species, decreasing = TRUE)]  ##       Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1:  virginica          6.5         3.0         5.55         2.0\n## 2: versicolor          5.9         2.8         4.35         1.3\n## 3:     setosa          5.0         3.4         1.50         0.2  DT  ##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica  Programming time vs readability  x  - c(2, 1, 2, 1, 2, 2, 1)\ny  - c(1, 3, 5, 7, 9, 11, 13)\nz  - c(2, 4, 6, 8, 10, 12, 14)\nDT  - data.table(x, y, z)\n\n# Mean of columns\nDT[, lapply(.SD, mean), by = .(x)]   ##    x        y        z\n## 1: 2 6.500000 7.500000\n## 2: 1 7.666667 8.666667  # Median of columns\nDT[, lapply(.SD, median), by = .(x)]  ##    x y z\n## 1: 2 7 8\n## 2: 1 7 8  Introducing  .SDcols  .SDcols  specifies the columns of  DT  that are included in  .SD .  grp  - c(6, 6, 8, 8, 8)\nQ1  - c(4, 3, 3, 5, 3)\nQ2  - c(1, 4, 1, 4, 4)\nQ3  - c(3, 1, 5, 5, 2)\nH1  - c(1, 2, 3, 2, 4)\nH2  - c(1, 4, 3, 4, 3)\nDT  - data.table(grp, Q1, Q2, Q3, H1, H2)\n\n# Calculate the sum of the Q columns\nDT[, lapply(.SD, sum), .SDcols = 2:4]  ##    Q1 Q2 Q3\n## 1: 18 14 16  # Calculate the sum of columns H1 and H2 \nDT[, lapply(.SD, sum), .SDcols = 5:6]  ##    H1 H2\n## 1: 12 15  # Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns. \nDT[, .SD[-1], .SDcols = 2:4, by = .(grp)]  ##    grp Q1 Q2 Q3\n## 1:   6  3  4  1\n## 2:   8  5  4  5\n## 3:   8  3  4  2  Mixing it together:  lapply ,  .SD ,  SDcols  and  .N  x  - c(2, 1, 2, 1, 2, 2, 1)\ny  - c(1, 3, 5, 7, 9, 11, 13)\nz  - c(2, 4, 6, 8, 10, 12, 14)\nDT  - data.table(x, y, z)\n\n# Sum of all columns and the number of rows\n# For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together.\nDT  ##    x  y  z\n## 1: 2  1  2\n## 2: 1  3  4\n## 3: 2  5  6\n## 4: 1  7  8\n## 5: 2  9 10\n## 6: 2 11 12\n## 7: 1 13 14  DT[,c(lapply(.SD, sum), .N), .SDcols = 1:3, by = x]  ##    x x  y  z N\n## 1: 2 8 26 30 4\n## 2: 1 3 23 26 3  # Cumulative sum of column x and y while grouping by x and z   8\nDT[,lapply(.SD, cumsum), .SDcols = 1:2, by = .(by1 = x, by2 = z   8)]  ##    by1   by2 x  y\n## 1:   2 FALSE 2  1\n## 2:   2 FALSE 4  6\n## 3:   1 FALSE 1  3\n## 4:   1 FALSE 2 10\n## 5:   2  TRUE 2  9\n## 6:   2  TRUE 4 20\n## 7:   1  TRUE 1 13  # Chaining\nDT[,lapply(.SD, cumsum), .SDcols = 1:2, by = .(by1 = x, by2 = z   8)][,lapply(.SD, max), .SDcols = 3:4, by = by1]  ##    by1 x  y\n## 1:   2 4 20\n## 2:   1 2 13  Adding, updating and removing columns  :=  is defined for use in  j  only.  # The data.table DT\nDT  - data.table(A = letters[c(1, 1, 1, 2, 2)], B = 1:5)\nDT  ##    A B\n## 1: a 1\n## 2: a 2\n## 3: a 3\n## 4: b 4\n## 5: b 5  # Add column by reference: Total\nDT[, ('Total') := sum(B), by = .(A)]  ##    A B Total\n## 1: a 1     6\n## 2: a 2     6\n## 3: a 3     6\n## 4: b 4     9\n## 5: b 5     9  # Add 1 to column B\nDT[c(2,4), ('B') := as.integer(1 + B)]  ##    A B Total\n## 1: a 1     6\n## 2: a 3     6\n## 3: a 3     6\n## 4: b 5     9\n## 5: b 5     9  # Add a new column Total2\nDT[2:4, ':='(Total2 = sum(B)), by = .(A)]  ##    A B Total Total2\n## 1: a 1     6     NA\n## 2: a 3     6      6\n## 3: a 3     6      6\n## 4: b 5     9      5\n## 5: b 5     9     NA  # Remove the Total column\nDT[, Total := NULL]  ##    A B Total2\n## 1: a 1     NA\n## 2: a 3      6\n## 3: a 3      6\n## 4: b 5      5\n## 5: b 5     NA  # Select the third column using `[[`\nDT[[3]]  ## [1] NA  6  6  5 NA  The functional form  # A data.table DT\nDT  - data.table(A = c(1, 1, 1, 2, 2), B = 1:5)\n\n# Update B, add C and D\nDT[, `:=`(B = B + 1,  C = A + B, D = 2)]  ##    A B C D\n## 1: 1 2 2 2\n## 2: 1 3 3 2\n## 3: 1 4 4 2\n## 4: 2 5 6 2\n## 5: 2 6 7 2  # Delete my_cols\nmy_cols  - c('B', 'C')\nDT[, (my_cols) := NULL]  ##    A D\n## 1: 1 2\n## 2: 1 2\n## 3: 1 2\n## 4: 2 2\n## 5: 2 2  # Delete column 2 by number\nDT[, 2 := NULL]  ##    A\n## 1: 1\n## 2: 1\n## 3: 1\n## 4: 2\n## 5: 2  Ready,  set , go!  The  set  function is used to repeatedly update a data.table by \nreference. You can think of the  set  function as a loopable.  A  - c(2, 2, 3, 5, 2, 5, 5, 4, 4, 1)\nB  - c(2, 1, 4, 2, 4, 3, 4, 5, 2, 4)\nC  - c(5, 2, 4, 1, 2, 2, 1, 2, 5, 2)\nD  - c(3, 3, 3, 1, 5, 4, 4, 1, 4, 3)\nDT  - data.table(A, B, C, D)\n\n# Set the seed\nset.seed(1)\n\n# Check the DT\nDT  ##     A B C D\n##  1: 2 2 5 3\n##  2: 2 1 2 3\n##  3: 3 4 4 3\n##  4: 5 2 1 1\n##  5: 2 4 2 5\n##  6: 5 3 2 4\n##  7: 5 4 1 4\n##  8: 4 5 2 1\n##  9: 4 2 5 4\n## 10: 1 4 2 3  # For loop with set\nfor (l in 2:4) set(DT, sample(10,3), l, NA)\n\n# Change the column names to lowercase\nsetnames(DT,c('A','B','C','D'), c('a','b','c','d'))\n\n# Print the resulting DT to the console\nDT  ##     a  b  c  d\n##  1: 2  2  5  3\n##  2: 2  1 NA  3\n##  3: 3 NA  4  3\n##  4: 5 NA  1  1\n##  5: 2 NA  2  5\n##  6: 5  3  2 NA\n##  7: 5  4  1  4\n##  8: 4  5 NA  1\n##  9: 4  2  5 NA\n## 10: 1  4 NA NA  The  set  family  # Define DT\nDT  - data.table(a = letters[c(1, 1, 1, 2, 2)], b = 1)\nDT  ##    a b\n## 1: a 1\n## 2: a 1\n## 3: a 1\n## 4: b 1\n## 5: b 1  # Add a postfix '_2' to all column names\nsetnames(DT, c(1:2), paste0(c('a','b'), '_2'))\nDT  ##    a_2 b_2\n## 1:   a   1\n## 2:   a   1\n## 3:   a   1\n## 4:   b   1\n## 5:   b   1  # Change column name 'a_2' to 'A2'\nsetnames(DT, 'a_2', 'A2')\nDT  ##    A2 b_2\n## 1:  a   1\n## 2:  a   1\n## 3:  a   1\n## 4:  b   1\n## 5:  b   1  # Reverse the order of the columns\nsetcolorder(DT, c('b_2','A2'))", 
            "title": "2, data.table yeoman"
        }, 
        {
            "location": "/Data_Wrangling/#3-datatable-expert", 
            "text": "Selecting rows the  data.table  way  # Convert iris to a data.table\niris  - data.table('Sepal.Length' = iris$Sepal.Length, 'Sepal.Width' = iris$Sepal.Width, 'Petal.Length' = iris$Petal.Length, 'Petal.Width' = iris$Petal.Width, 'Species' = iris$Species)\niris  ##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica  # Species is 'virginica'\nhead(iris[Species == 'virginica'], 20)  ##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##  1:          6.3         3.3          6.0         2.5 virginica\n##  2:          5.8         2.7          5.1         1.9 virginica\n##  3:          7.1         3.0          5.9         2.1 virginica\n##  4:          6.3         2.9          5.6         1.8 virginica\n##  5:          6.5         3.0          5.8         2.2 virginica\n##  6:          7.6         3.0          6.6         2.1 virginica\n##  7:          4.9         2.5          4.5         1.7 virginica\n##  8:          7.3         2.9          6.3         1.8 virginica\n##  9:          6.7         2.5          5.8         1.8 virginica\n## 10:          7.2         3.6          6.1         2.5 virginica\n## 11:          6.5         3.2          5.1         2.0 virginica\n## 12:          6.4         2.7          5.3         1.9 virginica\n## 13:          6.8         3.0          5.5         2.1 virginica\n## 14:          5.7         2.5          5.0         2.0 virginica\n## 15:          5.8         2.8          5.1         2.4 virginica\n## 16:          6.4         3.2          5.3         2.3 virginica\n## 17:          6.5         3.0          5.5         1.8 virginica\n## 18:          7.7         3.8          6.7         2.2 virginica\n## 19:          7.7         2.6          6.9         2.3 virginica\n## 20:          6.0         2.2          5.0         1.5 virginica  # Species is either 'virginica' or 'versicolor'\nhead(iris[Species %in% c('virginica', 'versicolor')], 20)  ##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n##  1:          7.0         3.2          4.7         1.4 versicolor\n##  2:          6.4         3.2          4.5         1.5 versicolor\n##  3:          6.9         3.1          4.9         1.5 versicolor\n##  4:          5.5         2.3          4.0         1.3 versicolor\n##  5:          6.5         2.8          4.6         1.5 versicolor\n##  6:          5.7         2.8          4.5         1.3 versicolor\n##  7:          6.3         3.3          4.7         1.6 versicolor\n##  8:          4.9         2.4          3.3         1.0 versicolor\n##  9:          6.6         2.9          4.6         1.3 versicolor\n## 10:          5.2         2.7          3.9         1.4 versicolor\n## 11:          5.0         2.0          3.5         1.0 versicolor\n## 12:          5.9         3.0          4.2         1.5 versicolor\n## 13:          6.0         2.2          4.0         1.0 versicolor\n## 14:          6.1         2.9          4.7         1.4 versicolor\n## 15:          5.6         2.9          3.6         1.3 versicolor\n## 16:          6.7         3.1          4.4         1.4 versicolor\n## 17:          5.6         3.0          4.5         1.5 versicolor\n## 18:          5.8         2.7          4.1         1.0 versicolor\n## 19:          6.2         2.2          4.5         1.5 versicolor\n## 20:          5.6         2.5          3.9         1.1 versicolor  Removing columns and adapting your column names  Refer to a regex cheat sheet for metacharacter.  # iris as a data.table\niris  - as.data.table(iris)\niris  ##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica  # Remove the 'Sepal.' prefix\n#gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x\nsetnames(iris, c('Sepal.Length', 'Sepal.Width'), c('Length','Width')) \n#gsub('^Sepal\\\\.','', iris)\n\n# Remove the two columns starting with 'Petal'\niris[, c('Petal.Length', 'Petal.Width') := NULL]  ##      Length Width   Species\n##   1:    5.1   3.5    setosa\n##   2:    4.9   3.0    setosa\n##   3:    4.7   3.2    setosa\n##   4:    4.6   3.1    setosa\n##   5:    5.0   3.6    setosa\n##  ---                       \n## 146:    6.7   3.0 virginica\n## 147:    6.3   2.5 virginica\n## 148:    6.5   3.0 virginica\n## 149:    6.2   3.4 virginica\n## 150:    5.9   3.0 virginica  Understanding automatic indexing  # Cleaned up iris data.table\niris2  - data.frame(Length = iris$Sepal.Length, Width = iris$Sepal.Width, Species = iris$Species)\niris2  - as.data.table(iris2)  # Area is greater than 20 square centimeters\niris2[ Width * Length   20 ], 20     Length Width    Species is_large\n 1:    5.4   3.9     setosa    FALSE\n 2:    5.8   4.0     setosa    FALSE\n 3:    5.7   4.4     setosa     TRUE\n 4:    5.4   3.9     setosa    FALSE\n 5:    5.7   3.8     setosa    FALSE\n 6:    5.2   4.1     setosa    FALSE\n 7:    5.5   4.2     setosa    FALSE\n 8:    7.0   3.2 versicolor    FALSE\n 9:    6.4   3.2 versicolor    FALSE\n10:    6.9   3.1 versicolor    FALSE\n11:    6.3   3.3 versicolor    FALSE\n12:    6.7   3.1 versicolor    FALSE\n13:    6.7   3.0 versicolor    FALSE\n14:    6.0   3.4 versicolor    FALSE\n15:    6.7   3.1 versicolor    FALSE\n16:    6.3   3.3  virginica    FALSE\n17:    7.1   3.0  virginica    FALSE\n18:    7.6   3.0  virginica    FALSE\n19:    7.3   2.9  virginica    FALSE\n20:    7.2   3.6  virginica     TRUE\n...  # Add new boolean column\niris2[, is_large := Width * Length   25]  # Now large observations with is_large\niris2[is_large == TRUE]     Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE  iris2[(is_large)] # Also OK     Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE  Selecting groups or parts of groups  # The 'keyed' data.table DT\nDT  - data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],\n                 B = c(5, 4, 1, 9,8 ,8, 6), \n                 C = 6:12)\nsetkey(DT, A, B)\n\n# Select the 'b' group\nDT['b']  ##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11  # 'b' and 'c' groups\nDT[c('b', 'c')]  ##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11\n## 4: c 6 12\n## 5: c 9  9  # The first row of the 'b' and 'c' groups\nDT[c('b', 'c'), mult = 'first']  ##    A B  C\n## 1: b 1  8\n## 2: c 6 12  # First and last row of the 'b' and 'c' groups\nDT[c('b', 'c'), .SD[c(1, .N)], by = .EACHI]  ##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9  # Copy and extend code for instruction 4: add printout\nDT[c('b', 'c'), { print(.SD); .SD[c(1, .N)] }, by = .EACHI]  ##    B  C\n## 1: 1  8\n## 2: 5  6\n## 3: 8 11\n##    B  C\n## 1: 6 12\n## 2: 9  9\n\n##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9  Rolling joins - part one  # Keyed data.table DT\nDT  - data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],                  B = c(5, 4, 1, 9, 8, 8, 6), \n                 C = 6:12, \n                 key = 'A,B')\n\n# Get the key of DT\nkey(DT)  ## [1] \"A\" \"B\"  # Row where A == 'b'   B == 6\nsetkey(DT, A, B)\nDT[.('b', 6)]  ##    A B  C\n## 1: b 6 NA  # Return the prevailing row\nDT[.('b',6), roll = TRUE]  ##    A B C\n## 1: b 6 6  # Return the nearest row\nDT[.('b',6), roll =+ Inf]  ##    A B C\n## 1: b 6 6  Rolling joins - part two  # Keyed data.table DT\nDT  - data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)],                  B = c(5, 4, 1, 9, 8, 8, 6), \n                 C = 6:12, \n                 key = 'A,B')\n\n# Look at the sequence (-2):10 for the 'b' group\nDT[.('b', (-2):10)]  ##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2 NA\n##  6: b  3 NA\n##  7: b  4 NA\n##  8: b  5  6\n##  9: b  6 NA\n## 10: b  7 NA\n## 11: b  8 11\n## 12: b  9 NA\n## 13: b 10 NA  # Add code: carry the prevailing values forwards\nDT[.('b', (-2):10), roll = TRUE]  ##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11  # Add code: carry the first observation backwards\nDT[.('b', (-2):10), roll = TRUE, rollends = TRUE]  ##     A  B  C\n##  1: b -2  8\n##  2: b -1  8\n##  3: b  0  8\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11", 
            "title": "3, data.table expert"
        }, 
        {
            "location": "/Data_Wrangling/#data-manipulation-in-r-with-dplyr", 
            "text": "", 
            "title": "Data Manipulation in R with dplyr"
        }, 
        {
            "location": "/Data_Wrangling/#1-introduction-to-dplyr", 
            "text": "Load the  dplyr  and  hflights  package  # Load the dplyr package\nlibrary(dplyr)\nlibrary(dtplyr)\n\n# Load the hflights package\n# A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011\nlibrary(hflights)\n\n# Call both head() and summary() on hflights\nhead(hflights)  ##      Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## 5424 2011     1          1         6    1400    1500            AA\n## 5425 2011     1          2         7    1401    1501            AA\n## 5426 2011     1          3         1    1352    1502            AA\n## 5427 2011     1          4         2    1403    1513            AA\n## 5428 2011     1          5         3    1405    1507            AA\n## 5429 2011     1          6         4    1359    1503            AA\n##      FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin\n## 5424       428  N576AA                60      40      -10        0    IAH\n## 5425       428  N557AA                60      45       -9        1    IAH\n## 5426       428  N541AA                70      48       -8       -8    IAH\n## 5427       428  N403AA                70      39        3        3    IAH\n## 5428       428  N492AA                62      44       -3        5    IAH\n## 5429       428  N262AA                64      45       -7       -1    IAH\n##      Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted\n## 5424  DFW      224      7      13         0                         0\n## 5425  DFW      224      6       9         0                         0\n## 5426  DFW      224      5      17         0                         0\n## 5427  DFW      224      9      22         0                         0\n## 5428  DFW      224      9       9         0                         0\n## 5429  DFW      224      6      13         0                         0  summary(hflights)  ##       Year          Month          DayofMonth      DayOfWeek    \n##  Min.   :2011   Min.   : 1.000   Min.   : 1.00   Min.   :1.000  \n##  1st Qu.:2011   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:2.000  \n##  Median :2011   Median : 7.000   Median :16.00   Median :4.000  \n##  Mean   :2011   Mean   : 6.514   Mean   :15.74   Mean   :3.948  \n##  3rd Qu.:2011   3rd Qu.: 9.000   3rd Qu.:23.00   3rd Qu.:6.000  \n##  Max.   :2011   Max.   :12.000   Max.   :31.00   Max.   :7.000  \n##                                                                 \n##     DepTime        ArrTime     UniqueCarrier        FlightNum   \n##  Min.   :   1   Min.   :   1   Length:227496      Min.   :   1  \n##  1st Qu.:1021   1st Qu.:1215   Class :character   1st Qu.: 855  \n##  Median :1416   Median :1617   Mode  :character   Median :1696  \n##  Mean   :1396   Mean   :1578                      Mean   :1962  \n##  3rd Qu.:1801   3rd Qu.:1953                      3rd Qu.:2755  \n##  Max.   :2400   Max.   :2400                      Max.   :7290  \n##  NA's   :2905   NA's   :3066                                    \n##    TailNum          ActualElapsedTime    AirTime         ArrDelay      \n##  Length:227496      Min.   : 34.0     Min.   : 11.0   Min.   :-70.000  \n##  Class :character   1st Qu.: 77.0     1st Qu.: 58.0   1st Qu.: -8.000  \n##  Mode  :character   Median :128.0     Median :107.0   Median :  0.000  \n##                     Mean   :129.3     Mean   :108.1   Mean   :  7.094  \n##                     3rd Qu.:165.0     3rd Qu.:141.0   3rd Qu.: 11.000  \n##                     Max.   :575.0     Max.   :549.0   Max.   :978.000  \n##                     NA's   :3622      NA's   :3622    NA's   :3622     \n##     DepDelay          Origin              Dest              Distance     \n##  Min.   :-33.000   Length:227496      Length:227496      Min.   :  79.0  \n##  1st Qu.: -3.000   Class :character   Class :character   1st Qu.: 376.0  \n##  Median :  0.000   Mode  :character   Mode  :character   Median : 809.0  \n##  Mean   :  9.445                                         Mean   : 787.8  \n##  3rd Qu.:  9.000                                         3rd Qu.:1042.0  \n##  Max.   :981.000                                         Max.   :3904.0  \n##  NA's   :2905                                                            \n##      TaxiIn           TaxiOut         Cancelled       CancellationCode  \n##  Min.   :  1.000   Min.   :  1.00   Min.   :0.00000   Length:227496     \n##  1st Qu.:  4.000   1st Qu.: 10.00   1st Qu.:0.00000   Class :character  \n##  Median :  5.000   Median : 14.00   Median :0.00000   Mode  :character  \n##  Mean   :  6.099   Mean   : 15.09   Mean   :0.01307                     \n##  3rd Qu.:  7.000   3rd Qu.: 18.00   3rd Qu.:0.00000                     \n##  Max.   :165.000   Max.   :163.00   Max.   :1.00000                     \n##  NA's   :3066      NA's   :2947                                         \n##     Diverted       \n##  Min.   :0.000000  \n##  1st Qu.:0.000000  \n##  Median :0.000000  \n##  Mean   :0.002853  \n##  3rd Qu.:0.000000  \n##  Max.   :1.000000  \n##  Convert  data.frame  to table  # Convert the hflights data.frame into a hflights tbl\nhflights  - tbl_df(hflights)\n\n# Display the hflights tbl\nhflights  ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *   int   int        int       int     int     int           chr \n## 1   2011     1          1         6    1400    1500            AA\n## 2   2011     1          2         7    1401    1501            AA\n## 3   2011     1          3         1    1352    1502            AA\n## 4   2011     1          4         2    1403    1513            AA\n## 5   2011     1          5         3    1405    1507            AA\n## 6   2011     1          6         4    1359    1503            AA\n## 7   2011     1          7         5    1359    1509            AA\n## 8   2011     1          8         6    1355    1454            AA\n## 9   2011     1          9         7    1443    1554            AA\n## 10  2011     1         10         1    1443    1553            AA\n## # ... with 227,486 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # Create the object carriers, containing only the UniqueCarrier variable of hflights\ncarriers  - hflights$UniqueCarrier  Changing labels of  hflights , part 1 of 2  # add\nlut  - c('AA' = 'American', 'AS' = 'Alaska', 'B6' = 'JetBlue', 'CO' = 'Continental', \n         'DL' = 'Delta', 'OO' = 'SkyWest', 'UA' = 'United', 'US' = 'US_Airways', \n         'WN' = 'Southwest', 'EV' = 'Atlantic_Southeast', 'F9' = 'Frontier', \n         'FL' = 'AirTran', 'MQ' = 'American_Eagle', 'XE' = 'ExpressJet', 'YV' = 'Mesa')\n\n# Use lut to translate the UniqueCarrier column of hflights\nhflights$UniqueCarrier  - lut[hflights$UniqueCarrier]\n\n# Inspect the resulting raw values of your variables\nglimpse(hflights)  ## Observations: 227,496\n## Variables: 21\n## $ Year               int  2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month              int  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth         int  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek          int  6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime            int  1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime            int  1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier      chr  \"American\", \"American\", \"American\", \"America...\n## $ FlightNum          int  428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum            chr  \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime  int  60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime            int  40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay           int  -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay           int  0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin             chr  \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest               chr  \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance           int  224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn             int  7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut            int  13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled          int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode   chr  \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted           int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  Changing labels of  hflights , part 2 of 2  # Build the lookup table: lut\nlut  - c( A  =  carrier ,  B  =  weather  , C  =  FFA  , D  =  security ,  E  =  not cancelled )\n\n# Add the Code column\nhflights$Code  - lut[hflights$CancellationCode]\n\n# Glimpse at hflights\nglimpse(hflights)  Result.  Observations: 227,496\nVariables: 22\n$ Year               int  2011, 2011, 2011, 2011, 2011, 2011,...\n$ Month              int  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n$ DayofMonth         int  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...\n$ DayOfWeek          int  6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,...\n$ DepTime            int  1400, 1401, 1352, 1403, 1405, 1359,...\n$ ArrTime            int  1500, 1501, 1502, 1513, 1507, 1503,...\n$ UniqueCarrier      chr   American ,  American ,  American ,...\n$ FlightNum          int  428, 428, 428, 428, 428, 428, 428, ...\n$ TailNum            chr   N576AA ,  N557AA ,  N541AA ,  N403...\n$ ActualElapsedTime  int  60, 60, 70, 70, 62, 64, 70, 59, 71,...\n$ AirTime            int  40, 45, 48, 39, 44, 45, 43, 40, 41,...\n$ ArrDelay           int  -10, -9, -8, 3, -3, -7, -1, -16, 44...\n$ DepDelay           int  0, 1, -8, 3, 5, -1, -1, -5, 43, 43,...\n$ Origin             chr   IAH ,  IAH ,  IAH ,  IAH ,  IAH , ...\n$ Dest               chr   DFW ,  DFW ,  DFW ,  DFW ,  DFW , ...\n$ Distance           int  224, 224, 224, 224, 224, 224, 224, ...\n$ TaxiIn             int  7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4...\n$ TaxiOut            int  13, 9, 17, 22, 9, 13, 15, 12, 22, 1...\n$ Cancelled          int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ CancellationCode   chr   ,  ,  ,  ,  ,  ,  ,  ,  ,...\n$ Diverted           int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ Code               chr  NA, NA, NA, NA, NA, NA, NA, NA, NA,...", 
            "title": "1, Introduction to dplyr"
        }, 
        {
            "location": "/Data_Wrangling/#2-select-and-mutate", 
            "text": "The five verbs and their meaning   select ; which returns a subset of the columns.  filter ; that is able to return a subset of the rows.  arrange ; that reorders the rows according to single or \n    multiple variables.  mutate ; used to add columns from existing data.  summarise ; which reduces each group to a single row by calculating \n    aggregate measures.   Choosing is not losing! The  select  verb  # Print out a tbl with the four columns of hflights related to delay\nselect(hflights, ActualElapsedTime, AirTime, ArrDelay, DepDelay)  ## # A tibble: 227,496 \u00d7 4\n##    ActualElapsedTime AirTime ArrDelay DepDelay\n## *               int     int      int      int \n## 1                 60      40      -10        0\n## 2                 60      45       -9        1\n## 3                 70      48       -8       -8\n## 4                 70      39        3        3\n## 5                 62      44       -3        5\n## 6                 64      45       -7       -1\n## 7                 70      43       -1       -1\n## 8                 59      40      -16       -5\n## 9                 71      41       44       43\n## 10                70      45       43       43\n## # ... with 227,486 more rows  # Print out hflights, nothing has changed!\nhflights  ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *   int   int        int       int     int     int           chr \n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # Print out the columns Origin up to Cancelled of hflights\nselect(hflights, 14:19)  ## # A tibble: 227,496 \u00d7 6\n##    Origin  Dest Distance TaxiIn TaxiOut Cancelled\n## *    chr   chr      int    int     int       int \n## 1     IAH   DFW      224      7      13         0\n## 2     IAH   DFW      224      6       9         0\n## 3     IAH   DFW      224      5      17         0\n## 4     IAH   DFW      224      9      22         0\n## 5     IAH   DFW      224      9       9         0\n## 6     IAH   DFW      224      6      13         0\n## 7     IAH   DFW      224     12      15         0\n## 8     IAH   DFW      224      7      12         0\n## 9     IAH   DFW      224      8      22         0\n## 10    IAH   DFW      224      6      19         0\n## # ... with 227,486 more rows  # Answer to last question: be concise!\nselect(hflights, 1:4, 12:21)  ## # A tibble: 227,496 \u00d7 14\n##     Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin  Dest\n## *   int   int        int       int      int      int    chr   chr \n## 1   2011     1          1         6      -10        0    IAH   DFW\n## 2   2011     1          2         7       -9        1    IAH   DFW\n## 3   2011     1          3         1       -8       -8    IAH   DFW\n## 4   2011     1          4         2        3        3    IAH   DFW\n## 5   2011     1          5         3       -3        5    IAH   DFW\n## 6   2011     1          6         4       -7       -1    IAH   DFW\n## 7   2011     1          7         5       -1       -1    IAH   DFW\n## 8   2011     1          8         6      -16       -5    IAH   DFW\n## 9   2011     1          9         7       44       43    IAH   DFW\n## 10  2011     1         10         1       43       43    IAH   DFW\n## # ... with 227,486 more rows, and 6 more variables: Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   Helper functions for variable selection  select :   starts_with(\"X\") ; every name that starts with  \"X\" ,  ends_with(\"X\") ; every name that ends with  \"X\" ,  contains(\"X\") ; every name that contains  \"X\" ,  matches(\"X\") ; every name that matches  \"X\" , where  \"X\"  can be a \n    regular expression,  num_range(\"x\", 1:5) ; the variables named  x01 ,  x02 ,  x03 , -  x04  and  x05 ,  one_of(x) ; every name that appears in  x , which should be a \n    character vector.    # Print out a tbl containing just ArrDelay and DepDelay\nselect(hflights, ArrDelay, DepDelay)  ## # A tibble: 227,496 \u00d7 2\n##    ArrDelay DepDelay\n## *      int      int \n## 1       -10        0\n## 2        -9        1\n## 3        -8       -8\n## 4         3        3\n## 5        -3        5\n## 6        -7       -1\n## 7        -1       -1\n## 8       -16       -5\n## 9        44       43\n## 10       43       43\n## # ... with 227,486 more rows  # Print out a tbl as described in the second instruction, using both helper functions and variable names\nselect(hflights, UniqueCarrier, ends_with('Num'), starts_with('Cancel'))  ## # A tibble: 227,496 \u00d7 5\n##    UniqueCarrier FlightNum TailNum Cancelled CancellationCode\n## *           chr       int     chr       int              chr \n## 1       American       428  N576AA         0                 \n## 2       American       428  N557AA         0                 \n## 3       American       428  N541AA         0                 \n## 4       American       428  N403AA         0                 \n## 5       American       428  N492AA         0                 \n## 6       American       428  N262AA         0                 \n## 7       American       428  N493AA         0                 \n## 8       American       428  N477AA         0                 \n## 9       American       428  N476AA         0                 \n## 10      American       428  N504AA         0                 \n## # ... with 227,486 more rows  # Print out a tbl as described in the third instruction, using only helper functions.\nselect(hflights, ends_with('Time'), ends_with('Delay'))  ## # A tibble: 227,496 \u00d7 6\n##    DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay\n## *     int     int               int     int      int      int \n## 1     1400    1500                60      40      -10        0\n## 2     1401    1501                60      45       -9        1\n## 3     1352    1502                70      48       -8       -8\n## 4     1403    1513                70      39        3        3\n## 5     1405    1507                62      44       -3        5\n## 6     1359    1503                64      45       -7       -1\n## 7     1359    1509                70      43       -1       -1\n## 8     1355    1454                59      40      -16       -5\n## 9     1443    1554                71      41       44       43\n## 10    1443    1553                70      45       43       43\n## # ... with 227,486 more rows  Comparison to basic R  # add\nex1r  - hflights[c('TaxiIn','TaxiOut','Distance')]\n\nex1d  - select(hflights, starts_with('Taxi'), Distance)\n\nex2r  - hflights[c('Year','Month','DayOfWeek','DepTime','ArrTime')]\n\nex2d  - select(hflights, Year, Month, DayOfWeek, DepTime, ArrTime)\n\nex3r  - hflights[c('TailNum','TaxiIn','TaxiOut')]\n\nex3d  - select(hflights, TailNum, starts_with('Taxi'))  Mutating is creating  # Add the new variable ActualGroundTime to a copy of hflights and save the result as g1\ng1  - mutate(hflights, ActualGroundTime = ActualElapsedTime - AirTime)\nglimpse(hflights)  ## Observations: 227,496\n## Variables: 21\n## $ Year               int  2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month              int  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth         int  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek          int  6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime            int  1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime            int  1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier      chr  \"American\", \"American\", \"American\", \"America...\n## $ FlightNum          int  428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum            chr  \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime  int  60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime            int  40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay           int  -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay           int  0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin             chr  \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest               chr  \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance           int  224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn             int  7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut            int  13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled          int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode   chr  \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted           int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  glimpse(g1)  ## Observations: 227,496\n## Variables: 22\n## $ Year               int  2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month              int  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth         int  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek          int  6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime            int  1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime            int  1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier      chr  \"American\", \"American\", \"American\", \"America...\n## $ FlightNum          int  428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum            chr  \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime  int  60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime            int  40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay           int  -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay           int  0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin             chr  \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest               chr  \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance           int  224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn             int  7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut            int  13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled          int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode   chr  \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted           int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ ActualGroundTime   int  20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...  # Add the new variable GroundTime to a g1; save the result as g2\ng2  - mutate(g1, GroundTime = TaxiIn + TaxiOut)\n\nhead(g1$ActualGroundTime == g2$GroundTime, 20)  ##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n## [15] TRUE TRUE TRUE TRUE TRUE TRUE  # Add the new variable AverageSpeed to g2; save the result as g3\ng3  - mutate(g2, AverageSpeed = Distance / AirTime * 60)\ng3  ## # A tibble: 227,496 \u00d7 24\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##     int   int        int       int     int     int           chr \n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 17 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int , ActualGroundTime  int , GroundTime  int ,\n## #   AverageSpeed  dbl   Add multiple variables using mutate  # Add a second variable loss_percent to the dataset: m1\nm1  - mutate(hflights, loss = ArrDelay - DepDelay, loss_percent = (ArrDelay - DepDelay)/DepDelay*100)\n\n# Copy and adapt the previous command to reduce redendancy: m2\nm2  - mutate(hflights, loss = ArrDelay - DepDelay, loss_percent = loss/DepDelay * 100)\n\n# Add the three variables as described in the third instruction: m3\nm3  - mutate(hflights, TotalTaxi = TaxiIn + TaxiOut, ActualGroundTime = ActualElapsedTime - AirTime, Diff = TotalTaxi - ActualGroundTime)\nglimpse(m3)  ## Observations: 227,496\n## Variables: 24\n## $ Year               int  2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month              int  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth         int  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek          int  6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime            int  1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime            int  1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier      chr  \"American\", \"American\", \"American\", \"America...\n## $ FlightNum          int  428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum            chr  \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime  int  60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime            int  40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay           int  -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay           int  0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin             chr  \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest               chr  \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance           int  224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn             int  7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut            int  13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled          int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode   chr  \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted           int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ TotalTaxi          int  20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ ActualGroundTime   int  20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ Diff               int  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...", 
            "title": "2, select and mutate"
        }, 
        {
            "location": "/Data_Wrangling/#3-filter-and-arrange", 
            "text": "Logical operators  filter :   x   y ;  TRUE  if  x  is less than  y .  x  = y ;  TRUE  if  x  is less than or equal to  y .  x == y ;  TRUE  if  x  equals  y .  x != y ;  TRUE  if  x  does not equal  y .  x  = y ;  TRUE  if  x  is greater than or equal to  y .  x   y ;  TRUE  if  x  is greater than  y .  x %in% c(a, b, c) ;  TRUE  if  x  is in the vector  c(a, b, c) .    # All flights that traveled 3000 miles or more\nfilter(hflights, Distance  = 3000)  ## # A tibble: 527 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##     int   int        int       int     int     int           chr \n## 1   2011     1         31         1     924    1413   Continental\n## 2   2011     1         30         7     925    1410   Continental\n## 3   2011     1         29         6    1045    1445   Continental\n## 4   2011     1         28         5    1516    1916   Continental\n## 5   2011     1         27         4     950    1344   Continental\n## 6   2011     1         26         3     944    1350   Continental\n## 7   2011     1         25         2     924    1337   Continental\n## 8   2011     1         24         1    1144    1605   Continental\n## 9   2011     1         23         7     926    1335   Continental\n## 10  2011     1         22         6     942    1340   Continental\n## # ... with 517 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # All flights flown by one of JetBlue, Southwest, or Delta\nfilter(hflights, UniqueCarrier %in% c('JetBlue','Southwest','Delta'))  ## # A tibble: 48,679 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##     int   int        int       int     int     int           chr \n## 1   2011     1          1         6     654    1124       JetBlue\n## 2   2011     1          1         6    1639    2110       JetBlue\n## 3   2011     1          2         7     703    1113       JetBlue\n## 4   2011     1          2         7    1604    2040       JetBlue\n## 5   2011     1          3         1     659    1100       JetBlue\n## 6   2011     1          3         1    1801    2200       JetBlue\n## 7   2011     1          4         2     654    1103       JetBlue\n## 8   2011     1          4         2    1608    2034       JetBlue\n## 9   2011     1          5         3     700    1103       JetBlue\n## 10  2011     1          5         3    1544    1954       JetBlue\n## # ... with 48,669 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # All flights where taxiing took longer than flying\nfilter(hflights, (TaxiIn + TaxiOut)   AirTime)  ## # A tibble: 1,389 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##     int   int        int       int     int     int           chr \n## 1   2011     1         24         1     731     904      American\n## 2   2011     1         30         7    1959    2132      American\n## 3   2011     1         24         1    1621    1749      American\n## 4   2011     1         10         1     941    1113      American\n## 5   2011     1         31         1    1301    1356   Continental\n## 6   2011     1         31         1    2113    2215   Continental\n## 7   2011     1         31         1    1434    1539   Continental\n## 8   2011     1         31         1     900    1006   Continental\n## 9   2011     1         30         7    1304    1408   Continental\n## 10  2011     1         30         7    2004    2128   Continental\n## # ... with 1,379 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   Combining tests using boolean operators  # All flights that departed before 5am or arrived after 10pm\nfilter(hflights, DepTime   500 | ArrTime   2200)  ## # A tibble: 27,799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##     int   int        int       int     int     int           chr \n## 1   2011     1          4         2    2100    2207      American\n## 2   2011     1         14         5    2119    2229      American\n## 3   2011     1         10         1    1934    2235      American\n## 4   2011     1         26         3    1905    2211      American\n## 5   2011     1         30         7    1856    2209      American\n## 6   2011     1          9         7    1938    2228        Alaska\n## 7   2011     1         31         1    1919    2231   Continental\n## 8   2011     1         31         1    2116    2344   Continental\n## 9   2011     1         31         1    1850    2211   Continental\n## 10  2011     1         31         1    2102    2216   Continental\n## # ... with 27,789 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # All flights that departed late but arrived ahead of schedule\nfilter(hflights, DepDelay   0   ArrDelay   0)  ## # A tibble: 27,712 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##     int   int        int       int     int     int           chr \n## 1   2011     1          2         7    1401    1501      American\n## 2   2011     1          5         3    1405    1507      American\n## 3   2011     1         18         2    1408    1508      American\n## 4   2011     1         18         2     721     827      American\n## 5   2011     1         12         3    2015    2113      American\n## 6   2011     1         13         4    2020    2116      American\n## 7   2011     1         26         3    2009    2103      American\n## 8   2011     1          1         6    1631    1736      American\n## 9   2011     1         10         1    1639    1740      American\n## 10  2011     1         12         3    1631    1739      American\n## # ... with 27,702 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # All cancelled weekend flights\nfilter(hflights, DayOfWeek %in% c(6,7)   Cancelled == 1)  ## # A tibble: 585 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##     int   int        int       int     int     int                chr \n## 1   2011     1          9         7      NA      NA           American\n## 2   2011     1         29         6      NA      NA        Continental\n## 3   2011     1          9         7      NA      NA        Continental\n## 4   2011     1          9         7      NA      NA              Delta\n## 5   2011     1          9         7      NA      NA            SkyWest\n## 6   2011     1          2         7      NA      NA          Southwest\n## 7   2011     1         29         6      NA      NA              Delta\n## 8   2011     1          9         7      NA      NA Atlantic_Southeast\n## 9   2011     1          1         6      NA      NA            AirTran\n## 10  2011     1          9         7      NA      NA            AirTran\n## # ... with 575 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # All flights that were cancelled after being delayed\nfilter(hflights, DepDelay   0   Cancelled == 1)  ## # A tibble: 40 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##     int   int        int       int     int     int           chr \n## 1   2011     1         26         3    1926      NA   Continental\n## 2   2011     1         11         2    1100      NA    US_Airways\n## 3   2011     1         19         3    1811      NA    ExpressJet\n## 4   2011     1          7         5    2028      NA    ExpressJet\n## 5   2011     2          4         5    1638      NA      American\n## 6   2011     2          8         2    1057      NA   Continental\n## 7   2011     2          2         3     802      NA    ExpressJet\n## 8   2011     2          9         3     904      NA    ExpressJet\n## 9   2011     2          1         2    1508      NA       SkyWest\n## 10  2011     3         31         4    1016      NA   Continental\n## # ... with 30 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   Blend together what you ve learned!  # Select the flights that had JFK as their destination: c1\nc1  - filter(hflights, Dest == 'JFK')\n\n# Combine the Year, Month and DayofMonth variables to create a Date column: c2\nc2  - mutate(c1, Date = paste(Year, Month, DayofMonth, sep = '-'))\n\n# Print out a selection of columns of c2\nselect(c2, Date, DepTime, ArrTime, TailNum)  ## # A tibble: 695 \u00d7 4\n##        Date DepTime ArrTime TailNum\n##        chr     int     int     chr \n## 1  2011-1-1     654    1124  N324JB\n## 2  2011-1-1    1639    2110  N324JB\n## 3  2011-1-2     703    1113  N324JB\n## 4  2011-1-2    1604    2040  N324JB\n## 5  2011-1-3     659    1100  N229JB\n## 6  2011-1-3    1801    2200  N206JB\n## 7  2011-1-4     654    1103  N267JB\n## 8  2011-1-4    1608    2034  N267JB\n## 9  2011-1-5     700    1103  N708JB\n## 10 2011-1-5    1544    1954  N644JB\n## # ... with 685 more rows  Arranging your data  # Definition of dtc\ndtc  - filter(hflights, Cancelled == 1, !is.na(DepDelay))\n\n# Arrange dtc by departure delays\narrange(dtc, DepDelay)  ## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##     int   int        int       int     int     int            chr \n## 1   2011     7         23         6     605      NA       Frontier\n## 2   2011     1         17         1     916      NA     ExpressJet\n## 3   2011    12          1         4     541      NA     US_Airways\n## 4   2011    10         12         3    2022      NA American_Eagle\n## 5   2011     7         29         5    1424      NA    Continental\n## 6   2011     9         29         4    1639      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     5          9         1     715      NA        SkyWest\n## 9   2011     1         20         4    1413      NA         United\n## 10  2011     1         17         1     831      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # Arrange dtc so that cancellation reasons are grouped\narrange(dtc, CancellationCode)  ## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##     int   int        int       int     int     int            chr \n## 1   2011     1         20         4    1413      NA         United\n## 2   2011     1          7         5    2028      NA     ExpressJet\n## 3   2011     2          4         5    1638      NA       American\n## 4   2011     2          8         2    1057      NA    Continental\n## 5   2011     2          1         2    1508      NA        SkyWest\n## 6   2011     2         21         1    2257      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     3         18         5     727      NA         United\n## 9   2011     4          4         1    1632      NA          Delta\n## 10  2011     4          8         5    1608      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # Arrange dtc according to carrier and departure delays\narrange(dtc, UniqueCarrier, DepDelay)  ## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##     int   int        int       int     int     int                chr \n## 1   2011     6         11         6    1649      NA            AirTran\n## 2   2011     8         18         4    1808      NA           American\n## 3   2011     2          4         5    1638      NA           American\n## 4   2011    10         12         3    2022      NA     American_Eagle\n## 5   2011     2          9         3     555      NA     American_Eagle\n## 6   2011     7         17         7    1917      NA     American_Eagle\n## 7   2011     4         30         6     612      NA Atlantic_Southeast\n## 8   2011     4         10         7    1147      NA Atlantic_Southeast\n## 9   2011     5         23         1     657      NA Atlantic_Southeast\n## 10  2011     9         29         4     723      NA Atlantic_Southeast\n## # ... with 58 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   Reverse the order of arranging  # Arrange according to carrier and decreasing departure delays\narrange(hflights, UniqueCarrier, desc(DepDelay))  ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##     int   int        int       int     int     int           chr \n## 1   2011     2         19         6    1902    2143       AirTran\n## 2   2011     3         14         1    2024    2309       AirTran\n## 3   2011     2         16         3    2349     227       AirTran\n## 4   2011    11         13         7    2312     213       AirTran\n## 5   2011     5         26         4    2353     305       AirTran\n## 6   2011     5         26         4    1922    2229       AirTran\n## 7   2011     4         28         4    1045    1328       AirTran\n## 8   2011     6          5         7    2207      52       AirTran\n## 9   2011     5          7         6    1009    1256       AirTran\n## 10  2011     7         25         1    2107      14       AirTran\n## # ... with 227,486 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # Arrange flights by total delay (normal order).\narrange(hflights, (ArrDelay+DepDelay))  ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##     int   int        int       int     int     int           chr \n## 1   2011     7          3         7    1914    2039    ExpressJet\n## 2   2011     8         31         3     934    1039       SkyWest\n## 3   2011     8         21         7     935    1039       SkyWest\n## 4   2011     8         28         7    2059    2206       SkyWest\n## 5   2011     8         29         1     935    1041       SkyWest\n## 6   2011    12         25         7     741     926       SkyWest\n## 7   2011     1         30         7     620     812       SkyWest\n## 8   2011     8          3         3    1741    1810    ExpressJet\n## 9   2011     8          4         4     930    1041       SkyWest\n## 10  2011     8         18         4     939    1043       SkyWest\n## # ... with 227,486 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int   # Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime \narrange(filter(hflights, Dest == 'DFW'   DepTime 800), desc(AirTime))  ## # A tibble: 799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##     int   int        int       int     int     int            chr \n## 1   2011    11         22         2     635     825       American\n## 2   2011     8         25         4     602     758 American_Eagle\n## 3   2011    10         12         3     559     738 American_Eagle\n## 4   2011     5          2         1     716     854       American\n## 5   2011     4          4         1     741     949       American\n## 6   2011     4          4         1     627     742 American_Eagle\n## 7   2011     6         21         2     726     848     ExpressJet\n## 8   2011     9          1         4     715     844       American\n## 9   2011     3         14         1     729     917    Continental\n## 10  2011    12          5         1     724     847    Continental\n## # ... with 789 more rows, and 14 more variables: FlightNum  int ,\n## #   TailNum  chr , ActualElapsedTime  int , AirTime  int , ArrDelay  int ,\n## #   DepDelay  int , Origin  chr , Dest  chr , Distance  int ,\n## #   TaxiIn  int , TaxiOut  int , Cancelled  int , CancellationCode  chr ,\n## #   Diverted  int", 
            "title": "3, filter and arrange"
        }, 
        {
            "location": "/Data_Wrangling/#4-summarise-and-the-pipe-operator", 
            "text": "The syntax of summarise  # Print out a summary with variables min_dist and max_dist\nsummarise(hflights, min_dist = min(Distance), max_dist = max(Distance))  ## # A tibble: 1 \u00d7 2\n##   min_dist max_dist\n##       int      int \n## 1       79     3904  # Print out a summary with variable max_div\nsummarise(filter(hflights, Diverted == 1), max_div = max(Distance))  ## # A tibble: 1 \u00d7 1\n##   max_div\n##      int \n## 1    3904  Aggregate functions  summarise :   min(x) ; minimum value of vector x.  max(x) ; maximum value of vector x.  mean(x) ; mean value of vector x.  median(x) ; median value of vector x.  quantile(x, p) ; pth quantile of vector x.  sd(x) ; standard deviation of vector x.  var(x) ; variance of vector x.  IQR(x) ; Inter Quartile Range (IQR) of vector x.  diff(range(x)) ; total range of vector x.    # Remove rows that have NA ArrDelay: temp1\ntemp1  - filter(hflights, !is.na(ArrDelay))\n\n# Generate summary about ArrDelay column of temp1\nsummarise(temp1, earliest = min(ArrDelay), average = mean(ArrDelay), latest = max(ArrDelay), sd = sd(ArrDelay))  ## # A tibble: 1 \u00d7 4\n##   earliest  average latest       sd\n##       int      dbl    int      dbl \n## 1      -70 7.094334    978 30.70852  # Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2\ntemp2  - filter(hflights, !is.na(TaxiIn), !is.na(TaxiOut))\n\n# Print the maximum taxiing difference of temp2 with summarise()\nsummarise(temp2, max_taxi_diff = max(abs(TaxiIn - TaxiOut)))  ## # A tibble: 1 \u00d7 1\n##   max_taxi_diff\n##            int \n## 1           160  dplyr  aggregate functions   first(x) ; the first element of vector  x .  last(x) ; the last element of vector  x .  nth(x, n) ; The  n th element of vector  x .  n() ; The number of rows in the data.frame or group of observations \n    that  summarise()  describes.  n_distinct(x) ; The number of unique values in vector  x .    # Generate summarizing statistics for hflights\nsummarise(hflights, n_obs = n(), n_carrier = n_distinct(UniqueCarrier), n_dest = n_distinct(Dest), dest100 = nth(Dest, 100))  ## # A tibble: 1 \u00d7 4\n##    n_obs n_carrier n_dest dest100\n##     int       int    int     chr \n## 1 227496        15    116     DFW  # Filter hflights to keep all American Airline flights: aa\naa  - filter(hflights, UniqueCarrier == 'American')\n\n# Generate summarizing statistics for aa \nsummarise(aa, n_flights = n(), n_canc = sum(Cancelled), p_canc = n_canc/n_flights*100, avg_delay = mean(ArrDelay, na.rm = TRUE))  ## # A tibble: 1 \u00d7 4\n##   n_flights n_canc   p_canc avg_delay\n##        int    int      dbl       dbl \n## 1      3244     60 1.849568 0.8917558  Overview of syntax  # Write the 'piped' version of the English sentences\nhflights % %\n    mutate(diff = TaxiOut - TaxiIn) % %\n    filter(!is.na(diff)) % %\n    summarise( avg = mean(diff))  ## # A tibble: 1 \u00d7 1\n##        avg\n##       dbl \n## 1 8.992064  Drive or fly? Part 1 of 2  # Part 1, concerning the selection and creation of columns\nd  - hflights % %\n  select(Dest, UniqueCarrier, Distance, ActualElapsedTime) % %  \n  mutate(RealTime = ActualElapsedTime + 100, mph = Distance / RealTime * 60)\n\n# Part 2, concerning flights that had an actual average speed of   70 mph.\nd % %\n  filter(!is.na(mph), mph   70) % %\n  summarise( n_less = n(), \n             n_dest = n_distinct(Dest), \n             min_dist = min(Distance), \n             max_dist = max(Distance))  ## # A tibble: 1 \u00d7 4\n##   n_less n_dest min_dist max_dist\n##     int    int      int      int \n## 1   6726     13       79      305  Drive or fly? Part 2 of 2  # Solve the exercise using a combination of dplyr verbs and % %\nhflights % %\n    #summarise(all_flights = n()) % %\n    filter(((Distance / (ActualElapsedTime + 100) * 60)   105) | Cancelled == 1 | Diverted == 1) % %\n    summarise(n_non = n(), p_non = n_non / 22751 *100, n_dest = n_distinct(Dest), min_dist = min(Distance), max_dist = max(Distance))  ## # A tibble: 1 \u00d7 5\n##   n_non    p_non n_dest min_dist max_dist\n##    int      dbl    int      int      int \n## 1 42400 186.3654    113       79     3904  Advanced piping exercise  # Count the number of overnight flights\nhflights % %\n    filter(ArrTime   DepTime   !is.na(DepTime)   !is.na(ArrTime)) % %\n    summarise(n = n())  ## # A tibble: 1 \u00d7 1\n##       n\n##    int \n## 1  2718", 
            "title": "4, summarise and the Pipe Operator"
        }, 
        {
            "location": "/Data_Wrangling/#5-group_by-and-working-with-data", 
            "text": "Unite and conquer using  group_by  # Make an ordered per-carrier summary of hflights\nhflights % %\n   group_by(UniqueCarrier) % %\n   summarise(n_flights = n(), \n             n_canc = sum(Cancelled == 1), \n             p_canc = mean(Cancelled == 1) * 100, \n             avg_delay = mean(ArrDelay, na.rm = TRUE)) % %\n   arrange(avg_delay, p_canc)  ## # A tibble: 15 \u00d7 5\n##         UniqueCarrier n_flights n_canc    p_canc  avg_delay\n##                  chr       int    int       dbl        dbl \n## 1          US_Airways      4082     46 1.1268986 -0.6307692\n## 2            American      3244     60 1.8495684  0.8917558\n## 3             AirTran      2139     21 0.9817672  1.8536239\n## 4              Alaska       365      0 0.0000000  3.1923077\n## 5                Mesa        79      1 1.2658228  4.0128205\n## 6               Delta      2641     42 1.5903067  6.0841374\n## 7         Continental     70032    475 0.6782614  6.0986983\n## 8      American_Eagle      4648    135 2.9044750  7.1529751\n## 9  Atlantic_Southeast      2204     76 3.4482759  7.2569543\n## 10          Southwest     45343    703 1.5504047  7.5871430\n## 11           Frontier       838      6 0.7159905  7.6682692\n## 12         ExpressJet     73053   1132 1.5495599  8.1865242\n## 13            SkyWest     16061    224 1.3946828  8.6934922\n## 14            JetBlue       695     18 2.5899281  9.8588410\n## 15             United      2072     34 1.6409266 10.4628628  # Make an ordered per-day summary of hflights\nhflights % % \n   group_by(DayOfWeek) % %\n   summarise(avg_taxi = mean(TaxiIn + TaxiOut, na.rm=TRUE)) % %\n   arrange(desc(avg_taxi))  ## # A tibble: 7 \u00d7 2\n##   DayOfWeek avg_taxi\n##        int      dbl \n## 1         1 21.77027\n## 2         2 21.43505\n## 3         4 21.26076\n## 4         3 21.19055\n## 5         5 21.15805\n## 6         7 20.93726\n## 7         6 20.43061  Combine  group_by  with  mutate  # Solution to first instruction\nhflights % %\n    filter(!is.na(ArrDelay)) % %\n    group_by(UniqueCarrier) % %\n    summarise(p_delay = sum(ArrDelay   0) / n()) % %\n    mutate(rank = rank(p_delay)) % %\n    arrange(rank)  ## # A tibble: 15 \u00d7 3\n##         UniqueCarrier   p_delay  rank\n##                  chr       dbl   dbl \n## 1            American 0.3030208     1\n## 2             AirTran 0.3112269     2\n## 3          US_Airways 0.3267990     3\n## 4  Atlantic_Southeast 0.3677511     4\n## 5      American_Eagle 0.3696714     5\n## 6               Delta 0.3871092     6\n## 7             JetBlue 0.3952452     7\n## 8              Alaska 0.4368132     8\n## 9           Southwest 0.4644557     9\n## 10               Mesa 0.4743590    10\n## 11        Continental 0.4907385    11\n## 12         ExpressJet 0.4943420    12\n## 13             United 0.4963109    13\n## 14            SkyWest 0.5350105    14\n## 15           Frontier 0.5564904    15  # Solution to second instruction\nhflights % %\n    filter(!is.na(ArrDelay), ArrDelay   0) % %\n    group_by(UniqueCarrier) % %\n    summarise(avg = mean(ArrDelay)) % %\n    mutate(rank = rank(avg)) % %\n    arrange(rank)  ## # A tibble: 15 \u00d7 3\n##         UniqueCarrier      avg  rank\n##                  chr      dbl   dbl \n## 1                Mesa 18.67568     1\n## 2            Frontier 18.68683     2\n## 3          US_Airways 20.70235     3\n## 4         Continental 22.13374     4\n## 5              Alaska 22.91195     5\n## 6             SkyWest 24.14663     6\n## 7          ExpressJet 24.19337     7\n## 8           Southwest 25.27750     8\n## 9             AirTran 27.85693     9\n## 10           American 28.49740    10\n## 11              Delta 32.12463    11\n## 12             United 32.48067    12\n## 13     American_Eagle 38.75135    13\n## 14 Atlantic_Southeast 40.24231    14\n## 15            JetBlue 45.47744    15  Advanced  group_by  exercises  # Which plane (by tail number) flew out of Houston the most times? How many times? adv1\nadv1  - hflights % %\n          group_by(TailNum) % %\n          summarise(n = n()) % %\n          filter(n == max(n))\n\n# How many airplanes only flew to one destination from Houston? adv2\nadv2  - hflights % %\n          group_by(TailNum) % %\n          summarise(ndest = n_distinct(Dest)) % %\n          filter(ndest == 1) % %\n          summarise(nplanes = n())\n\n# Find the most visited destination for each carrier: adv3\nadv3  - hflights % % \n          group_by(UniqueCarrier, Dest) % %\n          summarise(n = n()) % %\n          mutate(rank = rank(desc(n))) % %\n          filter(rank == 1)\n\n# Find the carrier that travels to each destination the most: adv4\nadv4  - hflights % % \n          group_by(Dest, UniqueCarrier) % %\n          summarise(n = n()) % %\n          mutate(rank = rank(desc(n))) % %\n          filter(rank == 1)  dplyr  deals with different types  # Use summarise to calculate n_carrier\ns2  - hflights % %\n    summarise(n_carrier = n_distinct(UniqueCarrier))  dplyr  and mySQL databases  Code only.  # set up a src that connects to the mysql database (src_mysql is provided by dplyr)\nmy_db  - src_mysql(dbname = 'dplyr', \n                  host = 'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com', \n                  port = 3306,\n                  user = 'dplyr',\n                  password = 'dplyr')\n\n# and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table\nnycflights  - tbl(my_db, 'dplyr')\n\n# glimpse at nycflights\nglimpse(nycflights)\n\n# Calculate the grouped summaries detailed in the instructions\nnycflights % %\n   group_by(carrier) % %\n   summarise(n_flights = n(), avg_delay = mean(arr_delay)) % %\n   arrange(avg_delay)", 
            "title": "5, group_by and working with data"
        }, 
        {
            "location": "/Data_Wrangling/#adding-tidyr-functions", 
            "text": "complete  drop_na  expand  extract  extract_numeric  complete  fill  full_seq  gather  nest  replace_na  separate  separate_rows  separate_rows_  smiths  spread  table1  unite  unnest  who", 
            "title": "Adding tidyr Functions"
        }, 
        {
            "location": "/Data_Wrangling/#extension-joining-data-in-r-with-dplyr", 
            "text": "## 1, Mutating Joins\n\n## 2, Filtering Joins and Set Operations\n\n## 3, Assembling Data\n\n## 4, Advanced Joining\n\n## 5, Case Study", 
            "title": "Extension: 'Joining Data in R with dplyr'"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/", 
            "text": "Plotting Packages\n\n\nData Type \n Dataset\n\n\nData Types\n\n\nFunctions\n\n\nDataset\n\n\n\n\n\n\nThe Basic Package\n\n\nBasic Plots, Options \n Parameters\n\n\nUnivariate Plots\n\n\nBivariate (Multivariate) Plots\n\n\nMultivariate Plots\n\n\nTimes Series\n\n\nRegressions and Residual Plots\n\n\n\n\n\n\nThe \nlattice\n and \nlatticeExtra\n Packages\n\n\nColoring\n\n\nDocumentation\n\n\nA note on reordering the levels (factors)\n\n\nUni-, Bi-, Multivariate Plots\n\n\n\n\n\n\nAdditional Packages\n\n\nThe \nsm\n Package (density)\n\n\nThe \ncar\n Package (scatter)\n\n\nThe \nvioplot\n Package (boxplot)\n\n\nThe \nvcd\n Package (count, correlation, mosaic)\n\n\nThe \nhexbin\n Package (scatter)\n\n\nThe \ncar\n Package (scatter)\n\n\nThe \nscatterplot3d\n Package\n\n\nThe \nrgl\n Package (interactive)\n\n\nThe \ncluster\n Package (dendrogram)\n\n\nThe \nextracat\n Package (splom)\n\n\nThe \nash\n Package (density)\n\n\nThe \nKernSmooth\n Package (density)\n\n\nThe \ncrorplot\n Package (correlation)\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nCode snippets and results.\n\n\nSome data might necessitate more specialized packages.\n\n\nFor explaining data, presenting results, reporting and publishing,\n\n    we can generate prettier graphics with \nggvis\n or \nggplot2\n, and\n\n    interactive packages such as \nshiny\n.\n\n\n\n\n\n\nPlotting Packages\n\n\nPackages used in bold.\n\n\nGraphics:\n\n\n\n\nmaps\n for grids and mapping.\n\n\ndiagram\n for flow charts.\n\n\nplotrix\n for ternary, polar plots.\n\n\ngplots\n.\n\n\npixmap\n, \npng\n, \nrtiff\n, \nReadImages\n, \nEBImage\n, \nRImageJ\n.\n\n\nleaflet\n.\n\n\n\n\nGrid:\n\n\n\n\nvcd\n for mosaic, ternary plots.\n\n\ngrImport\n for vectors.\n\n\nggplot2\n and extensions.\n\n\nlattice\n and \nlatticeExtra\n.\n\n\ngridBase\n.\n\n\n\n\ngrDevices:\n\n\n\n\nJavaGD\n.\n\n\nCairo\n.\n\n\ntikzDevice\n.\n\n\n\n\nInteractive:\n\n\n\n\nrgl\n.\n\n\nggvis\n.\n\n\niplots\n.\n\n\nrggobi\n.\n\n\n\n\nOthers:\n\n\n\n\nash\n for density plots.\n\n\ncluster\n for dendrograms.\n\n\ncopula\n for multivariate analyses.\n\n\ncorrplot\n for correlations.\n\n\ncompositions\n for geometries, ternary plots.\n\n\nextracat\n for missing values.\n\n\nsoiltexture\n for ternary plots and more.\n\n\nKernSmooth\n for histograms-density plots.\n\n\nopenair\n for polar, circular plots.\n\n\nsm\n for density plots.\n\n\ncar\n for scatter plots.\n\n\nvioplot\n for boxplots.\n\n\nvcd\n for mosaic plots and multivariate analyses.\n\n\nhexbin\n for scatter plots.\n\n\nscatterplot3d\n for 3D scatter plots.\n\n\ncluster\n for dendrograms.\n\n\nshiny\n for interactive plots.\n\n\nggvis\n.\n\n\n\n\nData Type \n Dataset\n\n\nData Types\n\n\n\n\ncontinuous vs categorical (or discrete).\n\n\ncontinuous: float, x-y-z, 3D, map coordinates, trianguar, lat-long,\n\n    polar, degree-distance, angle-vector.\n\n\ncategorical: integer, binary, dichotomic, dummy, factor,\n\n    ordinal (ordered).\n\n\n\n\nContinuous variable characteristics:\n\n\n\n\nasymmetry.\n\n\noutliers.\n\n\nmultimodality.\n\n\ngaps, missing values.\n\n\nheaping, redundance.\n\n\nrounding, integer.\n\n\nimpossibilities, anomalies.\n\n\nerrors.\n\n\n\n\n\n\nCategorical variable characteristics:\n\n\n\n\nunexpected pattern of results.\n\n\nuneven distribution.\n\n\nextra categories.\n\n\nunbalanced experiments.\n\n\nlarge numbers of categories.\n\n\nNA, errors, missings\n\n\nnominal: no fixed order.\n\n\nordinal: fixed order (scale of 1 to 5).\n\n\ndiscrete: counts, integers.\n\n\ndependencies, correlation, associations.\n\n\ncausal relationships, outliers, groups, clusters, gaps, barriers,\n\n    conditional relationship.\n\n\n\n\n\n\nUnivariate main plots:\n\n\n\n\nhistogram.\n\n\ndensity.\n\n\nqqmath chart.\n\n\nbox \n whickers chart.\n\n\nbar chart.\n\n\ndot.\n\n\n\n\nBivariate main plots:\n\n\n\n\nxy chart.\n\n\nqq chart.\n\n\n\n\nTrivariate main plots:\n\n\n\n\ncloud.\n\n\nwireframe.\n\n\ncountour.\n\n\nlevel.\n\n\n\n\nMultivariate main plots:\n\n\n\n\nsploms.\n\n\nparallel charts (coordinate).\n\n\n\n\nSpecialized plots:\n\n\n\n\nfrequencies, crosstabs: bar charts, mosaic plots, association plots.\n\n\ncorrelations: sploms, pairs, correlograms.\n\n\nt-tests, non-parrametric tests of group differences: box plot,\n\n    density plot.\n\n\nregression: scatter plot.\n\n\nANOVA: box plots, line plots.\n\n\n\n\nFunctions\n\n\nCreate a new variable\n\n\niris2 \n- within(iris, area \n- Petal.Width*Petal.Length)\nhead(iris2, 3)\n\n\n\n\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species area\n## 1          5.1         3.5          1.4         0.2  setosa 0.28\n## 2          4.9         3.0          1.4         0.2  setosa 0.28\n## 3          4.7         3.2          1.3         0.2  setosa 0.26\n\n\n\narea \n- with(iris, area \n- Petal.Width*Petal.Length)\nhead(area, 3)\n\n\n\n\n## [1] 0.28 0.28 0.26\n\n\n\nDataset\n\n\nFor most examples, we use the \nmtcars\n dataset.\n\n\nPrepare the dataset.\n\n\nattach(mtcars)\n\n\n\n\nGet data attached to a package (an example).\n\n\ndata(gvhd10, package = 'latticeExtra')\n\n\n\n\nThe Basic Package\n\n\nBasic Plots, Options \n Parameters\n\n\nStandardize the parameters (an example)\n\n\n# color and tick mark text orientation\npar(col = 'black', las = 1)\n\n\n\n\nGrid and layout\n\n\nOne plot.\n\n\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\n\n\n\n\n\nA grid of plots.\n\n\npar(mfrow = c(2, 1))\n\nplot(mpg, hp, ylab = 'horsepower', xlab = 'miles per gallon')\nboxplot(mpg ~ cyl, xlab = 'mile per gallon', ylab = 'number of cylinders', horizontal = TRUE)\n\npar(mfrow = c(1, 2))\n\nplot(mpg, hp, ylab = 'horsepower', xlab = 'miles per gallon')\nboxplot(mpg ~ cyl, xlab = 'mile per gallon', ylab = 'number of cylinders', horizontal = TRUE)\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nOther grids.\n\n\nlayout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))\n\nplot(mpg, xlab = 'observations', ylab = 'miles per gallon')\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\nboxplot(mpg ~ cyl, ylab = 'mile per gallon', xlab = 'number of cylinders')\n\n\n\n\n\n\n# view\nmatrix(c(1,2,1,3), 2, 2, byrow = TRUE)\n\n\n\n\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    3\n\n\n\nlayout(matrix(c(1,2,1,3), 2, 2, byrow = TRUE))\n\nhist(wt)\nhist(mpg)\nhist(disp)\n\n\n\n\n\n\nlayout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE),  widths = c(3,1), heights = c(1,2))\n\nhist(wt)\nhist(mpg)\nhist(disp)\n\n\n\n\n\n\nnf \n- layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths = lcm(12), heights = lcm(6))\nlayout.show(nf)\n\n\n\n\n\n\nplot(mpg, xlab = 'observations', ylab = 'miles per gallon')\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\nboxplot(mpg ~ cyl, ylab = 'mile per gallon', xlab = 'number of cylinders')\n\n\n\n\n\n\nGridview with additional packages.\n\n\nlibrary(vcd)\n\n\n\n\nmplot(A, B, C)\n\n\n\n\n\n\nSee the \nlattice\n and \nlatticeExtra\n packages for built-in\n\nfacet/gridview. \nggplot2\n as well.\n\n\nPlot and add ablines\n\n\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\n# abline(h = yvalues, v = xvalues)\nabline(lm(mpg ~ hp))\n\n# main = 'Title' or...\ntitle('Title')\n\n\n\n\n\n\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\nabline(h = c(20, 25))\nabline(v = c(50, 150))\nabline(v = seq(200, 300, 50), lty = 2, col = 'blue')\n\n\n\n\n\n\nAdd a legend\n\n\nboxplot(mpg ~ cyl, main = 'Title',\n   yaxt = 'n', xlab = 'mile per gallon', horizontal = TRUE, col = terrain.colors(3))\n\nlegend('topright', inset = 0.05, title = 'number of cylinders', c('4','6','8'), fill = terrain.colors(3), horiz = TRUE)\n\n\n\n\n\n\nSave\n\n\nmygraph \n- plot(hp, mpg, main = 'Title', xlab = 'horsepower', ylab = 'miles per gallon')\n\npdf('mygraph.pdf')\npng('mygraph.png')\njpeg('mygraph.jpg')\nbmp('mygraph.bmp')\npostscript('mygraph.ps')\n\n\n\n\nView in a new window\n\n\nTyping the function will open a new window to render the plot.\n\n\n\n\nwindows()\n for Windows.\n\n\nX11()\n for Linux.\n\n\nquartz()\n for OS X.\n\n\n\n\n\n\n\n# open the new windows\nwindows()\n\nplot(hp, mpg, main = 'Title', xlab = 'horsepower', ylab = 'miles per gallon')\n\n\n\n\nEnrich the plot, add text\n\n\nplot(hp, mpg,\n     main = 'Title', col.main = 'blue',\n     sub = 'figure 1', col.sub = 'blue',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     col.lab = 'red', cex.lab = 0.9,\n     xlim = c(50, 350),\n     ylim = c(0, 40))\n\ntext(100, 10, 'text 1') # x and y coordinate\nmtext('text 2', 4, line = 0.5) # pos = 1 (bottom), 2 (left), 3 (top), 4 (right); line (margin)\n\n\n\n\n\n\nWith \nlocator()\n, use the mouse; with 1 for 1 click, 2 for\n Find the\n\ncoordinates to be entered in the code. For example (after two clicks):\n\n\n locator(2)\n$x\n[1] 212.5308 293.7854\n\n$y\n[1] 33.34040 31.87281\n\n\n\n\nplot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\ntext(hp, mpg, row.names(mtcars), cex = 0.7, pos = 4, col = 'red')\n\n\n\n\n\n\nEnrich the plot, add symbols\n\n\nplot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\nsymbols(250, 20, squares = 1, add = TRUE, inches = 0.1, fg = 'red')\nsymbols(250, 25, circles = 1, add = TRUE, inches = 0.1, fg = 'red')\n\n\n\n\n\n\n#rectangles\n#stars\n#thermometers\n#boxplots\n\n\n\n\nCombine plots; change \npch =\n \n \ncol =\n\n\npar(mfrow = c(2,2))\n\n# 1\nplot(hp, mpg,\n     main = 'P1',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 1,\n     col = 'black')\n\n# 2\nplot(hp, mpg,\n     main = 'P2',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 3,\n     col = 'blue',\n     cex = 0.5)\n\n# 3\nplot(hp, mpg,\n     main = 'P3',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 5,\n     col = 'red',\n     cex = 2)\n\n# 4\nplot(hp, mpg,\n     main = 'P4',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 7,\n     col = 'green')\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1))\n\n\n\n\nChange \ncol =\n\n\n\n\nChange \npch =\n\n\n\n\npar(fig = c(0,0.8,0,0.8))\n\nplot(mtcars$wt, mtcars$mpg, xlab = 'Car Weight',   ylab = 'miles Per Gallon')\n\npar(fig = c(0,0.8,0.55,1), new = TRUE)\n\nboxplot(mtcars$wt, horizontal = TRUE, axes = FALSE)\n\npar(fig = c(0.65,1,0,0.8), new = TRUE)\n\nboxplot(mtcars$mpg, axes = FALSE)\n\nmtext('Enhanced Scatterplot', side = 3, outer = TRUE, line = -3)\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1))\n\n\n\n\nChange \ntype =\n; without dots\n\n\nx \n- c(1:5); y \n- x\n\npar(pch = 22, col = 'red') # plotting symbol and color\n\npar(mfrow = c(2,4)) # all plots on one page\nopts = c('p','l','o','b','c','s','S','h')\n\nfor (i in 1:length(opts)) {\n  heading = paste('type =',opts[i])\n  plot(x, y, type = 'n', main = heading)\n  lines(x, y, type = opts[i])\n}\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1), col = 'black')\n\n\n\n\nChange \ntype =\n; with dots\n\n\nx \n- c(1:5); y \n- x\n\npar(pch = 22, col = 'blue') # plotting symbol and color\n\npar(mfrow = c(2,4)) # all plots on one page\nopts = c('p','l','o','b','c','s','S','h')\n\nfor (i in 1:length(opts)) {\n  heading = paste('type =',opts[i])\n  plot(x, y, main = heading)\n  lines(x, y, type = opts[i])\n}\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1), col = 'black')\n\n\n\n\nAdd or modify the axes\n\n\nplot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     xaxt = 'n',\n     yaxt = 'n')\n\naxis(1, at = c(100, 200, 300), labels = NULL, pos = 15, lty = 'dashed', col = 'green', las = 2, tck = -0.05)\n\naxis(4, at = c(20, 30), labels = c('bt', 'up'), pos = 125, lty = 'dashed', col = 'blue', las = 2, tck = -0.05)\n\n\n\n\n\n\n# reverse\npar(las = 1)\n\n\n\n\nAdd layers to the first plot\n\n\nplot(mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\n# add lines\nlines(mpg[1:10], type = 'l', col = 'green')\n\n\n\n\n\n\nUnivariate Plots\n\n\nPlot; continuous\n\n\nplot(mpg, main = 'Title', xlab = 'observations', ylab = 'miles per gallon')\n\n\n\n\n\n\nPlot; categorical\n\n\nplot(cyl, main = 'Title', xlab = 'observations', ylab = 'cylinders')\n\n\n\n\n\n\nQQnorm; continuous\n\n\nqqnorm(mpg, main = 'Title', xlab = 'observations', ylab = 'cylinders')\n\n\n\n\n\n\nQQnorm; categorical\n\n\nqqnorm(cyl, main = 'Title', xlab = 'observations', ylab = 'cylinders')\n\n\n\n\n\n\nStripchart; continuous\n\n\nstripchart(mpg, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\nStripchart; categorical\n\n\nstripchart(cyl, main = 'Title', xlab = 'cylinders')\n\n\n\n\n\n\nBarplot (vertical); continuous\n\n\nbarplot(mpg[1:10], main = 'Title', xlab = 'observations', ylab = 'miles per gallon')\n\n\n\n\n\n\nBarplot (horizontal); categorical\n\n\nbarplot(cyl[1:10], main = 'Title', horiz = TRUE, xlab = 'cylinders', ylab = 'observations')\n\n\n\n\n\n\nBarplots options\n\n\nGroup with \ntable()\n.\n\n\ncounts \n- table(cyl)\ncounts\n\n\n\n\n## cyl\n##  4  6  8 \n## 11  7 14\n\n\n\nbarplot(counts, main = 'Title', horiz = TRUE, xlab = 'count', names.arg = c('4 Cyl', '6 Cyl', '8 Cyl'))\n\n\n\n\n\n\ncounts \n- table(vs, gear)\ncounts\n\n\n\n\n##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1\n\n\n\nbarplot(counts, main = 'Title', xlab = 'gearbox', col = c('darkblue', 'red'), legend = rownames(counts)) \n\n\n\n\n\n\ncounts \n- table(vs, gear)\ncounts\n\n\n\n\n##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1\n\n\n\nbarplot(counts, main = 'Title', xlab='gearbox', col = c('darkblue', 'red'), legend =  rownames(counts), beside = TRUE)\n\n\n\n\n\n\nGroup with \naggregate()\n.\n\n\naggregate(mtcars, by = list(cyl, vs), FUN = mean, na.rm = TRUE)\n\n\n\n\n##   Group.1 Group.2      mpg cyl   disp       hp     drat       wt     qsec\n## 1       4       0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000\n## 2       6       0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667\n## 3       8       0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214\n## 4       4       1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100\n## 5       6       1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500\n##   vs        am     gear     carb\n## 1  0 1.0000000 5.000000 2.000000\n## 2  0 1.0000000 4.333333 4.666667\n## 3  0 0.1428571 3.285714 3.500000\n## 4  1 0.7000000 4.000000 1.500000\n## 5  1 0.0000000 3.500000 2.500000\n\n\n\npar(las = 2) # make label text perpendicular to axis\n\npar(mar = c(5, 8, 4, 2)) # increase y-axis margin.\n\ncounts \n- table(mtcars$gear)\nbarplot(counts, main = 'Car Distribution', horiz = TRUE, names.arg = c('3 Gears', '4 Gears', '5   Gears'), cex.names = 0.8)\n\n\n\n\n\n\n# reverse\npar(las = 1)\n\n\n\n\nColors.\n\n\nlibrary(RColorBrewer)\n\npar(mfrow = c(2, 1))\n\nbarplot(iris$Petal.Length)\nbarplot(table(iris$Species, iris$Sepal.Length), col = brewer.pal(3, 'Set1'))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nPie Chart\n\n\nAvoid!\n\n\nDotchart; continuous\n\n\ndotchart(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'observations')\n\n\n\n\n\n\nDotchart; categorical\n\n\ndotchart(cyl, main = 'Title', xlab = 'cylinders', ylab = 'observations')\n\n\n\n\n\n\nDotchart options\n\n\ndotchart(mpg,labels = row.names(mtcars), cex = 0.7, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\n# sort by mpg\nx \n- mtcars[order(mpg),]\n\n# must be factors\nx$cyl \n- factor(x$cyl)\nx$color[x$cyl == 4] \n- 'red'\nx$color[x$cyl == 6] \n- 'blue'\nx$color[x$cyl == 8] \n- 'darkgreen'\n\ndotchart(x$mpg, labels = row.names(x), cex = 0.7, groups = x$cyl, main = 'Title',  xlab = 'miles per gallon', gcolor = 'black', color = x$color)\n\n\n\n\n\n\nMore with the \nhmisc\n package and \npanel.dotplot()\n and in the \nlattice\n\npackage section.\n\n\nBoxplot; continuous\n\n\nboxplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'observations')\n\n\n\n\n\n\nStem; continuous\n\n\nstem(mpg)\n\n\n\n\n## \n##   The decimal point is at the |\n## \n##   10 | 44\n##   12 | 3\n##   14 | 3702258\n##   16 | 438\n##   18 | 17227\n##   20 | 00445\n##   22 | 88\n##   24 | 4\n##   26 | 03\n##   28 | \n##   30 | 44\n##   32 | 49\n\n\n\nHistogram; continuous\n\n\nhist(mpg, main = 'Title', xlab = 'miles per gallon - bins', ylab = 'count')\n\n\n\n\n\n\nHistogram; categorical\n\n\nhist(cyl, main = 'Title', xlab = 'cylinders - bins', ylab = 'count')\n\n\n\n\n\n\nHistogram options\n\n\nhist(mpg, breaks = 12, col = 'red')\n\n\n\n\n\n\nx \n- mpg\n\nh \n- hist(x, breaks = 10, main = 'Title', xlab = 'miles per gallon')\n\nxfit \n- seq(min(x), max(x),length = 40)\nyfit \n- dnorm(xfit, mean = mean(x), sd = sd(x))\nyfit \n- yfit*diff(h$mids[1:2])*length(x)\n\nlines(xfit, yfit, col = 'blue', lwd = 2)\n\n\n\n\n\n\nColors.\n\n\nlibrary(RColorBrewer)\n\npar(mfrow = c(2, 3))\n\nhist(VADeaths, breaks = 10, col = brewer.pal(3, 'Set3'), main = '3, Set3')\nhist(VADeaths, breaks = 4, col = brewer.pal(3, 'Set2'), main = '3, Set2')\nhist(VADeaths, breaks = 8, col = brewer.pal(3, 'Set1'), main = '3, Set1')\nhist(VADeaths, breaks = 2, col = brewer.pal(8, 'Set3'), main = '8, Set3')\nhist(VADeaths, breaks = 10, col = brewer.pal(8, 'Greys'), main = '8, Greys')\nhist(VADeaths, breaks = 10, col = brewer.pal(8, 'Greens'), main = '8, Greens')\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nDensity Plot; continuous\n\n\nplot(density(mpg), main = 'Title')\n\n\n\n\n\n\nplot(density(mpg), main = 'Title')\n\npolygon(density(mpg), col = 'red', border = 'blue') \n\n\n\n\n\n\nd1 \n- density(mtcars$mpg)\nplot(d1)\nrug(mtcars$mpg)\n\nlines(density(mtcars$mpg, d1$bw/2), col = 'green')\nlines(density(mtcars$mpg, d1$bw/5), col = 'blue')\n\n\n\n\n\n\nBivariate (Multivariate) Plots\n\n\nPlot, continuous/continuous\n\n\nplot(mpg, hp, main = 'Title', xlab = 'miles per gallon', ylab = 'horsepowers')\n\n\n\n\n\n\nPlot, continuous/categorical\n\n\nplot(mpg, cyl, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\nPlot options\n\n\nplot(wt, mpg, main = 'Title', xlab = 'weight', ylab = 'miles per gallon ')\n\nabline(lm(mpg ~ wt), col = 'red') # regression\nlines(lowess(wt, mpg), col = 'blue') # lowess line\n\n\n\n\n\n\nSmoothScatter; continuous/continuous\n\n\nsmoothScatter(mpg, hp, main = 'Title', xlab = 'miles per gallon', ylab = 'horsepowers')\n\n\n\n\n\n\nSunflowerplot; categorical/categorical\n\n\nSpecial symbols at each location: one observation = one dot; more\n\nobservations = cross, star, etc.\n\n\nsunflowerplot(gear, cyl, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')\n\n\n\n\n\n\nBoxplot\n\n\nboxplot(mpg ~ cyl, main = 'Title',   xlab = 'cylinders', ylab = 'miles per gallon') \n\n\n\n\n\n\nColors.\n\n\nlibrary(RColorBrewer)\n\npar(mfrow = c(1, 2))\n\nboxplot(iris$Sepal.Length, col = 'red')\nboxplot(iris$Sepal.Length ~ iris$Species, col = topo.colors(3))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nlibrary(dplyr)\n\ndata(Pima.tr2, package = 'MASS')\n\nPimaV \n- select(Pima.tr2, glu:age)\nboxplot(scale(PimaV), pch = 16, outcol = 'red')\n\n\n\n\n\n\nBoxplot options\n\n\nfour \n- subset(mpg, cyl == 4)\nsix \n- subset(mpg, cyl == 6)\neight \n- subset(mpg, cyl == 8)\n\nboxplot(four, six, eight, main = 'Title', ylab = 'miles per gallon')\n\naxis(1, at = c(1, 2, 3), labels = c('4 Cyl', '6 Cyl', '8 Cyl'))\n\n\n\n\n\n\nDotchart\n\n\ncounts \n- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\ndotchart(counts, main = 'Title', xlab = 'count', ylab = 'cylinders/gearbox')\n\n\n\n\n\n\ncounts \n- table(cyl, gear)\ncounts\n\n\n\n\n##    gear\n## cyl  3  4  5\n##   4  1  8  2\n##   6  2  4  1\n##   8 12  0  2\n\n\n\ndotchart(counts, main = 'Title', xlab = 'count', ylab = 'gearbox/cylinders')\n\n\n\n\n\n\nBarplot with its options\n\n\nVertical or horizontal. The legend as well can be horizontal or\n\nvertical.\n\n\ncounts \n- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\nbarplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 20), col = terrain.colors(3))\n\nlegend('topleft', inset = .04, title = 'gearbox',\n   c('3','4','5'), fill = terrain.colors(3), horiz = TRUE)\n\n\n\n\n\n\ncounts \n- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\nbarplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 25), col = terrain.colors(3), legend = rownames(counts))\n\n\n\n\n\n\ncounts \n- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\nbarplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 20), col = terrain.colors(3), legend = rownames(counts), beside = TRUE)\n\n\n\n\n\n\nSpineplot\n\n\nCount\n = blocks; categorical (with factors).\n\n\ncyl2 \n- as.factor(cyl) # mandatory for the y\ngear2 \n- as.factor(gear)\n\nspineplot(gear2, cyl2, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')\n\n\n\n\n\n\nCount = blocks; continuous.\n\n\nspineplot(mpg, cyl2, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\nMosaicplot\n\n\nCount = blocks.\n\n\ncounts \n- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\nmosaicplot(counts, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')\n\n\n\n\n\n\nMultivariate Plots\n\n\nPairs\n\n\npairs( ~mpg + disp + hp)\n\n\n\n\n\n\nCoplot\n\n\ncoplot(mpg ~ hp | wt)\n\n\n\n\n\n\nCorrelograms\n\n\nlibrary(corrgram)\n\ncorrgram(mtcars, order = TRUE, lower.panel = panel.shade, upper.panel=panel.pie, text.panel = panel.txt, main = 'Car Milage Data in PC2/PC1 Order')\n\n\n\n\n\n\nPlot a dataset with colors\n\n\nlibrary(RColorBrewer)\n\nplot(iris, col = brewer.pal(3, 'Set1'))\n\n\n\n\n\n\nStars\n\n\nThe star branches are explanatory; be careful with the interpretation!\n\nWell-advised for visual and pattern exploration.\n\n\nmtcars[1:4, c(1, 4, 6)]\n\n\n\n\n##                 mpg  hp    wt\n## Mazda RX4      21.0 110 2.620\n## Mazda RX4 Wag  21.0 110 2.875\n## Datsun 710     22.8  93 2.320\n## Hornet 4 Drive 21.4 110 3.215\n\n\n\nstars(mtcars[1:4, c(1, 4, 6)])\n\n\n\n\n\n\nTrivariate plots\n\n\n\n\nimage()\n.\n\n\ncontour()\n.\n\n\nfilled.contour()\n.\n\n\npersp()\n.\n\n\nsymbols()\n.\n\n\n\n\nTimes Series\n\n\nAdd packages: \nzoo\n and \nxts\n.\n\n\nBasics\n\n\nplot(AirPassengers, type = 'l')\n\n\n\n\n\n\nChange the \ntype =\n\n\ny1 \n- rnorm(100)\n\npar(mfrow = c(2, 1))\n\nplot(y1, type = 'p', main = 'p vs l')\nplot(y1, type = 'l')\n\n\n\n\n\n\nplot(y1, type = 'l', main = 'l vs h')\nplot(y1, type = 'h')\n\n\n\n\n\n\nplot(y1, type = 'l', lty = 3, main = 'l 3 vs o')\nplot(y1, type = 'o')\n\n\n\n\n\n\nplot(y1, type = 'b', main = 'b vs c')\nplot(y1, type = 'c')\n\n\n\n\n\n\nplot(y1, type = 's', main = 's vs S')\nplot(y1, type = 'S')\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1, 1))\n\n\n\n\nAdd a box\n\n\ny1 \n- rnorm(100)\ny2 \n- rnorm(100)\n\npar(mfrow = (c(2, 1)))\n\nplot(y1, type = 'l', axes = FALSE, xlab = '', ylab = '', main = '')\n\nbox(col = 'gray')\n\nlines(x = c(20, 20, 40, 40), y = c(-7, max(y1), max(y1), -7), lwd = 3, col = 'gray')\n\nplot(y2, type = 'l', axes = FALSE, xlab = '', ylab = '', main = '')\n\nbox(col = 'gray')\n\nlines(x = c(20, 20, 40, 40), y = c(7, min(y2), min(y2), 7), lwd = 3, col = 'gray')\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1,1))\n\n\n\n\nAdd lines and text within the plot\n\n\ny1 \n- rnorm(100)\n\n# x goes from 0 to 100\n# xaxt = 'n' remove the x ticks\nplot(y1, type = 'l', lwd = 2, lty = 'longdash', main = 'Title', ylab = 'y', xlab = 'time', xaxt = 'n')\n\nabline(h = 0, lty = 'longdash')\n\nabline(v = 20, lty = 'longdash')\nabline(v = 50, lty = 'longdash')\nabline(v = 95, lty = 'longdash')\n\ntext(17, 1.5, srt = 90, adj = 0, labels = 'Tag 1', cex = 0.8)\ntext(47, 1.5, srt = 90, adj = 0, labels = 'Tag a', cex = 0.8)\ntext(92, 1.5, srt = 90, adj = 0, labels = 'Tag alpha', cex = 0.8)\n\n\n\n\n\n\nA comprehensive example\n\n\n# new data\nhead(Orange)\n\n\n\n\n##   Tree  age circumference\n## 1    1  118            30\n## 2    1  484            58\n## 3    1  664            87\n## 4    1 1004           115\n## 5    1 1231           120\n## 6    1 1372           142\n\n\n\n# convert factor to numeric for convenience\nOrange$Tree \n- as.numeric(Orange$Tree)\nntrees \n- max(Orange$Tree)\n\n# get the range for the x and y axis\nxrange \n- range(Orange$age)\nyrange \n- range(Orange$circumference)\n\n# set up the plot\nplot(xrange, yrange, type = 'n', xlab = 'Age (days)',\n   ylab = 'Circumference (mm)' )\ncolors \n- rainbow(ntrees)\nlinetype \n- c(1:ntrees)\nplotchar \n- seq(18, 18 + ntrees, 1)\n\n# add lines\nfor (i in 1:ntrees) {\n  tree \n- subset(Orange, Tree == i)\n  lines(tree$age, tree$circumference, type = 'b', lwd = 1.5,\n    lty = linetype[i], col = colors[i], pch = plotchar[i])\n}\n\n# add a title and subtitle\ntitle('Tree Growth', 'example of line plot')\n\n# add a legend\nlegend(xrange[1], yrange[2], 1:ntrees, cex = 0.8, col = colors,\n   pch = plotchar, lty = linetype, title = 'Tree')\n\n\n\n\n\n\nChange \nlty =\n\n\n\n\nRegressions and Residual Plots\n\n\n# first\nregr \n- lm(mpg ~ hp)\n\nsummary(regr)\n\n\n\n\n## \n## Call:\n## lm(formula = mpg ~ hp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7121 -2.1122 -0.8854  1.5819  8.2360 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(\n|t|)    \n## (Intercept) 30.09886    1.63392  18.421  \n 2e-16 ***\n## hp          -0.06823    0.01012  -6.742 1.79e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.863 on 30 degrees of freedom\n## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 \n## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n\n\nplot(mpg ~ hp)\nabline(regr)\n\n\n\n\n\n\npar(mfrow = c(2, 2))\n\n# then\nplot(regr)\n\n\n\n\n\n\n# reverse\npar(mfrow = c(1, 1))\n\n\n\n\nThe \nlattice\n and \nlatticeExtra\n Packages\n\n\nlibrary(lattice)\n\n\n\n\nColoring\n\n\n# Show the default settings\nshow.settings()\n\n\n\n\n\n\n# Save the default theme\nmytheme \n- trellis.par.get()\n\n# Turn the B\nW\ntrellis.par.set(canonical.theme(color = FALSE))\nshow.settings()\n\n\n\n\n\n\nDocumentation\n\n\n\n\nNational Park Service, Advanced\n\n    Graphics (Lattice)\n\n\nTreillis\n\n    Plots\n\n\n\n\nA note on reordering the levels (factors)\n\n\n# start\ncyl \n- mtcars$cyl\ncyl \n- as.factor(cyl)\ncyl\n\n\n\n\n##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n## Levels: 4 6 8\n\n\n\nlevels(cyl)\n\n\n\n\n## [1] \"4\" \"6\" \"8\"\n\n\n\n# option 1\ncyl \n- factor(cyl, levels = c('8', '6', '4'))\n# or levels = 3:1\n# or levels = letters[3:1]\nlevels(cyl)\n\n\n\n\n## [1] \"8\" \"6\" \"4\"\n\n\n\ncyl \n- mtcars$cyl\ncyl \n- as.factor(cyl)\n# option 2\ncyl \n- reorder(cyl, new.order = 3:1)\nlevels(cyl)\n\n\n\n\n## [1] \"8\" \"6\" \"4\"\n\n\n\nlibrary(lattice)\n\n# normalized x-axis for comparison\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2))\n\n\n\n\n\n\n# free x-axis\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2), scales = list(x = 'free'))\n\n\n\n\n\n\n# or\nbc.titanic \n- barchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2), scales = list(x = 'free'))\n\nbc.titanic\n\n\n\n\n\n\n# add bg grid\nupdate(bc.titanic, panel = function(...) {\n  panel.grid(h = 0, v = -1)\n  panel.barchart(...)\n})\n\n\n\n\n\n\n# remove lines\nupdate(bc.titanic, panel = function(...) {\n  panel.barchart(..., border = 'transparent')\n})\n\n\n\n\n\n\n# or\nupdate(bc.titanic, border = 'transparent')\n\n\n\n\n\n\nTitanic1 \n- as.data.frame(as.table(Titanic[, , 'Adult' ,]))\nTitanic1\n\n\n\n\n##    Class    Sex Survived Freq\n## 1    1st   Male       No  118\n## 2    2nd   Male       No  154\n## 3    3rd   Male       No  387\n## 4   Crew   Male       No  670\n## 5    1st Female       No    4\n## 6    2nd Female       No   13\n## 7    3rd Female       No   89\n## 8   Crew Female       No    3\n## 9    1st   Male      Yes   57\n## 10   2nd   Male      Yes   14\n## 11   3rd   Male      Yes   75\n## 12  Crew   Male      Yes  192\n## 13   1st Female      Yes  140\n## 14   2nd Female      Yes   80\n## 15   3rd Female      Yes   76\n## 16  Crew Female      Yes   20\n\n\n\nbarchart(Class ~ Freq | Sex, Titanic1, groups = Survived, stack = TRUE, auto.key = list(title = 'Survived', columns = 2))\n\n\n\n\n\n\nTitanic2 \n- reshape(Titanic1, direction = 'wide', v.names = 'Freq', idvar = c('Class', 'Sex'), timevar = 'Survived')\n\nnames(Titanic2) \n- c('Class', 'Sex', 'Dead', 'Alive')\n\nbarchart(Class ~ Dead + Alive | Sex, Titanic2, stack = TRUE, auto.key = list(columns = 2))\n\n\n\n\n\n\nUni-, Bi-, Multivariate Plots\n\n\nBarchart\n\n\nLike \nbarplot()\n.\n\n\n# y ~ x\nbarchart(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\n# y ~ x\nbarchart(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', horizontal = FALSE)\n\n\n\n\n\n\nbarchart(VADeaths, groups = FALSE, layout = c(1, 4), aspect = 0.7, reference =FALSE, main = 'Title', xlab = 'rate per 100')\n\n\n\n\n\n\ndata(postdoc, package = 'latticeExtra')\n\nbarchart(prop.table(postdoc, margin = 1), xlab = 'Proportion', auto.key = list(adj = 1))\n\n\n\n\n\n\nChange \nlayout = c(x, y, page)\n\n\nbarchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'horsepowers', ylab = 'cylinders - miles per gallon', layout = c(1,3))\n\n\n\n\n\n\nbarchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'cylinders - horsepowers', ylab = 'miles per gallon', layout = c(3,1))\n\n\n\n\n\n\nChange \naspect = 1\n\n\n1\n for square.\n\n\nbarchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1)\n\n\n\n\n\n\nColors\n\n\nbarchart(mpg ~ hp, group = cyl, auto.key = list(space = 'right'), main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\n\n\nshingle()\n; control the ranges.\n\n\nequal.count()\n; grid.\n\n\n\n\nDotplot\n\n\nLike \ndotchart()\n.\n\n\ndotplot(mpg, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(3,1))\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3)\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3, origin = 0)\n\n\n\n\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3, origin = 0, type = c('p', 'h'))\n\n\n\n\n\n\nSet \nauto.key\n.\n\n\n# maybe we'll want this later\nold.pars \n- trellis.par.get()\n\n#trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 12:14))\n\ntrellis.par.set(superpose.symbol = list(pch = c(1,3), col = 1))\n\n# Optionally put things back how they were\n#trellis.par.set(old.pars)\n\n\n\n\nUse \nauto.key\n.\n\n\ndotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), groups = vs, auto.key = list(space = 'right'))\n\n\n\n\n\n\ntrellis.par.set(old.pars)\n\n\n\n\ntrellis.par.set(superpose.symbol = list(pch = c(1,3), col = 1))\n\ndotplot(variety ~ yield | site, barley, layout = c(1, 6), aspect = c(0.7), groups = year, auto.key = list(space = 'right'))\n\n\n\n\n\n\ntrellis.par.set(old.pars)\n\n\n\n\nVertical.\n\n\ndotplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'cylinders', ylab = 'gearbox - miles per gallon', layout = c(1,3), aspect = 0.3)\n\n\n\n\n\n\nlibrary(readr)\ndensity \n- read_csv('density.csv')\ndensity$Density \n- as.numeric(density$Density)\n\ndotplot(reorder(MetropolitanArea, Density) ~ Density, density, type = c('p', 'h'), main = 'Title', xlab = 'Population Density (pop / sq.mi)')\n\n\n\n\n\n\ndotplot(reorder(MetropolitanArea, Density) ~ Density | Region, density, type = c('p', 'h'), strip = FALSE, strip.left = TRUE, layout = c(1, 3), scales = list(y = list(relation = 'free')), main = 'Title', xlab = 'Population Density (pop / sq.mi)')\n\n\n\n\n\n\nStripplot\n\n\nLike \nstripchart()\n.\n\n\nstripplot(mpg, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\nstripplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\nstripplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(1,3))\n\n\n\n\n\n\nstripplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(1,3), groups = vs, auto.key = list(space = 'right'))\n\n\n\n\n\n\nstripplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'cylinders', ylab = 'gearbox - miles per gallon', layout = c(1,3))\n\n\n\n\n\n\nHistogram\n\n\nLike \nhist()\n.\n\n\nhistogram(mpg, main = 'Title', xlab = 'miles per gallon')\n\n\n\n\n\n\nhistogram(~mpg | factor(cyl), layout = c(1, 3), main = 'Title', xlab = 'miles per gallon', ylab = 'density')\n\n\n\n\n\n\nDensityplot\n\n\nLike \nplot.density()\n.\n\n\ndensityplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'density')\n\n\n\n\n\n\ndensityplot(~mpg | factor(cyl), layout = c(1, 3), main = 'Title', xlab = 'miles per gallon', ylab = 'density')\n\n\n\n\n\n\nECDFplot\n\n\nlibrary(latticeExtra)\n\necdfplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = '')\n\n\n\n\n\n\nBWplot\n\n\nLike \nboxplot\n.\n\n\nbwplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'density')\n\n\n\n\n\n\nbwplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')\n\n\n\n\n\n\nbwplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3))\n\n\n\n\n\n\nbwplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'gearbox - cylinders', ylab = 'miles per gallon', layout = c(3,1))\n\n\n\n\n\n\nQQmath\n\n\nLike \nqqnorm()\n.\n\n\nqqmath(mpg, main = 'Title', ylab = 'miles per gallon')\n\n\n\n\n\n\nXYplot\n\n\nLike \nplot()\n.\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'horsepower', ylab = 'cylinders - miles per gallon', layout = c(1,3))\n\n\n\n\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1))\n\n\n\n\n\n\nXYplot options\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1)\n\n\n\n\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, scales = list(y = list(at = seq(10, 30, 10))))\n\n\n\n\n\n\nmeanmpg \n- mean(mpg)\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, panel = function(...) {\n  panel.xyplot(...)\n  panel.abline(h = meanmpg, lty = 'dashed')\n  panel.text(450, meanmpg + 1, 'avg', adj = c(1,  0), cex = 0.7)\n})\n\n\n\n\n\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, panel = function(x, y, ...) {\n    panel.lmline(x, y)\n    panel.xyplot(x, y, ...)\n})\n\n\n\n\n\n\n\n\npanel.points()\n.\n\n\npanel.lines()\n.\n\n\npanel.segments()\n.\n\n\npanel.arrows()\n.\n\n\npanel.rect()\n.\n\n\npanel.polygon()\n.\n\n\npanel.text()\n.\n\n\npanel.abline()\n.\n\n\npanel.lmline()\n.\n\n\npanel.xyplot()\n.\n\n\npanel.curve()\n.\n\n\npanel.rug()\n.\n\n\npanel.grid()\n.\n\n\npanel.bwplot()\n.\n\n\npanel.histogram()\n.\n\n\npanel.loess()\n.\n\n\npanel.violin()\n.\n\n\npanel.smoothScatter()\n.\n\n\n\n\npar.settings\n.\n\n\n\n\n\n\n\n\n\nlibrary(lattice)\n\ndata(SeatacWeather, package = 'latticeExtra')\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'l', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'p', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'l', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'o', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'r', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'g', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 's', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'S', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'h', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'a', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'smooth', lty = 1, col = 'black')\n\n\n\n\n\n\nxyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\nxyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', type = 'o')\n\n\n\n\n\n\nxyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', type = 'o', pch = 16, lty = 'dashed')\n\n\n\n\n\n\nxyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\ndata(USAge.df, package = 'latticeExtra')\n\nxyplot(Population ~ Age | factor(Year), USAge.df, groups = Sex, type = c('l', 'g'), auto.key = list(points = FALSE, lines = TRUE, columns = 2), aspect = 'xy', ylab = 'Population (millions)', subset = Year %in% seq(1905, 1975, by = 10))\n\n\n\n\n\n\nxyplot(Population ~ Year | factor(Age), USAge.df, groups = Sex, type = 'l', strip = FALSE, strip.left = TRUE, layout = c(1, 3), ylab = 'Population (millions)', auto.key = list(lines = TRUE, points = FALSE, columns = 2), subset = Age %in% c(0, 10, 20))\n\n\n\n\n\n\ndata(USCancerRates, package = 'latticeExtra')\n\nxyplot(rate.male ~ rate.female | state, USCancerRates, aspect = 'iso', pch = '.', cex = 2, index.cond = function(x, y) { median(y - x, na.rm = TRUE) }, scales = list(log = 2, at = c(75, 150, 300, 600)), panel = function(...) { \n  panel.grid(h = -1, v = -1)\n  panel.abline(0, 1)\n  panel.xyplot(...)\n  },\n  xlab = 'a',\n  ylab = 'b')\n\n\n\n\n\n\ndata(biocAccess, package = 'latticeExtra')\n\nbaxy \n- xyplot(log10(counts) ~ hour | month + weekday, biocAccess, type = c('p', 'a'), as.table = TRUE, pch = '.', cex = 2, col.line = 'black')\n\nbaxy\n\n\n\n\n\n\nlibrary(latticeExtra)\nuseOuterStrips(baxy)\n\n\n\n\n\n\nxyplot(sunspot.year, aspect = 'xy', strip = FALSE, strip.left = TRUE, cut = list(number = 4, overlap = 0.05))\n\n\n\n\n\n\ndata(biocAccess, package = 'latticeExtra')\n\nssd \n- stl(ts(biocAccess$counts[1:(24 * 30 *2)], frequency = 24), 'periodic')\n\nxyplot(ssd, main = 'Title', xlab = 'Time (Days)')\n\n\n\n\n\n\nSplom\n\n\nsplom(mtcars[c(1, 3, 6)], groups = cyl, data = mtcars, panel = panel.superpose, key = list(title = 'Three Cylinder Options', columns = 3, points = list(text = list(c('4 Cylinder', '6 Cylinder', '8 Cylinder')))))\n\n\n\n\n\n\ntrellis.par.set(superpose.symbol = list(pch = c(1,3, 22), col = 1, alpha = 0.5))\n\nsplom(~data.frame(mpg, disp, hp, drat, wt, qsec), data = mtcars, groups = cyl, pscales = 0, varnames = c('miles\\nper\\ngallon', 'displacement\\n(cu.in(', 'horsepower', 'rear\\naxle\\nratio', 'weight', '1/4\\nmile\\ntime'), auto.key = list(columns = 3, title = 'Title'))\n\n\n\n\n\n\ntrellis.par.set(old.pars)\n\n\n\n\nsplom(USArrests)\n\n\n\n\n\n\nsplom(~USArrests[c(3,1,2,4)] | state.region, pscales = 0, type = c('g', 'p', 'smooth'))\n\n\n\n\n\n\nParallel plot\n\n\nFor multivariate continuous data.\n\n\nparallelplot(~iris[1:4])\n\n\n\n\n\n\nparallelplot(~iris[1:4], horizontal.axis = FALSE)\n\n\n\n\n\n\nparallelplot(~iris[1:4], scales = list(x = list(rot = 90)))\n\n\n\n\n\n\nparallelplot(~iris[1:4] | Species, iris)\n\n\n\n\n\n\nparallelplot(~iris[1:4], iris, groups = Species,\n             horizontal.axis = FALSE, scales = list(x = list(rot = 90)))\n\n\n\n\n\n\nTrivariate plots\n\n\nLike \nimage()\n, \ncontour()\n, \nfilled.contour()\n, \npersp()\n, \nsymbols()\n.\n\n\n\n\nlevelplot()\n.\n\n\ncontourplot()\n.\n\n\ncloud()\n.\n\n\nwireframe()\n.\n\n\n\n\nAdditional Packages\n\n\nThe \nsm\n Package (density)\n\n\nlibrary(sm)\n\n\n\n\nDensity plot\n\n\n# create value labels\ncyl.f \n- factor(cyl, levels = c(4, 6, 8), labels = c('4 cyl', '6 cyl', '8 cyl'))\n\n# plot densities\nsm.density.compare(mpg, cyl, xlab = 'miles per gallon')\n\ntitle(main = 'Title')\n\n# add legend via mouse click\ncolfill \n- c(2:(2 + length(levels(cyl.f))))\nlegend(25, 0.19, levels(cyl.f), fill = colfill) \n\n\n\n\n\n\nThe \ncar\n Package (scatter)\n\n\nlibrary(car)\n\n\n\n\nScatter plot\n\n\nscatterplot(mpg ~ wt | cyl, data = mtcars,    xlab = 'weight', ylab = 'miles per gallon', labels = row.names(mtcars)) \n\n\n\n\n\n\nSplom\n\n\nscatterplotMatrix( ~mpg + disp + drat + wt | cyl, data = mtcars, main = 'Title')\n\n\n\n\n\n\nscatterplotMatrix == spm\n.\n\n\nspm( ~mpg + disp + drat + wt | cyl, data = mtcars, main = 'Title')\n\n\n\n\n\n\nThe \nvioplot\n Package (boxplot)\n\n\nlibrary(vioplot)\n\n\n\n\nViolin boxplot\n\n\nx1 \n- mpg[mtcars$cyl == 4]\nx2 \n- mpg[mtcars$cyl == 6]\nx3 \n- mpg[mtcars$cyl == 8]\n\nvioplot(x1, x2, x3, names = c('4 cyl', '6 cyl', '8 cyl'), col = 'green')\n\ntitle('Title')\n\n\n\n\n\n\nThe \nvcd\n Package (count, correlation, mosaic)\n\n\nlibrary(vcd)\n\n\n\n\nThe package provides a variety of methods for visualizing multivariate\n\ncategorical data.\n\n\nCount\n\n\ncounts \n- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2\n\n\n\nmosaic(counts, shade = TRUE, legend = TRUE) \n\n\n\n\n\n\nCorrelation\n\n\ncounts \n- table(gear, cyl)\ncounts\n\n\n\n\n##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2\n\n\n\nassoc(counts, shade = TRUE)\n\n\n\n\n\n\nMosaic\n\n\nucb \n- data.frame(UCBAdmissions)\nucb \n- within(ucb, Accept \n- factor(Admit, levels = c('Rejected', 'Admitted')))\n\nlibrary(vcd); library(grid)\n\ndoubledecker(xtabs(Freq~ Dept + Gender + Accept, data = ucb), gp = gpar(fill = c('grey90', 'steelblue')))\n\n\n\n\n\n\ndata(Fertility, package = 'AER')\n\ndoubledecker(morekids ~ age, data = Fertility, gp = gpar(fill = c('grey90', 'green')), spacing = spacing_equal(0))\n\n\n\n\n\n\ndoubledecker(morekids ~ gender1 + gender2, data = Fertility, gp = gpar(fill = c('grey90', 'green')))\n\n\n\n\n\n\ndoubledecker(morekids ~ age + gender1 + gender2, data = Fertility, gp = gpar(fill = c('grey90', 'green')), spacing = spacing_dimequal(c(0.1, 0, 0, 0)))\n\n\n\n\n\n\nThe \nhexbin\n Package (scatter)\n\n\nlibrary(hexbin)\n\n\n\n\nScatter plot\n\n\n# new data\ndata(NHANES)\n\n# compare\nplot(Serum.Iron ~ Transferin, NHANES, main = 'Title', xlab = 'Transferin', ylab = 'Iron')\n\n\n\n\n\n\n# with\nhexbinplot(Serum.Iron ~ Transferin, NHANES, main = 'Title', xlab = 'Transferin', ylab = 'Iron')\n\n\n\n\n\n\nhexbinplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')\n\n\n\n\n\n\nx \n- rnorm(1000)\ny \n- rnorm(1000)\n\nbin \n- hexbin(x, y, xbins = 50)\nplot(bin, main = 'Title') \n\n\n\n\n\n\nx \n- rnorm(1000)\ny \n- rnorm(1000)\n\nplot(x, y, main = 'Title', col =  rgb(0, 100, 0, 50, maxColorValue = 255), pch = 16)\n\n\n\n\n\n\ndata(Diamonds, package = 'Stat2Data')\n\na = hexbin(Diamonds$PricePerCt, Diamonds$Carat, xbins = 40)\n\nlibrary(RColorBrewer)\n\nplot(a)\n\n\n\n\n\n\nColors.\n\n\nrf \n- colorRampPalette(rev(brewer.pal(12, 'Set3')))\n\nhexbinplot(Diamonds$PricePerCt ~ Diamonds$Carat, colramp = rf)\n\n\n\n\n\n\nMix \nlattice\n and \nhexbin\n\n\ndata(gvhd10, package = 'latticeExtra')\n\nxyplot(asinh(SSC.H) ~ asinh(FL2.H), gvhd10, aspect = 1, panel = panel.hexbinplot, .aspect.ratio = 1, trans = sqrt)\n\n\n\n\n\n\nxyplot(asinh(SSC.H) ~ asinh(FL2.H) | Days, gvhd10, aspect = 1, panel = panel.hexbinplot, .aspect.ratio = 1, trans =sqrt)\n\n\n\n\n\n\nThe \ncar\n Package (scatter)\n\n\nlibrary(car)\n\n\n\n\nScatter plot\n\n\nscatterplotMatrix(~mpg + disp + drat + wt | cyl, data = mtcars,\n   main = 'Three Cylinder Options')\n\n\n\n\n\n\nThe \nscatterplot3d\n Package\n\n\nlibrary(scatterplot3d)\n\n\n\n\nScatter plot\n\n\nscatterplot3d(wt, disp, mpg, main = 'Title')\n\n\n\n\n\n\nscatterplot3d(wt, disp, mpg, pch = 16, highlight.3d = TRUE, type = 'h', main = 'Title')\n\n\n\n\n\n\ns3d \n- scatterplot3d(wt, disp, mpg, pch = 16, highlight.3d = TRUE, type = 'h', main = '   Title')\n\nfit \n- lm(mpg ~ wt + disp)\n\ns3d$plane3d(fit)\n\n\n\n\n\n\nThe \nrgl\n Package (interactive)\n\n\nlibrary(rgl)\n\n\n\n\nInteractive plot\n\n\nThe plot will open a new window.\n\n\nplot3d(wt, disp, mpg, col = 'red', size = 3)\n\n\n\n\nThe \ncluster\n Package (dendrogram)\n\n\nlibrary(cluster)\n\n\n\n\nDendrogram\n\n\nUse the \niris\n dataset.\n\n\nsubset \n- sample(1:150, 20)\ncS \n- as.character(Sp \n- iris$Species[subset])\ncS\n\n\n\n\n##  [1] \"setosa\"     \"versicolor\" \"setosa\"     \"virginica\"  \"virginica\" \n##  [6] \"setosa\"     \"setosa\"     \"setosa\"     \"virginica\"  \"setosa\"    \n## [11] \"versicolor\" \"versicolor\" \"virginica\"  \"setosa\"     \"versicolor\"\n## [16] \"versicolor\" \"setosa\"     \"virginica\"  \"versicolor\" \"versicolor\"\n\n\n\ncS[Sp == 'setosa'] \n- 'S'\ncS[Sp == 'versicolor'] \n- 'V'\ncS[Sp == 'virginica'] \n- 'g'\n\nai \n- agnes(iris[subset, 1:4])\n\nplot(ai, label = cS)\n\n\n\n\n\n\nThe \nextracat\n Package (splom)\n\n\nlibrary(extracat)\n\n\n\n\nSplom\n\n\nFor missing values. Binary matrix with reordering and filtering of rows\n\nand columns. The x-axis shows the frequency of NA. The y-axis shows the\n\nmarginal distribution of NA.\n\n\n# example 1\ndata(CHAIN, package = 'mi')\n\nvisna(CHAIN, sort = 'b')\n\n\n\n\n\n\nsummary(CHAIN)\n\n\n\n\n##    log_virus           age            income          healthy     \n##  Min.   : 0.000   Min.   :21.00   Min.   : 1.000   Min.   :16.67  \n##  1st Qu.: 0.000   1st Qu.:37.00   1st Qu.: 2.000   1st Qu.:35.00  \n##  Median : 0.000   Median :43.00   Median : 3.000   Median :45.37  \n##  Mean   : 4.324   Mean   :42.56   Mean   : 3.377   Mean   :44.40  \n##  3rd Qu.: 9.105   3rd Qu.:48.00   3rd Qu.: 5.000   3rd Qu.:54.89  \n##  Max.   :13.442   Max.   :70.00   Max.   :10.000   Max.   :70.11  \n##  NA's   :179      NA's   :24      NA's   :38       NA's   :24     \n##      mental           damage        treatment     \n##  Min.   :0.0000   Min.   :1.000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:0.0000  \n##  Median :0.0000   Median :4.000   Median :1.0000  \n##  Mean   :0.2717   Mean   :3.578   Mean   :0.8602  \n##  3rd Qu.:1.0000   3rd Qu.:5.000   3rd Qu.:2.0000  \n##  Max.   :1.0000   Max.   :5.000   Max.   :2.0000  \n##  NA's   :24       NA's   :63      NA's   :24\n\n\n\n# example 2\ndata(oly12, package = 'VGAMdata')\n\noly12d \n- oly12[, names(oly12) != 'DOB']\noly12a \n- oly12\n\nnames(oly12a) \n- abbreviate(names(oly12), 3)\n\nvisna(oly12a, sort = 'b')\n\n\n\n\n\n\n# example 3\ndata(freetrade, package = 'Amelia')\n\nfreetrade \n- within(freetrade, land1 \n- reorder(country, tariff, function(x) sum(is.na(x))))\n\nfluctile(xtabs(is.na(tariff) ~ land1 + year, data = freetrade))\n\n\n\n\n\n\n## viewport[base]\n\n\n\n# example 4\ndata(Pima.tr2, package = 'MASS')\n\nvisna(Pima.tr2, sort = 'b')\n\n\n\n\n\n\nThe \nash\n Package (density)\n\n\nlibrary(ash)\n\n\n\n\nDensity plot\n\n\nplot(ash1(bin1(mtcars$mpg, nbin = 50)), type = 'l')\n\n\n\n\n## [1] \"ash estimate nonzero outside interval ab\"\n\n\n\n\n\nThe \nKernSmooth\n Package (density)\n\n\nlibrary(KernSmooth)\n\n\n\n\nDensity plot\n\n\nwith(mtcars, {\n  hist(mpg, freq = FALSE, main = '', col = 'bisque2', ylab = '')\n  lines(density(mpg), lwd = 2)\n  ks1 \n- bkde(mpg, bandwidth = dpik(mpg))\n  lines(ks1, col = 'red', lty = 5, lwd = 2)})\n\n\n\n\n\n\nThe \ncrorplot\n Package (correlation)\n\n\nlibrary(corrplot)\n\n\n\n\nSplom\n\n\n# Create a correlation matrix for the dataset (9-14 are the '2' variables only)\ncorrelations \n- cor(mtcars)\n\ncorrplot(correlations)", 
            "title": "Plot Snippets for Exploratory (and some Explanatory) Analyses"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#data-type-dataset", 
            "text": "", 
            "title": "Data Type &amp; Dataset"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#data-types", 
            "text": "continuous vs categorical (or discrete).  continuous: float, x-y-z, 3D, map coordinates, trianguar, lat-long, \n    polar, degree-distance, angle-vector.  categorical: integer, binary, dichotomic, dummy, factor, \n    ordinal (ordered).   Continuous variable characteristics:   asymmetry.  outliers.  multimodality.  gaps, missing values.  heaping, redundance.  rounding, integer.  impossibilities, anomalies.  errors.    Categorical variable characteristics:   unexpected pattern of results.  uneven distribution.  extra categories.  unbalanced experiments.  large numbers of categories.  NA, errors, missings  nominal: no fixed order.  ordinal: fixed order (scale of 1 to 5).  discrete: counts, integers.  dependencies, correlation, associations.  causal relationships, outliers, groups, clusters, gaps, barriers, \n    conditional relationship.    Univariate main plots:   histogram.  density.  qqmath chart.  box   whickers chart.  bar chart.  dot.   Bivariate main plots:   xy chart.  qq chart.   Trivariate main plots:   cloud.  wireframe.  countour.  level.   Multivariate main plots:   sploms.  parallel charts (coordinate).   Specialized plots:   frequencies, crosstabs: bar charts, mosaic plots, association plots.  correlations: sploms, pairs, correlograms.  t-tests, non-parrametric tests of group differences: box plot, \n    density plot.  regression: scatter plot.  ANOVA: box plots, line plots.", 
            "title": "Data Types"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#functions", 
            "text": "Create a new variable  iris2  - within(iris, area  - Petal.Width*Petal.Length)\nhead(iris2, 3)  ##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species area\n## 1          5.1         3.5          1.4         0.2  setosa 0.28\n## 2          4.9         3.0          1.4         0.2  setosa 0.28\n## 3          4.7         3.2          1.3         0.2  setosa 0.26  area  - with(iris, area  - Petal.Width*Petal.Length)\nhead(area, 3)  ## [1] 0.28 0.28 0.26", 
            "title": "Functions"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#dataset", 
            "text": "For most examples, we use the  mtcars  dataset.  Prepare the dataset.  attach(mtcars)  Get data attached to a package (an example).  data(gvhd10, package = 'latticeExtra')", 
            "title": "Dataset"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-basic-package", 
            "text": "", 
            "title": "The Basic Package"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#basic-plots-options-parameters", 
            "text": "Standardize the parameters (an example)  # color and tick mark text orientation\npar(col = 'black', las = 1)  Grid and layout  One plot.  plot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')   A grid of plots.  par(mfrow = c(2, 1))\n\nplot(mpg, hp, ylab = 'horsepower', xlab = 'miles per gallon')\nboxplot(mpg ~ cyl, xlab = 'mile per gallon', ylab = 'number of cylinders', horizontal = TRUE)\n\npar(mfrow = c(1, 2))\n\nplot(mpg, hp, ylab = 'horsepower', xlab = 'miles per gallon')\nboxplot(mpg ~ cyl, xlab = 'mile per gallon', ylab = 'number of cylinders', horizontal = TRUE)   par(mfrow = c(1, 1))  Other grids.  layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))\n\nplot(mpg, xlab = 'observations', ylab = 'miles per gallon')\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\nboxplot(mpg ~ cyl, ylab = 'mile per gallon', xlab = 'number of cylinders')   # view\nmatrix(c(1,2,1,3), 2, 2, byrow = TRUE)  ##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    3  layout(matrix(c(1,2,1,3), 2, 2, byrow = TRUE))\n\nhist(wt)\nhist(mpg)\nhist(disp)   layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE),  widths = c(3,1), heights = c(1,2))\n\nhist(wt)\nhist(mpg)\nhist(disp)   nf  - layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths = lcm(12), heights = lcm(6))\nlayout.show(nf)   plot(mpg, xlab = 'observations', ylab = 'miles per gallon')\nplot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\nboxplot(mpg ~ cyl, ylab = 'mile per gallon', xlab = 'number of cylinders')   Gridview with additional packages.  library(vcd)  mplot(A, B, C)   See the  lattice  and  latticeExtra  packages for built-in \nfacet/gridview.  ggplot2  as well.  Plot and add ablines  plot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\n# abline(h = yvalues, v = xvalues)\nabline(lm(mpg ~ hp))\n\n# main = 'Title' or...\ntitle('Title')   plot(hp, mpg, xlab = 'horsepower', ylab = 'miles per gallon')\n\nabline(h = c(20, 25))\nabline(v = c(50, 150))\nabline(v = seq(200, 300, 50), lty = 2, col = 'blue')   Add a legend  boxplot(mpg ~ cyl, main = 'Title',\n   yaxt = 'n', xlab = 'mile per gallon', horizontal = TRUE, col = terrain.colors(3))\n\nlegend('topright', inset = 0.05, title = 'number of cylinders', c('4','6','8'), fill = terrain.colors(3), horiz = TRUE)   Save  mygraph  - plot(hp, mpg, main = 'Title', xlab = 'horsepower', ylab = 'miles per gallon')\n\npdf('mygraph.pdf')\npng('mygraph.png')\njpeg('mygraph.jpg')\nbmp('mygraph.bmp')\npostscript('mygraph.ps')  View in a new window  Typing the function will open a new window to render the plot.   windows()  for Windows.  X11()  for Linux.  quartz()  for OS X.    # open the new windows\nwindows()\n\nplot(hp, mpg, main = 'Title', xlab = 'horsepower', ylab = 'miles per gallon')  Enrich the plot, add text  plot(hp, mpg,\n     main = 'Title', col.main = 'blue',\n     sub = 'figure 1', col.sub = 'blue',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     col.lab = 'red', cex.lab = 0.9,\n     xlim = c(50, 350),\n     ylim = c(0, 40))\n\ntext(100, 10, 'text 1') # x and y coordinate\nmtext('text 2', 4, line = 0.5) # pos = 1 (bottom), 2 (left), 3 (top), 4 (right); line (margin)   With  locator() , use the mouse; with 1 for 1 click, 2 for  Find the \ncoordinates to be entered in the code. For example (after two clicks):   locator(2)\n$x\n[1] 212.5308 293.7854\n\n$y\n[1] 33.34040 31.87281  plot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\ntext(hp, mpg, row.names(mtcars), cex = 0.7, pos = 4, col = 'red')   Enrich the plot, add symbols  plot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\nsymbols(250, 20, squares = 1, add = TRUE, inches = 0.1, fg = 'red')\nsymbols(250, 25, circles = 1, add = TRUE, inches = 0.1, fg = 'red')   #rectangles\n#stars\n#thermometers\n#boxplots  Combine plots; change  pch =     col =  par(mfrow = c(2,2))\n\n# 1\nplot(hp, mpg,\n     main = 'P1',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 1,\n     col = 'black')\n\n# 2\nplot(hp, mpg,\n     main = 'P2',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 3,\n     col = 'blue',\n     cex = 0.5)\n\n# 3\nplot(hp, mpg,\n     main = 'P3',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 5,\n     col = 'red',\n     cex = 2)\n\n# 4\nplot(hp, mpg,\n     main = 'P4',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     pch = 7,\n     col = 'green')   # reverse\npar(mfrow = c(1,1))  Change  col =   Change  pch =   par(fig = c(0,0.8,0,0.8))\n\nplot(mtcars$wt, mtcars$mpg, xlab = 'Car Weight',   ylab = 'miles Per Gallon')\n\npar(fig = c(0,0.8,0.55,1), new = TRUE)\n\nboxplot(mtcars$wt, horizontal = TRUE, axes = FALSE)\n\npar(fig = c(0.65,1,0,0.8), new = TRUE)\n\nboxplot(mtcars$mpg, axes = FALSE)\n\nmtext('Enhanced Scatterplot', side = 3, outer = TRUE, line = -3)   # reverse\npar(mfrow = c(1,1))  Change  type = ; without dots  x  - c(1:5); y  - x\n\npar(pch = 22, col = 'red') # plotting symbol and color\n\npar(mfrow = c(2,4)) # all plots on one page\nopts = c('p','l','o','b','c','s','S','h')\n\nfor (i in 1:length(opts)) {\n  heading = paste('type =',opts[i])\n  plot(x, y, type = 'n', main = heading)\n  lines(x, y, type = opts[i])\n}   # reverse\npar(mfrow = c(1,1), col = 'black')  Change  type = ; with dots  x  - c(1:5); y  - x\n\npar(pch = 22, col = 'blue') # plotting symbol and color\n\npar(mfrow = c(2,4)) # all plots on one page\nopts = c('p','l','o','b','c','s','S','h')\n\nfor (i in 1:length(opts)) {\n  heading = paste('type =',opts[i])\n  plot(x, y, main = heading)\n  lines(x, y, type = opts[i])\n}   # reverse\npar(mfrow = c(1,1), col = 'black')  Add or modify the axes  plot(hp, mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon',\n     xaxt = 'n',\n     yaxt = 'n')\n\naxis(1, at = c(100, 200, 300), labels = NULL, pos = 15, lty = 'dashed', col = 'green', las = 2, tck = -0.05)\n\naxis(4, at = c(20, 30), labels = c('bt', 'up'), pos = 125, lty = 'dashed', col = 'blue', las = 2, tck = -0.05)   # reverse\npar(las = 1)  Add layers to the first plot  plot(mpg,\n     main = 'Title',\n     xlab = 'horsepower', \n     ylab = 'miles per gallon')\n\n# add lines\nlines(mpg[1:10], type = 'l', col = 'green')", 
            "title": "Basic Plots, Options &amp; Parameters"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#univariate-plots", 
            "text": "Plot; continuous  plot(mpg, main = 'Title', xlab = 'observations', ylab = 'miles per gallon')   Plot; categorical  plot(cyl, main = 'Title', xlab = 'observations', ylab = 'cylinders')   QQnorm; continuous  qqnorm(mpg, main = 'Title', xlab = 'observations', ylab = 'cylinders')   QQnorm; categorical  qqnorm(cyl, main = 'Title', xlab = 'observations', ylab = 'cylinders')   Stripchart; continuous  stripchart(mpg, main = 'Title', xlab = 'miles per gallon')   Stripchart; categorical  stripchart(cyl, main = 'Title', xlab = 'cylinders')   Barplot (vertical); continuous  barplot(mpg[1:10], main = 'Title', xlab = 'observations', ylab = 'miles per gallon')   Barplot (horizontal); categorical  barplot(cyl[1:10], main = 'Title', horiz = TRUE, xlab = 'cylinders', ylab = 'observations')   Barplots options  Group with  table() .  counts  - table(cyl)\ncounts  ## cyl\n##  4  6  8 \n## 11  7 14  barplot(counts, main = 'Title', horiz = TRUE, xlab = 'count', names.arg = c('4 Cyl', '6 Cyl', '8 Cyl'))   counts  - table(vs, gear)\ncounts  ##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1  barplot(counts, main = 'Title', xlab = 'gearbox', col = c('darkblue', 'red'), legend = rownames(counts))    counts  - table(vs, gear)\ncounts  ##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1  barplot(counts, main = 'Title', xlab='gearbox', col = c('darkblue', 'red'), legend =  rownames(counts), beside = TRUE)   Group with  aggregate() .  aggregate(mtcars, by = list(cyl, vs), FUN = mean, na.rm = TRUE)  ##   Group.1 Group.2      mpg cyl   disp       hp     drat       wt     qsec\n## 1       4       0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000\n## 2       6       0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667\n## 3       8       0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214\n## 4       4       1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100\n## 5       6       1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500\n##   vs        am     gear     carb\n## 1  0 1.0000000 5.000000 2.000000\n## 2  0 1.0000000 4.333333 4.666667\n## 3  0 0.1428571 3.285714 3.500000\n## 4  1 0.7000000 4.000000 1.500000\n## 5  1 0.0000000 3.500000 2.500000  par(las = 2) # make label text perpendicular to axis\n\npar(mar = c(5, 8, 4, 2)) # increase y-axis margin.\n\ncounts  - table(mtcars$gear)\nbarplot(counts, main = 'Car Distribution', horiz = TRUE, names.arg = c('3 Gears', '4 Gears', '5   Gears'), cex.names = 0.8)   # reverse\npar(las = 1)  Colors.  library(RColorBrewer)\n\npar(mfrow = c(2, 1))\n\nbarplot(iris$Petal.Length)\nbarplot(table(iris$Species, iris$Sepal.Length), col = brewer.pal(3, 'Set1'))   par(mfrow = c(1, 1))  Pie Chart  Avoid!  Dotchart; continuous  dotchart(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'observations')   Dotchart; categorical  dotchart(cyl, main = 'Title', xlab = 'cylinders', ylab = 'observations')   Dotchart options  dotchart(mpg,labels = row.names(mtcars), cex = 0.7, main = 'Title', xlab = 'miles per gallon')   # sort by mpg\nx  - mtcars[order(mpg),]\n\n# must be factors\nx$cyl  - factor(x$cyl)\nx$color[x$cyl == 4]  - 'red'\nx$color[x$cyl == 6]  - 'blue'\nx$color[x$cyl == 8]  - 'darkgreen'\n\ndotchart(x$mpg, labels = row.names(x), cex = 0.7, groups = x$cyl, main = 'Title',  xlab = 'miles per gallon', gcolor = 'black', color = x$color)   More with the  hmisc  package and  panel.dotplot()  and in the  lattice \npackage section.  Boxplot; continuous  boxplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'observations')   Stem; continuous  stem(mpg)  ## \n##   The decimal point is at the |\n## \n##   10 | 44\n##   12 | 3\n##   14 | 3702258\n##   16 | 438\n##   18 | 17227\n##   20 | 00445\n##   22 | 88\n##   24 | 4\n##   26 | 03\n##   28 | \n##   30 | 44\n##   32 | 49  Histogram; continuous  hist(mpg, main = 'Title', xlab = 'miles per gallon - bins', ylab = 'count')   Histogram; categorical  hist(cyl, main = 'Title', xlab = 'cylinders - bins', ylab = 'count')   Histogram options  hist(mpg, breaks = 12, col = 'red')   x  - mpg\n\nh  - hist(x, breaks = 10, main = 'Title', xlab = 'miles per gallon')\n\nxfit  - seq(min(x), max(x),length = 40)\nyfit  - dnorm(xfit, mean = mean(x), sd = sd(x))\nyfit  - yfit*diff(h$mids[1:2])*length(x)\n\nlines(xfit, yfit, col = 'blue', lwd = 2)   Colors.  library(RColorBrewer)\n\npar(mfrow = c(2, 3))\n\nhist(VADeaths, breaks = 10, col = brewer.pal(3, 'Set3'), main = '3, Set3')\nhist(VADeaths, breaks = 4, col = brewer.pal(3, 'Set2'), main = '3, Set2')\nhist(VADeaths, breaks = 8, col = brewer.pal(3, 'Set1'), main = '3, Set1')\nhist(VADeaths, breaks = 2, col = brewer.pal(8, 'Set3'), main = '8, Set3')\nhist(VADeaths, breaks = 10, col = brewer.pal(8, 'Greys'), main = '8, Greys')\nhist(VADeaths, breaks = 10, col = brewer.pal(8, 'Greens'), main = '8, Greens')   par(mfrow = c(1, 1))  Density Plot; continuous  plot(density(mpg), main = 'Title')   plot(density(mpg), main = 'Title')\n\npolygon(density(mpg), col = 'red', border = 'blue')    d1  - density(mtcars$mpg)\nplot(d1)\nrug(mtcars$mpg)\n\nlines(density(mtcars$mpg, d1$bw/2), col = 'green')\nlines(density(mtcars$mpg, d1$bw/5), col = 'blue')", 
            "title": "Univariate Plots"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#bivariate-multivariate-plots", 
            "text": "Plot, continuous/continuous  plot(mpg, hp, main = 'Title', xlab = 'miles per gallon', ylab = 'horsepowers')   Plot, continuous/categorical  plot(mpg, cyl, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   Plot options  plot(wt, mpg, main = 'Title', xlab = 'weight', ylab = 'miles per gallon ')\n\nabline(lm(mpg ~ wt), col = 'red') # regression\nlines(lowess(wt, mpg), col = 'blue') # lowess line   SmoothScatter; continuous/continuous  smoothScatter(mpg, hp, main = 'Title', xlab = 'miles per gallon', ylab = 'horsepowers')   Sunflowerplot; categorical/categorical  Special symbols at each location: one observation = one dot; more \nobservations = cross, star, etc.  sunflowerplot(gear, cyl, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')   Boxplot  boxplot(mpg ~ cyl, main = 'Title',   xlab = 'cylinders', ylab = 'miles per gallon')    Colors.  library(RColorBrewer)\n\npar(mfrow = c(1, 2))\n\nboxplot(iris$Sepal.Length, col = 'red')\nboxplot(iris$Sepal.Length ~ iris$Species, col = topo.colors(3))   par(mfrow = c(1, 1))  library(dplyr)\n\ndata(Pima.tr2, package = 'MASS')\n\nPimaV  - select(Pima.tr2, glu:age)\nboxplot(scale(PimaV), pch = 16, outcol = 'red')   Boxplot options  four  - subset(mpg, cyl == 4)\nsix  - subset(mpg, cyl == 6)\neight  - subset(mpg, cyl == 8)\n\nboxplot(four, six, eight, main = 'Title', ylab = 'miles per gallon')\n\naxis(1, at = c(1, 2, 3), labels = c('4 Cyl', '6 Cyl', '8 Cyl'))   Dotchart  counts  - table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  dotchart(counts, main = 'Title', xlab = 'count', ylab = 'cylinders/gearbox')   counts  - table(cyl, gear)\ncounts  ##    gear\n## cyl  3  4  5\n##   4  1  8  2\n##   6  2  4  1\n##   8 12  0  2  dotchart(counts, main = 'Title', xlab = 'count', ylab = 'gearbox/cylinders')   Barplot with its options  Vertical or horizontal. The legend as well can be horizontal or \nvertical.  counts  - table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  barplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 20), col = terrain.colors(3))\n\nlegend('topleft', inset = .04, title = 'gearbox',\n   c('3','4','5'), fill = terrain.colors(3), horiz = TRUE)   counts  - table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  barplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 25), col = terrain.colors(3), legend = rownames(counts))   counts  - table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  barplot(counts, main = 'Title', xlab = 'cylinders', ylab = 'count', ylim = c(0, 20), col = terrain.colors(3), legend = rownames(counts), beside = TRUE)   Spineplot  Count  = blocks; categorical (with factors).  cyl2  - as.factor(cyl) # mandatory for the y\ngear2  - as.factor(gear)\n\nspineplot(gear2, cyl2, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')   Count = blocks; continuous.  spineplot(mpg, cyl2, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   Mosaicplot  Count = blocks.  counts  - table(gear, cyl)\ncounts  ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2  mosaicplot(counts, main = 'Title', xlab = 'gearbox', ylab = 'cylinders')", 
            "title": "Bivariate (Multivariate) Plots"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#multivariate-plots", 
            "text": "Pairs  pairs( ~mpg + disp + hp)   Coplot  coplot(mpg ~ hp | wt)   Correlograms  library(corrgram)\n\ncorrgram(mtcars, order = TRUE, lower.panel = panel.shade, upper.panel=panel.pie, text.panel = panel.txt, main = 'Car Milage Data in PC2/PC1 Order')   Plot a dataset with colors  library(RColorBrewer)\n\nplot(iris, col = brewer.pal(3, 'Set1'))   Stars  The star branches are explanatory; be careful with the interpretation! \nWell-advised for visual and pattern exploration.  mtcars[1:4, c(1, 4, 6)]  ##                 mpg  hp    wt\n## Mazda RX4      21.0 110 2.620\n## Mazda RX4 Wag  21.0 110 2.875\n## Datsun 710     22.8  93 2.320\n## Hornet 4 Drive 21.4 110 3.215  stars(mtcars[1:4, c(1, 4, 6)])   Trivariate plots   image() .  contour() .  filled.contour() .  persp() .  symbols() .", 
            "title": "Multivariate Plots"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#times-series", 
            "text": "Add packages:  zoo  and  xts .  Basics  plot(AirPassengers, type = 'l')   Change the  type =  y1  - rnorm(100)\n\npar(mfrow = c(2, 1))\n\nplot(y1, type = 'p', main = 'p vs l')\nplot(y1, type = 'l')   plot(y1, type = 'l', main = 'l vs h')\nplot(y1, type = 'h')   plot(y1, type = 'l', lty = 3, main = 'l 3 vs o')\nplot(y1, type = 'o')   plot(y1, type = 'b', main = 'b vs c')\nplot(y1, type = 'c')   plot(y1, type = 's', main = 's vs S')\nplot(y1, type = 'S')   # reverse\npar(mfrow = c(1, 1))  Add a box  y1  - rnorm(100)\ny2  - rnorm(100)\n\npar(mfrow = (c(2, 1)))\n\nplot(y1, type = 'l', axes = FALSE, xlab = '', ylab = '', main = '')\n\nbox(col = 'gray')\n\nlines(x = c(20, 20, 40, 40), y = c(-7, max(y1), max(y1), -7), lwd = 3, col = 'gray')\n\nplot(y2, type = 'l', axes = FALSE, xlab = '', ylab = '', main = '')\n\nbox(col = 'gray')\n\nlines(x = c(20, 20, 40, 40), y = c(7, min(y2), min(y2), 7), lwd = 3, col = 'gray')   # reverse\npar(mfrow = c(1,1))  Add lines and text within the plot  y1  - rnorm(100)\n\n# x goes from 0 to 100\n# xaxt = 'n' remove the x ticks\nplot(y1, type = 'l', lwd = 2, lty = 'longdash', main = 'Title', ylab = 'y', xlab = 'time', xaxt = 'n')\n\nabline(h = 0, lty = 'longdash')\n\nabline(v = 20, lty = 'longdash')\nabline(v = 50, lty = 'longdash')\nabline(v = 95, lty = 'longdash')\n\ntext(17, 1.5, srt = 90, adj = 0, labels = 'Tag 1', cex = 0.8)\ntext(47, 1.5, srt = 90, adj = 0, labels = 'Tag a', cex = 0.8)\ntext(92, 1.5, srt = 90, adj = 0, labels = 'Tag alpha', cex = 0.8)   A comprehensive example  # new data\nhead(Orange)  ##   Tree  age circumference\n## 1    1  118            30\n## 2    1  484            58\n## 3    1  664            87\n## 4    1 1004           115\n## 5    1 1231           120\n## 6    1 1372           142  # convert factor to numeric for convenience\nOrange$Tree  - as.numeric(Orange$Tree)\nntrees  - max(Orange$Tree)\n\n# get the range for the x and y axis\nxrange  - range(Orange$age)\nyrange  - range(Orange$circumference)\n\n# set up the plot\nplot(xrange, yrange, type = 'n', xlab = 'Age (days)',\n   ylab = 'Circumference (mm)' )\ncolors  - rainbow(ntrees)\nlinetype  - c(1:ntrees)\nplotchar  - seq(18, 18 + ntrees, 1)\n\n# add lines\nfor (i in 1:ntrees) {\n  tree  - subset(Orange, Tree == i)\n  lines(tree$age, tree$circumference, type = 'b', lwd = 1.5,\n    lty = linetype[i], col = colors[i], pch = plotchar[i])\n}\n\n# add a title and subtitle\ntitle('Tree Growth', 'example of line plot')\n\n# add a legend\nlegend(xrange[1], yrange[2], 1:ntrees, cex = 0.8, col = colors,\n   pch = plotchar, lty = linetype, title = 'Tree')   Change  lty =", 
            "title": "Times Series"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#regressions-and-residual-plots", 
            "text": "# first\nregr  - lm(mpg ~ hp)\n\nsummary(regr)  ## \n## Call:\n## lm(formula = mpg ~ hp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7121 -2.1122 -0.8854  1.5819  8.2360 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr( |t|)    \n## (Intercept) 30.09886    1.63392  18.421    2e-16 ***\n## hp          -0.06823    0.01012  -6.742 1.79e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.863 on 30 degrees of freedom\n## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 \n## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07  plot(mpg ~ hp)\nabline(regr)   par(mfrow = c(2, 2))\n\n# then\nplot(regr)   # reverse\npar(mfrow = c(1, 1))", 
            "title": "Regressions and Residual Plots"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-lattice-and-latticeextra-packages", 
            "text": "library(lattice)", 
            "title": "The lattice and latticeExtra Packages"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#coloring", 
            "text": "# Show the default settings\nshow.settings()   # Save the default theme\nmytheme  - trellis.par.get()\n\n# Turn the B W\ntrellis.par.set(canonical.theme(color = FALSE))\nshow.settings()", 
            "title": "Coloring"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#documentation", 
            "text": "National Park Service, Advanced \n    Graphics (Lattice)  Treillis \n    Plots", 
            "title": "Documentation"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#a-note-on-reordering-the-levels-factors", 
            "text": "# start\ncyl  - mtcars$cyl\ncyl  - as.factor(cyl)\ncyl  ##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n## Levels: 4 6 8  levels(cyl)  ## [1] \"4\" \"6\" \"8\"  # option 1\ncyl  - factor(cyl, levels = c('8', '6', '4'))\n# or levels = 3:1\n# or levels = letters[3:1]\nlevels(cyl)  ## [1] \"8\" \"6\" \"4\"  cyl  - mtcars$cyl\ncyl  - as.factor(cyl)\n# option 2\ncyl  - reorder(cyl, new.order = 3:1)\nlevels(cyl)  ## [1] \"8\" \"6\" \"4\"  library(lattice)\n\n# normalized x-axis for comparison\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2))   # free x-axis\nbarchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2), scales = list(x = 'free'))   # or\nbc.titanic  - barchart(Class ~ Freq | Sex + Age, data = as.data.frame(Titanic), groups = Survived, stack = TRUE, layout = c(4, 1), auto.key = list(title = 'Survived', columns = 2), scales = list(x = 'free'))\n\nbc.titanic   # add bg grid\nupdate(bc.titanic, panel = function(...) {\n  panel.grid(h = 0, v = -1)\n  panel.barchart(...)\n})   # remove lines\nupdate(bc.titanic, panel = function(...) {\n  panel.barchart(..., border = 'transparent')\n})   # or\nupdate(bc.titanic, border = 'transparent')   Titanic1  - as.data.frame(as.table(Titanic[, , 'Adult' ,]))\nTitanic1  ##    Class    Sex Survived Freq\n## 1    1st   Male       No  118\n## 2    2nd   Male       No  154\n## 3    3rd   Male       No  387\n## 4   Crew   Male       No  670\n## 5    1st Female       No    4\n## 6    2nd Female       No   13\n## 7    3rd Female       No   89\n## 8   Crew Female       No    3\n## 9    1st   Male      Yes   57\n## 10   2nd   Male      Yes   14\n## 11   3rd   Male      Yes   75\n## 12  Crew   Male      Yes  192\n## 13   1st Female      Yes  140\n## 14   2nd Female      Yes   80\n## 15   3rd Female      Yes   76\n## 16  Crew Female      Yes   20  barchart(Class ~ Freq | Sex, Titanic1, groups = Survived, stack = TRUE, auto.key = list(title = 'Survived', columns = 2))   Titanic2  - reshape(Titanic1, direction = 'wide', v.names = 'Freq', idvar = c('Class', 'Sex'), timevar = 'Survived')\n\nnames(Titanic2)  - c('Class', 'Sex', 'Dead', 'Alive')\n\nbarchart(Class ~ Dead + Alive | Sex, Titanic2, stack = TRUE, auto.key = list(columns = 2))", 
            "title": "A note on reordering the levels (factors)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#uni-bi-multivariate-plots", 
            "text": "Barchart  Like  barplot() .  # y ~ x\nbarchart(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')   # y ~ x\nbarchart(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', horizontal = FALSE)   barchart(VADeaths, groups = FALSE, layout = c(1, 4), aspect = 0.7, reference =FALSE, main = 'Title', xlab = 'rate per 100')   data(postdoc, package = 'latticeExtra')\n\nbarchart(prop.table(postdoc, margin = 1), xlab = 'Proportion', auto.key = list(adj = 1))   Change  layout = c(x, y, page)  barchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'horsepowers', ylab = 'cylinders - miles per gallon', layout = c(1,3))   barchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'cylinders - horsepowers', ylab = 'miles per gallon', layout = c(3,1))   Change  aspect = 1  1  for square.  barchart(mpg ~ hp | factor(cyl), main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1)   Colors  barchart(mpg ~ hp, group = cyl, auto.key = list(space = 'right'), main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')    shingle() ; control the ranges.  equal.count() ; grid.   Dotplot  Like  dotchart() .  dotplot(mpg, main = 'Title', xlab = 'miles per gallon')   dotplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(3,1))   dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3)   dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3, origin = 0)   dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), aspect = 0.3, origin = 0, type = c('p', 'h'))   Set  auto.key .  # maybe we'll want this later\nold.pars  - trellis.par.get()\n\n#trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 12:14))\n\ntrellis.par.set(superpose.symbol = list(pch = c(1,3), col = 1))\n\n# Optionally put things back how they were\n#trellis.par.set(old.pars)  Use  auto.key .  dotplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3), groups = vs, auto.key = list(space = 'right'))   trellis.par.set(old.pars)  trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 1))\n\ndotplot(variety ~ yield | site, barley, layout = c(1, 6), aspect = c(0.7), groups = year, auto.key = list(space = 'right'))   trellis.par.set(old.pars)  Vertical.  dotplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'cylinders', ylab = 'gearbox - miles per gallon', layout = c(1,3), aspect = 0.3)   library(readr)\ndensity  - read_csv('density.csv')\ndensity$Density  - as.numeric(density$Density)\n\ndotplot(reorder(MetropolitanArea, Density) ~ Density, density, type = c('p', 'h'), main = 'Title', xlab = 'Population Density (pop / sq.mi)')   dotplot(reorder(MetropolitanArea, Density) ~ Density | Region, density, type = c('p', 'h'), strip = FALSE, strip.left = TRUE, layout = c(1, 3), scales = list(y = list(relation = 'free')), main = 'Title', xlab = 'Population Density (pop / sq.mi)')   Stripplot  Like  stripchart() .  stripplot(mpg, main = 'Title', xlab = 'miles per gallon')   stripplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   stripplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(1,3))   stripplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'gearbox - miles per gallon', ylab = 'cylinders', layout = c(1,3), groups = vs, auto.key = list(space = 'right'))   stripplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'cylinders', ylab = 'gearbox - miles per gallon', layout = c(1,3))   Histogram  Like  hist() .  histogram(mpg, main = 'Title', xlab = 'miles per gallon')   histogram(~mpg | factor(cyl), layout = c(1, 3), main = 'Title', xlab = 'miles per gallon', ylab = 'density')   Densityplot  Like  plot.density() .  densityplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'density')   densityplot(~mpg | factor(cyl), layout = c(1, 3), main = 'Title', xlab = 'miles per gallon', ylab = 'density')   ECDFplot  library(latticeExtra)\n\necdfplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = '')   BWplot  Like  boxplot .  bwplot(mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'density')   bwplot(factor(cyl) ~ mpg, main = 'Title', xlab = 'miles per gallon', ylab = 'cylinders')   bwplot(factor(cyl) ~ mpg | factor(gear), main = 'Title', xlab = 'miles per gallon', ylab = 'gearbox - cylinders', layout = c(1,3))   bwplot(mpg ~ factor(cyl) | factor(gear), main = 'Title', xlab = 'gearbox - cylinders', ylab = 'miles per gallon', layout = c(3,1))   QQmath  Like  qqnorm() .  qqmath(mpg, main = 'Title', ylab = 'miles per gallon')   XYplot  Like  plot() .  xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'horsepower', ylab = 'cylinders - miles per gallon', layout = c(1,3))   xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1))   XYplot options  xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1)   xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, scales = list(y = list(at = seq(10, 30, 10))))   meanmpg  - mean(mpg)\n\nxyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, panel = function(...) {\n  panel.xyplot(...)\n  panel.abline(h = meanmpg, lty = 'dashed')\n  panel.text(450, meanmpg + 1, 'avg', adj = c(1,  0), cex = 0.7)\n})   xyplot(mpg ~ disp | factor(cyl), main = 'Title', xlab = 'cylinder - horsepowers', ylab = 'miles per gallon', layout = c(3,1), aspect = 1, panel = function(x, y, ...) {\n    panel.lmline(x, y)\n    panel.xyplot(x, y, ...)\n})    panel.points() .  panel.lines() .  panel.segments() .  panel.arrows() .  panel.rect() .  panel.polygon() .  panel.text() .  panel.abline() .  panel.lmline() .  panel.xyplot() .  panel.curve() .  panel.rug() .  panel.grid() .  panel.bwplot() .  panel.histogram() .  panel.loess() .  panel.violin() .  panel.smoothScatter() .   par.settings .     library(lattice)\n\ndata(SeatacWeather, package = 'latticeExtra')\n\nxyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'l', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'p', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'l', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'o', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'r', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'g', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 's', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'S', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'h', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'a', lty = 1, col = 'black')   xyplot(min.temp + max.temp + precip ~ day | month, ylab = 'Temperature and Rainfall', data = SeatacWeather, layout = c(3,1), type = 'smooth', lty = 1, col = 'black')   xyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')   xyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', type = 'o')   xyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon', type = 'o', pch = 16, lty = 'dashed')   xyplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')   data(USAge.df, package = 'latticeExtra')\n\nxyplot(Population ~ Age | factor(Year), USAge.df, groups = Sex, type = c('l', 'g'), auto.key = list(points = FALSE, lines = TRUE, columns = 2), aspect = 'xy', ylab = 'Population (millions)', subset = Year %in% seq(1905, 1975, by = 10))   xyplot(Population ~ Year | factor(Age), USAge.df, groups = Sex, type = 'l', strip = FALSE, strip.left = TRUE, layout = c(1, 3), ylab = 'Population (millions)', auto.key = list(lines = TRUE, points = FALSE, columns = 2), subset = Age %in% c(0, 10, 20))   data(USCancerRates, package = 'latticeExtra')\n\nxyplot(rate.male ~ rate.female | state, USCancerRates, aspect = 'iso', pch = '.', cex = 2, index.cond = function(x, y) { median(y - x, na.rm = TRUE) }, scales = list(log = 2, at = c(75, 150, 300, 600)), panel = function(...) { \n  panel.grid(h = -1, v = -1)\n  panel.abline(0, 1)\n  panel.xyplot(...)\n  },\n  xlab = 'a',\n  ylab = 'b')   data(biocAccess, package = 'latticeExtra')\n\nbaxy  - xyplot(log10(counts) ~ hour | month + weekday, biocAccess, type = c('p', 'a'), as.table = TRUE, pch = '.', cex = 2, col.line = 'black')\n\nbaxy   library(latticeExtra)\nuseOuterStrips(baxy)   xyplot(sunspot.year, aspect = 'xy', strip = FALSE, strip.left = TRUE, cut = list(number = 4, overlap = 0.05))   data(biocAccess, package = 'latticeExtra')\n\nssd  - stl(ts(biocAccess$counts[1:(24 * 30 *2)], frequency = 24), 'periodic')\n\nxyplot(ssd, main = 'Title', xlab = 'Time (Days)')   Splom  splom(mtcars[c(1, 3, 6)], groups = cyl, data = mtcars, panel = panel.superpose, key = list(title = 'Three Cylinder Options', columns = 3, points = list(text = list(c('4 Cylinder', '6 Cylinder', '8 Cylinder')))))   trellis.par.set(superpose.symbol = list(pch = c(1,3, 22), col = 1, alpha = 0.5))\n\nsplom(~data.frame(mpg, disp, hp, drat, wt, qsec), data = mtcars, groups = cyl, pscales = 0, varnames = c('miles\\nper\\ngallon', 'displacement\\n(cu.in(', 'horsepower', 'rear\\naxle\\nratio', 'weight', '1/4\\nmile\\ntime'), auto.key = list(columns = 3, title = 'Title'))   trellis.par.set(old.pars)  splom(USArrests)   splom(~USArrests[c(3,1,2,4)] | state.region, pscales = 0, type = c('g', 'p', 'smooth'))   Parallel plot  For multivariate continuous data.  parallelplot(~iris[1:4])   parallelplot(~iris[1:4], horizontal.axis = FALSE)   parallelplot(~iris[1:4], scales = list(x = list(rot = 90)))   parallelplot(~iris[1:4] | Species, iris)   parallelplot(~iris[1:4], iris, groups = Species,\n             horizontal.axis = FALSE, scales = list(x = list(rot = 90)))   Trivariate plots  Like  image() ,  contour() ,  filled.contour() ,  persp() ,  symbols() .   levelplot() .  contourplot() .  cloud() .  wireframe() .", 
            "title": "Uni-, Bi-, Multivariate Plots"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#additional-packages", 
            "text": "", 
            "title": "Additional Packages"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-sm-package-density", 
            "text": "library(sm)  Density plot  # create value labels\ncyl.f  - factor(cyl, levels = c(4, 6, 8), labels = c('4 cyl', '6 cyl', '8 cyl'))\n\n# plot densities\nsm.density.compare(mpg, cyl, xlab = 'miles per gallon')\n\ntitle(main = 'Title')\n\n# add legend via mouse click\ncolfill  - c(2:(2 + length(levels(cyl.f))))\nlegend(25, 0.19, levels(cyl.f), fill = colfill)", 
            "title": "The sm Package (density)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-car-package-scatter", 
            "text": "library(car)  Scatter plot  scatterplot(mpg ~ wt | cyl, data = mtcars,    xlab = 'weight', ylab = 'miles per gallon', labels = row.names(mtcars))    Splom  scatterplotMatrix( ~mpg + disp + drat + wt | cyl, data = mtcars, main = 'Title')   scatterplotMatrix == spm .  spm( ~mpg + disp + drat + wt | cyl, data = mtcars, main = 'Title')", 
            "title": "The car Package (scatter)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-vioplot-package-boxplot", 
            "text": "library(vioplot)  Violin boxplot  x1  - mpg[mtcars$cyl == 4]\nx2  - mpg[mtcars$cyl == 6]\nx3  - mpg[mtcars$cyl == 8]\n\nvioplot(x1, x2, x3, names = c('4 cyl', '6 cyl', '8 cyl'), col = 'green')\n\ntitle('Title')", 
            "title": "The vioplot Package (boxplot)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-vcd-package-count-correlation-mosaic", 
            "text": "library(vcd)  The package provides a variety of methods for visualizing multivariate \ncategorical data.  Count  counts  - table(gear, cyl)\ncounts  ##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2  mosaic(counts, shade = TRUE, legend = TRUE)    Correlation  counts  - table(gear, cyl)\ncounts  ##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2  assoc(counts, shade = TRUE)   Mosaic  ucb  - data.frame(UCBAdmissions)\nucb  - within(ucb, Accept  - factor(Admit, levels = c('Rejected', 'Admitted')))\n\nlibrary(vcd); library(grid)\n\ndoubledecker(xtabs(Freq~ Dept + Gender + Accept, data = ucb), gp = gpar(fill = c('grey90', 'steelblue')))   data(Fertility, package = 'AER')\n\ndoubledecker(morekids ~ age, data = Fertility, gp = gpar(fill = c('grey90', 'green')), spacing = spacing_equal(0))   doubledecker(morekids ~ gender1 + gender2, data = Fertility, gp = gpar(fill = c('grey90', 'green')))   doubledecker(morekids ~ age + gender1 + gender2, data = Fertility, gp = gpar(fill = c('grey90', 'green')), spacing = spacing_dimequal(c(0.1, 0, 0, 0)))", 
            "title": "The vcd Package (count, correlation, mosaic)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-hexbin-package-scatter", 
            "text": "library(hexbin)  Scatter plot  # new data\ndata(NHANES)\n\n# compare\nplot(Serum.Iron ~ Transferin, NHANES, main = 'Title', xlab = 'Transferin', ylab = 'Iron')   # with\nhexbinplot(Serum.Iron ~ Transferin, NHANES, main = 'Title', xlab = 'Transferin', ylab = 'Iron')   hexbinplot(mpg ~ hp, main = 'Title', xlab = 'horsepowers', ylab = 'miles per gallon')   x  - rnorm(1000)\ny  - rnorm(1000)\n\nbin  - hexbin(x, y, xbins = 50)\nplot(bin, main = 'Title')    x  - rnorm(1000)\ny  - rnorm(1000)\n\nplot(x, y, main = 'Title', col =  rgb(0, 100, 0, 50, maxColorValue = 255), pch = 16)   data(Diamonds, package = 'Stat2Data')\n\na = hexbin(Diamonds$PricePerCt, Diamonds$Carat, xbins = 40)\n\nlibrary(RColorBrewer)\n\nplot(a)   Colors.  rf  - colorRampPalette(rev(brewer.pal(12, 'Set3')))\n\nhexbinplot(Diamonds$PricePerCt ~ Diamonds$Carat, colramp = rf)   Mix  lattice  and  hexbin  data(gvhd10, package = 'latticeExtra')\n\nxyplot(asinh(SSC.H) ~ asinh(FL2.H), gvhd10, aspect = 1, panel = panel.hexbinplot, .aspect.ratio = 1, trans = sqrt)   xyplot(asinh(SSC.H) ~ asinh(FL2.H) | Days, gvhd10, aspect = 1, panel = panel.hexbinplot, .aspect.ratio = 1, trans =sqrt)", 
            "title": "The hexbin Package (scatter)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-car-package-scatter_1", 
            "text": "library(car)  Scatter plot  scatterplotMatrix(~mpg + disp + drat + wt | cyl, data = mtcars,\n   main = 'Three Cylinder Options')", 
            "title": "The car Package (scatter)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-scatterplot3d-package", 
            "text": "library(scatterplot3d)  Scatter plot  scatterplot3d(wt, disp, mpg, main = 'Title')   scatterplot3d(wt, disp, mpg, pch = 16, highlight.3d = TRUE, type = 'h', main = 'Title')   s3d  - scatterplot3d(wt, disp, mpg, pch = 16, highlight.3d = TRUE, type = 'h', main = '   Title')\n\nfit  - lm(mpg ~ wt + disp)\n\ns3d$plane3d(fit)", 
            "title": "The scatterplot3d Package"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-rgl-package-interactive", 
            "text": "library(rgl)  Interactive plot  The plot will open a new window.  plot3d(wt, disp, mpg, col = 'red', size = 3)", 
            "title": "The rgl Package (interactive)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-cluster-package-dendrogram", 
            "text": "library(cluster)  Dendrogram  Use the  iris  dataset.  subset  - sample(1:150, 20)\ncS  - as.character(Sp  - iris$Species[subset])\ncS  ##  [1] \"setosa\"     \"versicolor\" \"setosa\"     \"virginica\"  \"virginica\" \n##  [6] \"setosa\"     \"setosa\"     \"setosa\"     \"virginica\"  \"setosa\"    \n## [11] \"versicolor\" \"versicolor\" \"virginica\"  \"setosa\"     \"versicolor\"\n## [16] \"versicolor\" \"setosa\"     \"virginica\"  \"versicolor\" \"versicolor\"  cS[Sp == 'setosa']  - 'S'\ncS[Sp == 'versicolor']  - 'V'\ncS[Sp == 'virginica']  - 'g'\n\nai  - agnes(iris[subset, 1:4])\n\nplot(ai, label = cS)", 
            "title": "The cluster Package (dendrogram)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-extracat-package-splom", 
            "text": "library(extracat)  Splom  For missing values. Binary matrix with reordering and filtering of rows \nand columns. The x-axis shows the frequency of NA. The y-axis shows the \nmarginal distribution of NA.  # example 1\ndata(CHAIN, package = 'mi')\n\nvisna(CHAIN, sort = 'b')   summary(CHAIN)  ##    log_virus           age            income          healthy     \n##  Min.   : 0.000   Min.   :21.00   Min.   : 1.000   Min.   :16.67  \n##  1st Qu.: 0.000   1st Qu.:37.00   1st Qu.: 2.000   1st Qu.:35.00  \n##  Median : 0.000   Median :43.00   Median : 3.000   Median :45.37  \n##  Mean   : 4.324   Mean   :42.56   Mean   : 3.377   Mean   :44.40  \n##  3rd Qu.: 9.105   3rd Qu.:48.00   3rd Qu.: 5.000   3rd Qu.:54.89  \n##  Max.   :13.442   Max.   :70.00   Max.   :10.000   Max.   :70.11  \n##  NA's   :179      NA's   :24      NA's   :38       NA's   :24     \n##      mental           damage        treatment     \n##  Min.   :0.0000   Min.   :1.000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:0.0000  \n##  Median :0.0000   Median :4.000   Median :1.0000  \n##  Mean   :0.2717   Mean   :3.578   Mean   :0.8602  \n##  3rd Qu.:1.0000   3rd Qu.:5.000   3rd Qu.:2.0000  \n##  Max.   :1.0000   Max.   :5.000   Max.   :2.0000  \n##  NA's   :24       NA's   :63      NA's   :24  # example 2\ndata(oly12, package = 'VGAMdata')\n\noly12d  - oly12[, names(oly12) != 'DOB']\noly12a  - oly12\n\nnames(oly12a)  - abbreviate(names(oly12), 3)\n\nvisna(oly12a, sort = 'b')   # example 3\ndata(freetrade, package = 'Amelia')\n\nfreetrade  - within(freetrade, land1  - reorder(country, tariff, function(x) sum(is.na(x))))\n\nfluctile(xtabs(is.na(tariff) ~ land1 + year, data = freetrade))   ## viewport[base]  # example 4\ndata(Pima.tr2, package = 'MASS')\n\nvisna(Pima.tr2, sort = 'b')", 
            "title": "The extracat Package (splom)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-ash-package-density", 
            "text": "library(ash)  Density plot  plot(ash1(bin1(mtcars$mpg, nbin = 50)), type = 'l')  ## [1] \"ash estimate nonzero outside interval ab\"", 
            "title": "The ash Package (density)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-kernsmooth-package-density", 
            "text": "library(KernSmooth)  Density plot  with(mtcars, {\n  hist(mpg, freq = FALSE, main = '', col = 'bisque2', ylab = '')\n  lines(density(mpg), lwd = 2)\n  ks1  - bkde(mpg, bandwidth = dpik(mpg))\n  lines(ks1, col = 'red', lty = 5, lwd = 2)})", 
            "title": "The KernSmooth Package (density)"
        }, 
        {
            "location": "/Plot_snippets_-_Basics/#the-crorplot-package-correlation", 
            "text": "library(corrplot)  Splom  # Create a correlation matrix for the dataset (9-14 are the '2' variables only)\ncorrelations  - cor(mtcars)\n\ncorrplot(correlations)", 
            "title": "The crorplot Package (correlation)"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/", 
            "text": "Documentation\n\n\nDataset\n\n\nThe \nggplot2\n Package\n\n\nSECTION 1\n\n\n1, Introduction\n\n\n2, Data\n\n\n3, Aesthetics\n\n\n4, Geometries\n\n\n5, qplot and wrap-up\n\n\nSECTION 2\n\n\n1, Statistics\n\n\n2, Coordinates and Facets\n\n\n3, Themes\n\n\n4, Best Practices\n\n\n5, Case Study\n\n\nSECTION 3\n\n\nSECTION 4 - Cheat List\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nCode snippets and results.\n\n\n\n\n\n\nDocumentation\n\n\n\n\nggplot2\n\n\nggthemes\n\n\nr4stats\n\n\n\n\nDataset\n\n\nFor most examples, we use the \nmtcars\n, \ndiamonds\n, \niris\n,\n\n\nChickWeight\n, \nrecess\n, \nfish\n, \nVocab\n, \nTitanic\n, \nmamsleep\n,\n\n\nbarley\n, \nadult\n datasets.\n\n\nThe \nggplot2\n Package\n\n\nlibrary(ggplot2)\n\n\n\n\nImport additional packages.\n\n\nlibrary(digest)\nlibrary(grid)\nlibrary(gtable)\nlibrary(MASS)\nlibrary(plyr)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(stats)\nlibrary(tidyr)\n\n\n\n\nFor this project, import additional packages.\n\n\nlibrary(ggthemes)\nlibrary(RColorBrewer)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(car)\nlibrary(Hmisc)\nlibrary(dplyr)\n\n\n\n\nSuggested additional packages\n\n\nSECTION 1\n\n\n1, Introduction\n\n\nExploring \nggplot2\n, part 1\n\n\n# basic plot\nggplot(mtcars, aes(x = cyl, y = mpg)) +\n  geom_point()\n\n\n\n\n\n\nExploring \nggplot2\n, part 2\n\n\n# cyl is a factor\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_point()\n\n\n\n\n\n\nExploring \nggplot2\n, part 3\n\n\n# scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\n\n\n# add color\nggplot(mtcars, aes(x = wt, y = mpg, col = disp)) +\n  geom_point()\n\n\n\n\n\n\n# change size\nggplot(mtcars, aes(x = wt, y = mpg, size = disp)) +\n  geom_point()\n\n\n\n\n\n\nExploring \nggplot2\n, part 4\n\n\n# Add geom_point() with +\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_point()\n\n\n\n\n\n\n# Add geom_point() and geom_smooth() with +\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_point() + geom_smooth()\n\n\n\n\n\n\nExploring \nggplot2\n, part 5\n\n\n# only the smooth line\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_smooth()\n\n\n\n\n\n\n# change col\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) + \n  geom_point()\n\n\n\n\n\n\n# change the alpha\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point(alpha = 0.4)\n\n\n\n\n\n\nExploring \nggplot2\n, part 6\n\n\n# 2 facets for comparison\nlibrary(gridExtra)\n\ndata(father.son, package = 'UsingR')\n\na \n- ggplot(father.son, aes(fheight, sheight)) +\n  geom_point() +\n  geom_smooth(method = 'lm', colour = 'red') +\n  geom_abline(slope = 1, intercept = 0)\n\nb \n- ggplot(father.son, aes(fheight, sheight)) +\n  geom_point() +\n  geom_smooth(method = 'lm', colour = 'red', se = FALSE) +\n  stat_smooth()\n\ngrid.arrange(a, b, nrow = 1)\n\n\n\n\n\n\n# load more data\ndata(oly12, package = 'VGAMdata')\n\n# 2 facets for comparison\nggplot(oly12, aes(Height, Weight)) +\n  geom_point(size = 1) +\n  facet_wrap(~Sex, ncol = 1)\n\n\n\n\n\n\n# create a new variable inside de data frame\noly12S \n- within(oly12, oly12$Sport \n- abbreviate(oly12$Sport, 12))\n\n# multiple facets or splom\nggplot(oly12S, aes(Height, Weight)) +\n  geom_point(size = 1) +\n  facet_wrap(~Sport) +\n  ggtitle('Weight and Height by Sport') \n\n\n\n\n\n\nUnderstanding the grammar, part 1\n\n\n# create the object containing the data and aes layers\ndia_plot \n- ggplot(diamonds, aes(x = Carat, y = PricePerCt))\n\n# add a geom layer\ndia_plot + \n  geom_point()\n\n\n\n\n\n\n# add the same geom layer, but with aes() inside\ndia_plot +\n  geom_point(aes(col = Clarity))\n\n\n\n\n\n\nUnderstanding the grammar, part 2\n\n\nset.seed(1)\n\n# create the object containing the data and aes layers\ndia_plot \n- ggplot(diamonds, aes(x = Carat, y = PricePerCt))\n\n# add geom_point() with alpha set to 0.2\ndia_plot \n- dia_plot +\n  geom_point(alpha = 0.2)\n\ndia_plot\n\n\n\n\n\n\n# plot dia_plot with additional geom_smooth() with se set to FALSE\ndia_plot +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n2, Data\n\n\nBase package and \nggplot2\n, part 1 - plot\n\n\n# basic plot\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)\n\n\n\n\n\n\n# change cyl inside mtcars to a factor\nmtcars$cyl \n- as.factor(mtcars$cyl)\n\n# make the same plot as in the first instruction\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)\n\n\n\n\n\n\nBase package and \nggplot2\n, part 2 - lm\n\n\ntransfer to other\n\n\n# Basic plot\nmtcars$cyl \n- as.factor(mtcars$cyl)\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)\n\n# use lm() to calculate a linear model and save it as carModel\ncarModel \n- lm(mpg ~ wt, data = mtcars)\n\n# Call abline() with carModel as first argument and lty as second\nabline(carModel, lty = 2)\n\n# plot each subset efficiently with lapply\nlapply(mtcars$cyl, function(x) {\n  abline(lm(mpg ~ wt, mtcars, subset = (cyl == x)), col = x)\n  })\n\n\n\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n## \n## [[8]]\n## NULL\n## \n## [[9]]\n## NULL\n## \n## [[10]]\n## NULL\n## \n## [[11]]\n## NULL\n## \n## [[12]]\n## NULL\n## \n## [[13]]\n## NULL\n## \n## [[14]]\n## NULL\n## \n## [[15]]\n## NULL\n## \n## [[16]]\n## NULL\n## \n## [[17]]\n## NULL\n## \n## [[18]]\n## NULL\n## \n## [[19]]\n## NULL\n## \n## [[20]]\n## NULL\n## \n## [[21]]\n## NULL\n## \n## [[22]]\n## NULL\n## \n## [[23]]\n## NULL\n## \n## [[24]]\n## NULL\n## \n## [[25]]\n## NULL\n## \n## [[26]]\n## NULL\n## \n## [[27]]\n## NULL\n## \n## [[28]]\n## NULL\n## \n## [[29]]\n## NULL\n## \n## [[30]]\n## NULL\n## \n## [[31]]\n## NULL\n## \n## [[32]]\n## NULL\n\n\n\n# draw the legend of the plot\nlegend(x = 5, y = 33, legend = levels(mtcars$cyl), col = 1:3, pch = 1, bty = 'n')\n\n\n\n\n\n\nBase package and \nggplot2\n, part 3\n\n\n# scatter plot\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) + \n  geom_point()\n\n\n\n\n\n\n# include the lines of the linear models, per cyl\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\n# include a lm for the entire dataset in its whole\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE) +\n  geom_smooth(aes(group = 1), method = 'lm', se = FALSE, linetype = 2)\n\n\n\n\n\n\nVariables to visuals, part 1\n\n\niris.tidy \n- iris %\n%\n  gather(key, Value, -Species) %\n%\n  separate(key, c('Part', 'Measure'), '\\\\.')\n\n# create 2 facets\nggplot(iris.tidy, aes(x = Species, y = Value, col = Part)) +\n  geom_jitter() + facet_grid(. ~ Measure)\n\n\n\n\n\n\nVariables to visuals, part 2\n\n\n# Add a new column, Flower, to iris that contains unique ids\niris$Flower \n- 1:nrow(iris)\n\niris.wide \n- iris %\n%\n  gather(key, value, -Species, -Flower) %\n%\n  separate(key, c('Part', 'Measure'), '\\\\.') %\n%\n  spread(Measure, value)\n\n# create 3 facets\nggplot(iris.wide, aes(x = Length, y = Width, col = Part)) + \n  geom_jitter() +\n  facet_grid(. ~ Species)\n\n\n\n\n\n\n3, Aesthetics\n\n\nAll about aesthetics, part 1\n\n\n# map cyl to y\nggplot(mtcars, aes(x = mpg, y = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to x\nggplot(mtcars, aes(y = mpg, x = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to col\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# change shape and size of the points\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(shape = 1, size = 4)\n\n\n\n\n\n\nAll about aesthetics, part 2\n\n\n# map cyl to fill\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# Change shape, size and alpha of the points in the above plot\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(shape = 16, size = 6, alpha = 0.6)\n\n\n\n\n\n\nAll about aesthetics, part 3\n\n\n# map cyl to size\nggplot(mtcars, aes(x = wt, y = mpg, size = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to alpha\nggplot(mtcars, aes(x = wt, y = mpg, alpha = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to shape \nggplot(mtcars, aes(x = wt, y = mpg, shape = cyl, label = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# map cyl to labels\nggplot(mtcars, aes(x = wt, y = mpg, label = cyl)) +\n  geom_text()\n\n\n\n\n\n\nAll about attributes, part 1\n\n\n# define a hexadecimal color\nmy_color \n- '#123456'\n\n# set the color aesthetic \nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point()\n\n\n\n\n\n\n# set the color aesthetic and attribute \nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(col = my_color)\n\n\n\n\n\n\n# set the fill aesthetic and color, size and shape attributes\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(size = 10, shape = 23, col = my_color)\n\n\n\n\n\n\nAll about attributes, part 2\n\n\n# draw points with alpha 0.5\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n# raw points with shape 24 and color yellow\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(shape = 24, col = 'yellow')\n\n\n\n\n\n\n# draw text with label x, color red and size 10\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_text(label = 'x', col = 'red', size = 10)\n\n\n\n\n\n\nGoing all out\n\n\n# Map mpg onto x, qsec onto y and factor(cyl) onto col\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl))) +\n  geom_point()\n\n\n\n\n\n\n# Add mapping: factor(am) onto shape\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am))) +\n  geom_point()\n\n\n\n\n\n\n# Add mapping: (hp/wt) onto size\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am), size = hp/wt)) +\n  geom_point()\n\n\n\n\n\n\n# Add mapping: rownames(mtcars) onto label\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am), size = hp/wt)) +\n  geom_text(aes(label = rownames(mtcars)))\n\n\n\n\n\n\nPosition\n\n\n# base layers\ncyl.am \n- ggplot(mtcars, aes(x = factor(cyl), fill = factor(am)))\n\n# add geom (position = 'stack'' by default)\ncyl.am + \n  geom_bar(position = 'stack')\n\n\n\n\n\n\n# show proportion\ncyl.am + \n  geom_bar(position = 'fill')\n\n\n\n\n\n\n# dodging\ncyl.am + \n  geom_bar(position = 'dodge')\n\n\n\n\n\n\n# clean up the axes with scale_ functions\nval = c('#E41A1C', '#377EB8')\nlab = c('Manual', 'Automatic')\n\ncyl.am + geom_bar(position = 'dodge', ) +\n  scale_x_discrete('Cylinders') +\n  scale_y_continuous('Number') +\n  scale_fill_manual('Transmission', values = val, labels = lab)\n\n\n\n\n\n\nSetting a dummy aesthetic\n\n\n# add a new column called group\nmtcars$group \n- 0\n\n# create jittered plot of mtcars: mpg onto x, group onto y\nggplot(mtcars, aes(x = mpg, y = group)) +   geom_jitter()\n\n\n\n\n\n\n# change the y aesthetic limits\nggplot(mtcars, aes(x = mpg, y = group)) +\n  geom_jitter() +\n  scale_y_continuous(limits = c(-2, 2))\n\n\n\n\n\n\nOverplotting 1 - Point shape and transparency\n\n\n# basic scatter plot: wt on x-axis and mpg on y-axis; map cyl to col\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4)\n\n\n\n\n\n\n# hollow circles - an improvement\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4, shape = 1)\n\n\n\n\n\n\n# add transparency - very nice\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4, shape = 1, alpha = 0.6)\n\n\n\n\n\n\nOverplotting 2 - alpha with large datasets\n\n\n# scatter plot: carat (x), price (y), clarity (col)\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point()\n\n\n\n\n\n\n# adjust for overplotting\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n# scatter plot: clarity (x), carat (y), price (col)\nggplot(diamonds, aes(x = Clarity, y = Carat, col = PricePerCt)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n# dot plot with jittering\nggplot(diamonds, aes(x = Clarity, y = Carat, col = PricePerCt)) +\n  geom_point(alpha = 0.5, position = 'jitter')\n\n\n\n\n\n\n4, Geometries\n\n\nScatter plots and jittering (1)\n\n\n# plot the cyl on the x-axis and wt on the y-axis\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_point()\n\n\n\n\n\n\n# Use geom_jitter() instead of geom_point()\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_jitter()\n\n\n\n\n\n\n# Define the position object using position_jitter(): posn.j\nposn.j \n-  position_jitter(0.1)\n\n# Use posn.j in geom_point()\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_point(position = posn.j)\n\n\n\n\n\n\nScatter plots and jittering (2)\n\n\n# scatter plot of vocabulary (y) against education (x). Use geom_point()\nggplot(Vocab, aes(x = education, y = vocabulary)) + \n  geom_point()\n\n\n\n\n\n\n# use geom_jitter() instead of geom_point()\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter()\n\n\n\n\n\n\n# set alpha to a very low 0.2\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2)\n\n\n\n\n\n\n# set the shape to 1\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2, shape = 1)\n\n\n\n\n\n\nHistograms\n\n\n# univariate histogram\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram()\n\n\n\n\n\n\n# change the bin width to 1\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n# change the y aesthetic to density\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1)\n\n\n\n\n\n\n# custom color code\nmyBlue \n- '#377EB8'\n\n# Change the fill color to myBlue\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = myBlue)\n\n\n\n\n\n\nPosition\n\n\nmtcars$am \n- as.factor(mtcars$am)\n\n# bar plot of cyl, filled according to am\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar()\n\n\n\n\n\n\n# change the position argument to stack\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'stack')\n\n\n\n\n\n\n# change the position argument to fill\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'fill')\n\n\n\n\n\n\n# change the position argument to dodge\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'dodge')\n\n\n\n\n\n\nOverlapping bar plots\n\n\n# bar plot of cyl, filled according to am\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar()\n\n\n\n\n\n\n# change the position argument to 'dodge'\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'dodge')\n\n\n\n\n\n\n# define posn_d with position_dodge()\nposn_d \n- position_dodge(0.2)\n\n# change the position argument to posn_d\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = posn_d)\n\n\n\n\n\n\n# use posn_d as position and adjust alpha to 0.6\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = posn_d, alpha = 0.6)\n\n\n\n\n\n\nOverlapping histograms\n\n\n# histogram, add coloring defined by cyl \nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n# change position to identity \nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(binwidth = 1, position = 'identity')\n\n\n\n\n\n\n# change geom to freqpoly (position is identity by default) \nggplot(mtcars, aes(mpg, col = cyl)) +\n  geom_freqpoly(binwidth = 1)\n\n\n\n\n\n\nFacets or splom histograms\n\n\n# load the package\nlibrary(reshape2)\n\n# load new data\ndata(uniranks, package = 'GDAdata')\n\n# name the variables\nnames(uniranks)[c(5, 6, 8, 8, 10, 11, 13)] \n- c('AvTeach', 'NSSTeach', 'SpendperSt', 'StudentStaffR', 'Careers', 'VAddScore', 'NSSFeedb')\n\n# reshape the data frame\nur2 \n- melt(uniranks[, c(3, 5:13)], id.vars = 'UniGroup', variable.name = 'uniV', value.name = 'uniX')\n\n\n\n\n# Splom\nggplot(ur2, aes(uniX)) +\n  geom_histogram() +\n  xlab('') +\n  ylab('') +\n  facet_grid(UniGroup ~ uniV, scales = 'free_x')\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\ndata(Pima.tr2, package = 'MASS')\n\nh1 \n- ggplot(Pima.tr2, aes(glu)) + geom_histogram()\nh2 \n- ggplot(Pima.tr2, aes(bp)) + geom_histogram()\nh3 \n- ggplot(Pima.tr2, aes(skin)) + geom_histogram()\nh4 \n- ggplot(Pima.tr2, aes(bmi)) + geom_histogram()\nh5 \n- ggplot(Pima.tr2, aes(ped)) + geom_histogram()\nh6 \n- ggplot(Pima.tr2, aes(age)) + geom_histogram()\n\ngrid.arrange(h1, h2, h3, h4, h5, h6, nrow = 2)\n\n\n\n\n\n\nBar plots with color ramp, part 1\n\n\n# Example of how to use a brewed color palette\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar() + \n  scale_fill_brewer(palette = 'Set1')\n\n\n\n\n\n\nVocab$education \n- as.factor(Vocab$education)\nVocab$vocabulary \n- as.factor(Vocab$vocabulary)\n\n# Plot education on x and vocabulary on fill\n# Use the default brewed color palette\nggplot(Vocab, aes(x = education, fill = vocabulary)) + geom_bar(position = 'fill') + scale_fill_brewer(palette = 'Set3')\n\n\n\n\n\n\nBar plots with color ramp, part 2\n\n\n# Definition of a set of blue colors\nblues \n- brewer.pal(9, 'Blues')\n\n# Make a color range using colorRampPalette() and the set of blues\nblue_range \n- colorRampPalette(blues)\n\n# Use blue_range to adjust the color of the bars, use scale_fill_manual()\nggplot(Vocab, aes(x = education, fill = vocabulary)) + \n  geom_bar(position = 'fill') +\n  scale_fill_manual(values = blue_range(11))\n\n\n\n\n\n\nOverlapping histograms (2)\n\n\n# histogram\nggplot(mtcars, aes(mpg)) + geom_histogram(binwidth = 1)\n\n\n\n\n\n\n# expand the histogram to fill using am\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n# change the position argument to 'dodge'\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'dodge', binwidth = 1)\n\n\n\n\n\n\n# change the position argument to 'fill'\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'fill', binwidth = 1)\n\n\n\n\n\n\n# change the position argument to 'identity' and set alpha to 0.4\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'identity', binwidth = 1, alpha = 0.4)\n\n\n\n\n\n\n# change fill to cyl\nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(position = 'identity', binwidth = 1, alpha = 0.4)\n\n\n\n\n\n\nLine plots\n\n\n# plot unemploy as a function of date using a line plot\nggplot(economics, aes(x = date, y = unemploy)) +\n  geom_line()  \n\n\n\n\n\n\n# adjust plot to represent the fraction of total population that is unemployed\nggplot(economics, aes(x = date, y = unemploy/pop)) +\n  geom_line()\n\n\n\n\n\n\nPeriods of recession\n\n\n# draw the recess periods\nggplot(economics, aes(x = date, y = unemploy/pop)) +\n  geom_line() +\n  geom_rect(data = recess, inherit.aes = FALSE, aes(xmin = begin, xmax = end, ymin = -Inf, ymax = +Inf), fill = 'red', alpha = 0.2)\n\n\n\n\n\n\nMultiple time series, part 1\n\n\n# use gather to go from fish to fish.tidy.\nfish.tidy \n- gather(fish, Species, Capture, -Year)\n\n\n\n\nMultiple time series, part 2\n\n\n# plot\nggplot(fish.tidy, aes(x = Year, y = Capture, col = Species)) +\n  geom_line()\n\n\n\n\n\n\n5, qplot and wrap-up\n\n\nUsing \nqplot\n\n\n# the old way\nplot(mpg ~ wt, data = mtcars)\n\n\n\n\n\n\n# using ggplot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(shape = 1)\n\n\n\n\n\n\n# Using qplot\nqplot(wt, mpg, data = mtcars)\n\n\n\n\n\n\nUsing aesthetics\n\n\n# Categorical: cyl\nqplot(wt, mpg, data = mtcars, size = cyl)\n\n\n\n\n\n\n# gear\nqplot(wt, mpg, data = mtcars, size = gear)\n\n\n\n\n\n\n# Continuous: hp\nqplot(wt, mpg, data = mtcars, col = hp)\n\n\n\n\n\n\n# qsec\nqplot(wt, mpg, data = mtcars, size = qsec)\n\n\n\n\n\n\nChoosing geoms, part 1\n\n\n# qplot() with x only\nqplot(factor(cyl), data = mtcars)\n\n\n\n\n\n\n# qplot() with x and y\nqplot(factor(cyl), factor(vs), data = mtcars)\n\n\n\n\n\n\n# qplot() with geom set to jitter manually\nqplot(factor(cyl), factor(vs), data = mtcars, geom = 'jitter')\n\n\n\n\n\n\nChoosing geoms, part 2 - dotplot\n\n\n# make a dot plot with ggplot\nggplot(mtcars, aes(cyl, wt, fill = am)) + \n  geom_dotplot(stackdir = 'center', binaxis = 'y')\n\n\n\n\n\n\n# qplot with geom 'dotplot', binaxis = 'y' and stackdir = 'center'\nqplot(as.numeric(cyl), wt, data = mtcars, fill = am, geom = 'dotplot', stackdir = 'center', binaxis = 'y')\n\n\n\n\n\n\nChicken weight\n\n\n# base\nggplot(ChickWeight, aes(x = Time, y = weight)) +\n  geom_line(aes(group = Chick))\n\n\n\n\n\n\n# color\nggplot(ChickWeight, aes(x = Time, y = weight, col = Diet)) +\n  geom_line(aes(group = Chick))\n\n\n\n\n\n\n# lines\nggplot(ChickWeight, aes(x = Time, y = weight, col = Diet)) +\n  geom_line(aes(group = Chick), alpha = 0.3) +\n  geom_smooth(lwd = 2, se = FALSE)\n\n\n\n\n\n\nTitanic\n\n\n# Use ggplot() for the first instruction\nggplot(titanic, aes(x = factor(Pclass), fill = factor(Sex))) +\n  geom_bar(position = 'dodge')\n\n\n\n\n\n\n# Use ggplot() for the second instruction\nggplot(titanic, aes(x = factor(Pclass), fill = factor(Sex))) +\n  geom_bar(position = 'dodge') +\n  facet_grid('. ~ Survived')\n\n\n\n\n\n\n# position jitter\nposn.j \n- position_jitter(0.5, 0)\n\n# Use ggplot() for the last instruction\nggplot(titanic, aes(x = factor(Pclass), y = Age, col = factor(Sex))) +\n  geom_jitter(size = 3, alpha = 0.5, position = posn.j) +\n  facet_grid('. ~ Survived')\n\n\n\n\n\n\nSECTION 2\n\n\n1, Statistics\n\n\nSmoothing\n\n\n# scatter plot with LOESS smooth with a CI ribbon\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n# scatter plot with LOESS smooth without CI\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n# scatter plot with an OLS linear model\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n# scatter plot with an OLS linear model without points\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\nGrouping variables\n\n\n# cyl as a factor variable\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\n# set the group aesthetic\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl), group = 1)) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = F)\n\n\n\n\n\n\n# add a second smooth layer in which the group aesthetic is set\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'lm', se = FALSE, aes(group = 1))\n\n\n\n\n\n\nModifying \nstat_smooth\n\n\n# change the LOESS span\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 0.7, method = 'auto')\n\n\n\n\n\n\n# method = 'auto' is by default\n\n\n\n\n# set the model to the default LOESS and use a span of 0.7\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1), col = 'black', span = 0.7)\n\n\n\n\n\n\n# set col to 'All', inside the aes layer\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1, col = 'All cyl'), span = 0.7)\n\n\n\n\n\n\n# add `scale_color_manual` to change the colors\nmyColors \n- c(brewer.pal(3, 'Dark2'), 'black')\n\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1, col = 'All cyl'), span = 0.7) +\n  scale_color_manual('Cylinders', values = myColors)\n\n\n\n\n\n\nModifying \nstat_smooth\n (2)\n\n\n# jittered scatter plot, add a linear model (lm) smooth\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2) +\n  stat_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\n# only lm, colored by year\nggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) +\n  stat_smooth(method = 'lm', se = FALSE)\n\n\n\n\n\n\n# set a color brewer palette\nggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) + \n  stat_smooth(method = 'lm', se = FALSE) +\n  scale_color_brewer('Accent')\n\n\n\n\n\n\n# change col and group, specify alpha, size and geom, and add scale_color_gradient\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + \n  stat_smooth(method = 'lm', se = FALSE, alpha = 0.6, size = 2, geom = 'path') +\n  scale_color_brewer('Blues') +\n  scale_color_gradientn(colors = brewer.pal(9, 'YlOrRd'))\n\n\n\n\n\n\nQuantiles\n\n\n# use stat_quantile instead of stat_smooth\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) +  stat_quantile(alpha = 0.6, size = 2) + \n  scale_color_gradientn(colors = brewer.pal(9,'YlOrRd'))\n\n\n\n\n\n\n# set quantile to 0.5\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + \n  stat_quantile(alpha = 0.6, size = 2, quantiles = c(0.5)) + \n  scale_color_gradientn(colors = brewer.pal(9,'YlOrRd'))\n\n\n\n\n\n\nSum\n\n\n# plot with linear and loess model\np \n- ggplot(Vocab, aes(x = education, y = vocabulary)) + \n  stat_smooth(method = 'loess', aes(col = 'red'), se = F) + \n  stat_smooth(method = 'lm', aes(col = 'blue'), se = F) + \n  scale_color_discrete('Model', labels = c('red' = 'LOESS', 'blue' = 'lm'))\n\np\n\n\n\n\n\n\n# add stat_sum (by overall proportion)\np + \n  stat_sum()\n\n\n\n\n\n\n#aes(group = 1)\n\n\n\n\n# set size range\np + \n  stat_sum() + \n  scale_size(range = c(1,10))\n\n\n\n\n\n\n# proportional within years of education; set group aesthetic\np + \n  stat_sum(aes(group = education))\n\n\n\n\n\n\n# set the n\np + \n  stat_sum(aes(group = education, size = ..n..))\n\n\n\n\n\n\nPreparations\n\n\n# convert cyl and am to factors\nmtcars$cyl \n- as.factor(mtcars$cyl)\nmtcars$am \n- as.factor(mtcars$am)\n\n# define positions\nposn.d \n- position_dodge(width = 0.1)\nposn.jd \n- position_jitterdodge(jitter.width = 0.1, dodge.width = 0.2)\nposn.j \n- position_jitter(width = 0.2)\n\n# base layers\nwt.cyl.am \n- ggplot(mtcars, aes(x = cyl, y = wt, col = am, group = am, fill = am))\n\n\n\n\nPlotting variations\n\n\n# base layer\nwt.cyl.am \n- ggplot(mtcars, aes(x = cyl,  y = wt, col = am, fill = am, group = am))\n\n\n\n\n# jittered, dodged scatter plot with transparent points\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6)\n\n\n\n\n\n\n# mean and sd\nwt.cyl.am +\n  geom_point(position = posn.jd, alpha = 0.6) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = posn.d)\n\n\n\n\n\n\n# mean and 95% CI\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6) + \n  stat_summary(fun.data = mean_cl_normal, position = posn.d)\n\n\n\n\n\n\n# mean and SD with T-tipped error bars\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6) + \n  stat_summary(geom = 'point', fun.y = mean, position = posn.d) + \n  stat_summary(geom = 'errorbar', fun.data = mean_sdl, fun.args = list(mult = 1), width = 0.1, position = posn.d)\n\n\n\n\n\n\n2, Coordinates and Facets\n\n\nZooming In\n\n\n# basic\np \n- ggplot(mtcars, aes(x = wt, y = hp, col = am)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\n# add scale_x_continuous\np + \n  scale_x_continuous(limits = c(3, 6), expand = c(0,0))\n\n\n\n\n\n\n# zoom in\np + \n  coord_cartesian(xlim = c(3, 6))\n\n\n\n\n\n\nAspect Ratio\n\n\n# scatter plot\nbase.plot \n- ggplot(iris, aes(y = Sepal.Width, x = Sepal.Length, col = Species)) + \n  geom_jitter() + \n  geom_smooth(method = 'lm', se = FALSE)\n\n\n\n\n# default aspect ratio\n# fix aspect ratio (1:1)\nbase.plot + \n  coord_equal()\n\n\n\n\n\n\nbase.plot + \n  coord_fixed()\n\n\n\n\n\n\nPie Charts\n\n\n# stacked bar plot\nthin.bar \n- ggplot(mtcars, aes(x = 1, fill = cyl)) + \n  geom_bar()\n\nthin.bar\n\n\n\n\n\n\n# convert thin.bar to pie chart\nthin.bar + \n  coord_polar(theta = 'y')\n\n\n\n\n\n\n# create stacked bar plot\nwide.bar \n- ggplot(mtcars, aes(x = 1, fill = cyl)) + \n  geom_bar(width = 1)\n\nwide.bar\n\n\n\n\n\n\n# Convert wide.bar to pie chart\nwide.bar + coord_polar(theta = 'y')\n\n\n\n\n\n\nFacets: the basics\n\n\n# scatter plot\np \n- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point()\n\n\n\n\n# separate rows according am\n# facet_grid(rows ~ cols)\np + \n  facet_grid(am ~ .)\n\n\n\n\n\n\n# separate columns according to cyl\n# facet_grid(rows ~ cols)\np + facet_grid(. ~ cyl)\n\n\n\n\n\n\n# separate by both columns and rows \n# facet_grid(rows ~ cols)\np + \n  facet_grid(am ~ cyl)\n\n\n\n\n\n\nMany variables\n\n\n# create the `cyl_am` col and `myCol` vector\nmtcars$cyl_am \n- paste(mtcars$cyl, mtcars$am, sep = '_')\n\nmyCol \n- rbind(brewer.pal(9, 'Blues')[c(3,6,8)],\n               brewer.pal(9, 'Reds')[c(3,6,8)])\n\n\n\n\n# scatter plot, add color scale\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) + \n  geom_point() + \n  scale_color_manual(values = myCol)\n\n\n\n\n\n\n# facet according on rows and columns\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) +\n  geom_point() + \n  scale_color_manual(values = myCol) + \n  facet_grid(gear ~ vs)\n\n\n\n\n\n\n# add more variables\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am, size = disp)) + \n  geom_point() + \n  scale_color_manual(values = myCol) + \n  facet_grid(gear ~ vs)\n\n\n\n\n\n\nDropping levels\n\n\n# scatter plot\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point()\n\n\n\n\n\n\n# facet rows according to `vore`\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point() + \n  facet_grid(vore ~ .)\n\n\n\n\n\n\n# specify scale and space arguments to free up rows\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point() + \n  facet_grid(vore ~ ., scale = 'free_y', space = 'free_y')\n\n\n\n\n\n\n3, Themes\n\n\nRectangles\n\n\n# separate columns according to cyl\n# facet_grid(rows ~ cols)\nmtcars$cyl \n- c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4)\n\nmtcars$Cylinders \n- factor(mtcars$cyl)\n\nz \n- ggplot(mtcars, aes(x = wt, y = mpg, col = Cylinders)) + \n  geom_point(size = 2, alpha = 0.7) + \n  facet_grid(. ~ cyl) + \n  labs(x = 'Weight (lb/1000)', y = 'Miles/(US) gallon') + \n  geom_smooth(method = 'lm', se = FALSE) +\n  theme_base() +\n  scale_colour_economist()\nz\n\n\n\n\n\n\n# change the plot background color to myPink (#FEE0D2)\nmyPink \n- '#FEE0D2'\n\nz + \n  theme(plot.background = element_rect(fill = myPink))\n\n\n\n\n\n\n# adjust the border to be a black line of size 3\nz + \n  theme(plot.background = element_rect(fill = myPink, color = 'black', size = 3))\n\n\n\n\n\n\n# adjust the border to be a black line of size 3\nz + \n  theme(plot.background = element_rect(color = 'black', size = 3))\n\n\n\n\n\n\n# set panel.background, legend.key, legend.background and strip.background to element_blank()\nz + \n  theme(plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank())\n\n\n\n\n\n\nLines\n\n\n# Extend z with theme() and three arguments\nz +\n    theme(panel.grid = element_blank(), axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'))\n\n\n\n\n\n\nText\n\n\n# extend z with theme() function and four arguments\nmyRed \n- '#99000D'\n\nz +\n    theme(strip.text = element_text(size = 16, color = myRed), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'))\n\n\n\n\n\n\nLegends\n\n\n# move legend by position\nz + \n  theme(legend.position = c(0.85, 0.85))\n\n\n\n\n\n\n# change direction\nz + \n  theme(legend.direction = 'horizontal')\n\n\n\n\n\n\n# change location by name\nz + \n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n# remove legend entirely\nz + \n  theme(legend.position = 'none')\n\n\n\n\n\n\nPositions\n\n\n# increase spacing between facets\nz + \n  theme(panel.margin.x = unit(2, 'cm'))\n\n\n\n\n\n\n# add code to remove any excess plot margin space\nz + \n  theme(panel.margin.x = unit(2, 'cm'), plot.margin = unit(c(0,0,0,0), 'cm'))\n\n\n\n\n\n\nUpdate Themestheme update\n\n\n# theme layer saved as an object, theme_pink\ntheme_pink \n- theme(panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank(), plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.grid = element_blank(), axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'), strip.text = element_text(size = 16, color = myRed), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'), legend.position = 'none')\n\n\n\n\nz2 \n- z\n\n# apply theme_pink to z2\nz2 + \n  theme_pink\n\n\n\n\n\n\n# change code so that old theme is saved as old\nold \n- theme_update(panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank(), plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.grid = element_blank(),axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'), strip.text = element_text(size = 16, color = myRed), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'), legend.position = 'none')\n\n\n\n\n# display the plot z2\ntheme_set(theme_pink)\n\nz2 + \n  theme_pink\n\n\n\n\n\n\n# restore the old plot\ntheme_set(old)\n\nz2\n\n\n\n\n\n\nExploring ggthemes\n\n\n# apply theme_tufte\n# set the theme with theme_set\ntheme_set(theme_tufte())\n\n# or apply it in the ggplot command\nz2 + \n  theme_tufte()\n\n\n\n\n\n\n# apply theme_tufte, modified\n# set the theme with theme_set\ntheme_set(theme_tufte() + \n  theme(legend.position = c(0.9, 0.9), axis.title = element_text(face = 'italic', size = 12),  legend.title = element_text(face = 'italic', size = 12)))\n\n# or apply it in the ggplot command\nz2 + \n  theme_tufte() +\n  theme(legend.position = c(0.9, 0.9),        axis.title = element_text(face = 'italic', size = 12), legend.title = element_text(face = 'italic', size = 12))\n\n\n\n\n\n\n# apply theme_igray\n# set the theme with `theme_set`\ntheme_set(theme_igray())\n\n# or apply it in the ggplot command\nz2 + \n  theme_igray()\n\n\n\n\n\n\n# apply `theme_igray`, modified\n# set the theme with `theme_set`\ntheme_set(theme_igray() + \n  theme(legend.position = c(0.9, 0.9), legend.key = element_blank(), legend.background = element_rect(fill = 'grey90')))\n\nz2 + \n  # Or apply it in the ggplot command\n  theme_igray() +\n  theme(legend.position = c(0.9, 0.9),\n        legend.key = element_blank(),\n        legend.background = element_rect(fill = 'grey90'))\n\n\n\n\n\n\n4, Best Practices\n\n\nBar Plots (1)\n\n\n# base layers\nm \n- ggplot(mtcars, aes(x = cyl, y = wt))\n\n\n\n\n# dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar', fill = 'skyblue') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1)\n\n\n\n\n\n\nBar Plots (2)\n\n\n# base layers\nm \n- ggplot(mtcars, aes(x = cyl,y = wt, col = am, fill = am))\n\n\n\n\n# dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1)\n\n\n\n\n\n\n# set position dodge in each `stat` function\nm + \n  stat_summary(fun.y = mean, geom = 'bar', position = 'dodge') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1, position = 'dodge')\n\n\n\n\n\n\n# set your dodge `posn` manually\nposn.d \n- position_dodge(0.9)\n\n\n\n\n# redraw dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar', position = posn.d) +  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1, position = posn.d)\n\n\n\n\n\n\nBar Plots (3)\n\n\n# base layers\nmtcars.cyl \n- mtcars %\n% group_by(cyl) %\n% summarise(wt.avg = mean(wt))\nmtcars.cyl\n\n\n\n\n## # A tibble: 3 \u00d7 2\n##     cyl   wt.avg\n##   \ndbl\n    \ndbl\n\n## 1     4 2.285727\n## 2     6 3.117143\n## 3     8 3.999214\n\n\n\nm \n- ggplot(mtcars.cyl, aes(x = cyl, y = wt.avg))\nm\n\n\n\n\n\n\n# draw bar plot\nm + \n  geom_bar(stat = 'identity', fill = 'skyblue')\n\n\n\n\n\n\nPie Charts (1)\n\n\n# bar chart to pie chart\nggplot(mtcars, aes(x = cyl, fill = am)) + geom_bar(position = 'fill')\n\n\n\n\n\n\nggplot(mtcars, aes(x = cyl, fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl)\n\n\n\n\n\n\nggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl)\n\n\n\n\n\n\nggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl) + coord_polar(theta = 'y')\n\n\n\n\n\n\nggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill', width = 1) + facet_grid(. ~ cyl) + coord_polar(theta = 'y')\n\n\n\n\n\n\nParallel coordinate plot\n\n\n# parallel coordinates plot using `GGally`\n# all columns except `am` (`am` column is the 9th)\ngroup_by_am \n- 9\nmy_names_am \n- (1:11)[-group_by_am]\n\n\n\n\n# parallel plot; each variable plotted as a z-score transformation\nggparcoord(mtcars, columns = my_names_am, groupColumn = group_by_am, alpha = 0.8)\n\n\n\n\n\n\n# scaled between 0-1 and most discriminating variable first\nggparcoord(mtcars, columns = my_names_am, groupColumn = group_by_am, alpha = 0.8, scale = 'uniminmax', order = 'anyClass')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species') # xlab, ylab, scale_x_discrete, them\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'uniminmax')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'globalminmax')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', mapping = aes(size = 1))\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', alphaLines = 0.3)\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'center')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scaleSummary = 'median', missing = 'exclude')\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', order = 'allClass') # or custom filter\n\n\n\n\n\n\nggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'std')\n\n\n\n\n\n\nSplom\n\n\nlibrary(dplyr)\n\ndata(Pima.tr2, package = 'MASS')\nPimaV \n- select(Pima.tr2, glu:age)\n\nggpairs(PimaV, diag = list(continuous = 'density'), axisLabels = 'show')\n\n\n\n\n\n\nHeat Maps\n\n\n# create color palette\nmyColors \n- brewer.pal(9, 'Reds')\n\n\n\n\n# heat map\nggplot(barley, aes(x = year, y = variety, fill = yield)) + \n  geom_tile()\n\n\n\n\n\n\n# add facet_wrap(~ variable); not like facet_grid(. ~ variable)\nggplot(barley, aes(x = year, y = variety, fill = yield)) + \n  geom_tile() + \n  facet_wrap( ~ site, ncol = 1)\n\n\n\n\n\n\n# \nggplot(barley, aes(x = year, y = variety, fill = yield)) + geom_tile() + facet_wrap( ~ site, ncol = 1) + \n  scale_fill_gradientn(colors = myColors)\n\n\n\n\n\n\nHeat Maps Alternatives (1)\n\n\n# line plots\nggplot(barley, aes(x = year, y = yield, col = variety, group = variety)) + geom_line() + \n  facet_wrap(facets = ~ site, nrow = 1)\n\n\n\n\n\n\nHeat Maps Alternatives (2)\n\n\n# overlapping ribbon plot\nggplot(barley, aes(x = year, y = yield, col = site, group = site, fill = site)) + geom_line() + \n  stat_summary(fun.y = mean, geom = 'line') + \n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'ribbon', col = NA, alpha = 0.1)\n\n\n\n\n\n\n5, Case Study\n\n\nSort and order\n\n\n# reorder\ndata(Cars93, package = 'MASS')\n\nCars93 \n- within(Cars93, TypeWt \n- reorder(Type, Weight, mean))\n\nCars93 \n- within(Cars93, Type1 \n- factor(Type, levels = c('Small', 'Sporty', 'Compact', 'Midsize', 'Large', 'Van')))\n\nwith(Cars93, table(TypeWt, Type1))\n\n\n\n\n##          Type1\n## TypeWt    Small Sporty Compact Midsize Large Van\n##   Small      21      0       0       0     0   0\n##   Sporty      0     14       0       0     0   0\n##   Compact     0      0      16       0     0   0\n##   Midsize     0      0       0      22     0   0\n##   Large       0      0       0       0    11   0\n##   Van         0      0       0       0     0   9\n\n\n\nggplot(Cars93, aes(TypeWt, 100/MPG.city)) + \n  geom_boxplot() + \n  ylab('Gallons per 100 miles') + \n  xlab('Car type')\n\n\n\n\n\n\nCars93 \n- within(Cars93, {\n  levels(Type1) \n- c('Small', 'Large', 'Midsize', 'Small', 'Sporty', 'Large')\n})\n\nggplot(Cars93, aes(TypeWt, 100/MPG.city)) + \n  geom_boxplot() + \n  ylab('Gallons per 100 miles') + \n  xlab('Car type')\n\n\n\n\n\n\nEnsemble plots\n\n\nlibrary(gridExtra)\n\ndata(Fertility, package = 'AER')\n\np0 \n- ggplot(Fertility) + geom_histogram(binwidth = 1) + ylab('')\np1 \n- p0 + aes(x = age)\np2 \n- p0 + aes(x = work) + xlab('Weeks worked in 1979')\n\nk \n- ggplot(Fertility) + \n  geom_bar() + ylab('') + \n  ylim(0, 250000)\n\np3 \n- k + aes(x = morekids) + \n  xlab('has more children')\np4 \n- k + aes(x = gender1) + \n  xlab('first child')\np5 \n- k + aes(x = gender2) + \n  xlab('second child')\np6 \n- k + aes(x = afam) + \n  xlab('African-American')\np7 \n- k + aes(x = hispanic) + \n  xlab('Hispanic')\np8 \n- k + aes(x = other) + \n  xlab('other race')\n\ngrid.arrange(arrangeGrob(p1, p2, ncol = 2, widths = c(3, 3)), arrangeGrob(p3, p4, p5, p6, p7, p8, ncol = 6), nrow = 2, heights = c(1.25, 1))\n\n\n\n\n\n\nExploring Data\n\n\n# histogram\nggplot(adult, aes(x = SRAGE_P)) + \n  geom_histogram()\n\n\n\n\n\n\n# histogram\nggplot(adult, aes(x = BMI_P)) + \n  geom_histogram()\n\n\n\n\n\n\n# color, default binwidth\nggplot(adult,aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\nData Cleaning\n\n\n# remove individual aboves 84\nadult \n- adult[adult$SRAGE_P \n= 84, ] \n\n# remove individuals with a BMI below 16 and above or equal to 52\nadult \n- adult[adult$BMI_P \n= 16 \n adult$BMI_P \n 52, ]\n\n# relabel race\nadult$RACEHPR2 \n- factor(adult$RACEHPR2, labels = c('Latino', 'Asian', 'African American', 'White'))\n\n# relabel the BMI categories variable\nadult$RBMI \n- factor(adult$RBMI, labels = c('Under-weight', 'Normal-weight', 'Over-weight', 'Obese'))\n\n\n\n\nMultiple Histograms\n\n\n# color palette BMI_fill\nBMI_fill \n- scale_fill_brewer('BMI Category', palette = 'Reds')\n\n\n\n\n# histogram, add BMI_fill and customizations\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(binwidth = 1) + \n  BMI_fill + facet_grid(RBMI ~ .) + \n  theme_classic()\n\n\n\n\n\n\nAlternatives\n\n\n# count histogram\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(binwidth = 1) +\n  BMI_fill\n\n\n\n\n\n\n# density histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1) +\n  BMI_fill\n\n\n\n\n\n\n# faceted count histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(binwidth = 1) +\n  BMI_fill + facet_grid(RBMI ~ .)\n\n\n\n\n\n\n# faceted density histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1) +\n  BMI_fill + facet_grid(RBMI ~ .)\n\n\n\n\n\n\n# density histogram with `position = 'fill'`\nggplot(adult, aes (x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1, position = 'fill') +\n  BMI_fill\n\n\n\n\n\n\n# accurate histogram\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..count../sum(..count..)), binwidth = 1, position = 'fill') +\n  BMI_fill\n\n\n\n\n\n\nDo Things Manually\n\n\n# an attempt to facet the accurate frequency histogram from before (failed)\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..count../sum(..count..)), binwidth = 1, position = 'fill') +\n  BMI_fill +\n  facet_grid(RBMI ~ .)\n\n\n\n\n\n\n# create DF with `table()`\nDF \n- table(adult$RBMI, adult$SRAGE_P)\n\n# use apply on DF to get frequency of each group\nDF_freq \n- apply(DF, 2, function(x) x/sum(x))\n\n# melt on DF to create DF_melted\nDF_melted \n- melt(DF_freq)\n\n# change names of DF_melted\nnames(DF_melted) \n- c('FILL', 'X', 'value')\n\n\n\n\n# add code to make this a faceted plot\nggplot(DF_melted, aes(x = X, y = value, fill = FILL)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  BMI_fill + \n  facet_grid(FILL ~ .)\n\n\n\n\n\n\nMerimeko/Mosaic Plot\n\n\n# The initial contingency table\nDF \n- as.data.frame.matrix(table(adult$SRAGE_P, adult$RBMI))\n\n# Add the columns groupsSum, xmax and xmin. Remove groupSum again.\nDF$groupSum \n- rowSums(DF)\nDF$xmax \n- cumsum(DF$groupSum)\nDF$xmin \n- DF$xmax - DF$groupSum\n# The groupSum column needs to be removed, don't remove this line\nDF$groupSum \n- NULL\n\n# Copy row names to variable X\nDF$X \n- row.names(DF)\n\n# Melt the dataset\nDF_melted \n- melt(DF, id.vars = c('X', 'xmin', 'xmax'), variable.name = 'FILL')\n\n# dplyr call to calculate ymin and ymax - don't change\nDF_melted \n- DF_melted %\n% \n  group_by(X) %\n% \n  mutate(ymax = cumsum(value/sum(value)),\n         ymin = ymax - value/sum(value))\n\n# Plot rectangles - don't change.\nggplot(DF_melted, aes(ymin = ymin, \n                 ymax = ymax,\n                 xmin = xmin, \n                 xmax = xmax, \n                 fill = FILL)) + \n  geom_rect(colour = 'white') +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  BMI_fill +\n  theme_tufte()\n\n\n\n\n\n\nAdding statistics\n\n\n# perform chi.sq test (`RBMI` and `SRAGE_P`)\nresults \n- chisq.test(table(adult$RBMI, adult$SRAGE_P))\n\n# melt results$residuals and store as resid\nresid \n- melt(results$residuals)\n\n# change names of resid\nnames(resid) \n- c('FILL', 'X', 'residual')\n\n# merge the two datasets\nDF_all \n- merge(DF_melted, resid)\n\n\n\n\n# update plot command\nggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) +\n  geom_rect() +\n  scale_fill_gradient2() +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  theme_tufte()\n\n\n\n\n\n\nAdding text\n\n\n# position for labels on x axis\nDF_all$xtext \n- DF_all$xmin + (DF_all$xmax - DF_all$xmin) / 2\n\n# position for labels on y axis\nindex \n- DF_all$xmax == max(DF_all$xmax)\nDF_all$ytext \n- DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2\n\n\n\n\n# plot\nggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) + \n  geom_rect(col = 'white') +\n  # geom_text for ages (i.e. the x axis)\n  geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) +\n  # geom_text for BMI (i.e. the fill axis)\n  geom_text(aes(x = max(xmax), y = ytext, label = FILL), size = 3, hjust = 1, show.legend  = FALSE) +\n  scale_fill_gradient2() +\n  theme_tufte() +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\nGeneralizations\n\n\n# script generalized into a function\nmosaicGG \n- function(data, X, FILL) {\n  # Proportions in raw data\n  DF \n- as.data.frame.matrix(table(data[[X]], data[[FILL]]))\n  DF$groupSum \n- rowSums(DF)\n  DF$xmax \n- cumsum(DF$groupSum)\n  DF$xmin \n- DF$xmax - DF$groupSum\n  DF$X \n- row.names(DF)\n  DF$groupSum \n- NULL\n  DF_melted \n- melt(DF, id = c('X', 'xmin', 'xmax'), variable.name = 'FILL')\n\n  DF_melted \n- DF_melted %\n% \n    group_by(X) %\n% \n    mutate(ymax = cumsum(value/sum(value)),\n           ymin = ymax - value/sum(value))\n\n  # Chi-sq test\n  results \n- chisq.test(table(data[[FILL]], data[[X]])) # fill and then x\n  resid \n- melt(results$residuals)\n  names(resid) \n- c('FILL', 'X', 'residual')\n  # Merge data\n  DF_all \n- merge(DF_melted, resid)\n   # Positions for labels\n  DF_all$xtext \n- DF_all$xmin + (DF_all$xmax - DF_all$xmin)/2\n  index \n- DF_all$xmax == max(DF_all$xmax)\n  DF_all$ytext \n- DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2\n\n  # plot\n  g \n- ggplot(DF_all, aes(ymin = ymin,  ymax = ymax, xmin = xmin, \n                          xmax = xmax, fill = residual)) + \n    geom_rect(col = 'white') +\n    geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) + geom_text(aes(x = max(xmax),  y = ytext, label = FILL), size = 3, hjust = 1, show.legend = FALSE) +\n    scale_fill_gradient2('Residuals') +\n    scale_x_continuous('Individuals', expand = c(0,0)) +\n    scale_y_continuous('Proportion', expand = c(0,0)) +\n    theme_tufte() +\n    theme(legend.position = 'bottom')\n  print(g)\n}\n\n\n\n\n# BMI described by age (in x)\nmosaicGG(adult, 'SRAGE_P','RBMI')\n\n\n\n\n\n\n# poverty described by age (in x)\nmosaicGG(adult, 'SRAGE_P', 'POVLL')\n\n\n\n\n\n\n# `am` described by `cyl` (in x)\nmosaicGG(mtcars, 'cyl', 'am')\n\n\n\n\n\n\n# `Vocab` described by education\nmosaicGG(Vocab, 'education', 'vocabulary')\n\n\n\n\n\n\nSECTION 3\n\n\nSECTION 4 - Cheat List\n\n\nggplot(data, aes(x = , y = ), col = , fill = , size = , labels = , alpha\n\n= , shape = , line = , position = \njitter\n)\n\n\n+ geom_point()\n\n+ geom_point(aes(), col = , position = posn.j)\n\n+ geom_jitter()\n\n+ facet_grid(. ~ x) # y ~ x\n\n+ scale_x_continous('Sepal Length', limits = c(2, 8), breaks = seq(2, 8, 3))\n\n+ scale_color_discrete('Species', labels = c('a', 'b', 'c'))\n\n+ labs(x = , y = , col = )\n\n\n\nposn.j \n- position_jitter(width = 0.1)\n\n\nData\n\n\n\n\ndiamonds\n, prices of 50,000 round cut diamonds.\n\n\neconomics\n, economics_long, US economic time series.\n\n\nfaithfuld\n, 2d density estimate of Old Faithful data.\n\n\nluv_colours\n, colors().\n\n\nmidwest\n, midwest demographics.\n\n\nmpg\n, fuel economy data from 1999 and 2008 for 38 popular models\n\n    of car.\n\n\nmsleep\n, an updated and expanded version of the mammals\n\n    sleep dataset.\n\n\npresidential\n, terms of 11 presidents from Eisenhower to Obama.\n\n\nseals\n, vector field of seal movements.\n\n\ntxhousing\n, Housing sales in TX.\n\n\n\n\nAesthetics\n\n\n\n\nx-axis.\n\n\ny-asix.\n\n\ncolor.\n\n\nfill.\n\n\nsize (points, lines).\n\n\nlabels.\n\n\nalpha.\n\n\nshape (points).\n\n\nlinetype (lines).\n\n\naes\n, Define aesth.etic mappings.\n\n\naes_\n (aes_q, aes_string), Define aesthetic mappings from\n\n    strings, or quoted calls and formulas.\n\n\naes_all\n, Given a character vector, create a set of\n\n    identity mappings.\n\n\naes_auto\n, Automatic aesthetic mapping.\n\n\naes_colour_fill_alpha\n (color, colour, fill), Colour related\n\n    aesthetics: colour, fill and alpha.\n\n\naes_group_order\n (group), Aesthetics: group.\n\n    aes_linetype_size_shape (linetype, shape, size), Differentiation\n\n    related aesthetics: linetype, size, shape.\n\n\naes_position\n (x, xend, xmax, xmin, y, yend, ymax, ymin), Position\n\n    related aesthetics: x, y, xmin, xmax, ymin, ymax, xend, yend.\n\n\n\n\nPosition\n\n\n\n\nposition_dodge\n, Adjust position by dodging overlaps to the side.\n\n\nposition_fill\n (position_stack), Stack overlapping objects on top\n\n    of one another.\n\n\nposition_identity\n, Don\nt adjust position\n\n\nposition_nudge\n, Nudge points.\n\n\nposition_jitter\n, Jitter points to avoid overplotting.\n\n\nposition_jitterdodge\n, Adjust position by simultaneously dodging\n\n    and jittering.\n\n\n\n\nScales\n\n\n\n\nexpand_limits\n, Expand the plot limits with data.\n\n\nguides\n, Set guides for each scale.\n\n\nguide_legend\n, Legend guide.\n\n\nguide_colourbar\n (guide_colorbar), Continuous colour bar guide.\n\n\nlims\n (xlim, ylim), Convenience functions to set the axis limits.\n\n\nscale_alpha\n (scale_alpha_continuous, scale_alpha_discrete),\n\n    Alpha scales.\n\n\nscale_colour_brewer\n (scale_color_brewer,\n\n    scale_color_distiller, scale_colour_distiller,\n\n    scale_fill_brewer, scale_fill_distiller), Sequential, diverging\n\n    and qualitative colour scales from colorbrewer.org\n\n\nscale_colour_gradient\n (scale_color_continuous,\n\n    scale_color_gradient, scale_color_gradient2,\n\n    scale_color_gradientn, scale_colour_continuous,\n\n    scale_colour_date, scale_colour_datetime,\n\n    scale_colour_gradient2, scale_colour_gradientn,\n\n    scale_fill_continuous, scale_fill_date, scale_fill_datetime,\n\n    scale_fill_gradient,\n\n    scale_fill_gradient2, scale_fill_gradientn).\n\n\nscale_colour_grey\n (scale_color_grey, scale_fill_grey),\n\n    Sequential grey colour scale.\n\n\nscale_colour_hue\n (scale_color_discrete, scale_color_hue,\n\n    scale_colour_discrete, scale_fill_discrete, scale_fill_hue),\n\n    Qualitative colour scale with evenly spaced hues.\n\n\nscale_identity\n (scale_alpha_identity, scale_color_identity,\n\n    scale_colour_identity, scale_fill_identity,\n\n    scale_linetype_identity, scale_shape_identity,\n\n    scale_size_identity), Use values without scaling.\n\n\nscale_manual\n (scale_alpha_manual, scale_color_manual,\n\n    scale_colour_manual, scale_fill_manual, scale_linetype_manual,\n\n    scale_shape_manual, scale_size_manual), Create your own\n\n    discrete scale.\n\n\nscale_linetype\n (scale_linetype_continuous,\n\n    scale_linetype_discrete), Scale for line patterns.\n\n\nscale_shape\n (scale_shape_continuous, scale_shape_discrete),\n\n    Scale for shapes, aka glyphs.\n\n\nscale_size\n (scale_radius, scale_size_area,\n\n    scale_size_continuous, scale_size_date, scale_size_datetime,\n\n    scale_size_discrete), Scale size (area or radius).\n\n\nscale_x_discrete\n (scale_y_discrete), Discrete position.\n\n\nlabs\n (ggtitle, xlab, ylab), Change axis labels and legend titles.\n\n\nupdate_labels\n, Update axis/legend labels.\n\n\n\n\nGeometries\n\n\n\n\npoint.\n\n\nline.\n\n\nhistogram.\n\n\nbar.\n\n\nboxplot.\n\n\ngeom_abline\n (geom_hline, geom_vline), Lines: horizontal,\n\n    vertical, and specified by slope and intercept.\n\n\ngeom_bar\n (stat_count), Bars, rectangles with bases on x-axis\n\n\ngeom_bin2d\n (stat_bin2d, stat_bin_2d), Add heatmap of 2d\n\n    bin counts.\n\n\ngeom_blank\n, Blank, draws nothing.\n\n\ngeom_boxplot\n (stat_boxplot), Box and whiskers plot.\n\n\ngeom_contour\n (stat_contour), Display contours of a 3d surface\n\n    in 2d.\n\n\ngeom_count\n(stat_sum), Count the number of observations at\n\n    each location.\n\n\ngeom_crossbar\n (geom_errorbar, geom_linerange, geom_pointrange),\n\n    Vertical intervals: lines, crossbars \n errorbars.\n\n\ngeom_density\n (stat_density), Display a smooth density estimate.\n\n\ngeom_density_2d\n (geom_density2d, stat_density2d,\n\n    stat_density_2d), Contours from a 2d density estimate.\n\n\ngeom_dotplot\n, Dot plot\n\n\ngeom_errorbarh\n, Horizontal error bars.\n\n\ngeom_freqpoly\n (geom_histogram, stat_bin), Histograms and\n\n    frequency polygons.\n\n\ngeom_hex\n (stat_bin_hex, stat_binhex), Hexagon binning.\n\n\ngeom_jitter\n, Points, jittered to reduce overplotting.\n\n\ngeom_label\n (geom_text), Textual annotations.\n\n\ngeom_map\n, Polygons from a reference map.\n\n\ngeom_path\n (geom_line, geom_step), Connect observations.\n\n\ngeom_point\n, Points, as for a scatterplot.\n\n\ngeom_polygon\n, Polygon, a filled path.\n\n\ngeom_quantile\n (stat_quantile), Add quantile lines from a\n\n    quantile regression.\n\n\ngeom_raster\n (geom_rect, geom_tile), Draw rectangles.\n\n\ngeom_ribbon\n (geom_area), Ribbons and area plots.\n\n\ngeom_rug\n, Marginal rug plots.\n\n\ngeom_segment\n (geom_curve), Line segments and curves.\n\n\ngeom_smooth\n (stat_smooth), Add a smoothed conditional mean.\n\n\ngeom_violin\n (stat_ydensity), Violin plot.\n\n\n\n\nFacets\n\n\n\n\ncolumns.\n\n\nrows.\n\n\nfacet_grid\n, Lay out panels in a grid.\n\n\nfacet_null\n, Facet specification: a single panel.\n\n\nfacet_wrap\n, Wrap a 1d ribbon of panels into 2d.\n\n\nlabeller\n, Generic labeller function for facets.\n\n\nlabel_bquote\n, Backquoted labeller.\n\n\n\n\nAnnotation\n\n\n\n\nannotate\n, Create an annotation layer.\n\n\nannotation_custom\n, Annotation: Custom grob.\n\n\nannotation_logticks\n, Annotation: log tick marks.\n\n\nannotation_map\n, Annotation: maps.\n\n\nannotation_raster\n, Annotation: High-performance\n\n    rectangular tiling.\n\n\nborders\n, Create a layer of map borders.\n\n\n\n\nFortify\n\n\n\n\nfortify\n, Fortify a model with data.\n\n\nfortify-multcomp\n (fortify.cld, fortify.confint.glht, fortify.glht,\n\n    fortify.summary.glht), Fortify methods for objects produced by.\n\n\nfortify.lm\n, Supplement the data fitted to a linear model with\n\n    model fit statistics.\n\n\nfortify.map\n, Fortify method for map objects.\n\n\nfortify.sp\n (fortify.Line, fortify.Lines, fortify.Polygon,\n\n    fortify.Polygons, fortify.SpatialLinesDataFrame,\n\n    fortify.SpatialPolygons, fortify.SpatialPolygonsDataFrame), Fortify\n\n    method for classes from the sp package.\n\n\nmap_data\n, Create a data frame of map data.\n\n\n\n\nStatistics\n\n\n\n\nbinning.\n\n\nsmoothing.\n\n\ndescriptive.\n\n\ninferential.\n\n\nstat_ecdf\n, Empirical Cumulative Density Function.\n\n\nstat_ellipse\n, Plot data ellipses.\n\n\nstat_function\n, Superimpose a function.\n\n\nstat_identity\n, Identity statistic.\n\n\nstat_qq\n (geom_qq), Calculation for quantile-quantile plot.\n\n\nstat_summary_2d\n (stat_summary2d, stat_summary_hex), Bin and\n\n    summarise in 2d (rectangle \n hexagons)\n\n\nstat_unique\n, Remove duplicates.\n\n\nCoordinates.\n\n\ncartesian.\n\n\nfixes.\n\n\npolar.\n\n\nlimites.\n\n\ncoord_cartesian\n, Cartesian coordinates.\n\n\ncoord_fixed\n (coord_equal), Cartesian coordinates with fixed\n\n    relationship between x and y scales.\n\n\ncoord_flip\n, Flipped cartesian coordinates.\n\n\ncoord_map\n (coord_quickmap), Map projections.\n\n\ncoord_polar\n, Polar coordinates.\n\n\ncoord_trans\n, Transformed cartesian coordinate system.\n\n\n\n\nThemes\n\n\n\n\ntheme_bw\n\n\ntheme_grey\n\n\ntheme_classic\n\n\ntheme_minimal\n\n\nggthemes", 
            "title": "Plot Snippets - ggplot2"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#dataset", 
            "text": "For most examples, we use the  mtcars ,  diamonds ,  iris ,  ChickWeight ,  recess ,  fish ,  Vocab ,  Titanic ,  mamsleep ,  barley ,  adult  datasets.", 
            "title": "Dataset"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#the-ggplot2-package", 
            "text": "library(ggplot2)  Import additional packages.  library(digest)\nlibrary(grid)\nlibrary(gtable)\nlibrary(MASS)\nlibrary(plyr)\nlibrary(reshape2)\nlibrary(scales)\nlibrary(stats)\nlibrary(tidyr)  For this project, import additional packages.  library(ggthemes)\nlibrary(RColorBrewer)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(car)\nlibrary(Hmisc)\nlibrary(dplyr)  Suggested additional packages", 
            "title": "The ggplot2 Package"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#section-1", 
            "text": "", 
            "title": "SECTION 1"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#1-introduction", 
            "text": "Exploring  ggplot2 , part 1  # basic plot\nggplot(mtcars, aes(x = cyl, y = mpg)) +\n  geom_point()   Exploring  ggplot2 , part 2  # cyl is a factor\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_point()   Exploring  ggplot2 , part 3  # scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point()   # add color\nggplot(mtcars, aes(x = wt, y = mpg, col = disp)) +\n  geom_point()   # change size\nggplot(mtcars, aes(x = wt, y = mpg, size = disp)) +\n  geom_point()   Exploring  ggplot2 , part 4  # Add geom_point() with +\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_point()   # Add geom_point() and geom_smooth() with +\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_point() + geom_smooth()   Exploring  ggplot2 , part 5  # only the smooth line\nggplot(diamonds, aes(x = Carat, y = PricePerCt)) +\n  geom_smooth()   # change col\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) + \n  geom_point()   # change the alpha\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point(alpha = 0.4)   Exploring  ggplot2 , part 6  # 2 facets for comparison\nlibrary(gridExtra)\n\ndata(father.son, package = 'UsingR')\n\na  - ggplot(father.son, aes(fheight, sheight)) +\n  geom_point() +\n  geom_smooth(method = 'lm', colour = 'red') +\n  geom_abline(slope = 1, intercept = 0)\n\nb  - ggplot(father.son, aes(fheight, sheight)) +\n  geom_point() +\n  geom_smooth(method = 'lm', colour = 'red', se = FALSE) +\n  stat_smooth()\n\ngrid.arrange(a, b, nrow = 1)   # load more data\ndata(oly12, package = 'VGAMdata')\n\n# 2 facets for comparison\nggplot(oly12, aes(Height, Weight)) +\n  geom_point(size = 1) +\n  facet_wrap(~Sex, ncol = 1)   # create a new variable inside de data frame\noly12S  - within(oly12, oly12$Sport  - abbreviate(oly12$Sport, 12))\n\n# multiple facets or splom\nggplot(oly12S, aes(Height, Weight)) +\n  geom_point(size = 1) +\n  facet_wrap(~Sport) +\n  ggtitle('Weight and Height by Sport')    Understanding the grammar, part 1  # create the object containing the data and aes layers\ndia_plot  - ggplot(diamonds, aes(x = Carat, y = PricePerCt))\n\n# add a geom layer\ndia_plot + \n  geom_point()   # add the same geom layer, but with aes() inside\ndia_plot +\n  geom_point(aes(col = Clarity))   Understanding the grammar, part 2  set.seed(1)\n\n# create the object containing the data and aes layers\ndia_plot  - ggplot(diamonds, aes(x = Carat, y = PricePerCt))\n\n# add geom_point() with alpha set to 0.2\ndia_plot  - dia_plot +\n  geom_point(alpha = 0.2)\n\ndia_plot   # plot dia_plot with additional geom_smooth() with se set to FALSE\ndia_plot +\n  geom_smooth(se = FALSE)", 
            "title": "1, Introduction"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#2-data", 
            "text": "Base package and  ggplot2 , part 1 - plot  # basic plot\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)   # change cyl inside mtcars to a factor\nmtcars$cyl  - as.factor(mtcars$cyl)\n\n# make the same plot as in the first instruction\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)   Base package and  ggplot2 , part 2 - lm  transfer to other  # Basic plot\nmtcars$cyl  - as.factor(mtcars$cyl)\nplot(mtcars$wt, mtcars$mpg, col = mtcars$cyl)\n\n# use lm() to calculate a linear model and save it as carModel\ncarModel  - lm(mpg ~ wt, data = mtcars)\n\n# Call abline() with carModel as first argument and lty as second\nabline(carModel, lty = 2)\n\n# plot each subset efficiently with lapply\nlapply(mtcars$cyl, function(x) {\n  abline(lm(mpg ~ wt, mtcars, subset = (cyl == x)), col = x)\n  })  ## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n## \n## [[8]]\n## NULL\n## \n## [[9]]\n## NULL\n## \n## [[10]]\n## NULL\n## \n## [[11]]\n## NULL\n## \n## [[12]]\n## NULL\n## \n## [[13]]\n## NULL\n## \n## [[14]]\n## NULL\n## \n## [[15]]\n## NULL\n## \n## [[16]]\n## NULL\n## \n## [[17]]\n## NULL\n## \n## [[18]]\n## NULL\n## \n## [[19]]\n## NULL\n## \n## [[20]]\n## NULL\n## \n## [[21]]\n## NULL\n## \n## [[22]]\n## NULL\n## \n## [[23]]\n## NULL\n## \n## [[24]]\n## NULL\n## \n## [[25]]\n## NULL\n## \n## [[26]]\n## NULL\n## \n## [[27]]\n## NULL\n## \n## [[28]]\n## NULL\n## \n## [[29]]\n## NULL\n## \n## [[30]]\n## NULL\n## \n## [[31]]\n## NULL\n## \n## [[32]]\n## NULL  # draw the legend of the plot\nlegend(x = 5, y = 33, legend = levels(mtcars$cyl), col = 1:3, pch = 1, bty = 'n')   Base package and  ggplot2 , part 3  # scatter plot\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) + \n  geom_point()   # include the lines of the linear models, per cyl\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE)   # include a lm for the entire dataset in its whole\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE) +\n  geom_smooth(aes(group = 1), method = 'lm', se = FALSE, linetype = 2)   Variables to visuals, part 1  iris.tidy  - iris % %\n  gather(key, Value, -Species) % %\n  separate(key, c('Part', 'Measure'), '\\\\.')\n\n# create 2 facets\nggplot(iris.tidy, aes(x = Species, y = Value, col = Part)) +\n  geom_jitter() + facet_grid(. ~ Measure)   Variables to visuals, part 2  # Add a new column, Flower, to iris that contains unique ids\niris$Flower  - 1:nrow(iris)\n\niris.wide  - iris % %\n  gather(key, value, -Species, -Flower) % %\n  separate(key, c('Part', 'Measure'), '\\\\.') % %\n  spread(Measure, value)\n\n# create 3 facets\nggplot(iris.wide, aes(x = Length, y = Width, col = Part)) + \n  geom_jitter() +\n  facet_grid(. ~ Species)", 
            "title": "2, Data"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#3-aesthetics", 
            "text": "All about aesthetics, part 1  # map cyl to y\nggplot(mtcars, aes(x = mpg, y = cyl)) +\n  geom_point()   # map cyl to x\nggplot(mtcars, aes(y = mpg, x = cyl)) +\n  geom_point()   # map cyl to col\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point()   # change shape and size of the points\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(shape = 1, size = 4)   All about aesthetics, part 2  # map cyl to fill\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point()   # Change shape, size and alpha of the points in the above plot\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(shape = 16, size = 6, alpha = 0.6)   All about aesthetics, part 3  # map cyl to size\nggplot(mtcars, aes(x = wt, y = mpg, size = cyl)) +\n  geom_point()   # map cyl to alpha\nggplot(mtcars, aes(x = wt, y = mpg, alpha = cyl)) +\n  geom_point()   # map cyl to shape \nggplot(mtcars, aes(x = wt, y = mpg, shape = cyl, label = cyl)) +\n  geom_point()   # map cyl to labels\nggplot(mtcars, aes(x = wt, y = mpg, label = cyl)) +\n  geom_text()   All about attributes, part 1  # define a hexadecimal color\nmy_color  - '#123456'\n\n# set the color aesthetic \nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point()   # set the color aesthetic and attribute \nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(col = my_color)   # set the fill aesthetic and color, size and shape attributes\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(size = 10, shape = 23, col = my_color)   All about attributes, part 2  # draw points with alpha 0.5\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(alpha = 0.5)   # raw points with shape 24 and color yellow\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_point(shape = 24, col = 'yellow')   # draw text with label x, color red and size 10\nggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +\n  geom_text(label = 'x', col = 'red', size = 10)   Going all out  # Map mpg onto x, qsec onto y and factor(cyl) onto col\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl))) +\n  geom_point()   # Add mapping: factor(am) onto shape\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am))) +\n  geom_point()   # Add mapping: (hp/wt) onto size\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am), size = hp/wt)) +\n  geom_point()   # Add mapping: rownames(mtcars) onto label\nggplot(mtcars, aes(x = mpg, y = qsec, col = factor(cyl), shape = factor(am), size = hp/wt)) +\n  geom_text(aes(label = rownames(mtcars)))   Position  # base layers\ncyl.am  - ggplot(mtcars, aes(x = factor(cyl), fill = factor(am)))\n\n# add geom (position = 'stack'' by default)\ncyl.am + \n  geom_bar(position = 'stack')   # show proportion\ncyl.am + \n  geom_bar(position = 'fill')   # dodging\ncyl.am + \n  geom_bar(position = 'dodge')   # clean up the axes with scale_ functions\nval = c('#E41A1C', '#377EB8')\nlab = c('Manual', 'Automatic')\n\ncyl.am + geom_bar(position = 'dodge', ) +\n  scale_x_discrete('Cylinders') +\n  scale_y_continuous('Number') +\n  scale_fill_manual('Transmission', values = val, labels = lab)   Setting a dummy aesthetic  # add a new column called group\nmtcars$group  - 0\n\n# create jittered plot of mtcars: mpg onto x, group onto y\nggplot(mtcars, aes(x = mpg, y = group)) +   geom_jitter()   # change the y aesthetic limits\nggplot(mtcars, aes(x = mpg, y = group)) +\n  geom_jitter() +\n  scale_y_continuous(limits = c(-2, 2))   Overplotting 1 - Point shape and transparency  # basic scatter plot: wt on x-axis and mpg on y-axis; map cyl to col\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4)   # hollow circles - an improvement\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4, shape = 1)   # add transparency - very nice\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +\n  geom_point(size = 4, shape = 1, alpha = 0.6)   Overplotting 2 - alpha with large datasets  # scatter plot: carat (x), price (y), clarity (col)\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point()   # adjust for overplotting\nggplot(diamonds, aes(x = Carat, y = PricePerCt, col = Clarity)) +\n  geom_point(alpha = 0.5)   # scatter plot: clarity (x), carat (y), price (col)\nggplot(diamonds, aes(x = Clarity, y = Carat, col = PricePerCt)) +\n  geom_point(alpha = 0.5)   # dot plot with jittering\nggplot(diamonds, aes(x = Clarity, y = Carat, col = PricePerCt)) +\n  geom_point(alpha = 0.5, position = 'jitter')", 
            "title": "3, Aesthetics"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#4-geometries", 
            "text": "Scatter plots and jittering (1)  # plot the cyl on the x-axis and wt on the y-axis\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_point()   # Use geom_jitter() instead of geom_point()\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_jitter()   # Define the position object using position_jitter(): posn.j\nposn.j  -  position_jitter(0.1)\n\n# Use posn.j in geom_point()\nggplot(mtcars, aes(x = cyl, y = wt)) +\n  geom_point(position = posn.j)   Scatter plots and jittering (2)  # scatter plot of vocabulary (y) against education (x). Use geom_point()\nggplot(Vocab, aes(x = education, y = vocabulary)) + \n  geom_point()   # use geom_jitter() instead of geom_point()\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter()   # set alpha to a very low 0.2\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2)   # set the shape to 1\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2, shape = 1)   Histograms  # univariate histogram\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram()   # change the bin width to 1\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 1)   # change the y aesthetic to density\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1)   # custom color code\nmyBlue  - '#377EB8'\n\n# Change the fill color to myBlue\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = myBlue)   Position  mtcars$am  - as.factor(mtcars$am)\n\n# bar plot of cyl, filled according to am\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar()   # change the position argument to stack\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'stack')   # change the position argument to fill\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'fill')   # change the position argument to dodge\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'dodge')   Overlapping bar plots  # bar plot of cyl, filled according to am\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar()   # change the position argument to 'dodge'\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = 'dodge')   # define posn_d with position_dodge()\nposn_d  - position_dodge(0.2)\n\n# change the position argument to posn_d\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = posn_d)   # use posn_d as position and adjust alpha to 0.6\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar(position = posn_d, alpha = 0.6)   Overlapping histograms  # histogram, add coloring defined by cyl \nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(binwidth = 1)   # change position to identity \nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(binwidth = 1, position = 'identity')   # change geom to freqpoly (position is identity by default) \nggplot(mtcars, aes(mpg, col = cyl)) +\n  geom_freqpoly(binwidth = 1)   Facets or splom histograms  # load the package\nlibrary(reshape2)\n\n# load new data\ndata(uniranks, package = 'GDAdata')\n\n# name the variables\nnames(uniranks)[c(5, 6, 8, 8, 10, 11, 13)]  - c('AvTeach', 'NSSTeach', 'SpendperSt', 'StudentStaffR', 'Careers', 'VAddScore', 'NSSFeedb')\n\n# reshape the data frame\nur2  - melt(uniranks[, c(3, 5:13)], id.vars = 'UniGroup', variable.name = 'uniV', value.name = 'uniX')  # Splom\nggplot(ur2, aes(uniX)) +\n  geom_histogram() +\n  xlab('') +\n  ylab('') +\n  facet_grid(UniGroup ~ uniV, scales = 'free_x')   library(ggplot2)\nlibrary(gridExtra)\ndata(Pima.tr2, package = 'MASS')\n\nh1  - ggplot(Pima.tr2, aes(glu)) + geom_histogram()\nh2  - ggplot(Pima.tr2, aes(bp)) + geom_histogram()\nh3  - ggplot(Pima.tr2, aes(skin)) + geom_histogram()\nh4  - ggplot(Pima.tr2, aes(bmi)) + geom_histogram()\nh5  - ggplot(Pima.tr2, aes(ped)) + geom_histogram()\nh6  - ggplot(Pima.tr2, aes(age)) + geom_histogram()\n\ngrid.arrange(h1, h2, h3, h4, h5, h6, nrow = 2)   Bar plots with color ramp, part 1  # Example of how to use a brewed color palette\nggplot(mtcars, aes(x = cyl, fill = am)) +\n  geom_bar() + \n  scale_fill_brewer(palette = 'Set1')   Vocab$education  - as.factor(Vocab$education)\nVocab$vocabulary  - as.factor(Vocab$vocabulary)\n\n# Plot education on x and vocabulary on fill\n# Use the default brewed color palette\nggplot(Vocab, aes(x = education, fill = vocabulary)) + geom_bar(position = 'fill') + scale_fill_brewer(palette = 'Set3')   Bar plots with color ramp, part 2  # Definition of a set of blue colors\nblues  - brewer.pal(9, 'Blues')\n\n# Make a color range using colorRampPalette() and the set of blues\nblue_range  - colorRampPalette(blues)\n\n# Use blue_range to adjust the color of the bars, use scale_fill_manual()\nggplot(Vocab, aes(x = education, fill = vocabulary)) + \n  geom_bar(position = 'fill') +\n  scale_fill_manual(values = blue_range(11))   Overlapping histograms (2)  # histogram\nggplot(mtcars, aes(mpg)) + geom_histogram(binwidth = 1)   # expand the histogram to fill using am\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(binwidth = 1)   # change the position argument to 'dodge'\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'dodge', binwidth = 1)   # change the position argument to 'fill'\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'fill', binwidth = 1)   # change the position argument to 'identity' and set alpha to 0.4\nggplot(mtcars, aes(mpg, fill = am)) +\n  geom_histogram(position = 'identity', binwidth = 1, alpha = 0.4)   # change fill to cyl\nggplot(mtcars, aes(mpg, fill = cyl)) +\n  geom_histogram(position = 'identity', binwidth = 1, alpha = 0.4)   Line plots  # plot unemploy as a function of date using a line plot\nggplot(economics, aes(x = date, y = unemploy)) +\n  geom_line()     # adjust plot to represent the fraction of total population that is unemployed\nggplot(economics, aes(x = date, y = unemploy/pop)) +\n  geom_line()   Periods of recession  # draw the recess periods\nggplot(economics, aes(x = date, y = unemploy/pop)) +\n  geom_line() +\n  geom_rect(data = recess, inherit.aes = FALSE, aes(xmin = begin, xmax = end, ymin = -Inf, ymax = +Inf), fill = 'red', alpha = 0.2)   Multiple time series, part 1  # use gather to go from fish to fish.tidy.\nfish.tidy  - gather(fish, Species, Capture, -Year)  Multiple time series, part 2  # plot\nggplot(fish.tidy, aes(x = Year, y = Capture, col = Species)) +\n  geom_line()", 
            "title": "4, Geometries"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#5-qplot-and-wrap-up", 
            "text": "Using  qplot  # the old way\nplot(mpg ~ wt, data = mtcars)   # using ggplot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(shape = 1)   # Using qplot\nqplot(wt, mpg, data = mtcars)   Using aesthetics  # Categorical: cyl\nqplot(wt, mpg, data = mtcars, size = cyl)   # gear\nqplot(wt, mpg, data = mtcars, size = gear)   # Continuous: hp\nqplot(wt, mpg, data = mtcars, col = hp)   # qsec\nqplot(wt, mpg, data = mtcars, size = qsec)   Choosing geoms, part 1  # qplot() with x only\nqplot(factor(cyl), data = mtcars)   # qplot() with x and y\nqplot(factor(cyl), factor(vs), data = mtcars)   # qplot() with geom set to jitter manually\nqplot(factor(cyl), factor(vs), data = mtcars, geom = 'jitter')   Choosing geoms, part 2 - dotplot  # make a dot plot with ggplot\nggplot(mtcars, aes(cyl, wt, fill = am)) + \n  geom_dotplot(stackdir = 'center', binaxis = 'y')   # qplot with geom 'dotplot', binaxis = 'y' and stackdir = 'center'\nqplot(as.numeric(cyl), wt, data = mtcars, fill = am, geom = 'dotplot', stackdir = 'center', binaxis = 'y')   Chicken weight  # base\nggplot(ChickWeight, aes(x = Time, y = weight)) +\n  geom_line(aes(group = Chick))   # color\nggplot(ChickWeight, aes(x = Time, y = weight, col = Diet)) +\n  geom_line(aes(group = Chick))   # lines\nggplot(ChickWeight, aes(x = Time, y = weight, col = Diet)) +\n  geom_line(aes(group = Chick), alpha = 0.3) +\n  geom_smooth(lwd = 2, se = FALSE)   Titanic  # Use ggplot() for the first instruction\nggplot(titanic, aes(x = factor(Pclass), fill = factor(Sex))) +\n  geom_bar(position = 'dodge')   # Use ggplot() for the second instruction\nggplot(titanic, aes(x = factor(Pclass), fill = factor(Sex))) +\n  geom_bar(position = 'dodge') +\n  facet_grid('. ~ Survived')   # position jitter\nposn.j  - position_jitter(0.5, 0)\n\n# Use ggplot() for the last instruction\nggplot(titanic, aes(x = factor(Pclass), y = Age, col = factor(Sex))) +\n  geom_jitter(size = 3, alpha = 0.5, position = posn.j) +\n  facet_grid('. ~ Survived')", 
            "title": "5, qplot and wrap-up"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#section-2", 
            "text": "", 
            "title": "SECTION 2"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#1-statistics", 
            "text": "Smoothing  # scatter plot with LOESS smooth with a CI ribbon\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth()   # scatter plot with LOESS smooth without CI\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(se = FALSE)   # scatter plot with an OLS linear model\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = 'lm')   # scatter plot with an OLS linear model without points\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_smooth(method = 'lm', se = FALSE)   Grouping variables  # cyl as a factor variable\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE)   # set the group aesthetic\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl), group = 1)) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = F)   # add a second smooth layer in which the group aesthetic is set\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'lm', se = FALSE, aes(group = 1))   Modifying  stat_smooth  # change the LOESS span\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(se = FALSE, span = 0.7, method = 'auto')   # method = 'auto' is by default  # set the model to the default LOESS and use a span of 0.7\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1), col = 'black', span = 0.7)   # set col to 'All', inside the aes layer\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1, col = 'All cyl'), span = 0.7)   # add `scale_color_manual` to change the colors\nmyColors  - c(brewer.pal(3, 'Dark2'), 'black')\n\nggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +\n  geom_point() +\n  stat_smooth(method = 'lm', se = FALSE) +\n  stat_smooth(method = 'auto', se = FALSE, aes(group = 1, col = 'All cyl'), span = 0.7) +\n  scale_color_manual('Cylinders', values = myColors)   Modifying  stat_smooth  (2)  # jittered scatter plot, add a linear model (lm) smooth\nggplot(Vocab, aes(x = education, y = vocabulary)) +\n  geom_jitter(alpha = 0.2) +\n  stat_smooth(method = 'lm', se = FALSE)   # only lm, colored by year\nggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) +\n  stat_smooth(method = 'lm', se = FALSE)   # set a color brewer palette\nggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) + \n  stat_smooth(method = 'lm', se = FALSE) +\n  scale_color_brewer('Accent')   # change col and group, specify alpha, size and geom, and add scale_color_gradient\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + \n  stat_smooth(method = 'lm', se = FALSE, alpha = 0.6, size = 2, geom = 'path') +\n  scale_color_brewer('Blues') +\n  scale_color_gradientn(colors = brewer.pal(9, 'YlOrRd'))   Quantiles  # use stat_quantile instead of stat_smooth\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) +  stat_quantile(alpha = 0.6, size = 2) + \n  scale_color_gradientn(colors = brewer.pal(9,'YlOrRd'))   # set quantile to 0.5\nggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + \n  stat_quantile(alpha = 0.6, size = 2, quantiles = c(0.5)) + \n  scale_color_gradientn(colors = brewer.pal(9,'YlOrRd'))   Sum  # plot with linear and loess model\np  - ggplot(Vocab, aes(x = education, y = vocabulary)) + \n  stat_smooth(method = 'loess', aes(col = 'red'), se = F) + \n  stat_smooth(method = 'lm', aes(col = 'blue'), se = F) + \n  scale_color_discrete('Model', labels = c('red' = 'LOESS', 'blue' = 'lm'))\n\np   # add stat_sum (by overall proportion)\np + \n  stat_sum()   #aes(group = 1)  # set size range\np + \n  stat_sum() + \n  scale_size(range = c(1,10))   # proportional within years of education; set group aesthetic\np + \n  stat_sum(aes(group = education))   # set the n\np + \n  stat_sum(aes(group = education, size = ..n..))   Preparations  # convert cyl and am to factors\nmtcars$cyl  - as.factor(mtcars$cyl)\nmtcars$am  - as.factor(mtcars$am)\n\n# define positions\nposn.d  - position_dodge(width = 0.1)\nposn.jd  - position_jitterdodge(jitter.width = 0.1, dodge.width = 0.2)\nposn.j  - position_jitter(width = 0.2)\n\n# base layers\nwt.cyl.am  - ggplot(mtcars, aes(x = cyl, y = wt, col = am, group = am, fill = am))  Plotting variations  # base layer\nwt.cyl.am  - ggplot(mtcars, aes(x = cyl,  y = wt, col = am, fill = am, group = am))  # jittered, dodged scatter plot with transparent points\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6)   # mean and sd\nwt.cyl.am +\n  geom_point(position = posn.jd, alpha = 0.6) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = posn.d)   # mean and 95% CI\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6) + \n  stat_summary(fun.data = mean_cl_normal, position = posn.d)   # mean and SD with T-tipped error bars\nwt.cyl.am + \n  geom_point(position = posn.jd, alpha = 0.6) + \n  stat_summary(geom = 'point', fun.y = mean, position = posn.d) + \n  stat_summary(geom = 'errorbar', fun.data = mean_sdl, fun.args = list(mult = 1), width = 0.1, position = posn.d)", 
            "title": "1, Statistics"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#2-coordinates-and-facets", 
            "text": "Zooming In  # basic\np  - ggplot(mtcars, aes(x = wt, y = hp, col = am)) + \n  geom_point() + \n  geom_smooth()  # add scale_x_continuous\np + \n  scale_x_continuous(limits = c(3, 6), expand = c(0,0))   # zoom in\np + \n  coord_cartesian(xlim = c(3, 6))   Aspect Ratio  # scatter plot\nbase.plot  - ggplot(iris, aes(y = Sepal.Width, x = Sepal.Length, col = Species)) + \n  geom_jitter() + \n  geom_smooth(method = 'lm', se = FALSE)  # default aspect ratio\n# fix aspect ratio (1:1)\nbase.plot + \n  coord_equal()   base.plot + \n  coord_fixed()   Pie Charts  # stacked bar plot\nthin.bar  - ggplot(mtcars, aes(x = 1, fill = cyl)) + \n  geom_bar()\n\nthin.bar   # convert thin.bar to pie chart\nthin.bar + \n  coord_polar(theta = 'y')   # create stacked bar plot\nwide.bar  - ggplot(mtcars, aes(x = 1, fill = cyl)) + \n  geom_bar(width = 1)\n\nwide.bar   # Convert wide.bar to pie chart\nwide.bar + coord_polar(theta = 'y')   Facets: the basics  # scatter plot\np  - ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point()  # separate rows according am\n# facet_grid(rows ~ cols)\np + \n  facet_grid(am ~ .)   # separate columns according to cyl\n# facet_grid(rows ~ cols)\np + facet_grid(. ~ cyl)   # separate by both columns and rows \n# facet_grid(rows ~ cols)\np + \n  facet_grid(am ~ cyl)   Many variables  # create the `cyl_am` col and `myCol` vector\nmtcars$cyl_am  - paste(mtcars$cyl, mtcars$am, sep = '_')\n\nmyCol  - rbind(brewer.pal(9, 'Blues')[c(3,6,8)],\n               brewer.pal(9, 'Reds')[c(3,6,8)])  # scatter plot, add color scale\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) + \n  geom_point() + \n  scale_color_manual(values = myCol)   # facet according on rows and columns\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) +\n  geom_point() + \n  scale_color_manual(values = myCol) + \n  facet_grid(gear ~ vs)   # add more variables\nggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am, size = disp)) + \n  geom_point() + \n  scale_color_manual(values = myCol) + \n  facet_grid(gear ~ vs)   Dropping levels  # scatter plot\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point()   # facet rows according to `vore`\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point() + \n  facet_grid(vore ~ .)   # specify scale and space arguments to free up rows\nggplot(mamsleep, aes(x = time, y = name, col = sleep)) + \n  geom_point() + \n  facet_grid(vore ~ ., scale = 'free_y', space = 'free_y')", 
            "title": "2, Coordinates and Facets"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#3-themes", 
            "text": "Rectangles  # separate columns according to cyl\n# facet_grid(rows ~ cols)\nmtcars$cyl  - c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4)\n\nmtcars$Cylinders  - factor(mtcars$cyl)\n\nz  - ggplot(mtcars, aes(x = wt, y = mpg, col = Cylinders)) + \n  geom_point(size = 2, alpha = 0.7) + \n  facet_grid(. ~ cyl) + \n  labs(x = 'Weight (lb/1000)', y = 'Miles/(US) gallon') + \n  geom_smooth(method = 'lm', se = FALSE) +\n  theme_base() +\n  scale_colour_economist()\nz   # change the plot background color to myPink (#FEE0D2)\nmyPink  - '#FEE0D2'\n\nz + \n  theme(plot.background = element_rect(fill = myPink))   # adjust the border to be a black line of size 3\nz + \n  theme(plot.background = element_rect(fill = myPink, color = 'black', size = 3))   # adjust the border to be a black line of size 3\nz + \n  theme(plot.background = element_rect(color = 'black', size = 3))   # set panel.background, legend.key, legend.background and strip.background to element_blank()\nz + \n  theme(plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank())   Lines  # Extend z with theme() and three arguments\nz +\n    theme(panel.grid = element_blank(), axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'))   Text  # extend z with theme() function and four arguments\nmyRed  - '#99000D'\n\nz +\n    theme(strip.text = element_text(size = 16, color = myRed), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'))   Legends  # move legend by position\nz + \n  theme(legend.position = c(0.85, 0.85))   # change direction\nz + \n  theme(legend.direction = 'horizontal')   # change location by name\nz + \n  theme(legend.position = 'bottom')   # remove legend entirely\nz + \n  theme(legend.position = 'none')   Positions  # increase spacing between facets\nz + \n  theme(panel.margin.x = unit(2, 'cm'))   # add code to remove any excess plot margin space\nz + \n  theme(panel.margin.x = unit(2, 'cm'), plot.margin = unit(c(0,0,0,0), 'cm'))   Update Themestheme update  # theme layer saved as an object, theme_pink\ntheme_pink  - theme(panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank(), plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.grid = element_blank(), axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'), strip.text = element_text(size = 16, color = myRed), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'), legend.position = 'none')  z2  - z\n\n# apply theme_pink to z2\nz2 + \n  theme_pink   # change code so that old theme is saved as old\nold  - theme_update(panel.background = element_blank(), legend.key = element_blank(), legend.background = element_blank(), strip.background = element_blank(), plot.background = element_rect(fill = myPink, color = 'black', size = 3), panel.grid = element_blank(),axis.line = element_line(color = 'black'), axis.ticks = element_line(color = 'black'), strip.text = element_text(size = 16, color = myRed), axis.title.y = element_text(color = myRed, hjust = 0, face = 'italic'), axis.title.x = element_text(color = myRed, hjust = 0, face = 'italic'), axis.text = element_text(color = 'black'), legend.position = 'none')  # display the plot z2\ntheme_set(theme_pink)\n\nz2 + \n  theme_pink   # restore the old plot\ntheme_set(old)\n\nz2   Exploring ggthemes  # apply theme_tufte\n# set the theme with theme_set\ntheme_set(theme_tufte())\n\n# or apply it in the ggplot command\nz2 + \n  theme_tufte()   # apply theme_tufte, modified\n# set the theme with theme_set\ntheme_set(theme_tufte() + \n  theme(legend.position = c(0.9, 0.9), axis.title = element_text(face = 'italic', size = 12),  legend.title = element_text(face = 'italic', size = 12)))\n\n# or apply it in the ggplot command\nz2 + \n  theme_tufte() +\n  theme(legend.position = c(0.9, 0.9),        axis.title = element_text(face = 'italic', size = 12), legend.title = element_text(face = 'italic', size = 12))   # apply theme_igray\n# set the theme with `theme_set`\ntheme_set(theme_igray())\n\n# or apply it in the ggplot command\nz2 + \n  theme_igray()   # apply `theme_igray`, modified\n# set the theme with `theme_set`\ntheme_set(theme_igray() + \n  theme(legend.position = c(0.9, 0.9), legend.key = element_blank(), legend.background = element_rect(fill = 'grey90')))\n\nz2 + \n  # Or apply it in the ggplot command\n  theme_igray() +\n  theme(legend.position = c(0.9, 0.9),\n        legend.key = element_blank(),\n        legend.background = element_rect(fill = 'grey90'))", 
            "title": "3, Themes"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#4-best-practices", 
            "text": "Bar Plots (1)  # base layers\nm  - ggplot(mtcars, aes(x = cyl, y = wt))  # dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar', fill = 'skyblue') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1)   Bar Plots (2)  # base layers\nm  - ggplot(mtcars, aes(x = cyl,y = wt, col = am, fill = am))  # dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1)   # set position dodge in each `stat` function\nm + \n  stat_summary(fun.y = mean, geom = 'bar', position = 'dodge') + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1, position = 'dodge')   # set your dodge `posn` manually\nposn.d  - position_dodge(0.9)  # redraw dynamite plot\nm + \n  stat_summary(fun.y = mean, geom = 'bar', position = posn.d) +  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'errorbar', width = 0.1, position = posn.d)   Bar Plots (3)  # base layers\nmtcars.cyl  - mtcars % % group_by(cyl) % % summarise(wt.avg = mean(wt))\nmtcars.cyl  ## # A tibble: 3 \u00d7 2\n##     cyl   wt.avg\n##    dbl      dbl \n## 1     4 2.285727\n## 2     6 3.117143\n## 3     8 3.999214  m  - ggplot(mtcars.cyl, aes(x = cyl, y = wt.avg))\nm   # draw bar plot\nm + \n  geom_bar(stat = 'identity', fill = 'skyblue')   Pie Charts (1)  # bar chart to pie chart\nggplot(mtcars, aes(x = cyl, fill = am)) + geom_bar(position = 'fill')   ggplot(mtcars, aes(x = cyl, fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl)   ggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl)   ggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill') + facet_grid(. ~ cyl) + coord_polar(theta = 'y')   ggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = 'fill', width = 1) + facet_grid(. ~ cyl) + coord_polar(theta = 'y')   Parallel coordinate plot  # parallel coordinates plot using `GGally`\n# all columns except `am` (`am` column is the 9th)\ngroup_by_am  - 9\nmy_names_am  - (1:11)[-group_by_am]  # parallel plot; each variable plotted as a z-score transformation\nggparcoord(mtcars, columns = my_names_am, groupColumn = group_by_am, alpha = 0.8)   # scaled between 0-1 and most discriminating variable first\nggparcoord(mtcars, columns = my_names_am, groupColumn = group_by_am, alpha = 0.8, scale = 'uniminmax', order = 'anyClass')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species') # xlab, ylab, scale_x_discrete, them   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'uniminmax')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'globalminmax')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', mapping = aes(size = 1))   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', alphaLines = 0.3)   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'center')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scaleSummary = 'median', missing = 'exclude')   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', order = 'allClass') # or custom filter   ggparcoord(iris, columns = 1:4, groupColumn = 'Species', scale = 'std')   Splom  library(dplyr)\n\ndata(Pima.tr2, package = 'MASS')\nPimaV  - select(Pima.tr2, glu:age)\n\nggpairs(PimaV, diag = list(continuous = 'density'), axisLabels = 'show')   Heat Maps  # create color palette\nmyColors  - brewer.pal(9, 'Reds')  # heat map\nggplot(barley, aes(x = year, y = variety, fill = yield)) + \n  geom_tile()   # add facet_wrap(~ variable); not like facet_grid(. ~ variable)\nggplot(barley, aes(x = year, y = variety, fill = yield)) + \n  geom_tile() + \n  facet_wrap( ~ site, ncol = 1)   # \nggplot(barley, aes(x = year, y = variety, fill = yield)) + geom_tile() + facet_wrap( ~ site, ncol = 1) + \n  scale_fill_gradientn(colors = myColors)   Heat Maps Alternatives (1)  # line plots\nggplot(barley, aes(x = year, y = yield, col = variety, group = variety)) + geom_line() + \n  facet_wrap(facets = ~ site, nrow = 1)   Heat Maps Alternatives (2)  # overlapping ribbon plot\nggplot(barley, aes(x = year, y = yield, col = site, group = site, fill = site)) + geom_line() + \n  stat_summary(fun.y = mean, geom = 'line') + \n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = 'ribbon', col = NA, alpha = 0.1)", 
            "title": "4, Best Practices"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#5-case-study", 
            "text": "Sort and order  # reorder\ndata(Cars93, package = 'MASS')\n\nCars93  - within(Cars93, TypeWt  - reorder(Type, Weight, mean))\n\nCars93  - within(Cars93, Type1  - factor(Type, levels = c('Small', 'Sporty', 'Compact', 'Midsize', 'Large', 'Van')))\n\nwith(Cars93, table(TypeWt, Type1))  ##          Type1\n## TypeWt    Small Sporty Compact Midsize Large Van\n##   Small      21      0       0       0     0   0\n##   Sporty      0     14       0       0     0   0\n##   Compact     0      0      16       0     0   0\n##   Midsize     0      0       0      22     0   0\n##   Large       0      0       0       0    11   0\n##   Van         0      0       0       0     0   9  ggplot(Cars93, aes(TypeWt, 100/MPG.city)) + \n  geom_boxplot() + \n  ylab('Gallons per 100 miles') + \n  xlab('Car type')   Cars93  - within(Cars93, {\n  levels(Type1)  - c('Small', 'Large', 'Midsize', 'Small', 'Sporty', 'Large')\n})\n\nggplot(Cars93, aes(TypeWt, 100/MPG.city)) + \n  geom_boxplot() + \n  ylab('Gallons per 100 miles') + \n  xlab('Car type')   Ensemble plots  library(gridExtra)\n\ndata(Fertility, package = 'AER')\n\np0  - ggplot(Fertility) + geom_histogram(binwidth = 1) + ylab('')\np1  - p0 + aes(x = age)\np2  - p0 + aes(x = work) + xlab('Weeks worked in 1979')\n\nk  - ggplot(Fertility) + \n  geom_bar() + ylab('') + \n  ylim(0, 250000)\n\np3  - k + aes(x = morekids) + \n  xlab('has more children')\np4  - k + aes(x = gender1) + \n  xlab('first child')\np5  - k + aes(x = gender2) + \n  xlab('second child')\np6  - k + aes(x = afam) + \n  xlab('African-American')\np7  - k + aes(x = hispanic) + \n  xlab('Hispanic')\np8  - k + aes(x = other) + \n  xlab('other race')\n\ngrid.arrange(arrangeGrob(p1, p2, ncol = 2, widths = c(3, 3)), arrangeGrob(p3, p4, p5, p6, p7, p8, ncol = 6), nrow = 2, heights = c(1.25, 1))   Exploring Data  # histogram\nggplot(adult, aes(x = SRAGE_P)) + \n  geom_histogram()   # histogram\nggplot(adult, aes(x = BMI_P)) + \n  geom_histogram()   # color, default binwidth\nggplot(adult,aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(binwidth = 1)   Data Cleaning  # remove individual aboves 84\nadult  - adult[adult$SRAGE_P  = 84, ] \n\n# remove individuals with a BMI below 16 and above or equal to 52\nadult  - adult[adult$BMI_P  = 16   adult$BMI_P   52, ]\n\n# relabel race\nadult$RACEHPR2  - factor(adult$RACEHPR2, labels = c('Latino', 'Asian', 'African American', 'White'))\n\n# relabel the BMI categories variable\nadult$RBMI  - factor(adult$RBMI, labels = c('Under-weight', 'Normal-weight', 'Over-weight', 'Obese'))  Multiple Histograms  # color palette BMI_fill\nBMI_fill  - scale_fill_brewer('BMI Category', palette = 'Reds')  # histogram, add BMI_fill and customizations\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(binwidth = 1) + \n  BMI_fill + facet_grid(RBMI ~ .) + \n  theme_classic()   Alternatives  # count histogram\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(binwidth = 1) +\n  BMI_fill   # density histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1) +\n  BMI_fill   # faceted count histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(binwidth = 1) +\n  BMI_fill + facet_grid(RBMI ~ .)   # faceted density histogram\nggplot(adult, aes(x = SRAGE_P, fill= factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1) +\n  BMI_fill + facet_grid(RBMI ~ .)   # density histogram with `position = 'fill'`\nggplot(adult, aes (x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..density..), binwidth = 1, position = 'fill') +\n  BMI_fill   # accurate histogram\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..count../sum(..count..)), binwidth = 1, position = 'fill') +\n  BMI_fill   Do Things Manually  # an attempt to facet the accurate frequency histogram from before (failed)\nggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + \n  geom_histogram(aes(y = ..count../sum(..count..)), binwidth = 1, position = 'fill') +\n  BMI_fill +\n  facet_grid(RBMI ~ .)   # create DF with `table()`\nDF  - table(adult$RBMI, adult$SRAGE_P)\n\n# use apply on DF to get frequency of each group\nDF_freq  - apply(DF, 2, function(x) x/sum(x))\n\n# melt on DF to create DF_melted\nDF_melted  - melt(DF_freq)\n\n# change names of DF_melted\nnames(DF_melted)  - c('FILL', 'X', 'value')  # add code to make this a faceted plot\nggplot(DF_melted, aes(x = X, y = value, fill = FILL)) +\n  geom_bar(stat = 'identity', position = 'stack') +\n  BMI_fill + \n  facet_grid(FILL ~ .)   Merimeko/Mosaic Plot  # The initial contingency table\nDF  - as.data.frame.matrix(table(adult$SRAGE_P, adult$RBMI))\n\n# Add the columns groupsSum, xmax and xmin. Remove groupSum again.\nDF$groupSum  - rowSums(DF)\nDF$xmax  - cumsum(DF$groupSum)\nDF$xmin  - DF$xmax - DF$groupSum\n# The groupSum column needs to be removed, don't remove this line\nDF$groupSum  - NULL\n\n# Copy row names to variable X\nDF$X  - row.names(DF)\n\n# Melt the dataset\nDF_melted  - melt(DF, id.vars = c('X', 'xmin', 'xmax'), variable.name = 'FILL')\n\n# dplyr call to calculate ymin and ymax - don't change\nDF_melted  - DF_melted % % \n  group_by(X) % % \n  mutate(ymax = cumsum(value/sum(value)),\n         ymin = ymax - value/sum(value))\n\n# Plot rectangles - don't change.\nggplot(DF_melted, aes(ymin = ymin, \n                 ymax = ymax,\n                 xmin = xmin, \n                 xmax = xmax, \n                 fill = FILL)) + \n  geom_rect(colour = 'white') +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  BMI_fill +\n  theme_tufte()   Adding statistics  # perform chi.sq test (`RBMI` and `SRAGE_P`)\nresults  - chisq.test(table(adult$RBMI, adult$SRAGE_P))\n\n# melt results$residuals and store as resid\nresid  - melt(results$residuals)\n\n# change names of resid\nnames(resid)  - c('FILL', 'X', 'residual')\n\n# merge the two datasets\nDF_all  - merge(DF_melted, resid)  # update plot command\nggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) +\n  geom_rect() +\n  scale_fill_gradient2() +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  theme_tufte()   Adding text  # position for labels on x axis\nDF_all$xtext  - DF_all$xmin + (DF_all$xmax - DF_all$xmin) / 2\n\n# position for labels on y axis\nindex  - DF_all$xmax == max(DF_all$xmax)\nDF_all$ytext  - DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2  # plot\nggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) + \n  geom_rect(col = 'white') +\n  # geom_text for ages (i.e. the x axis)\n  geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) +\n  # geom_text for BMI (i.e. the fill axis)\n  geom_text(aes(x = max(xmax), y = ytext, label = FILL), size = 3, hjust = 1, show.legend  = FALSE) +\n  scale_fill_gradient2() +\n  theme_tufte() +\n  theme(legend.position = 'bottom')   Generalizations  # script generalized into a function\nmosaicGG  - function(data, X, FILL) {\n  # Proportions in raw data\n  DF  - as.data.frame.matrix(table(data[[X]], data[[FILL]]))\n  DF$groupSum  - rowSums(DF)\n  DF$xmax  - cumsum(DF$groupSum)\n  DF$xmin  - DF$xmax - DF$groupSum\n  DF$X  - row.names(DF)\n  DF$groupSum  - NULL\n  DF_melted  - melt(DF, id = c('X', 'xmin', 'xmax'), variable.name = 'FILL')\n\n  DF_melted  - DF_melted % % \n    group_by(X) % % \n    mutate(ymax = cumsum(value/sum(value)),\n           ymin = ymax - value/sum(value))\n\n  # Chi-sq test\n  results  - chisq.test(table(data[[FILL]], data[[X]])) # fill and then x\n  resid  - melt(results$residuals)\n  names(resid)  - c('FILL', 'X', 'residual')\n  # Merge data\n  DF_all  - merge(DF_melted, resid)\n   # Positions for labels\n  DF_all$xtext  - DF_all$xmin + (DF_all$xmax - DF_all$xmin)/2\n  index  - DF_all$xmax == max(DF_all$xmax)\n  DF_all$ytext  - DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2\n\n  # plot\n  g  - ggplot(DF_all, aes(ymin = ymin,  ymax = ymax, xmin = xmin, \n                          xmax = xmax, fill = residual)) + \n    geom_rect(col = 'white') +\n    geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) + geom_text(aes(x = max(xmax),  y = ytext, label = FILL), size = 3, hjust = 1, show.legend = FALSE) +\n    scale_fill_gradient2('Residuals') +\n    scale_x_continuous('Individuals', expand = c(0,0)) +\n    scale_y_continuous('Proportion', expand = c(0,0)) +\n    theme_tufte() +\n    theme(legend.position = 'bottom')\n  print(g)\n}  # BMI described by age (in x)\nmosaicGG(adult, 'SRAGE_P','RBMI')   # poverty described by age (in x)\nmosaicGG(adult, 'SRAGE_P', 'POVLL')   # `am` described by `cyl` (in x)\nmosaicGG(mtcars, 'cyl', 'am')   # `Vocab` described by education\nmosaicGG(Vocab, 'education', 'vocabulary')", 
            "title": "5, Case Study"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#section-3", 
            "text": "", 
            "title": "SECTION 3"
        }, 
        {
            "location": "/Plot_snippets_-_ggplot2/#section-4-cheat-list", 
            "text": "ggplot(data, aes(x = , y = ), col = , fill = , size = , labels = , alpha \n= , shape = , line = , position =  jitter )  + geom_point()\n\n+ geom_point(aes(), col = , position = posn.j)\n\n+ geom_jitter()\n\n+ facet_grid(. ~ x) # y ~ x\n\n+ scale_x_continous('Sepal Length', limits = c(2, 8), breaks = seq(2, 8, 3))\n\n+ scale_color_discrete('Species', labels = c('a', 'b', 'c'))\n\n+ labs(x = , y = , col = )  posn.j  - position_jitter(width = 0.1)  Data   diamonds , prices of 50,000 round cut diamonds.  economics , economics_long, US economic time series.  faithfuld , 2d density estimate of Old Faithful data.  luv_colours , colors().  midwest , midwest demographics.  mpg , fuel economy data from 1999 and 2008 for 38 popular models \n    of car.  msleep , an updated and expanded version of the mammals \n    sleep dataset.  presidential , terms of 11 presidents from Eisenhower to Obama.  seals , vector field of seal movements.  txhousing , Housing sales in TX.   Aesthetics   x-axis.  y-asix.  color.  fill.  size (points, lines).  labels.  alpha.  shape (points).  linetype (lines).  aes , Define aesth.etic mappings.  aes_  (aes_q, aes_string), Define aesthetic mappings from \n    strings, or quoted calls and formulas.  aes_all , Given a character vector, create a set of \n    identity mappings.  aes_auto , Automatic aesthetic mapping.  aes_colour_fill_alpha  (color, colour, fill), Colour related \n    aesthetics: colour, fill and alpha.  aes_group_order  (group), Aesthetics: group. \n    aes_linetype_size_shape (linetype, shape, size), Differentiation \n    related aesthetics: linetype, size, shape.  aes_position  (x, xend, xmax, xmin, y, yend, ymax, ymin), Position \n    related aesthetics: x, y, xmin, xmax, ymin, ymax, xend, yend.   Position   position_dodge , Adjust position by dodging overlaps to the side.  position_fill  (position_stack), Stack overlapping objects on top \n    of one another.  position_identity , Don t adjust position  position_nudge , Nudge points.  position_jitter , Jitter points to avoid overplotting.  position_jitterdodge , Adjust position by simultaneously dodging \n    and jittering.   Scales   expand_limits , Expand the plot limits with data.  guides , Set guides for each scale.  guide_legend , Legend guide.  guide_colourbar  (guide_colorbar), Continuous colour bar guide.  lims  (xlim, ylim), Convenience functions to set the axis limits.  scale_alpha  (scale_alpha_continuous, scale_alpha_discrete), \n    Alpha scales.  scale_colour_brewer  (scale_color_brewer, \n    scale_color_distiller, scale_colour_distiller, \n    scale_fill_brewer, scale_fill_distiller), Sequential, diverging \n    and qualitative colour scales from colorbrewer.org  scale_colour_gradient  (scale_color_continuous, \n    scale_color_gradient, scale_color_gradient2, \n    scale_color_gradientn, scale_colour_continuous, \n    scale_colour_date, scale_colour_datetime, \n    scale_colour_gradient2, scale_colour_gradientn, \n    scale_fill_continuous, scale_fill_date, scale_fill_datetime, \n    scale_fill_gradient, \n    scale_fill_gradient2, scale_fill_gradientn).  scale_colour_grey  (scale_color_grey, scale_fill_grey), \n    Sequential grey colour scale.  scale_colour_hue  (scale_color_discrete, scale_color_hue, \n    scale_colour_discrete, scale_fill_discrete, scale_fill_hue), \n    Qualitative colour scale with evenly spaced hues.  scale_identity  (scale_alpha_identity, scale_color_identity, \n    scale_colour_identity, scale_fill_identity, \n    scale_linetype_identity, scale_shape_identity, \n    scale_size_identity), Use values without scaling.  scale_manual  (scale_alpha_manual, scale_color_manual, \n    scale_colour_manual, scale_fill_manual, scale_linetype_manual, \n    scale_shape_manual, scale_size_manual), Create your own \n    discrete scale.  scale_linetype  (scale_linetype_continuous, \n    scale_linetype_discrete), Scale for line patterns.  scale_shape  (scale_shape_continuous, scale_shape_discrete), \n    Scale for shapes, aka glyphs.  scale_size  (scale_radius, scale_size_area, \n    scale_size_continuous, scale_size_date, scale_size_datetime, \n    scale_size_discrete), Scale size (area or radius).  scale_x_discrete  (scale_y_discrete), Discrete position.  labs  (ggtitle, xlab, ylab), Change axis labels and legend titles.  update_labels , Update axis/legend labels.   Geometries   point.  line.  histogram.  bar.  boxplot.  geom_abline  (geom_hline, geom_vline), Lines: horizontal, \n    vertical, and specified by slope and intercept.  geom_bar  (stat_count), Bars, rectangles with bases on x-axis  geom_bin2d  (stat_bin2d, stat_bin_2d), Add heatmap of 2d \n    bin counts.  geom_blank , Blank, draws nothing.  geom_boxplot  (stat_boxplot), Box and whiskers plot.  geom_contour  (stat_contour), Display contours of a 3d surface \n    in 2d.  geom_count (stat_sum), Count the number of observations at \n    each location.  geom_crossbar  (geom_errorbar, geom_linerange, geom_pointrange), \n    Vertical intervals: lines, crossbars   errorbars.  geom_density  (stat_density), Display a smooth density estimate.  geom_density_2d  (geom_density2d, stat_density2d, \n    stat_density_2d), Contours from a 2d density estimate.  geom_dotplot , Dot plot  geom_errorbarh , Horizontal error bars.  geom_freqpoly  (geom_histogram, stat_bin), Histograms and \n    frequency polygons.  geom_hex  (stat_bin_hex, stat_binhex), Hexagon binning.  geom_jitter , Points, jittered to reduce overplotting.  geom_label  (geom_text), Textual annotations.  geom_map , Polygons from a reference map.  geom_path  (geom_line, geom_step), Connect observations.  geom_point , Points, as for a scatterplot.  geom_polygon , Polygon, a filled path.  geom_quantile  (stat_quantile), Add quantile lines from a \n    quantile regression.  geom_raster  (geom_rect, geom_tile), Draw rectangles.  geom_ribbon  (geom_area), Ribbons and area plots.  geom_rug , Marginal rug plots.  geom_segment  (geom_curve), Line segments and curves.  geom_smooth  (stat_smooth), Add a smoothed conditional mean.  geom_violin  (stat_ydensity), Violin plot.   Facets   columns.  rows.  facet_grid , Lay out panels in a grid.  facet_null , Facet specification: a single panel.  facet_wrap , Wrap a 1d ribbon of panels into 2d.  labeller , Generic labeller function for facets.  label_bquote , Backquoted labeller.   Annotation   annotate , Create an annotation layer.  annotation_custom , Annotation: Custom grob.  annotation_logticks , Annotation: log tick marks.  annotation_map , Annotation: maps.  annotation_raster , Annotation: High-performance \n    rectangular tiling.  borders , Create a layer of map borders.   Fortify   fortify , Fortify a model with data.  fortify-multcomp  (fortify.cld, fortify.confint.glht, fortify.glht, \n    fortify.summary.glht), Fortify methods for objects produced by.  fortify.lm , Supplement the data fitted to a linear model with \n    model fit statistics.  fortify.map , Fortify method for map objects.  fortify.sp  (fortify.Line, fortify.Lines, fortify.Polygon, \n    fortify.Polygons, fortify.SpatialLinesDataFrame, \n    fortify.SpatialPolygons, fortify.SpatialPolygonsDataFrame), Fortify \n    method for classes from the sp package.  map_data , Create a data frame of map data.   Statistics   binning.  smoothing.  descriptive.  inferential.  stat_ecdf , Empirical Cumulative Density Function.  stat_ellipse , Plot data ellipses.  stat_function , Superimpose a function.  stat_identity , Identity statistic.  stat_qq  (geom_qq), Calculation for quantile-quantile plot.  stat_summary_2d  (stat_summary2d, stat_summary_hex), Bin and \n    summarise in 2d (rectangle   hexagons)  stat_unique , Remove duplicates.  Coordinates.  cartesian.  fixes.  polar.  limites.  coord_cartesian , Cartesian coordinates.  coord_fixed  (coord_equal), Cartesian coordinates with fixed \n    relationship between x and y scales.  coord_flip , Flipped cartesian coordinates.  coord_map  (coord_quickmap), Map projections.  coord_polar , Polar coordinates.  coord_trans , Transformed cartesian coordinate system.   Themes   theme_bw  theme_grey  theme_classic  theme_minimal  ggthemes", 
            "title": "SECTION 4 - Cheat List"
        }, 
        {
            "location": "/Plot_snippets_-_ggvis/", 
            "text": "Documentation\n\n\nDataset\n\n\nThe \nggvis\n Package\n\n\n1, The Grammar of Graphics\n\n\n2, Lines and Syntax\n\n\n3, Transformations\n\n\n4, Interactivity and Layers\n\n\n5, Customizing Axes, Legends, and\n\n    Scales\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nCode snippets and results.\n\n\nggvis\n generates html outputs. Graphics presented here are images:\n\n    .png files; .gif files when specified.\n\n\n\n\n\n\nDocumentation\n\n\n\n\nggvis\n Overview\n\n\nggvis\n Cookbook\n\n\n\n\nDataset\n\n\nFor most examples, we use the \nmtcars\n, \npressure\n, \nfaithful\n datasets.\n\n\nThe \nggvis\n Package\n\n\nlibrary(ggvis)\n\n\n\n\n1, The Grammar of Graphics\n\n\nStart to explore \n\n\n# change the code below to plot the disp variable of mtcars on the x axis\nmtcars %\n%\n  ggvis(~disp, ~mpg) %\n%\n  layer_points()\n\n\n\n\n\n\nggvis\n and its capabilities \n\n\n# Change the code below to make a graph with red points\nmtcars %\n%\n  ggvis(~wt, ~mpg, fill := \nred\n) %\n%\n  layer_points()\n\n\n\n\n\n\n# Change the code below draw smooths instead of points\nmtcars %\n%\n  ggvis(~wt, ~mpg) %\n%\n  layer_smooths()\n\n\n\n\n\n\n# Change the code below to make a graph containing both points and a smoothed summary line\nmtcars %\n%\n  ggvis(~wt, ~mpg) %\n%\n  layer_points() %\n%\n  layer_smooths()\n\n\n\n\n\n\nggvis\n grammar ~ graphics grammar\n\n\n# Make a scatterplot of the pressure dataset\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_points\n\n\n\n\n\n\n# Adapt the code you wrote for the first challenge: show bars instead of points\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_bars\n\n\n\n\n\n\n# Adapt the code you wrote for the first challenge: show lines instead of points\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_lines\n\n\n\n\n\n\n# Adapt the code you wrote for the first challenge: map the fill property to the temperature variable\npressure %\n%\n  ggvis(~temperature, ~pressure, fill = ~temperature) %\n%\n  layer_points\n\n\n\n\n\n\n# Extend the code you wrote for the previous challenge: map the size property to the pressure variable\npressure %\n%\n  ggvis(~temperature, ~pressure, fill = ~temperature, size = ~pressure) %\n%\n  layer_points\n\n\n\n\n\n\n4 essential components of a graph\n\n\nfaithful %\n%\n    ggvis(~waiting, ~eruptions, fill := \nred\n) %\n%\n    layer_points() %\n%\n    add_axis(\ny\n, title = \nDuration of eruption (m)\n,\n             values = c(2, 3, 4, 5), subdivide = 9) %\n%\n    add_axis(\nx\n, title = \nTime since previous eruption (m)\n)\n\n\n\n\n\n\n2, Lines and Syntax\n\n\nThree operators: \n%\n%\n, \n=\n and \n:=\n\n\nlayer_points(ggvis(faithful, ~waiting, ~eruptions))\n\n\n\n\n\n\n# Rewrite the code with the pipe operator     \nfaithful %\n%\n  ggvis(~waiting, ~eruptions) %\n%\n  layer_points()\n\n\n\n\n\n\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_points()\n\n\n\n\n\n\n# Modify this graph to map the size property to the pressure variable\npressure %\n%\n  ggvis(~temperature, ~pressure, size = ~pressure) %\n%\n  layer_points()\n\n\n\n\n\n\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_points()\n\n\n\n\n\n\n# Modify this graph by setting the size property\npressure %\n%\n  ggvis(~temperature, ~pressure, size := 100) %\n%\n  layer_points()\n\n\n\n\n\n\npressure %\n% \n  ggvis(~temperature, ~pressure, fill = \nred\n) %\n%\n  layer_points()\n\n\n\n\n\n\n# Fix this code to set the fill property to red\npressure %\n% \n  ggvis(~temperature, ~pressure, fill := \nred\n) %\n%\n  layer_points()\n\n\n\n\n\n\nReferring to different objects\n\n\nred \n- \ngreen\n\npressure$red \n- pressure$temperature\n\npressure %\n%\n  ggvis(~temperature, ~pressure, fill := red) %\n%\n  layer_points()\n\n\n\n\n\n\nProperties for points\n\n\n# Change the code to set the fills using pressure$black\npressure %\n%\n  ggvis(~temperature, ~pressure, fill := ~'black') %\n%\n  layer_points()\n\n\n\n\n\n\n# Plot the faithful data as described in the second instruction\nfaithful %\n% \n  ggvis(~waiting, ~eruptions, size = ~eruptions, opacity := 0.5, fill := \nblue\n, stroke := \nblack\n) %\n% \n  layer_points()\n\n\n\n\n\n\n# Plot the faithful data as described in the third instruction\nfaithful %\n% \n  ggvis(~waiting, ~eruptions, size := 100, fill := \nred\n, fillOpacity = ~eruptions, stroke := \nred\n, shape := \ncross\n) %\n%\n  layer_points()\n\n\n\n\n\n\nProperties for lines\n\n\n# Change the code below to use the lines mark\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_lines()\n\n\n\n\n\n\n# Set the properties described in the second instruction in the graph below\npressure %\n%\n  ggvis(~temperature, ~pressure, stroke := \nred\n, strokeWidth := 2, strokeDash := 6) %\n%\n  layer_lines()\n\n\n\n\n\n\nDisplay model fits\n\n\nlayer_lines()\n will always connect the points in your plot from the\n\nleftmost point to the rightmost point. This can be undesirable if you\n\nare trying to plot a specific shape.\n\n\nlayer_paths()\n: this mark connects the points in the order that they\n\nappear in the data set. So the paths mark will connect the point that\n\ncorresponds to the first row of the data to the point that corresponds\n\nto the second row of data, and so on - no matter where those points\n\nappear in the graph.\n\n\n# change the third line of code to plot a map of Texas\nlibrary(maps)\nlibrary(ggplot2)\n\ntexas \n- ggplot2::map_data(\nstate\n, region = \ntexas\n)\n\ntexas %\n%\n  ggvis(~long, ~lat) %\n%\n  layer_paths()\n\n\n\n\n\n\n# Same plot, but set the fill property of the texas map to dark orange\ntexas %\n%\n  ggvis(~long, ~lat, fill := \ndarkorange\n) %\n%\n  layer_paths()\n\n\n\n\n\n\ncompute_smooth()\n to simplify model fits\n\n\ncompute_model_prediction()\n is a useful function to use with line\n\ngraphs. It takes a data frame as input and returns a new data frame as\n\noutput. The new data frame will contain the x and y values of a line\n\nfitted to the data in the original data frame.\n\n\nGenerate the x and y coordinates for a LOESS smooth line.\n\n\nfaithful %\n%\n  compute_model_prediction(eruptions ~ waiting, model = \nlm\n) %\n%\n  head(10)\n\n\n\n\n##       pred_    resp_\n## 1  43.00000 1.377986\n## 2  43.67089 1.428724\n## 3  44.34177 1.479461\n## 4  45.01266 1.530199\n## 5  45.68354 1.580937\n## 6  46.35443 1.631674\n## 7  47.02532 1.682412\n## 8  47.69620 1.733150\n## 9  48.36709 1.783888\n## 10 49.03797 1.834625\n\n\n\n# Compute the x and y coordinates for a loess smooth line that predicts mpg with the wt\nmtcars %\n%\n  compute_smooth(mpg ~ wt) %\n%\n  head(10)\n\n\n\n\n##       pred_    resp_\n## 1  1.513000 32.08897\n## 2  1.562506 31.68786\n## 3  1.612013 31.28163\n## 4  1.661519 30.87037\n## 5  1.711025 30.45419\n## 6  1.760532 30.03318\n## 7  1.810038 29.60745\n## 8  1.859544 29.17711\n## 9  1.909051 28.74224\n## 10 1.958557 28.30017\n\n\n\n#model = \nloess\n is set by default\n\n\n\n\n3, Transformations\n\n\nHistograms (1)\n\n\n# Build a histogram of the waiting variable of the faithful data set.\nfaithful %\n%\n  ggvis(~waiting) %\n%\n  layer_histograms()\n\n\n\n\n\n\n# Build the same histogram, but with a binwidth (width argument) of 5 units\nfaithful %\n%\n  ggvis(~waiting) %\n%\n  layer_histograms(width = 5)\n\n\n\n\n\n\nHistograms (2)\n\n\nfaithful %\n%\n  ggvis(~waiting) %\n%\n  layer_histograms(width = 5)\n\n\n\n\n\n\n# Transform the code: just compute the bins instead of plotting a histogram\nfaithful %\n%\n  compute_bin(~waiting, width = 5)\n\n\n\n\n##    count_ x_ xmin_ xmax_ width_\n## 1      13 45  42.5  47.5      5\n## 2      24 50  47.5  52.5      5\n## 3      29 55  52.5  57.5      5\n## 4      21 60  57.5  62.5      5\n## 5      13 65  62.5  67.5      5\n## 6      13 70  67.5  72.5      5\n## 7      42 75  72.5  77.5      5\n## 8      58 80  77.5  82.5      5\n## 9      38 85  82.5  87.5      5\n## 10     17 90  87.5  92.5      5\n## 11      4 95  92.5  97.5      5\n\n\n\n# Combine the solution to the first challenge with layer_rects() to build a histogram\nfaithful %\n%\n  compute_bin(~waiting, width = 5) %\n%\n  ggvis(x = ~xmin_, x2 = ~xmax_, y = 0, y2 = ~count_) %\n%\n  layer_rects()\n\n\n\n\n\n\nDensity plots\n\n\n# Combine compute_density() with layer_lines() to make a density plot of the waiting variable.\nfaithful %\n%\n  compute_density(~waiting) %\n%\n  ggvis(~pred_,~resp_) %\n%\n  layer_lines()\n\n\n\n\n\n\n# Build a density plot directly using layer_densities. Use the correct variables and properties.\nfaithful %\n%\n  ggvis(~waiting) %\n%\n  layer_densities(fill := \ngreen\n)\n\n\n\n\n\n\nShortcuts\n\n\n# Complete the code to plot a bar graph of the cyl factor.\nmtcars %\n%\n  ggvis(~factor(cyl)) %\n%\n  layer_bars()\n\n\n\n\n\n\n# Adapt the solution to the first challenge to just calculate the count values. No plotting!\nmtcars %\n%\n  compute_count(~factor(cyl))\n\n\n\n\n##   count_ x_\n## 1     11  4\n## 2      7  6\n## 3     14  8\n\n\n\nggvis\n and \ngroup_by\n\n\nmtcars %\n%\n  group_by(am) %\n%\n  ggvis(~mpg, ~wt, stroke = ~factor(am)) %\n%\n  layer_smooths()\n\n\n\n\n\n\n# Change the code to plot a unique smooth line for each value of the cyl variable.\nmtcars %\n%\n  group_by(cyl) %\n%\n  ggvis(~mpg, ~wt, stroke = ~factor(cyl)) %\n%\n  layer_smooths()\n\n\n\n\n\n\n# Adapt the graph to contain a separate density for each value of cyl.\nmtcars %\n%\n  group_by(cyl) %\n%\n  ggvis(~mpg) %\n%\n  layer_densities()\n\n\n\n\n\n\n# Copy and alter the solution to the second challenge to map the fill property to a categorical version of cyl.\nmtcars %\n%\n  group_by(cyl) %\n%\n  ggvis(~mpg) %\n%\n  layer_densities(fill = ~factor(cyl))\n\n\n\n\n\n\ngroup_by()\n versus \ninteraction()\n\n\nmtcars %\n%\n  group_by(cyl) %\n%\n  ggvis(~mpg) %\n%\n  layer_densities(fill = ~factor(cyl))\n\n\n\n\n\n\n# Alter the graph: separate density for each unique combination of 'cyl' and 'am'.\nmtcars %\n%\n  group_by(cyl,am) %\n%\n  ggvis(~mpg, fill = ~factor(cyl)) %\n%\n  layer_densities()\n\n#factor(cyl),factor(am)\n\n\n\n\n\n\nmtcars %\n%\n  group_by(cyl, am) %\n%\n  ggvis(~mpg, fill = ~factor(cyl)) %\n%\n  layer_densities()\n\n\n\n\n\n\n# Update the graph to map `fill` to the unique combinations of the grouping variables.\nmtcars %\n%\n  group_by(cyl, am) %\n%\n  ggvis(~mpg, fill = ~interaction(cyl,am)) %\n% \n  layer_densities()\n\n\n\n\n\n\nChaining is a virtue\n\n\nmtcars %\n%\n    group_by(cyl, am) %\n%\n    ggvis(~mpg, fill = ~interaction(cyl, am)) %\n%\n    layer_densities()\n\n\n\n\n\n\nThis call is exactly equivalent to the following piece of code that is\n\nvery hard to read:\n\n\nlayer_densities(ggvis(group_by(mtcars, cyl, am), ~mpg, fill = ~interaction(cyl, am)))\n\n\n\n\n\n\n4, Interactivity and Layers\n\n\nThe basics of interactive plots\n\n\n# Run this code and inspect the output. Follow the link in the instructions for the interactive version\nfaithful %\n% \n  ggvis(~waiting, ~eruptions, fillOpacity := 0.5, \n        shape := input_select(label = \nChoose shape:\n, choices = c(\ncircle\n, \nsquare\n, \ncross\n, \ndiamond\n, \ntriangle-up\n, \ntriangle-down\n))) %\n% \n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\n# Copy the first code chunk and alter the code to make the fill property interactive using a select box\nfaithful %\n%\n  ggvis(~waiting, ~eruptions, fillOpacity := 0.5, shape := input_select(label = \nChoose shape:\n, choices = c(\ncircle\n, \nsquare\n, \ncross\n, \ndiamond\n, \ntriangle-up\n, \ntriangle-down\n)), fill := input_select(label = \nChoose color:\n, choices = c(\nblack\n,\nred\n,\nblue\n,\ngreen\n))) %\n%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\n# Add radio buttons to control the fill of the plot\nmtcars %\n% \n  ggvis(~mpg, ~wt, fill := input_radiobuttons(label = \nChoose color:\n, choices = c(\nblack\n,\nred\n,\nblue\n,\ngreen\n))) %\n%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\nInput widgets in more detail\n\n\nmtcars %\n%\n  ggvis(~mpg, ~wt, fill := input_radiobuttons(label = \nChoose color:\n, choices = c(\nblack\n, \nred\n, \nblue\n, \ngreen\n))) %\n%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\n# Change the radiobuttons widget to a text widget \nmtcars %\n%\n  ggvis(~mpg, ~wt, fill := input_text(label = \nChoose color:\n, value = c(\nblack\n))) %\n%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\nmtcars %\n%\n  ggvis(~mpg, ~wt) %\n%\n  layer_points()\n\n\n\n\n\n\n# Map the fill property to a select box that returns variable names\nmtcars %\n%\n  ggvis(~mpg, ~wt, fill = input_select(label = \nChoose fill variable:\n, choices = names(mtcars), map = as.name)) %\n%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\nInput widgets in more detail (2)\n\n\n# Map the fill property to a select box that returns variable names\nmtcars %\n%\n  ggvis(~mpg, ~wt, fill = input_select(label = \nChoose fill variable:\n, choices = names(mtcars), map = as.name)) %\n%\n  layer_points()\n\n\n\n\n.gif file:\n\n\n\n\nControl parameters and values\n\n\n# Map the bindwidth to a numeric field (\nChoose a binwidth:\n)\nmtcars %\n%\n  ggvis(~mpg) %\n%\n  layer_histograms(width = input_numeric(value = 1, label = \nChoose a binwidth:\n))\n\n\n\n\n.gif file:\n\n\n\n\n# Map the binwidth to a slider bar (\nChoose a binwidth:\n) with the correct specifications\nmtcars %\n%\n  ggvis(~mpg) %\n%\n  layer_histograms(width = input_slider(min = 1, max = 20, label = \nChoose a binwidth:\n))\n\n\n\n\n.gif file:\n\n\n\n\nMulti-layered plots and their properties\n\n\n# Add a layer of points to the graph below.\npressure %\n%\n  ggvis(~temperature, ~pressure, stroke := \nskyblue\n) %\n% layer_lines() %\n%\n  layer_points()\n\n\n\n\n\n\n# Copy and adapt the solution to the first instruction below so that only the lines layer uses a skyblue stroke.\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_lines(stroke := \nskyblue\n) %\n%\n  layer_points()\n\n\n\n\n\n\n# Rewrite the code below so that only the points layer uses the shape property.\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_lines(stroke := \nskyblue\n) %\n%\n  layer_points(shape := \ntriangle-up\n)\n\n\n\n\n\n\n# Refactor the code for the graph below to make it as concise as possible\npressure %\n%\n  ggvis(~temperature, ~pressure, stroke := \nskyblue\n, strokeOpacity := 0.5, strokeWidth := 5) %\n%\n  layer_lines() %\n%\n  layer_points(fill = ~temperature, shape := \ntriangle-up\n, size := 300)\n\n\n\n\n\n\nMulti-layered plots and their properties (2)\n\n\n# Rewrite the code below so that only the points layer uses the shape property.\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_lines(stroke := \nskyblue\n) %\n%\n  layer_points(shape := \ntriangle-up\n)\n\n\n\n\n\n\n# Refactor the code for the graph below to make it as concise as possible\npressure %\n%\n  ggvis(~temperature, ~pressure, stroke := \nskyblue\n,\n        strokeOpacity := 0.5, strokeWidth := 5) %\n%\n  layer_lines() %\n%\n  layer_points(fill = ~temperature, shape := \ntriangle-up\n, size := 300)\n\n\n\n\n\n\nThere is no limit on the number of layers!\n\n\n# Create a graph containing a scatterplot, a linear model and a smooth line.\npressure %\n%\n  ggvis(~temperature, ~pressure) %\n%\n  layer_points() %\n%\n  layer_lines(stroke := \nblack\n, opacity := 0.5) %\n%\n  layer_model_predictions(model = \nlm\n, stroke := \nnavy\n) %\n%\n  layer_smooths(stroke := \nskyblue\n)\n\n\n\n\n\n\nTaking local and global to the next level\n\n\npressure %\n%\n  ggvis(~temperature, ~pressure, stroke := \ndarkred\n) %\n%\n  layer_lines(stroke := \norange\n, strokeDash := 5, strokeWidth := 5) %\n%\n  layer_points(shape := \ncircle\n, size := 100, fill := \nlightgreen\n) %\n%\n  layer_smooths()\n\n\n\n\n\n\n5, Customizing Axes, Legends, and Scales\n\n\nAxes\n\n\n# add the title of the x axis: \nTime since previous eruption (m)\n\nfaithful %\n% \n  ggvis(~waiting, ~eruptions) %\n% \n  layer_points() %\n% \n  add_axis(\ny\n, title = \nDuration of eruption (m)\n) %\n%\n  add_axis(\nx\n, title = \nTime since previous eruption (m)\n)\n\n\n\n\n\n\n# Add to the code to place a labelled tick mark at 50, 60, 70, 80, 90 on the x axis.\nfaithful %\n% \n  ggvis(~waiting, ~eruptions) %\n% \n  layer_points() %\n% \n  add_axis(\ny\n, title = \nDuration of eruption (m)\n, values = c(2, 3, 4, 5), subdivide = 9) %\n% \n  add_axis(\nx\n, title = \nTime since previous eruption (m)\n, values = c(50, 60, 70, 80, 90), subdivide = 9)\n\n\n\n\n\n\n# Add to the code below to change the location of the y axis\nfaithful %\n% \n  ggvis(~waiting, ~eruptions) %\n% \n  layer_points() %\n%\n  add_axis(\nx\n, orient = \ntop\n) %\n% add_axis(\ny\n, orient = \nright\n)\n\n\n\n\n\n\nLegends\n\n\n# Add a legend to the plot below: use the correct title and orientation\nfaithful %\n% \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions))) %\n% \n  layer_points() %\n%\n  add_legend(\nfill\n, title = \n~ duration (m)\n, orient = \nleft\n)\n\n\n\n\n\n\n#add_legend(vis, scales = NULL, orient = \nright\n, title = NULL, format = NULL, values = NULL, properties = NULL)\n\n# Use add_legend() to combine the legends in the plot below. Adjust its properties as instructed.\nfaithful %\n% \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), \n        size = ~round(eruptions))  %\n%\n  layer_points() %\n%\n  add_legend(c(\nfill\n, \nshape\n, \nsize\n), title = \n~ duration (m)\n, values = c(2,3,4,5))\n\n\n\n\n\n\nLegends (2)\n\n\n# Fix the legend\nfaithful %\n% \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), \n        size = ~round(eruptions)) %\n% \n    layer_points() %\n% \n    add_legend(c(\nfill\n, \nshape\n, \nsize\n), title = \n~ duration (m)\n)\n\n\n\n\n\n\nScale types\n\n\n# Add to the code below to make the stroke color range from \ndarkred\n to \norange\n.\nmtcars %\n% \n  ggvis(~wt, ~mpg, fill = ~disp, stroke = ~disp, strokeWidth := 2) %\n%\n  layer_points() %\n%\n  scale_numeric(\nfill\n, range = c(\nred\n, \nyellow\n)) %\n% scale_numeric(\nstroke\n, range = c(\ndarkred\n, \norange\n))\n\n\n\n\n\n\n# Change the graph below to make the fill colors range from green to beige.\nmtcars %\n% ggvis(~wt, ~mpg, fill = ~hp) %\n%\n  layer_points() %\n% scale_numeric(\nfill\n, range = c(\ngreen\n, \nbeige\n))\n\n\n\n\n\n\n# Create a scale that will map `factor(cyl)` to a new range of colors: purple, blue, and green. \nmtcars %\n% ggvis(~wt, ~mpg, fill = ~factor(cyl)) %\n%\n  layer_points() %\n% scale_nominal(\nfill\n, range = c(\npurple\n, \nblue\n, \ngreen\n))\n\n\n\n\n\n\nAdjust any visual property\n\n\n# Add a scale that limits the range of opacity from 0.2 to 1. \nmtcars %\n%\n  ggvis(x = ~wt, y = ~mpg, fill = ~factor(cyl), opacity = ~hp) %\n%\n  layer_points() %\n%\n  scale_numeric(\nopacity\n, range = c(0.2,1))\n\n\n\n\n\n\n# Add a second scale that will expand the x axis to cover data values from 0 to 6.\nmtcars %\n%\n  ggvis(~wt, ~mpg, fill = ~disp) %\n%\n  layer_points() %\n%\n  scale_numeric(\ny\n, domain = c(0, NA)) %\n%\n  scale_numeric(\nx\n, domain = c(0, 6))\n\n\n\n\n\n\nAdjust any visual property (2)\n\n\n# Add a second scale to set domain for x\nmtcars %\n%\n  ggvis(~wt, ~mpg, fill = ~disp) %\n%\n  layer_points() %\n%\n  scale_numeric(\ny\n, domain = c(0, NA)) %\n%\n  scale_numeric(\nx\n, domain = c(0, 6))\n\n\n\n\n\n\n=\n versus \n:=\n\n\n# Set the fill value to the color variable instead of mapping it, and see what happens\nmtcars$color \n- c(\nred\n, \nteal\n, \n#cccccc\n, \ntan\n)\n\nmtcars %\n%\n  ggvis(x = ~wt, y = ~mpg, fill = ~color) %\n%\n  layer_points()\n\n\n\n\n\n\nmtcars %\n%\n  ggvis(x = ~wt, y = ~mpg, fill := ~color) %\n%\n  layer_points()", 
            "title": "Plot Snippets - ggvis"
        }, 
        {
            "location": "/Plot_snippets_-_ggvis/#dataset", 
            "text": "For most examples, we use the  mtcars ,  pressure ,  faithful  datasets.", 
            "title": "Dataset"
        }, 
        {
            "location": "/Plot_snippets_-_ggvis/#the-ggvis-package", 
            "text": "library(ggvis)", 
            "title": "The ggvis Package"
        }, 
        {
            "location": "/Plot_snippets_-_ggvis/#1-the-grammar-of-graphics", 
            "text": "Start to explore   # change the code below to plot the disp variable of mtcars on the x axis\nmtcars % %\n  ggvis(~disp, ~mpg) % %\n  layer_points()   ggvis  and its capabilities   # Change the code below to make a graph with red points\nmtcars % %\n  ggvis(~wt, ~mpg, fill :=  red ) % %\n  layer_points()   # Change the code below draw smooths instead of points\nmtcars % %\n  ggvis(~wt, ~mpg) % %\n  layer_smooths()   # Change the code below to make a graph containing both points and a smoothed summary line\nmtcars % %\n  ggvis(~wt, ~mpg) % %\n  layer_points() % %\n  layer_smooths()   ggvis  grammar ~ graphics grammar  # Make a scatterplot of the pressure dataset\npressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_points   # Adapt the code you wrote for the first challenge: show bars instead of points\npressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_bars   # Adapt the code you wrote for the first challenge: show lines instead of points\npressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_lines   # Adapt the code you wrote for the first challenge: map the fill property to the temperature variable\npressure % %\n  ggvis(~temperature, ~pressure, fill = ~temperature) % %\n  layer_points   # Extend the code you wrote for the previous challenge: map the size property to the pressure variable\npressure % %\n  ggvis(~temperature, ~pressure, fill = ~temperature, size = ~pressure) % %\n  layer_points   4 essential components of a graph  faithful % %\n    ggvis(~waiting, ~eruptions, fill :=  red ) % %\n    layer_points() % %\n    add_axis( y , title =  Duration of eruption (m) ,\n             values = c(2, 3, 4, 5), subdivide = 9) % %\n    add_axis( x , title =  Time since previous eruption (m) )", 
            "title": "1, The Grammar of Graphics"
        }, 
        {
            "location": "/Plot_snippets_-_ggvis/#2-lines-and-syntax", 
            "text": "Three operators:  % % ,  =  and  :=  layer_points(ggvis(faithful, ~waiting, ~eruptions))   # Rewrite the code with the pipe operator     \nfaithful % %\n  ggvis(~waiting, ~eruptions) % %\n  layer_points()   pressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_points()   # Modify this graph to map the size property to the pressure variable\npressure % %\n  ggvis(~temperature, ~pressure, size = ~pressure) % %\n  layer_points()   pressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_points()   # Modify this graph by setting the size property\npressure % %\n  ggvis(~temperature, ~pressure, size := 100) % %\n  layer_points()   pressure % % \n  ggvis(~temperature, ~pressure, fill =  red ) % %\n  layer_points()   # Fix this code to set the fill property to red\npressure % % \n  ggvis(~temperature, ~pressure, fill :=  red ) % %\n  layer_points()   Referring to different objects  red  -  green \npressure$red  - pressure$temperature\n\npressure % %\n  ggvis(~temperature, ~pressure, fill := red) % %\n  layer_points()   Properties for points  # Change the code to set the fills using pressure$black\npressure % %\n  ggvis(~temperature, ~pressure, fill := ~'black') % %\n  layer_points()   # Plot the faithful data as described in the second instruction\nfaithful % % \n  ggvis(~waiting, ~eruptions, size = ~eruptions, opacity := 0.5, fill :=  blue , stroke :=  black ) % % \n  layer_points()   # Plot the faithful data as described in the third instruction\nfaithful % % \n  ggvis(~waiting, ~eruptions, size := 100, fill :=  red , fillOpacity = ~eruptions, stroke :=  red , shape :=  cross ) % %\n  layer_points()   Properties for lines  # Change the code below to use the lines mark\npressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_lines()   # Set the properties described in the second instruction in the graph below\npressure % %\n  ggvis(~temperature, ~pressure, stroke :=  red , strokeWidth := 2, strokeDash := 6) % %\n  layer_lines()   Display model fits  layer_lines()  will always connect the points in your plot from the \nleftmost point to the rightmost point. This can be undesirable if you \nare trying to plot a specific shape.  layer_paths() : this mark connects the points in the order that they \nappear in the data set. So the paths mark will connect the point that \ncorresponds to the first row of the data to the point that corresponds \nto the second row of data, and so on - no matter where those points \nappear in the graph.  # change the third line of code to plot a map of Texas\nlibrary(maps)\nlibrary(ggplot2)\n\ntexas  - ggplot2::map_data( state , region =  texas )\n\ntexas % %\n  ggvis(~long, ~lat) % %\n  layer_paths()   # Same plot, but set the fill property of the texas map to dark orange\ntexas % %\n  ggvis(~long, ~lat, fill :=  darkorange ) % %\n  layer_paths()   compute_smooth()  to simplify model fits  compute_model_prediction()  is a useful function to use with line \ngraphs. It takes a data frame as input and returns a new data frame as \noutput. The new data frame will contain the x and y values of a line \nfitted to the data in the original data frame.  Generate the x and y coordinates for a LOESS smooth line.  faithful % %\n  compute_model_prediction(eruptions ~ waiting, model =  lm ) % %\n  head(10)  ##       pred_    resp_\n## 1  43.00000 1.377986\n## 2  43.67089 1.428724\n## 3  44.34177 1.479461\n## 4  45.01266 1.530199\n## 5  45.68354 1.580937\n## 6  46.35443 1.631674\n## 7  47.02532 1.682412\n## 8  47.69620 1.733150\n## 9  48.36709 1.783888\n## 10 49.03797 1.834625  # Compute the x and y coordinates for a loess smooth line that predicts mpg with the wt\nmtcars % %\n  compute_smooth(mpg ~ wt) % %\n  head(10)  ##       pred_    resp_\n## 1  1.513000 32.08897\n## 2  1.562506 31.68786\n## 3  1.612013 31.28163\n## 4  1.661519 30.87037\n## 5  1.711025 30.45419\n## 6  1.760532 30.03318\n## 7  1.810038 29.60745\n## 8  1.859544 29.17711\n## 9  1.909051 28.74224\n## 10 1.958557 28.30017  #model =  loess  is set by default", 
            "title": "2, Lines and Syntax"
        }, 
        {
            "location": "/Plot_snippets_-_ggvis/#3-transformations", 
            "text": "Histograms (1)  # Build a histogram of the waiting variable of the faithful data set.\nfaithful % %\n  ggvis(~waiting) % %\n  layer_histograms()   # Build the same histogram, but with a binwidth (width argument) of 5 units\nfaithful % %\n  ggvis(~waiting) % %\n  layer_histograms(width = 5)   Histograms (2)  faithful % %\n  ggvis(~waiting) % %\n  layer_histograms(width = 5)   # Transform the code: just compute the bins instead of plotting a histogram\nfaithful % %\n  compute_bin(~waiting, width = 5)  ##    count_ x_ xmin_ xmax_ width_\n## 1      13 45  42.5  47.5      5\n## 2      24 50  47.5  52.5      5\n## 3      29 55  52.5  57.5      5\n## 4      21 60  57.5  62.5      5\n## 5      13 65  62.5  67.5      5\n## 6      13 70  67.5  72.5      5\n## 7      42 75  72.5  77.5      5\n## 8      58 80  77.5  82.5      5\n## 9      38 85  82.5  87.5      5\n## 10     17 90  87.5  92.5      5\n## 11      4 95  92.5  97.5      5  # Combine the solution to the first challenge with layer_rects() to build a histogram\nfaithful % %\n  compute_bin(~waiting, width = 5) % %\n  ggvis(x = ~xmin_, x2 = ~xmax_, y = 0, y2 = ~count_) % %\n  layer_rects()   Density plots  # Combine compute_density() with layer_lines() to make a density plot of the waiting variable.\nfaithful % %\n  compute_density(~waiting) % %\n  ggvis(~pred_,~resp_) % %\n  layer_lines()   # Build a density plot directly using layer_densities. Use the correct variables and properties.\nfaithful % %\n  ggvis(~waiting) % %\n  layer_densities(fill :=  green )   Shortcuts  # Complete the code to plot a bar graph of the cyl factor.\nmtcars % %\n  ggvis(~factor(cyl)) % %\n  layer_bars()   # Adapt the solution to the first challenge to just calculate the count values. No plotting!\nmtcars % %\n  compute_count(~factor(cyl))  ##   count_ x_\n## 1     11  4\n## 2      7  6\n## 3     14  8  ggvis  and  group_by  mtcars % %\n  group_by(am) % %\n  ggvis(~mpg, ~wt, stroke = ~factor(am)) % %\n  layer_smooths()   # Change the code to plot a unique smooth line for each value of the cyl variable.\nmtcars % %\n  group_by(cyl) % %\n  ggvis(~mpg, ~wt, stroke = ~factor(cyl)) % %\n  layer_smooths()   # Adapt the graph to contain a separate density for each value of cyl.\nmtcars % %\n  group_by(cyl) % %\n  ggvis(~mpg) % %\n  layer_densities()   # Copy and alter the solution to the second challenge to map the fill property to a categorical version of cyl.\nmtcars % %\n  group_by(cyl) % %\n  ggvis(~mpg) % %\n  layer_densities(fill = ~factor(cyl))   group_by()  versus  interaction()  mtcars % %\n  group_by(cyl) % %\n  ggvis(~mpg) % %\n  layer_densities(fill = ~factor(cyl))   # Alter the graph: separate density for each unique combination of 'cyl' and 'am'.\nmtcars % %\n  group_by(cyl,am) % %\n  ggvis(~mpg, fill = ~factor(cyl)) % %\n  layer_densities()\n\n#factor(cyl),factor(am)   mtcars % %\n  group_by(cyl, am) % %\n  ggvis(~mpg, fill = ~factor(cyl)) % %\n  layer_densities()   # Update the graph to map `fill` to the unique combinations of the grouping variables.\nmtcars % %\n  group_by(cyl, am) % %\n  ggvis(~mpg, fill = ~interaction(cyl,am)) % % \n  layer_densities()   Chaining is a virtue  mtcars % %\n    group_by(cyl, am) % %\n    ggvis(~mpg, fill = ~interaction(cyl, am)) % %\n    layer_densities()   This call is exactly equivalent to the following piece of code that is \nvery hard to read:  layer_densities(ggvis(group_by(mtcars, cyl, am), ~mpg, fill = ~interaction(cyl, am)))", 
            "title": "3, Transformations"
        }, 
        {
            "location": "/Plot_snippets_-_ggvis/#4-interactivity-and-layers", 
            "text": "The basics of interactive plots  # Run this code and inspect the output. Follow the link in the instructions for the interactive version\nfaithful % % \n  ggvis(~waiting, ~eruptions, fillOpacity := 0.5, \n        shape := input_select(label =  Choose shape: , choices = c( circle ,  square ,  cross ,  diamond ,  triangle-up ,  triangle-down ))) % % \n  layer_points()  .gif file:   # Copy the first code chunk and alter the code to make the fill property interactive using a select box\nfaithful % %\n  ggvis(~waiting, ~eruptions, fillOpacity := 0.5, shape := input_select(label =  Choose shape: , choices = c( circle ,  square ,  cross ,  diamond ,  triangle-up ,  triangle-down )), fill := input_select(label =  Choose color: , choices = c( black , red , blue , green ))) % %\n  layer_points()  .gif file:   # Add radio buttons to control the fill of the plot\nmtcars % % \n  ggvis(~mpg, ~wt, fill := input_radiobuttons(label =  Choose color: , choices = c( black , red , blue , green ))) % %\n  layer_points()  .gif file:   Input widgets in more detail  mtcars % %\n  ggvis(~mpg, ~wt, fill := input_radiobuttons(label =  Choose color: , choices = c( black ,  red ,  blue ,  green ))) % %\n  layer_points()  .gif file:   # Change the radiobuttons widget to a text widget \nmtcars % %\n  ggvis(~mpg, ~wt, fill := input_text(label =  Choose color: , value = c( black ))) % %\n  layer_points()  .gif file:   mtcars % %\n  ggvis(~mpg, ~wt) % %\n  layer_points()   # Map the fill property to a select box that returns variable names\nmtcars % %\n  ggvis(~mpg, ~wt, fill = input_select(label =  Choose fill variable: , choices = names(mtcars), map = as.name)) % %\n  layer_points()  .gif file:   Input widgets in more detail (2)  # Map the fill property to a select box that returns variable names\nmtcars % %\n  ggvis(~mpg, ~wt, fill = input_select(label =  Choose fill variable: , choices = names(mtcars), map = as.name)) % %\n  layer_points()  .gif file:   Control parameters and values  # Map the bindwidth to a numeric field ( Choose a binwidth: )\nmtcars % %\n  ggvis(~mpg) % %\n  layer_histograms(width = input_numeric(value = 1, label =  Choose a binwidth: ))  .gif file:   # Map the binwidth to a slider bar ( Choose a binwidth: ) with the correct specifications\nmtcars % %\n  ggvis(~mpg) % %\n  layer_histograms(width = input_slider(min = 1, max = 20, label =  Choose a binwidth: ))  .gif file:   Multi-layered plots and their properties  # Add a layer of points to the graph below.\npressure % %\n  ggvis(~temperature, ~pressure, stroke :=  skyblue ) % % layer_lines() % %\n  layer_points()   # Copy and adapt the solution to the first instruction below so that only the lines layer uses a skyblue stroke.\npressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_lines(stroke :=  skyblue ) % %\n  layer_points()   # Rewrite the code below so that only the points layer uses the shape property.\npressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_lines(stroke :=  skyblue ) % %\n  layer_points(shape :=  triangle-up )   # Refactor the code for the graph below to make it as concise as possible\npressure % %\n  ggvis(~temperature, ~pressure, stroke :=  skyblue , strokeOpacity := 0.5, strokeWidth := 5) % %\n  layer_lines() % %\n  layer_points(fill = ~temperature, shape :=  triangle-up , size := 300)   Multi-layered plots and their properties (2)  # Rewrite the code below so that only the points layer uses the shape property.\npressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_lines(stroke :=  skyblue ) % %\n  layer_points(shape :=  triangle-up )   # Refactor the code for the graph below to make it as concise as possible\npressure % %\n  ggvis(~temperature, ~pressure, stroke :=  skyblue ,\n        strokeOpacity := 0.5, strokeWidth := 5) % %\n  layer_lines() % %\n  layer_points(fill = ~temperature, shape :=  triangle-up , size := 300)   There is no limit on the number of layers!  # Create a graph containing a scatterplot, a linear model and a smooth line.\npressure % %\n  ggvis(~temperature, ~pressure) % %\n  layer_points() % %\n  layer_lines(stroke :=  black , opacity := 0.5) % %\n  layer_model_predictions(model =  lm , stroke :=  navy ) % %\n  layer_smooths(stroke :=  skyblue )   Taking local and global to the next level  pressure % %\n  ggvis(~temperature, ~pressure, stroke :=  darkred ) % %\n  layer_lines(stroke :=  orange , strokeDash := 5, strokeWidth := 5) % %\n  layer_points(shape :=  circle , size := 100, fill :=  lightgreen ) % %\n  layer_smooths()", 
            "title": "4, Interactivity and Layers"
        }, 
        {
            "location": "/Plot_snippets_-_ggvis/#5-customizing-axes-legends-and-scales", 
            "text": "Axes  # add the title of the x axis:  Time since previous eruption (m) \nfaithful % % \n  ggvis(~waiting, ~eruptions) % % \n  layer_points() % % \n  add_axis( y , title =  Duration of eruption (m) ) % %\n  add_axis( x , title =  Time since previous eruption (m) )   # Add to the code to place a labelled tick mark at 50, 60, 70, 80, 90 on the x axis.\nfaithful % % \n  ggvis(~waiting, ~eruptions) % % \n  layer_points() % % \n  add_axis( y , title =  Duration of eruption (m) , values = c(2, 3, 4, 5), subdivide = 9) % % \n  add_axis( x , title =  Time since previous eruption (m) , values = c(50, 60, 70, 80, 90), subdivide = 9)   # Add to the code below to change the location of the y axis\nfaithful % % \n  ggvis(~waiting, ~eruptions) % % \n  layer_points() % %\n  add_axis( x , orient =  top ) % % add_axis( y , orient =  right )   Legends  # Add a legend to the plot below: use the correct title and orientation\nfaithful % % \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions))) % % \n  layer_points() % %\n  add_legend( fill , title =  ~ duration (m) , orient =  left )   #add_legend(vis, scales = NULL, orient =  right , title = NULL, format = NULL, values = NULL, properties = NULL)\n\n# Use add_legend() to combine the legends in the plot below. Adjust its properties as instructed.\nfaithful % % \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), \n        size = ~round(eruptions))  % %\n  layer_points() % %\n  add_legend(c( fill ,  shape ,  size ), title =  ~ duration (m) , values = c(2,3,4,5))   Legends (2)  # Fix the legend\nfaithful % % \n  ggvis(~waiting, ~eruptions, opacity := 0.6, fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), \n        size = ~round(eruptions)) % % \n    layer_points() % % \n    add_legend(c( fill ,  shape ,  size ), title =  ~ duration (m) )   Scale types  # Add to the code below to make the stroke color range from  darkred  to  orange .\nmtcars % % \n  ggvis(~wt, ~mpg, fill = ~disp, stroke = ~disp, strokeWidth := 2) % %\n  layer_points() % %\n  scale_numeric( fill , range = c( red ,  yellow )) % % scale_numeric( stroke , range = c( darkred ,  orange ))   # Change the graph below to make the fill colors range from green to beige.\nmtcars % % ggvis(~wt, ~mpg, fill = ~hp) % %\n  layer_points() % % scale_numeric( fill , range = c( green ,  beige ))   # Create a scale that will map `factor(cyl)` to a new range of colors: purple, blue, and green. \nmtcars % % ggvis(~wt, ~mpg, fill = ~factor(cyl)) % %\n  layer_points() % % scale_nominal( fill , range = c( purple ,  blue ,  green ))   Adjust any visual property  # Add a scale that limits the range of opacity from 0.2 to 1. \nmtcars % %\n  ggvis(x = ~wt, y = ~mpg, fill = ~factor(cyl), opacity = ~hp) % %\n  layer_points() % %\n  scale_numeric( opacity , range = c(0.2,1))   # Add a second scale that will expand the x axis to cover data values from 0 to 6.\nmtcars % %\n  ggvis(~wt, ~mpg, fill = ~disp) % %\n  layer_points() % %\n  scale_numeric( y , domain = c(0, NA)) % %\n  scale_numeric( x , domain = c(0, 6))   Adjust any visual property (2)  # Add a second scale to set domain for x\nmtcars % %\n  ggvis(~wt, ~mpg, fill = ~disp) % %\n  layer_points() % %\n  scale_numeric( y , domain = c(0, NA)) % %\n  scale_numeric( x , domain = c(0, 6))   =  versus  :=  # Set the fill value to the color variable instead of mapping it, and see what happens\nmtcars$color  - c( red ,  teal ,  #cccccc ,  tan )\n\nmtcars % %\n  ggvis(x = ~wt, y = ~mpg, fill = ~color) % %\n  layer_points()   mtcars % %\n  ggvis(x = ~wt, y = ~mpg, fill := ~color) % %\n  layer_points()", 
            "title": "5, Customizing Axes, Legends, and Scales"
        }, 
        {
            "location": "/Plot_snippets_-_Colours/", 
            "text": "Default colours\n\n\nBasic colours\n\n\nRColorBrewer examples\n\n\nBuilding a palette\n\n\nGrabing colours\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDefault colours\n\n\n# Set\npalette('default')\npalette()\n\n\n\n\n## [1] \"black\"   \"red\"     \"green3\"  \"blue\"    \"cyan\"    \"magenta\" \"yellow\" \n## [8] \"gray\"\n\n\n\n# Show\npar(mfrow = c(1, 2))\n\nn \n- 8\npie(rep(1, n), col = FALSE, main = 'colourless')\nn \n- 8\npie(rep(1, n), col = palette(), main = 'default')\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nBasic colours\n\n\n# 8 types times 9 tones\nn \n- 9\nrainbow(n, s = 1, v = 1, start = 0, end = max(1, n - 1)/n, alpha = 1)\n\n\n\n\n## [1] \"#FF0000FF\" \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\"\n## [7] \"#0000FFFF\" \"#AA00FFFF\" \"#FF00AAFF\"\n\n\n\nm \n- (1:n)/n\ngray(m)\n\n\n\n\n## [1] \"#1C1C1C\" \"#393939\" \"#555555\" \"#717171\" \"#8E8E8E\" \"#AAAAAA\" \"#C6C6C6\"\n## [8] \"#E3E3E3\" \"#FFFFFF\"\n\n\n\nhsv(m, s = 1, v = 1, alpha = 1)\n\n\n\n\n## [1] \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" \"#0000FFFF\"\n## [7] \"#AA00FFFF\" \"#FF00AAFF\" \"#FF0000FF\"\n\n\n\nblues9\n\n\n\n\n## [1] \"#F7FBFF\" \"#DEEBF7\" \"#C6DBEF\" \"#9ECAE1\" \"#6BAED6\" \"#4292C6\" \"#2171B5\"\n## [8] \"#08519C\" \"#08306B\"\n\n\n\nheat.colors(n, alpha = 1)\n\n\n\n\n## [1] \"#FF0000FF\" \"#FF2A00FF\" \"#FF5500FF\" \"#FF8000FF\" \"#FFAA00FF\" \"#FFD500FF\"\n## [7] \"#FFFF00FF\" \"#FFFF40FF\" \"#FFFFBFFF\"\n\n\n\nterrain.colors(n, alpha = 1)\n\n\n\n\n## [1] \"#00A600FF\" \"#3EBB00FF\" \"#8BD000FF\" \"#E6E600FF\" \"#E8C32EFF\" \"#EBB25EFF\"\n## [7] \"#EDB48EFF\" \"#F0C9C0FF\" \"#F2F2F2FF\"\n\n\n\ncm.colors(n, alpha = 1)\n\n\n\n\n## [1] \"#80FFFFFF\" \"#9FFFFFFF\" \"#BFFFFFFF\" \"#DFFFFFFF\" \"#FFFFFFFF\" \"#FFDFFFFF\"\n## [7] \"#FFBFFFFF\" \"#FF9FFFFF\" \"#FF80FFFF\"\n\n\n\ntopo.colors(n, alpha = 1)\n\n\n\n\n## [1] \"#4C00FFFF\" \"#004CFFFF\" \"#00E5FFFF\" \"#00FF4DFF\" \"#4DFF00FF\" \"#E6FF00FF\"\n## [7] \"#FFFF00FF\" \"#FFDE59FF\" \"#FFE0B3FF\"\n\n\n\n# Show\npar(mfrow = c(2, 2))\n\npie(rep(1, n), col = rainbow(n, alpha = 1), main = 'rainbow')\npie(rep(1, n), col = gray(m), main = 'gray')\npie(rep(1, n), col = hsv(m, alpha = 1), main = 'hsv')\npie(rep(1, n), col = blues9, main = 'blues9')\n\n\n\n\n\n\npie(rep(1, n), col = heat.colors(n, alpha = 1), main = 'heat')\npie(rep(1, n), col = terrain.colors(n, alpha = 1), main = 'terrain')\npie(rep(1, n), col = cm.colors(n), main = 'cm.colors')\npie(rep(1, n), col = topo.colors(n, alpha = 1), main = 'topo')\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\nRColorBrewer examples\n\n\nlibrary(RColorBrewer)\n\n# Show all\ndisplay.brewer.all()\n\n\n\n\n\n\n# Pick a palette\nn \n- 8\ncolors \n- brewer.pal(n, \nBuPu\n)\ncolors\n\n\n\n\n## [1] \"#F7FCFD\" \"#E0ECF4\" \"#BFD3E6\" \"#9EBCDA\" \"#8C96C6\" \"#8C6BB1\" \"#88419D\"\n## [8] \"#6E016B\"\n\n\n\npar(mfrow = c(1, 2))\npie(rep(1, n), col = colors, main = 'Sequential RdPu')\n\n# Interpolate these colors\npal \n- colorRampPalette(brewer.pal(n, 'RdPu'))\npal(8)\n\n\n\n\n## [1] \"#FFF7F3\" \"#FDE0DD\" \"#FCC5C0\" \"#FA9FB5\" \"#F768A1\" \"#DD3497\" \"#AE017E\"\n## [8] \"#7A0177\"\n\n\n\npie(rep(1, n), col = pal(8), main = 'Interpolated RdPu')\n\n\n\n\n\n\n# Apply\ndata(volcano)\npar(mfrow = c(2, 1))\nimage(volcano, col = pal(8))\nimage(volcano, col = pal(30))\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n# Show samples\npar(mfrow = c(2, 2))\n\nn = 9\npie(rep(1, n), col = brewer.pal(n, 'RdPu'), main = 'Sequential RdPu')\nn = 9\npie(rep(1, n), col = brewer.pal(n, 'Set1'), main = 'Qualitative Set1')\nn = 12\npie(rep(1, n), col = brewer.pal(n, 'Paired'), main = 'Qualitative Paired')\nn = 11\npie(rep(1, n), col = brewer.pal(n, 'RdBu'), main = 'Divergent RdBu')\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n# Show\nn = 8\ndarkcols \n- brewer.pal(n, 'Dark2')\npie(rep(1, n), col = darkcols, main = 'Dark2')\n\n\n\n\n\n\nBuilding a palette\n\n\n# All\nhead(colors())\n\n\n\n\n## [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n## [5] \"antiquewhite2\" \"antiquewhite3\"\n\n\n\nlength(colors()) # 657\n\n\n\n\n## [1] 657\n\n\n\n# Create\nmycols \n- colors()[c(8, 5, 30, 53, 118, 72)] #\n# or\n# mycols \n- c('aquamarine', 'antiquewhite2', 'blue4', 'chocolate1', 'deeppink2', 'cyan4')\n\n# Show\nn = 6\npie(rep(1, n), col = mycols, main = 'mycols')\n\n\n\n\n\n\n# Generate randomly\ncl \n- colors(distinct = TRUE)\nset.seed(15887) # to set random generator seed\nmycols2 \n- sample(cl, 7)\n\n# Show\nn = 7\npie(rep(1, n), col = mycols2, main = 'mycols2 (random)')\n\n\n\n\n\n\nGrabing colours\n\n\n\n\nGrab Website Colors\n\n\nRGB Color Codes\n\n    Chart", 
            "title": "Plot Snippets - Colours"
        }, 
        {
            "location": "/Plot_snippets_-_Colours/#basic-colours", 
            "text": "# 8 types times 9 tones\nn  - 9\nrainbow(n, s = 1, v = 1, start = 0, end = max(1, n - 1)/n, alpha = 1)  ## [1] \"#FF0000FF\" \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\"\n## [7] \"#0000FFFF\" \"#AA00FFFF\" \"#FF00AAFF\"  m  - (1:n)/n\ngray(m)  ## [1] \"#1C1C1C\" \"#393939\" \"#555555\" \"#717171\" \"#8E8E8E\" \"#AAAAAA\" \"#C6C6C6\"\n## [8] \"#E3E3E3\" \"#FFFFFF\"  hsv(m, s = 1, v = 1, alpha = 1)  ## [1] \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" \"#0000FFFF\"\n## [7] \"#AA00FFFF\" \"#FF00AAFF\" \"#FF0000FF\"  blues9  ## [1] \"#F7FBFF\" \"#DEEBF7\" \"#C6DBEF\" \"#9ECAE1\" \"#6BAED6\" \"#4292C6\" \"#2171B5\"\n## [8] \"#08519C\" \"#08306B\"  heat.colors(n, alpha = 1)  ## [1] \"#FF0000FF\" \"#FF2A00FF\" \"#FF5500FF\" \"#FF8000FF\" \"#FFAA00FF\" \"#FFD500FF\"\n## [7] \"#FFFF00FF\" \"#FFFF40FF\" \"#FFFFBFFF\"  terrain.colors(n, alpha = 1)  ## [1] \"#00A600FF\" \"#3EBB00FF\" \"#8BD000FF\" \"#E6E600FF\" \"#E8C32EFF\" \"#EBB25EFF\"\n## [7] \"#EDB48EFF\" \"#F0C9C0FF\" \"#F2F2F2FF\"  cm.colors(n, alpha = 1)  ## [1] \"#80FFFFFF\" \"#9FFFFFFF\" \"#BFFFFFFF\" \"#DFFFFFFF\" \"#FFFFFFFF\" \"#FFDFFFFF\"\n## [7] \"#FFBFFFFF\" \"#FF9FFFFF\" \"#FF80FFFF\"  topo.colors(n, alpha = 1)  ## [1] \"#4C00FFFF\" \"#004CFFFF\" \"#00E5FFFF\" \"#00FF4DFF\" \"#4DFF00FF\" \"#E6FF00FF\"\n## [7] \"#FFFF00FF\" \"#FFDE59FF\" \"#FFE0B3FF\"  # Show\npar(mfrow = c(2, 2))\n\npie(rep(1, n), col = rainbow(n, alpha = 1), main = 'rainbow')\npie(rep(1, n), col = gray(m), main = 'gray')\npie(rep(1, n), col = hsv(m, alpha = 1), main = 'hsv')\npie(rep(1, n), col = blues9, main = 'blues9')   pie(rep(1, n), col = heat.colors(n, alpha = 1), main = 'heat')\npie(rep(1, n), col = terrain.colors(n, alpha = 1), main = 'terrain')\npie(rep(1, n), col = cm.colors(n), main = 'cm.colors')\npie(rep(1, n), col = topo.colors(n, alpha = 1), main = 'topo')   par(mfrow = c(1, 1))", 
            "title": "Basic colours"
        }, 
        {
            "location": "/Plot_snippets_-_Colours/#rcolorbrewer-examples", 
            "text": "library(RColorBrewer)\n\n# Show all\ndisplay.brewer.all()   # Pick a palette\nn  - 8\ncolors  - brewer.pal(n,  BuPu )\ncolors  ## [1] \"#F7FCFD\" \"#E0ECF4\" \"#BFD3E6\" \"#9EBCDA\" \"#8C96C6\" \"#8C6BB1\" \"#88419D\"\n## [8] \"#6E016B\"  par(mfrow = c(1, 2))\npie(rep(1, n), col = colors, main = 'Sequential RdPu')\n\n# Interpolate these colors\npal  - colorRampPalette(brewer.pal(n, 'RdPu'))\npal(8)  ## [1] \"#FFF7F3\" \"#FDE0DD\" \"#FCC5C0\" \"#FA9FB5\" \"#F768A1\" \"#DD3497\" \"#AE017E\"\n## [8] \"#7A0177\"  pie(rep(1, n), col = pal(8), main = 'Interpolated RdPu')   # Apply\ndata(volcano)\npar(mfrow = c(2, 1))\nimage(volcano, col = pal(8))\nimage(volcano, col = pal(30))   par(mfrow = c(1, 1))  # Show samples\npar(mfrow = c(2, 2))\n\nn = 9\npie(rep(1, n), col = brewer.pal(n, 'RdPu'), main = 'Sequential RdPu')\nn = 9\npie(rep(1, n), col = brewer.pal(n, 'Set1'), main = 'Qualitative Set1')\nn = 12\npie(rep(1, n), col = brewer.pal(n, 'Paired'), main = 'Qualitative Paired')\nn = 11\npie(rep(1, n), col = brewer.pal(n, 'RdBu'), main = 'Divergent RdBu')   par(mfrow = c(1, 1))  # Show\nn = 8\ndarkcols  - brewer.pal(n, 'Dark2')\npie(rep(1, n), col = darkcols, main = 'Dark2')", 
            "title": "RColorBrewer examples"
        }, 
        {
            "location": "/Plot_snippets_-_Colours/#building-a-palette", 
            "text": "# All\nhead(colors())  ## [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n## [5] \"antiquewhite2\" \"antiquewhite3\"  length(colors()) # 657  ## [1] 657  # Create\nmycols  - colors()[c(8, 5, 30, 53, 118, 72)] #\n# or\n# mycols  - c('aquamarine', 'antiquewhite2', 'blue4', 'chocolate1', 'deeppink2', 'cyan4')\n\n# Show\nn = 6\npie(rep(1, n), col = mycols, main = 'mycols')   # Generate randomly\ncl  - colors(distinct = TRUE)\nset.seed(15887) # to set random generator seed\nmycols2  - sample(cl, 7)\n\n# Show\nn = 7\npie(rep(1, n), col = mycols2, main = 'mycols2 (random)')", 
            "title": "Building a palette"
        }, 
        {
            "location": "/Plot_snippets_-_Colours/#grabing-colours", 
            "text": "Grab Website Colors  RGB Color Codes \n    Chart", 
            "title": "Grabing colours"
        }, 
        {
            "location": "/Tables/", 
            "text": "Markdown tables\n\n\nExample 1\n\n\nExample 2\n\n\n\n\n\n\nThe \nxtable\n package\n\n\nExample 1\n\n\n\n\n\n\nThe \nknitr::kable\n function\n\n\nExample 1\n\n\n\n\n\n\nThe \npander::pandoc.table\n\n    function\n\n\nExample 1\n\n\n\n\n\n\nThe \nhtmlTable\n package\n\n\nExample 1\n\n\nExample 2\n\n\nExample 3\n\n\nExample 4\n\n\nExample 5\n\n\nExample 6\n\n\nExample 7\n\n\n\n\n\n\nThe \nztable\n package\n\n\nExample 1\n\n\n\n\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: \npygments\n syntax, the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nMarkdown tables\n\n\nExample 1\n\n\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\n: Demonstration of pipe table syntax.\n\n\n\n\n\nDemonstration of pipe table syntax.\n\n\n\n\n\n\nRight\n\n\nLeft\n\n\nDefault\n\n\nCenter\n\n\n\n\n\n\n\n\n\n\n12\n\n\n12\n\n\n12\n\n\n12\n\n\n\n\n\n\n123\n\n\n123\n\n\n123\n\n\n123\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\nExample 2\n\n\n: Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+\n\n\n\n\n\nSample grid table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFruit\n\n\nPrice\n\n\nAdvantages\n\n\n\n\n\n\n\n\n\n\nBananas\n\n\n$1.34\n\n\n\n\nbuilt-in wrapper\n\n\nbright color\n\n\n\n\n\n\n\n\nOranges\n\n\n$2.10\n\n\n\n\ncures scurvy\n\n\ntasty\n\n\n\n\n\n\n\n\n\n\n\nThe \nxtable\n package\n\n\nExample 1\n\n\nlibrary(xtable)\n\n# given the data in the first row\nprint(xtable(output,\n             caption = 'A test table', \n             align = c('l', 'c', 'r')),\n      type = 'html')\n\n\n\n\nThe \nknitr::kable\n function\n\n\nExample 1\n\n\nlibrary(knitr)\n\n# given the data in the first row\nkable(output, \n      caption = 'A test table', \n      align = c('c', 'r'))\n\n\n\n\n\n\nA test table\n\n\n\n\n\n\n\n\n1st header\n\n\n2nd header\n\n\n\n\n\n\n\n\n\n\n1st row\n\n\nContent A\n\n\nContent B\n\n\n\n\n\n\n2nd row\n\n\nContent C\n\n\nContent D\n\n\n\n\n\n\n\n\n\nThe \npander::pandoc.table\n function\n\n\nExample 1\n\n\nlibrary(pander)\n\n# given the data in the first row\npandoc.table(output, \n             emphasize.rows = 1, \n             emphasize.strong.cols = 2)\n\n\n\n\nThe \nhtmlTable\n package\n\n\nhtmlTable\n on GitHub.\n\n\nExample 1\n\n\noutput \n- \n  matrix(paste('Content', LETTERS[1:16]), \n         ncol = 4, byrow = TRUE)\n\nlibrary(htmlTable)\n\nhtmlTable(output,\n          header = paste(c('1st', '2nd', '3rd', '4th'), 'header'),\n          rnames = paste(c('1st', '2nd', '3rd', '4th'), 'row'),\n          rgroup = c('Group A', 'Group B'),\n          n.rgroup = c(2,2),\n          cgroup = c('Cgroup 1', 'Cgroup 2\ndagger;'),\n          n.cgroup = c(2,2), \n          caption = 'Basic table with both column spanners (groups) and row groups',\n          tfoot = '\ndagger; A table footer commment')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic table with both column spanners (groups) and row groups\n\n\n\n\n\n\n\n\n\n\n\n\nCgroup 1\n\n\n\n\n\u00a0\n\n\n\n\nCgroup 2\u2020\n\n\n\n\n\n\n\n\n\n\n\n\n1st header\n\n\n\n\n2nd header\n\n\n\n\n\u00a0\n\n\n\n\n3rd header\n\n\n\n\n4th header\n\n\n\n\n\n\n\n\n\n\n\n\nGroup A\n\n\n\n\n\n\n\n\n\u00a0\u00a01st row\n\n\n\n\nContent A\n\n\n\n\nContent B\n\n\n\n\n\u00a0\n\n\n\n\nContent C\n\n\n\n\nContent D\n\n\n\n\n\n\n\n\n\u00a0\u00a02nd row\n\n\n\n\nContent E\n\n\n\n\nContent F\n\n\n\n\n\u00a0\n\n\n\n\nContent G\n\n\n\n\nContent H\n\n\n\n\n\n\n\n\nGroup B\n\n\n\n\n\n\n\n\n\u00a0\u00a03rd row\n\n\n\n\nContent I\n\n\n\n\nContent J\n\n\n\n\n\u00a0\n\n\n\n\nContent K\n\n\n\n\nContent L\n\n\n\n\n\n\n\n\n\u00a0\u00a04th row\n\n\n\n\nContent M\n\n\n\n\nContent N\n\n\n\n\n\u00a0\n\n\n\n\nContent O\n\n\n\n\nContent P\n\n\n\n\n\n\n\n\n\n\n\n\n\u2020 A table footer commment\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('\nDelta;\nsub\nint\n/sub\n correspnds to the change since start',\n                                '\nDelta;\nsub\nstd\n/sub\n corresponds to the change compared to national average'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst period\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\nSecond period\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\nThird period\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('\nDelta;\nsub\nint\n/sub\n correspnds to the change since start',\n                                '\nDelta;\nsub\nstd\n/sub\n corresponds to the change compared to national average'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst period\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\nSecond period\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\nThird period\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 4\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          col.columns = c(rep('#E6E6F0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n                    tfoot = txtMergeLines('\nDelta;\nsub\nint\n/sub\n correspnds to the change since start',\n                                          '\nDelta;\nsub\nstd\n/sub\n corresponds to the change compared to national average'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst period\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\nSecond period\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\nThird period\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1),\n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          # I use the \nnbsp; - the no breaking space as I don't want to have a\n          # row break in the row group. This adds a little space in the table\n          # when used together with the cspan.rgroup=1.\n          rgroup = c('1st\nnbsp;period', '2nd\nnbsp;period', '3rd\nnbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('\nDelta;\nsub\nint\n/sub\n correspnds to the change since start',\n                                '\nDelta;\nsub\nstd\n/sub\n corresponds to the change compared to national average'),\n          cspan.rgroup = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\n1st\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n2nd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n3rd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(out_mx,\n          caption = 'Average age in Sweden counties over a period of\n                     15 years. The Norbotten county is typically known\n                     for having a negative migration pattern compared to\n                     Stockholm, while Uppsala has a proportionally large \n                     population of students.',\n          pos.rowlabel = 'bottom',\n          rowlabel='Year', \n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('1st\nnbsp;period', '2nd\nnbsp;period', '3rd\nnbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('\nDelta;\nsub\nint\n/sub\n correspnds to the change since start',\n                                '\nDelta;\nsub\nstd\n/sub\n corresponds to the change compared to national average'),\n          cspan.rgroup = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\nYear\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\n1st\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n\n0.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n\n0.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n\n1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n2nd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n\n1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n\n-1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n\n1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n\n1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n3rd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n\n2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n\n2.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n\n2.5\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n\n-1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7\n\n\nlibrary(htmlTable)\n\n# given the data in the first row\nhtmlTable(out_mx,\n          caption = 'Average age in Sweden counties over a period of\n                     15 years. The Norbotten county is typically known\n                     for having a negative migration pattern compared to\n                     Stockholm, while Uppsala has a proportionally large \n                     population of students.',\n          pos.rowlabel = 'bottom',\n          rowlabel = 'Year', \n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4), rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('1st\nnbsp;period', '2nd\nnbsp;period', '3rd\nnbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines('\nDelta;\nsub\nint\n/sub\n correspnds to the change since start',\n                                '\nDelta;\nsub\nstd\n/sub\n corresponds to the change compared to national average'),\n          cspan.rgroup = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\nYear\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\n1st\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n\n0.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n\n0.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n\n1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n2nd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n\n1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n\n-1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n\n1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n\n1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n3rd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n\n2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n\n2.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n\n2.5\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n\n-1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \nztable\n package\n\n\nThe package can also export to \nL\na\nT\ne\nX\n.\n\n\nExample 1\n\n\nlibrary(ztable)\n\noptions(ztable.type='html')\n\n# given the data in the first row\nzt \n- ztable(out_mx, \n             caption = 'Average age in Sweden counties over a period of\n             15 years. The Norbotten county is typically known\n             for having a negative migration pattern compared to\n             Stockholm, while Uppsala has a proportionally large \n             population of students.',\n             zebra.type = 1,\n             zebra = 'peach',\n             align=paste(rep('r', ncol(out_mx) + 1), collapse = ''))\n# zt \n- addcgroup(zt,\n#                 cgroup = cgroup,\n#                 n.cgroup = n.cgroup)\n# Causes an error:\n# Error in if (result \n= length(vlines)) { : \nzt \n- addrgroup(zt, \n                rgroup = c('1st\nnbsp;period', '2nd\nnbsp;period', '3rd\nnbsp;period'),\n                n.rgroup = rep(5, 3))\n\nprint(zt)", 
            "title": "Tables"
        }, 
        {
            "location": "/Tables/#example-1", 
            "text": "| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\n: Demonstration of pipe table syntax.   Demonstration of pipe table syntax.    Right  Left  Default  Center      12  12  12  12    123  123  123  123    1  1  1  1", 
            "title": "Example 1"
        }, 
        {
            "location": "/Tables/#example-2", 
            "text": ": Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+   Sample grid table.         Fruit  Price  Advantages      Bananas  $1.34   built-in wrapper  bright color     Oranges  $2.10   cures scurvy  tasty", 
            "title": "Example 2"
        }, 
        {
            "location": "/Tables/#the-xtable-package", 
            "text": "", 
            "title": "The xtable package"
        }, 
        {
            "location": "/Tables/#example-1_1", 
            "text": "library(xtable)\n\n# given the data in the first row\nprint(xtable(output,\n             caption = 'A test table', \n             align = c('l', 'c', 'r')),\n      type = 'html')", 
            "title": "Example 1"
        }, 
        {
            "location": "/Tables/#the-knitrkable-function", 
            "text": "", 
            "title": "The knitr::kable function"
        }, 
        {
            "location": "/Tables/#example-1_2", 
            "text": "library(knitr)\n\n# given the data in the first row\nkable(output, \n      caption = 'A test table', \n      align = c('c', 'r'))   A test table     1st header  2nd header      1st row  Content A  Content B    2nd row  Content C  Content D", 
            "title": "Example 1"
        }, 
        {
            "location": "/Tables/#the-panderpandoctable-function", 
            "text": "", 
            "title": "The pander::pandoc.table function"
        }, 
        {
            "location": "/Tables/#example-1_3", 
            "text": "library(pander)\n\n# given the data in the first row\npandoc.table(output, \n             emphasize.rows = 1, \n             emphasize.strong.cols = 2)", 
            "title": "Example 1"
        }, 
        {
            "location": "/Tables/#the-htmltable-package", 
            "text": "htmlTable  on GitHub.", 
            "title": "The htmlTable package"
        }, 
        {
            "location": "/Tables/#example-1_4", 
            "text": "output  - \n  matrix(paste('Content', LETTERS[1:16]), \n         ncol = 4, byrow = TRUE)\n\nlibrary(htmlTable)\n\nhtmlTable(output,\n          header = paste(c('1st', '2nd', '3rd', '4th'), 'header'),\n          rnames = paste(c('1st', '2nd', '3rd', '4th'), 'row'),\n          rgroup = c('Group A', 'Group B'),\n          n.rgroup = c(2,2),\n          cgroup = c('Cgroup 1', 'Cgroup 2 dagger;'),\n          n.cgroup = c(2,2), \n          caption = 'Basic table with both column spanners (groups) and row groups',\n          tfoot = ' dagger; A table footer commment')      \nBasic table with both column spanners (groups) and row groups      \nCgroup 1  \n\u00a0  \nCgroup 2\u2020      \n1st header  \n2nd header  \n\u00a0  \n3rd header  \n4th header      \nGroup A    \n\u00a0\u00a01st row  \nContent A  \nContent B  \n\u00a0  \nContent C  \nContent D    \n\u00a0\u00a02nd row  \nContent E  \nContent F  \n\u00a0  \nContent G  \nContent H    \nGroup B    \n\u00a0\u00a03rd row  \nContent I  \nContent J  \n\u00a0  \nContent K  \nContent L    \n\u00a0\u00a04th row  \nContent M  \nContent N  \n\u00a0  \nContent O  \nContent P      \n\u2020 A table footer commment", 
            "title": "Example 1"
        }, 
        {
            "location": "/Tables/#example-2_1", 
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines(' Delta; sub int /sub  correspnds to the change since start',\n                                ' Delta; sub std /sub  corresponds to the change compared to national average'))        \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \nFirst period    \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \nSecond period    \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \nThird period    \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average", 
            "title": "Example 2"
        }, 
        {
            "location": "/Tables/#example-3", 
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines(' Delta; sub int /sub  correspnds to the change since start',\n                                ' Delta; sub std /sub  corresponds to the change compared to national average'))        \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \nFirst period    \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \nSecond period    \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \nThird period    \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average", 
            "title": "Example 3"
        }, 
        {
            "location": "/Tables/#example-4", 
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1), \n          col.columns = c(rep('#E6E6F0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('First period', 'Second period', 'Third period'),\n          n.rgroup = rep(5, 3),\n                    tfoot = txtMergeLines(' Delta; sub int /sub  correspnds to the change since start',\n                                          ' Delta; sub std /sub  corresponds to the change compared to national average'))        \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \nFirst period    \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \nSecond period    \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \nThird period    \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average", 
            "title": "Example 4"
        }, 
        {
            "location": "/Tables/#example-5", 
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(txtRound(mx, 1),\n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          # I use the  nbsp; - the no breaking space as I don't want to have a\n          # row break in the row group. This adds a little space in the table\n          # when used together with the cspan.rgroup=1.\n          rgroup = c('1st nbsp;period', '2nd nbsp;period', '3rd nbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines(' Delta; sub int /sub  correspnds to the change since start',\n                                ' Delta; sub std /sub  corresponds to the change compared to national average'),\n          cspan.rgroup = 1)        \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \n1st\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \n2nd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n3rd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average", 
            "title": "Example 5"
        }, 
        {
            "location": "/Tables/#example-6", 
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(out_mx,\n          caption = 'Average age in Sweden counties over a period of\n                     15 years. The Norbotten county is typically known\n                     for having a negative migration pattern compared to\n                     Stockholm, while Uppsala has a proportionally large \n                     population of students.',\n          pos.rowlabel = 'bottom',\n          rowlabel='Year', \n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4),\n                          rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('1st nbsp;period', '2nd nbsp;period', '3rd nbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines(' Delta; sub int /sub  correspnds to the change since start',\n                                ' Delta; sub std /sub  corresponds to the change compared to national average'),\n          cspan.rgroup = 1)      \nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.      \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen    \nYear  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \n1st\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0   0.8   \n\u00a0  \n41.9  \n0.0   0.4   \n\u00a0  \n37.3  \n0.0   -1.6   \n\u00a0  \n40.1  \n0.0   -1.4   \n\u00a0  \n37.2  \n0.0   -1.7   \n\u00a0  \n39.3  \n0.0   -2.2     \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3   1.0   \n\u00a0  \n42.2  \n0.3   0.6   \n\u00a0  \n37.4  \n0.1   -1.6   \n\u00a0  \n40.1  \n0.0   -1.5   \n\u00a0  \n37.5  \n0.3   -1.5   \n\u00a0  \n39.4  \n0.1   -2.2     \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5   1.0   \n\u00a0  \n42.5  \n0.6   0.8   \n\u00a0  \n37.5  \n0.2   -1.7   \n\u00a0  \n40.1  \n0.0   -1.6   \n\u00a0  \n37.6  \n0.4   -1.6   \n\u00a0  \n39.6  \n0.3   -2.1     \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8   1.2   \n\u00a0  \n42.8  \n0.9   1.0   \n\u00a0  \n37.6  \n0.3   -1.7   \n\u00a0  \n40.2  \n0.1   -1.6   \n\u00a0  \n37.8  \n0.6   -1.5   \n\u00a0  \n39.7  \n0.4   -2.1     \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0   1.3   \n\u00a0  \n43.0  \n1.1   1.1   \n\u00a0  \n37.7  \n0.4   -1.7   \n\u00a0  \n40.2  \n0.1   -1.7   \n\u00a0  \n38.0  \n0.8   -1.4   \n\u00a0  \n39.8  \n0.5   -2.1     \n2nd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2   1.3   \n\u00a0  \n43.1  \n1.2   1.1   \n\u00a0  \n37.8  \n0.5   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.1  \n0.9   -1.5   \n\u00a0  \n40.0  \n0.7   -2.0     \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4   1.4   \n\u00a0  \n43.4  \n1.5   1.4   \n\u00a0  \n37.9  \n0.6   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.3  \n1.1   -1.4   \n\u00a0  \n40.1  \n0.8   -1.9     \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6   1.5   \n\u00a0  \n43.5  \n1.6   1.4   \n\u00a0  \n37.9  \n0.6   -1.9   \n\u00a0  \n40.2  \n0.1   -1.9   \n\u00a0  \n38.5  \n1.3   -1.3   \n\u00a0  \n40.4  \n1.1   -1.7     \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8   1.7   \n\u00a0  \n43.8  \n1.9   1.7   \n\u00a0  \n37.8  \n0.5   -2.0   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.6  \n1.4   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0   1.8   \n\u00a0  \n44.0  \n2.1   1.9   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.7  \n1.5   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n3rd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2   2.0   \n\u00a0  \n44.2  \n2.3   2.1   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.8  \n1.6   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4   2.1   \n\u00a0  \n44.4  \n2.5   2.3   \n\u00a0  \n37.8  \n0.5   -2.2   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.9  \n1.7   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6   2.2   \n\u00a0  \n44.5  \n2.6   2.3   \n\u00a0  \n37.9  \n0.6   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.0  \n1.8   -1.1   \n\u00a0  \n40.7  \n1.4   -1.5     \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.6  \n2.7   2.4   \n\u00a0  \n37.9  \n0.6   -2.3   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.1  \n1.9   -1.1   \n\u00a0  \n40.8  \n1.5   -1.4     \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.7  \n2.8   2.5   \n\u00a0  \n38.0  \n0.7   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.2  \n2.0   -1.0   \n\u00a0  \n40.9  \n1.6   -1.3       \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average", 
            "title": "Example 6"
        }, 
        {
            "location": "/Tables/#example-7", 
            "text": "library(htmlTable)\n\n# given the data in the first row\nhtmlTable(out_mx,\n          caption = 'Average age in Sweden counties over a period of\n                     15 years. The Norbotten county is typically known\n                     for having a negative migration pattern compared to\n                     Stockholm, while Uppsala has a proportionally large \n                     population of students.',\n          pos.rowlabel = 'bottom',\n          rowlabel = 'Year', \n          col.rgroup = c('none', '#FFFFCC'),\n          col.columns = c(rep('#EFEFF0', 4), rep('none', ncol(mx) - 4)),\n          align = 'rrrr|r',\n          cgroup = cgroup,\n          n.cgroup = n.cgroup,\n          rgroup = c('1st nbsp;period', '2nd nbsp;period', '3rd nbsp;period'),\n          n.rgroup = rep(5, 3),\n          tfoot = txtMergeLines(' Delta; sub int /sub  correspnds to the change since start',\n                                ' Delta; sub std /sub  corresponds to the change compared to national average'),\n          cspan.rgroup = 1)      \nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.      \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen    \nYear  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \n1st\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0   0.8   \n\u00a0  \n41.9  \n0.0   0.4   \n\u00a0  \n37.3  \n0.0   -1.6   \n\u00a0  \n40.1  \n0.0   -1.4   \n\u00a0  \n37.2  \n0.0   -1.7   \n\u00a0  \n39.3  \n0.0   -2.2     \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3   1.0   \n\u00a0  \n42.2  \n0.3   0.6   \n\u00a0  \n37.4  \n0.1   -1.6   \n\u00a0  \n40.1  \n0.0   -1.5   \n\u00a0  \n37.5  \n0.3   -1.5   \n\u00a0  \n39.4  \n0.1   -2.2     \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5   1.0   \n\u00a0  \n42.5  \n0.6   0.8   \n\u00a0  \n37.5  \n0.2   -1.7   \n\u00a0  \n40.1  \n0.0   -1.6   \n\u00a0  \n37.6  \n0.4   -1.6   \n\u00a0  \n39.6  \n0.3   -2.1     \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8   1.2   \n\u00a0  \n42.8  \n0.9   1.0   \n\u00a0  \n37.6  \n0.3   -1.7   \n\u00a0  \n40.2  \n0.1   -1.6   \n\u00a0  \n37.8  \n0.6   -1.5   \n\u00a0  \n39.7  \n0.4   -2.1     \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0   1.3   \n\u00a0  \n43.0  \n1.1   1.1   \n\u00a0  \n37.7  \n0.4   -1.7   \n\u00a0  \n40.2  \n0.1   -1.7   \n\u00a0  \n38.0  \n0.8   -1.4   \n\u00a0  \n39.8  \n0.5   -2.1     \n2nd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2   1.3   \n\u00a0  \n43.1  \n1.2   1.1   \n\u00a0  \n37.8  \n0.5   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.1  \n0.9   -1.5   \n\u00a0  \n40.0  \n0.7   -2.0     \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4   1.4   \n\u00a0  \n43.4  \n1.5   1.4   \n\u00a0  \n37.9  \n0.6   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.3  \n1.1   -1.4   \n\u00a0  \n40.1  \n0.8   -1.9     \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6   1.5   \n\u00a0  \n43.5  \n1.6   1.4   \n\u00a0  \n37.9  \n0.6   -1.9   \n\u00a0  \n40.2  \n0.1   -1.9   \n\u00a0  \n38.5  \n1.3   -1.3   \n\u00a0  \n40.4  \n1.1   -1.7     \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8   1.7   \n\u00a0  \n43.8  \n1.9   1.7   \n\u00a0  \n37.8  \n0.5   -2.0   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.6  \n1.4   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0   1.8   \n\u00a0  \n44.0  \n2.1   1.9   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.7  \n1.5   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n3rd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2   2.0   \n\u00a0  \n44.2  \n2.3   2.1   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.8  \n1.6   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4   2.1   \n\u00a0  \n44.4  \n2.5   2.3   \n\u00a0  \n37.8  \n0.5   -2.2   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.9  \n1.7   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6   2.2   \n\u00a0  \n44.5  \n2.6   2.3   \n\u00a0  \n37.9  \n0.6   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.0  \n1.8   -1.1   \n\u00a0  \n40.7  \n1.4   -1.5     \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.6  \n2.7   2.4   \n\u00a0  \n37.9  \n0.6   -2.3   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.1  \n1.9   -1.1   \n\u00a0  \n40.8  \n1.5   -1.4     \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.7  \n2.8   2.5   \n\u00a0  \n38.0  \n0.7   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.2  \n2.0   -1.0   \n\u00a0  \n40.9  \n1.6   -1.3       \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average", 
            "title": "Example 7"
        }, 
        {
            "location": "/Tables/#the-ztable-package", 
            "text": "The package can also export to  L a T e X .", 
            "title": "The ztable package"
        }, 
        {
            "location": "/Tables/#example-1_5", 
            "text": "library(ztable)\n\noptions(ztable.type='html')\n\n# given the data in the first row\nzt  - ztable(out_mx, \n             caption = 'Average age in Sweden counties over a period of\n             15 years. The Norbotten county is typically known\n             for having a negative migration pattern compared to\n             Stockholm, while Uppsala has a proportionally large \n             population of students.',\n             zebra.type = 1,\n             zebra = 'peach',\n             align=paste(rep('r', ncol(out_mx) + 1), collapse = ''))\n# zt  - addcgroup(zt,\n#                 cgroup = cgroup,\n#                 n.cgroup = n.cgroup)\n# Causes an error:\n# Error in if (result  = length(vlines)) { : \nzt  - addrgroup(zt, \n                rgroup = c('1st nbsp;period', '2nd nbsp;period', '3rd nbsp;period'),\n                n.rgroup = rep(5, 3))\n\nprint(zt)", 
            "title": "Example 1"
        }, 
        {
            "location": "/Charts/", 
            "text": "Charts \n Colors\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nCharts \n Colors\n\n\nCharts\n\n\nColors\n\n\n\n\n\n\n\n\n\n\n\n\nCharts\n\n\n\n\nColors", 
            "title": "Charts & Colors"
        }, 
        {
            "location": "/Charts/#charts", 
            "text": "", 
            "title": "Charts"
        }, 
        {
            "location": "/Charts/#colors", 
            "text": "", 
            "title": "Colors"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/", 
            "text": "1, Variables\n\n\n2, Histograms and Distributions\n\n\n3, Scales of Measurement\n\n\n4, Measures of Central Tendency\n\n\n5, Measures of Variability\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, Variables\n\n\nNominal variables in R\n\n\n# Create a numeric vector with the identifiers of the participants of your survey\nparticipants_1 \n- c(2, 3, 5, 7, 11, 13, 17)\n\n# Check what type of values R thinks the vector consists of\nclass(participants_1)\n\n\n\n\n## [1] \"numeric\"\n\n\n\n# Transform the numeric vector to a factor vector\nparticipants_2 \n- factor(participants_1)\n\n# Check what type of values R thinks the vector consists of now\nclass(participants_2)\n\n\n\n\n## [1] \"factor\"\n\n\n\nOrdinal variables in R\n\n\n# Create a vector of temperature observations\ntemperature_vector \n- c('High', 'Low', 'High', 'Low', 'Medium')\ntemperature_vector\n\n\n\n\n## [1] \"High\"   \"Low\"    \"High\"   \"Low\"    \"Medium\"\n\n\n\n# Specify that they are ordinal variables with the given levels\nfactor_temperature_vector \n- factor(temperature_vector, order = TRUE, levels = c('Low', 'Medium', 'High'))\nfactor_temperature_vector\n\n\n\n\n## [1] High   Low    High   Low    Medium\n## Levels: Low \n Medium \n High\n\n\n\nInterval and Ratio variables in R\n\n\n# Assign to the variable 'longitudes' a vector with the longitudes\n# This is an interval variable.\nlongitudes \n- c(10, 20, 30, 40)\n\n# Assign the times it takes for an athlete to run 100 meters to the variable 'chronos'\n# This is a ratio variable.\nchronos \n- c(10.60, 10.12, 9.58, 11.1)\n\n\n\n\n2, Histograms and Distributions\n\n\nCreating histograms in R\n\n\nlibrary(XLConnectJars)\nlibrary(XLConnect)\n\n# Read in the data set and assign to the object\nimpact \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'impact', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(impact)\n\n\n\n\n##   subject condition verbal_memory_baseline visual_memory_baseline\n## 1       1   control                     95                     88\n## 2       2   control                     90                     82\n## 3       3   control                     87                     77\n## 4       4   control                     84                     72\n## 5       5   control                     92                     77\n## 6       6   control                     89                     79\n##   visual.motor_speed_baseline reaction_time_baseline\n## 1                       35.29                   0.42\n## 2                       31.47                   0.63\n## 3                       30.87                   0.56\n## 4                       41.87                   0.66\n## 5                       33.28                   0.56\n## 6                       40.73                   0.81\n##   impulse_control_baseline total_symptom_baseline verbal_memory_retest\n## 1                       11                      0                   97\n## 2                        7                      0                   86\n## 3                        8                      0                   90\n## 4                        7                      0                   85\n## 5                        7                      1                   87\n## 6                        6                      0                   91\n##   visual_memory_retest visual.motor_speed_retest reaction_time_retest\n## 1                   86                     35.61                 0.65\n## 2                   80                     37.01                 0.49\n## 3                   79                     20.15                 0.75\n## 4                   70                     33.26                 0.19\n## 5                   77                     28.34                 0.59\n## 6                   85                     33.47                 0.48\n##   impulse_control_retest total_symptom_retest\n## 1                     10                    0\n## 2                      7                    0\n## 3                      9                    0\n## 4                      8                    0\n## 5                      8                    1\n## 6                      5                    0\n\n\n\n# Use the describe() function to see some summary information per variable\n#describe(impact)\nsummary(impact)\n\n\n\n\n##     subject       condition         verbal_memory_baseline\n##  Min.   : 1.00   Length:40          Min.   :75.00         \n##  1st Qu.:10.75   Class :character   1st Qu.:85.00         \n##  Median :20.50   Mode  :character   Median :91.00         \n##  Mean   :20.50                      Mean   :89.75         \n##  3rd Qu.:30.25                      3rd Qu.:95.00         \n##  Max.   :40.00                      Max.   :98.00         \n##  visual_memory_baseline visual.motor_speed_baseline reaction_time_baseline\n##  Min.   :59.00          Min.   :26.29               Min.   :0.4200        \n##  1st Qu.:68.75          1st Qu.:31.59               1st Qu.:0.5675        \n##  Median :75.00          Median :33.50               Median :0.6500        \n##  Mean   :74.88          Mean   :34.03               Mean   :0.6670        \n##  3rd Qu.:81.25          3rd Qu.:36.44               3rd Qu.:0.7325        \n##  Max.   :91.00          Max.   :41.87               Max.   :1.2000        \n##  impulse_control_baseline total_symptom_baseline verbal_memory_retest\n##  Min.   : 2.000           Min.   :0.00           Min.   :59          \n##  1st Qu.: 7.000           1st Qu.:0.00           1st Qu.:74          \n##  Median : 8.500           Median :0.00           Median :85          \n##  Mean   : 8.275           Mean   :0.05           Mean   :82          \n##  3rd Qu.:10.000           3rd Qu.:0.00           3rd Qu.:91          \n##  Max.   :12.000           Max.   :1.00           Max.   :97          \n##  visual_memory_retest visual.motor_speed_retest reaction_time_retest\n##  Min.   :54.00        Min.   :20.15             Min.   :0.1900      \n##  1st Qu.:66.75        1st Qu.:30.33             1st Qu.:0.5575      \n##  Median :72.00        Median :35.15             Median :0.6500      \n##  Mean   :71.90        Mean   :35.83             Mean   :0.6730      \n##  3rd Qu.:79.00        3rd Qu.:39.41             3rd Qu.:0.7325      \n##  Max.   :86.00        Max.   :60.77             Max.   :1.3000      \n##  impulse_control_retest total_symptom_retest\n##  Min.   : 1.00          Min.   : 0.00       \n##  1st Qu.: 5.00          1st Qu.: 0.00       \n##  Median : 7.00          Median : 7.00       \n##  Mean   : 6.75          Mean   :13.88       \n##  3rd Qu.: 9.00          3rd Qu.:27.00       \n##  Max.   :12.00          Max.   :43.00\n\n\n\n# Select the variable 'verbal_memory_baseline' from the 'impact' data.frame and assign it to the variable 'verbal_baseline'\nverbal_baseline \n- impact$verbal_memory_baseline\nverbal_baseline\n\n\n\n\n##  [1] 95 90 87 84 92 89 78 97 93 90 89 97 79 86 85 85 98 95 96 92 79 85 97\n## [24] 89 75 75 84 93 88 97 93 96 84 89 95 95 97 95 92 95\n\n\n\n# Plot a histogram of the verbal_baseline variable that you have just created\nhist(verbal_baseline, main = 'Distribution of verbal memory baseline scores', xlab = 'score', ylab = 'frequency')\n\n\n\n\n\n\nLet us go wine tasting (red wine)\n\n\n# Read in the data set and assign to the object\nred_wine_data \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'red_wine_data', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(red_wine_data)\n\n\n\n\n##   subject condition Ratings\n## 1       1 Australia      77\n## 2       2 Australia      82\n## 3       3 Australia      75\n## 4       4 Australia      92\n## 5       5 Australia      83\n## 6       6 Australia      75\n\n\n\n# Print basic statistical properties of the red_wine_data data.frame. Use the describe() function\n#describe(red_wine_data)\nsummary(red_wine_data)\n\n\n\n\n##     subject       condition            Ratings     \n##  Min.   :  1.0   Length:400         Min.   :39.00  \n##  1st Qu.:100.8   Class :character   1st Qu.:67.00  \n##  Median :200.5   Mode  :character   Median :74.00  \n##  Mean   :200.5                      Mean   :73.94  \n##  3rd Qu.:300.2                      3rd Qu.:81.00  \n##  Max.   :400.0                      Max.   :98.00\n\n\n\n# Split the data.frame in subsets for each country and assign these subsets to the variables below\nred_usa \n- subset(red_wine_data, red_wine_data$condition == 'USA')\nred_france \n- subset(red_wine_data, red_wine_data$condition == 'France')\nred_australia \n- subset(red_wine_data, red_wine_data$condition == 'Australia')\nred_argentina \n- subset(red_wine_data, red_wine_data$condition == 'Argentina')\n\n# Select only the Ratings variable for each of these subsets and assign them to the variables below\nred_ratings_usa \n- red_usa$Ratings\nred_ratings_france \n- red_france$Ratings\nred_ratings_australia \n- red_australia$Ratings\nred_ratings_argentina \n- red_argentina$Ratings\n\n## Create a 2 by 2 matrix of histograms\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\npar(mfrow = c(2,2))\n\n# Plot four histograms, one for each subject\nhist(red_ratings_usa)\nhist(red_ratings_france)\nhist(red_ratings_australia)\nhist(red_ratings_argentina)\n\n\n\n\n\n\nLet us go wine tasting (white wine)\n\n\n# Read in the data set and assign to the object\nwhite_wine_data \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'white_wine_data', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(white_wine_data)\n\n\n\n\n##   condition Ratings\n## 1 Australia    85.6\n## 2 Australia    85.6\n## 3 Australia    85.6\n## 4 Australia    85.6\n## 5 Australia    85.6\n## 6 Australia    85.6\n\n\n\n# Assign the scores for each country to a variable\nwhite_ratings_france \n- subset(white_wine_data, white_wine_data$condition == 'France')$Ratings\nwhite_ratings_argentina \n- subset(white_wine_data, white_wine_data$condition == 'Argentina')$Ratings\nwhite_ratings_australia \n- subset(white_wine_data, white_wine_data$condition == 'Australia')$Ratings\nwhite_ratings_usa \n- subset(white_wine_data, white_wine_data$condition == 'USA')$Ratings\n\n# Plot a histogram for each of the countries\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\npar(mfrow = c(2,2))\n\nhist(white_ratings_usa, main = 'USA white ratings', xlab = 'score')\nhist(white_ratings_australia, main = 'Australia white ratings', xlab = 'score')\nhist(white_ratings_argentina, main = 'Argentina white ratings', xlab = 'score')\nhist(white_ratings_france, main = 'France white ratings', xlab = 'score')\n\n\n\n\n\n\n3, Scales of Measurement\n\n\nConverting a distribution to Z-scale\n\n\n# Read in the data set and assign to the object\nratings_australia \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'ratings_australia', header = TRUE, startCol = 1, startRow = 1)\n\nratings_australia \n- as.vector(ratings_australia$ratings_australia)\n\n\n\n\n# Print the ratings for the Australian red wine\nratings_australia\n\n\n\n\n##   [1] 77 82 75 92 83 75 84 86 85 79 92 84 77 65 89 81 81 88 87 85 87 86 82\n##  [24] 67 85 81 80 71 78 84 91 80 84 81 71 78 78 81 89 86 80 79 86 85 76 76\n##  [47] 84 86 80 87 84 77 83 73 91 95 78 74 85 80 98 81 86 81 76 82 68 91 82\n##  [70] 96 84 76 85 74 72 83 78 81 82 77 77 80 89 70 85 83 88 79 84 83 77 89\n##  [93] 89 86 92 85 72 77 72 78\n\n\n\n# Convert these ratings to Z-scores. Use the `scale()` function\nz_scores_australia \n- scale(ratings_australia)\n\n# Plot both the original data and the scaled data in histograms next to each other\npar(mfrow = c(1,2))\n\n# Plot the histogram for the original scores\nhist(ratings_australia)\n\n# Plot the histogram for the Z-scores\nhist(z_scores_australia)\n\n\n\n\n\n\n4, Measures of Central Tendency\n\n\nThe mean of a Fibonacci sequence\n\n\n# create a vector that contains the Fibonacci elements\nfibonacci \n- c(0, 1, 1, 2, 3, 5, 8, 13) \n\n# calculate the mean manually. Use the sum() and the length() functions\nmean \n- sum(fibonacci)/length(fibonacci)\nmean\n\n\n\n\n## [1] 4.125\n\n\n\n# calculate the mean the easy way\nmean_check \n- mean(fibonacci)\nmean_check\n\n\n\n\n## [1] 4.125\n\n\n\nSetting up histograms\n\n\n# Read in the data set and assign to the object\nwine_data \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wine_data', header = TRUE, startCol = 1, startRow = 1)\n\nhead(wine_data)\n\n\n\n\n##   condition Ratings\n## 1       Red      77\n## 2       Red      82\n## 3       Red      75\n## 4       Red      92\n## 5       Red      83\n## 6       Red      75\n\n\n\n# create the two subsets\nred_wine \n- subset(wine_data, wine_data$condition == 'Red')\nwhite_wine \n- subset(wine_data, wine_data$condition == 'White')\n\n# Plot the histograms of the ratings of both subsets\npar(mfrow = c(1,2))\nhist(red_wine$Ratings, main = 'Shiraz', xlab = 'Ratings')\nhist(white_wine$Ratings, main = 'Pinot Grigio', xlab = 'Ratings')\n\n\n\n\n\n\nRobustness to outliers\n\n\n# create the outlier and add it to the dataset\noutlier \n- data.frame(condition = 'Red', Ratings = 0)\n\nred_wine_extreme \n- rbind(red_wine, outlier)\n\n# calculate the difference in means and display it afterwards\ndiff_means \n- mean(red_wine$Ratings) - mean(red_wine_extreme$Ratings)\n\ndiff_means\n\n\n\n\n## [1] 0.8093069\n\n\n\n# calculate the difference in medians and display it afterwards\ndiff_medians \n- median(red_wine$Ratings) - median(red_wine_extreme$Ratings)\n\ndiff_medians\n\n\n\n\n## [1] 0\n\n\n\n5, Measures of Variability\n\n\nMichael Jordan\ns first NBA season - Global overview\n\n\n# Read in the data set and assign to the object\ndata_jordan \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'data_jordan', header = TRUE, startCol = 1, startRow = 1)\n\nhead(data_jordan)\n\n\n\n\n##   game points\n## 1    1     16\n## 2    2     21\n## 3    3     37\n## 4    4     25\n## 5    5     17\n## 6    6     25\n\n\n\n# Make a scatterplot of the data on which a horizontal line with height equal to the mean is drawn.\nmean_jordan \n- mean(data_jordan$points)\nplot(data_jordan$game, data_jordan$points,main = '1st NBA season of Michael Jordan')\n\nabline(h = mean_jordan)\n\n\n\n\n\n\nMichael Jordan\ns first NBA season - Calculate the variance manually\n\n\n# Calculate the differences with respect to the mean \ndiff \n- data_jordan$points - mean(data_jordan$points)\n\n# Calculate the squared differences\nsquared_diff \n- diff^2\n\n# Combine all pieces of the puzzle in order to acquire the variance\nvariance \n- sum(squared_diff)/(length(data_jordan$points) - 1)\nvariance\n\n\n\n\n## [1] 66.73427\n\n\n\n# Compare your result to the correct solution. You can find the correct solution by calculating it with the `var()` function.\nvar(data_jordan$points)\n\n\n\n\n## [1] 66.73427", 
            "title": "Statistics with R, Course One, Introduction"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#2-histograms-and-distributions", 
            "text": "Creating histograms in R  library(XLConnectJars)\nlibrary(XLConnect)\n\n# Read in the data set and assign to the object\nimpact  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'impact', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(impact)  ##   subject condition verbal_memory_baseline visual_memory_baseline\n## 1       1   control                     95                     88\n## 2       2   control                     90                     82\n## 3       3   control                     87                     77\n## 4       4   control                     84                     72\n## 5       5   control                     92                     77\n## 6       6   control                     89                     79\n##   visual.motor_speed_baseline reaction_time_baseline\n## 1                       35.29                   0.42\n## 2                       31.47                   0.63\n## 3                       30.87                   0.56\n## 4                       41.87                   0.66\n## 5                       33.28                   0.56\n## 6                       40.73                   0.81\n##   impulse_control_baseline total_symptom_baseline verbal_memory_retest\n## 1                       11                      0                   97\n## 2                        7                      0                   86\n## 3                        8                      0                   90\n## 4                        7                      0                   85\n## 5                        7                      1                   87\n## 6                        6                      0                   91\n##   visual_memory_retest visual.motor_speed_retest reaction_time_retest\n## 1                   86                     35.61                 0.65\n## 2                   80                     37.01                 0.49\n## 3                   79                     20.15                 0.75\n## 4                   70                     33.26                 0.19\n## 5                   77                     28.34                 0.59\n## 6                   85                     33.47                 0.48\n##   impulse_control_retest total_symptom_retest\n## 1                     10                    0\n## 2                      7                    0\n## 3                      9                    0\n## 4                      8                    0\n## 5                      8                    1\n## 6                      5                    0  # Use the describe() function to see some summary information per variable\n#describe(impact)\nsummary(impact)  ##     subject       condition         verbal_memory_baseline\n##  Min.   : 1.00   Length:40          Min.   :75.00         \n##  1st Qu.:10.75   Class :character   1st Qu.:85.00         \n##  Median :20.50   Mode  :character   Median :91.00         \n##  Mean   :20.50                      Mean   :89.75         \n##  3rd Qu.:30.25                      3rd Qu.:95.00         \n##  Max.   :40.00                      Max.   :98.00         \n##  visual_memory_baseline visual.motor_speed_baseline reaction_time_baseline\n##  Min.   :59.00          Min.   :26.29               Min.   :0.4200        \n##  1st Qu.:68.75          1st Qu.:31.59               1st Qu.:0.5675        \n##  Median :75.00          Median :33.50               Median :0.6500        \n##  Mean   :74.88          Mean   :34.03               Mean   :0.6670        \n##  3rd Qu.:81.25          3rd Qu.:36.44               3rd Qu.:0.7325        \n##  Max.   :91.00          Max.   :41.87               Max.   :1.2000        \n##  impulse_control_baseline total_symptom_baseline verbal_memory_retest\n##  Min.   : 2.000           Min.   :0.00           Min.   :59          \n##  1st Qu.: 7.000           1st Qu.:0.00           1st Qu.:74          \n##  Median : 8.500           Median :0.00           Median :85          \n##  Mean   : 8.275           Mean   :0.05           Mean   :82          \n##  3rd Qu.:10.000           3rd Qu.:0.00           3rd Qu.:91          \n##  Max.   :12.000           Max.   :1.00           Max.   :97          \n##  visual_memory_retest visual.motor_speed_retest reaction_time_retest\n##  Min.   :54.00        Min.   :20.15             Min.   :0.1900      \n##  1st Qu.:66.75        1st Qu.:30.33             1st Qu.:0.5575      \n##  Median :72.00        Median :35.15             Median :0.6500      \n##  Mean   :71.90        Mean   :35.83             Mean   :0.6730      \n##  3rd Qu.:79.00        3rd Qu.:39.41             3rd Qu.:0.7325      \n##  Max.   :86.00        Max.   :60.77             Max.   :1.3000      \n##  impulse_control_retest total_symptom_retest\n##  Min.   : 1.00          Min.   : 0.00       \n##  1st Qu.: 5.00          1st Qu.: 0.00       \n##  Median : 7.00          Median : 7.00       \n##  Mean   : 6.75          Mean   :13.88       \n##  3rd Qu.: 9.00          3rd Qu.:27.00       \n##  Max.   :12.00          Max.   :43.00  # Select the variable 'verbal_memory_baseline' from the 'impact' data.frame and assign it to the variable 'verbal_baseline'\nverbal_baseline  - impact$verbal_memory_baseline\nverbal_baseline  ##  [1] 95 90 87 84 92 89 78 97 93 90 89 97 79 86 85 85 98 95 96 92 79 85 97\n## [24] 89 75 75 84 93 88 97 93 96 84 89 95 95 97 95 92 95  # Plot a histogram of the verbal_baseline variable that you have just created\nhist(verbal_baseline, main = 'Distribution of verbal memory baseline scores', xlab = 'score', ylab = 'frequency')   Let us go wine tasting (red wine)  # Read in the data set and assign to the object\nred_wine_data  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'red_wine_data', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(red_wine_data)  ##   subject condition Ratings\n## 1       1 Australia      77\n## 2       2 Australia      82\n## 3       3 Australia      75\n## 4       4 Australia      92\n## 5       5 Australia      83\n## 6       6 Australia      75  # Print basic statistical properties of the red_wine_data data.frame. Use the describe() function\n#describe(red_wine_data)\nsummary(red_wine_data)  ##     subject       condition            Ratings     \n##  Min.   :  1.0   Length:400         Min.   :39.00  \n##  1st Qu.:100.8   Class :character   1st Qu.:67.00  \n##  Median :200.5   Mode  :character   Median :74.00  \n##  Mean   :200.5                      Mean   :73.94  \n##  3rd Qu.:300.2                      3rd Qu.:81.00  \n##  Max.   :400.0                      Max.   :98.00  # Split the data.frame in subsets for each country and assign these subsets to the variables below\nred_usa  - subset(red_wine_data, red_wine_data$condition == 'USA')\nred_france  - subset(red_wine_data, red_wine_data$condition == 'France')\nred_australia  - subset(red_wine_data, red_wine_data$condition == 'Australia')\nred_argentina  - subset(red_wine_data, red_wine_data$condition == 'Argentina')\n\n# Select only the Ratings variable for each of these subsets and assign them to the variables below\nred_ratings_usa  - red_usa$Ratings\nred_ratings_france  - red_france$Ratings\nred_ratings_australia  - red_australia$Ratings\nred_ratings_argentina  - red_argentina$Ratings\n\n## Create a 2 by 2 matrix of histograms\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\npar(mfrow = c(2,2))\n\n# Plot four histograms, one for each subject\nhist(red_ratings_usa)\nhist(red_ratings_france)\nhist(red_ratings_australia)\nhist(red_ratings_argentina)   Let us go wine tasting (white wine)  # Read in the data set and assign to the object\nwhite_wine_data  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'white_wine_data', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(white_wine_data)  ##   condition Ratings\n## 1 Australia    85.6\n## 2 Australia    85.6\n## 3 Australia    85.6\n## 4 Australia    85.6\n## 5 Australia    85.6\n## 6 Australia    85.6  # Assign the scores for each country to a variable\nwhite_ratings_france  - subset(white_wine_data, white_wine_data$condition == 'France')$Ratings\nwhite_ratings_argentina  - subset(white_wine_data, white_wine_data$condition == 'Argentina')$Ratings\nwhite_ratings_australia  - subset(white_wine_data, white_wine_data$condition == 'Australia')$Ratings\nwhite_ratings_usa  - subset(white_wine_data, white_wine_data$condition == 'USA')$Ratings\n\n# Plot a histogram for each of the countries\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\npar(mfrow = c(2,2))\n\nhist(white_ratings_usa, main = 'USA white ratings', xlab = 'score')\nhist(white_ratings_australia, main = 'Australia white ratings', xlab = 'score')\nhist(white_ratings_argentina, main = 'Argentina white ratings', xlab = 'score')\nhist(white_ratings_france, main = 'France white ratings', xlab = 'score')", 
            "title": "2, Histograms and Distributions"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#3-scales-of-measurement", 
            "text": "Converting a distribution to Z-scale  # Read in the data set and assign to the object\nratings_australia  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'ratings_australia', header = TRUE, startCol = 1, startRow = 1)\n\nratings_australia  - as.vector(ratings_australia$ratings_australia)  # Print the ratings for the Australian red wine\nratings_australia  ##   [1] 77 82 75 92 83 75 84 86 85 79 92 84 77 65 89 81 81 88 87 85 87 86 82\n##  [24] 67 85 81 80 71 78 84 91 80 84 81 71 78 78 81 89 86 80 79 86 85 76 76\n##  [47] 84 86 80 87 84 77 83 73 91 95 78 74 85 80 98 81 86 81 76 82 68 91 82\n##  [70] 96 84 76 85 74 72 83 78 81 82 77 77 80 89 70 85 83 88 79 84 83 77 89\n##  [93] 89 86 92 85 72 77 72 78  # Convert these ratings to Z-scores. Use the `scale()` function\nz_scores_australia  - scale(ratings_australia)\n\n# Plot both the original data and the scaled data in histograms next to each other\npar(mfrow = c(1,2))\n\n# Plot the histogram for the original scores\nhist(ratings_australia)\n\n# Plot the histogram for the Z-scores\nhist(z_scores_australia)", 
            "title": "3, Scales of Measurement"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#4-measures-of-central-tendency", 
            "text": "The mean of a Fibonacci sequence  # create a vector that contains the Fibonacci elements\nfibonacci  - c(0, 1, 1, 2, 3, 5, 8, 13) \n\n# calculate the mean manually. Use the sum() and the length() functions\nmean  - sum(fibonacci)/length(fibonacci)\nmean  ## [1] 4.125  # calculate the mean the easy way\nmean_check  - mean(fibonacci)\nmean_check  ## [1] 4.125  Setting up histograms  # Read in the data set and assign to the object\nwine_data  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wine_data', header = TRUE, startCol = 1, startRow = 1)\n\nhead(wine_data)  ##   condition Ratings\n## 1       Red      77\n## 2       Red      82\n## 3       Red      75\n## 4       Red      92\n## 5       Red      83\n## 6       Red      75  # create the two subsets\nred_wine  - subset(wine_data, wine_data$condition == 'Red')\nwhite_wine  - subset(wine_data, wine_data$condition == 'White')\n\n# Plot the histograms of the ratings of both subsets\npar(mfrow = c(1,2))\nhist(red_wine$Ratings, main = 'Shiraz', xlab = 'Ratings')\nhist(white_wine$Ratings, main = 'Pinot Grigio', xlab = 'Ratings')   Robustness to outliers  # create the outlier and add it to the dataset\noutlier  - data.frame(condition = 'Red', Ratings = 0)\n\nred_wine_extreme  - rbind(red_wine, outlier)\n\n# calculate the difference in means and display it afterwards\ndiff_means  - mean(red_wine$Ratings) - mean(red_wine_extreme$Ratings)\n\ndiff_means  ## [1] 0.8093069  # calculate the difference in medians and display it afterwards\ndiff_medians  - median(red_wine$Ratings) - median(red_wine_extreme$Ratings)\n\ndiff_medians  ## [1] 0", 
            "title": "4, Measures of Central Tendency"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#5-measures-of-variability", 
            "text": "Michael Jordan s first NBA season - Global overview  # Read in the data set and assign to the object\ndata_jordan  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'data_jordan', header = TRUE, startCol = 1, startRow = 1)\n\nhead(data_jordan)  ##   game points\n## 1    1     16\n## 2    2     21\n## 3    3     37\n## 4    4     25\n## 5    5     17\n## 6    6     25  # Make a scatterplot of the data on which a horizontal line with height equal to the mean is drawn.\nmean_jordan  - mean(data_jordan$points)\nplot(data_jordan$game, data_jordan$points,main = '1st NBA season of Michael Jordan')\n\nabline(h = mean_jordan)   Michael Jordan s first NBA season - Calculate the variance manually  # Calculate the differences with respect to the mean \ndiff  - data_jordan$points - mean(data_jordan$points)\n\n# Calculate the squared differences\nsquared_diff  - diff^2\n\n# Combine all pieces of the puzzle in order to acquire the variance\nvariance  - sum(squared_diff)/(length(data_jordan$points) - 1)\nvariance  ## [1] 66.73427  # Compare your result to the correct solution. You can find the correct solution by calculating it with the `var()` function.\nvar(data_jordan$points)  ## [1] 66.73427", 
            "title": "5, Measures of Variability"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Two,_Student_s_T-test/", 
            "text": "1, Introduction to t-tests\n\n\n2, Independent t-tests\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, Introduction to t-tests\n\n\n\n\nTest p-values for significance with z-tests, t-tests.\n\n\nSingle sample t-test: group of people from a particular geographic\n\n    region perform on a well-known test of intelligence. In particular,\n\n    you are interested in finding out whether or not this group scores\n\n    significantly higher than the overall population on an IQ test. This\n\n    is a form of Null Hypothesis Significance Testing (NHST), where the\n\n    null hypothesis is that there\ns no difference between this group and\n\n    the overall population.\n\n\nDependent t-test: single group of voters to rate their likelihood of\n\n    voting for the candidate before the speech and again after the\n\n    speech; understand if voters from a particular neighborhood are\n\n    likely to vote differently when compared to the overall\n\n    voting population.\n\n\nIndependent t-test: significant difference in preferences between\n\n    these two groups; compare liberals and convervatives.\n\n\nt-distribution, observed value, expected value, standard error.\n\n\n\n\nGenerate density plots of different t-distributions\n\n\n# Generate a vector of 100 values between -4 and 4\nx \n- seq(-4, 4, length = 100)\n\n# Simulate the t-distribution\ny_1 \n- dt(x, 4)\ny_2 \n- dt(x, 6)\ny_3 \n- dt(x, 8)\ny_4 \n- dt(x, 10)\ny_5 \n- dt(x, 12)\n\n# Plot the t-distributions\nplot(x, y_1, type = 'l', lwd = 2, xlab = 'T value', ylab = 'Density', main = 'Comparison of t-distributions')\n\nlines(x, y_2, col = 'red')\n#lines(x, y_3, col = 'orange')\n#lines(x, y_4, col = 'green')\n#lines(x, y_5, col = 'blue')\n\n# Add a legend\nlegend('topright', c('df = 4', 'df = 6', 'df = 8', 'df = 10', 'df = 12'), title = 'T distributions', col = c('black', 'red', 'orange', 'green', 'blue'), lty = 1)\n\n\n\n\nThe working memory dataset\n\n\nConduct a dependent (or paired) t-test on the \nworking memory\n dataset.\n\nThis dataset consists of the intelligence scores for subjects before and\n\nafter training, as well as for a control group. Our goal is to assess\n\nwhether intelligence training results in significantly different\n\nintelligence scores for the individuals.\n\n\nThe observations of individuals before and after training are two\n\nsamples from the same group at different points in time, which calls for\n\na dependent t-test. This will test whether or not the difference in mean\n\nintelligence scores before and after training are significant.\n\n\nlibrary(XLConnectJars)\nlibrary(XLConnect)\n\n# Read in the data set and assign to the object\nwm \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(wm)\n\n\n\n\n# Create a subset for the data that contains information on those subject who trained\nwm_t \n- subset(wm, wm$train == 1)\n\n # Summary statistics \ndescribe(wm_t)\n\n# Create a boxplot with pre- and post-training groups \nboxplot(wm_t$pre, wm_t$post, main = \nBoxplot\n, xlab = \nPre and Post Training\n, ylab = \nIntelligence Score\n, col = c(\nred\n, \ngreen\n))\n\n\n\n\nPerforming dependent t-tests manually in R (1)\n\n\nConducting a dependent t-test, also known as a paired t-test, requires\n\nthe following steps:\n\n\n\n\nDefine null and alternative hypotheses\n\n\nDecide significance level \n\u03b1\n\n\nCompute observed t-value\n\n\nFind critical value\n\n\nCompare observed value to critical value\n\n\n\n\nWe\nre performing a Null Hypothesis Significance Test (NHST), so our null\n\nhypothesis is that there\ns no effect (i.e. training has no impact on\n\nintelligence scores). The alternative hypothesis is that training\n\nresults in signficantly different intelligence scores. We\nll use a\n\nsignificance level of 0.05, which is very common in statistics.\n\n\nCompute the observed t-value.\n\n\n# Define the sample size\nn \n- dim(wm_t)[1]\n\n# Calculate the degrees of freedom\ndf \n- n - 1\n\n # Find the critical t-value\nt_crit \n- abs(qt(0.025, df))\n\n# Calculate the mean of the difference in scores. The differences are already in the dataset under the column 'gain'.\nmean_diff \n- sum(wm_t$gain)/n\n\n# Calculate the standard deviation\nxdt \n- sum(wm_t$gain^2)\nxdt2 \n- xdt/n\nsd_diff2 \n- sqrt((xdt - xdt2)/(n - 1))\nsd_diff \n- sqrt((sum(wm_t$gain^2) - ((sum(wm_t$gain))^2/n))/(n - 1))\nsd_diff2\nsd_diff\n\n\n\n\nPerforming dependent t-tests manually in R (2)\n\n\nNow that we\nve determined our null and alternative hypotheses, decided\n\non a significance level, and computed our observed t-value, all that\n\nremains is to calculate the critical value for this test and compare it\n\nto our observed t-value. This will tell us whether we have sufficient\n\nevidence to reject our null hypothesis. We\nll even go one step further\n\nand compute an effect size with Cohen\ns d!\n\n\nThe critical value is the point on the relevant t-distribution that\n\ndetermines whether the value we observed is extreme enough to warrant\n\nrejecting the null hypothesis. Recall that a t-distribution is defined\n\nby its degrees of freedom, which in turn is equal to the sample size\n\nminus 1. In this example, we have 80 subjects so the relevant\n\nt-distribution has 79 degrees of freedom.\n\n\nWe\nre performing a two-tailed t-test in this situation since we care\n\nabout detecting a significant effect in either the positive or negative\n\ndirection. In other words, we want to know if training significantly\n\nincreases or decreases intelligence, however, given that our observed\n\nt-value is positive (14.49) the right-hand is the only relevant value\n\nhere.\n\n\nFurthermore, since our desired significance level (i.e. alpha) is 0.05,\n\nour critical value is the point on our t-distribution at which 0.025\n\n(0.05 / 2) of its total area of 1 is to the right and thus 0.975 (1 -\n\n0.025) of its total area is to the left.\n\n\nThis point is called the 0.975 quantile and is computed for a\n\nt-distrbution.\n\n\n# The variables from the previous exercise are still preloaded, type ls() in the console to see them\nn \n- dim(wm_t)[1]\ndf \n- n - 1\nt_crit \n- abs(qt(0.025, df))\nmean_diff \n- sum(wm_t$gain)/n\nsd_diff \n- sqrt((sum(wm_t$gain^2) - ((sum(wm_t$gain))^2/n))/(n - 1))\n\n# Calculate the t-value for this test\nt_value \n- mean_diff/(sd_diff/sqrt(n))\n\n# Check whether or not the mean difference is statistically significant\nt_value\nt_crit\n\n# Calculate the confidence interval\nconf_upper \n- mean_diff + t_crit * (sd_diff/sqrt(n))\nconf_lower \n- mean_diff - t_crit * (sd_diff/sqrt(n))\nconf_upper\nconf_lower\n\n# Calculate Cohen's d\ncohens_d \n- mean_diff/sd_diff\ncohens_d\n\n\n\n\nLetting R do all the dirty work: Dependent t-tests\n\n\nThe \nCohensD\n function (not showns).\n\n\nCohen\ns d Determine that the difference pre- and post-training is\n\nstatistically significant; and the effect size, meaning the effect of\n\ntraining on intelligence gains particularly strong or not.\n\n\nCohen\ns d is unbiased by sample size. Cohen\ns d provides a standardized\n\ndifference between two means. Cohen\ns d is calculated by subtracting one\n\ngroup mean from the other, then dividing by the pooled standard\n\ndeviation.\n\n\n# Conduct a paired t-test using the t.test function\nt.test(wm_t$post, wm_t$pre, paired = TRUE)\n\n# Calculate Cohen's d\ncohensD(wm_t$post, wm_t$pre, method = 'paired')\n\n\n\n\n2, Independent t-tests\n\n\nAn independent t-test is appropriate when you want to compare the the\n\nmeans for two independent groups.\n\n\nPreliminary statistics\n\n\nFor independent t-tests you will revisit the working memory dataset from\n\nthe previous chapter. In this dataset, subjects were randomly assigned\n\nto four different training groups that trained for 8, 12, 17 and 19\n\ndays.\n\n\n# Read in the data set and assign to the object\nwm_t \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm_t', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(wm_t)\n\n\n\n\nAdd statistical functions.\n\n\n# Load the psych(ology) package\nlibrary(psych)\n\n\n\n\nLevene\ns test for homogeneity of variance.\n\n\n# Create subsets for each training time\nwm_t08 \n- subset(wm_t, cond == 't08')\nwm_t12 \n- subset(wm_t, cond == 't12')\nwm_t17 \n- subset(wm_t, cond == 't17')\nwm_t19 \n- subset(wm_t, cond == 't19')\n\n# Summary statistics of the change in training scores before and after exercise\ndescribe(wm_t08)\ndescribe(wm_t12)\ndescribe(wm_t17)\ndescribe(wm_t19)\n\n# Create a boxplot of the different training times\nggplot(wm_t, aes(x = cond, y = gain, fill = cond)) + geom_boxplot()\n\n# Levene's test\nleveneTest(wm_t$gain ~ wm_t$cond)\n\n\n\n\nConducting an independent t-test manually (1)\n\n\nPerform an independent t-test the same way we did for the dependent\n\nt-test in the previous chapter. Continuing with the working memory\n\nexample, our null hypothesis is that the difference in intelligence\n\nscore gain between the group that trained for 8 days and the group that\n\ntrained for 19 days is equal to zero. If our observed t-value is\n\nsufficiently large, we can reject the null in favor of the alternative\n\nhypothesis, which would imply a significant difference in intelligence\n\ngain between the two training groups.\n\n\nCalculation of the observed t-value for an independent t-test is similar\n\nto the dependent t-test, but involves slightly different formulas.\n\n\n# Calculate mean difference by subtracting the gain for t08 by the gain for t19\nmean_t08 \n- mean(wm_t08$gain)\nmean_t19 \n- mean(wm_t19$gain)\nmean_diff \n- (mean_t19 - mean_t08)\n\n# Calculate degrees of freedom\nn_t08 \n- dim(wm_t08)[1]\nn_t19 \n- dim(wm_t19)[1]\ndf \n- n_t08 + n_t19 - 2\n\n# Calculate the pooled standard error\nvar_t08 \n- (sum((wm_t08$gain - mean_t08)^2))/(n_t08 - 1)\nvar_t19 \n- (sum((wm_t19$gain - mean_t19)^2))/(n_t19 - 1)\nse_pooled \n- sqrt((var_t08/n_t08) + (var_t19/n_t19))\n\n\n\n\nConducting an independent t-test manually (2)\n\n\nCompute the observed t-value. Then we will determine the p-value using\n\nthe relevant t-distribution. If you recall, in the last chapter we\n\ncalculated the critical value. The p-value is simply an alternative\n\napproach to hypothesis testing and determining the significance of your\n\nresults. It\ns good to practice both! Finally, we will finish by\n\ncalculating effect size via Cohen\ns d.\n\n\n# All variables from the previous exercises are preloaded in your workspace\n# Type ls() to see them\n# Calculate the t-value\nt_value \n- mean_diff/se_pooled\n\n# Calculate p-value\n#two-tail test, 0.05/2 = 0.025\np_value \n- 2*(1-pt(t_value,df = df))\n\n# Calculate Cohen's d\nsd_t08 \n- sd(wm_t08$gain)\nsd_t19 \n- sd(wm_t19$gain)\npooled_sd \n- (sd_t08 + sd_t19) / 2\ncohens_d \n- mean_diff/pooled_sd\n\n\n\n\nLetting R do all the dirty work: Independent t-tests\n\n\n# Conduct an independent t-test \nt.test(wm_t19$gain, wm_t08$gain,var.equal = TRUE)\n\n# Calculate Cohen's d\ncohensD(wm_t19$gain, wm_t08$gain, method = 'pooled')", 
            "title": "Statistics with R, Course Two, Student's t-test"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Two,_Student_s_T-test/#2-independent-t-tests", 
            "text": "An independent t-test is appropriate when you want to compare the the \nmeans for two independent groups.  Preliminary statistics  For independent t-tests you will revisit the working memory dataset from \nthe previous chapter. In this dataset, subjects were randomly assigned \nto four different training groups that trained for 8, 12, 17 and 19 \ndays.  # Read in the data set and assign to the object\nwm_t  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm_t', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(wm_t)  Add statistical functions.  # Load the psych(ology) package\nlibrary(psych)  Levene s test for homogeneity of variance.  # Create subsets for each training time\nwm_t08  - subset(wm_t, cond == 't08')\nwm_t12  - subset(wm_t, cond == 't12')\nwm_t17  - subset(wm_t, cond == 't17')\nwm_t19  - subset(wm_t, cond == 't19')\n\n# Summary statistics of the change in training scores before and after exercise\ndescribe(wm_t08)\ndescribe(wm_t12)\ndescribe(wm_t17)\ndescribe(wm_t19)\n\n# Create a boxplot of the different training times\nggplot(wm_t, aes(x = cond, y = gain, fill = cond)) + geom_boxplot()\n\n# Levene's test\nleveneTest(wm_t$gain ~ wm_t$cond)  Conducting an independent t-test manually (1)  Perform an independent t-test the same way we did for the dependent \nt-test in the previous chapter. Continuing with the working memory \nexample, our null hypothesis is that the difference in intelligence \nscore gain between the group that trained for 8 days and the group that \ntrained for 19 days is equal to zero. If our observed t-value is \nsufficiently large, we can reject the null in favor of the alternative \nhypothesis, which would imply a significant difference in intelligence \ngain between the two training groups.  Calculation of the observed t-value for an independent t-test is similar \nto the dependent t-test, but involves slightly different formulas.  # Calculate mean difference by subtracting the gain for t08 by the gain for t19\nmean_t08  - mean(wm_t08$gain)\nmean_t19  - mean(wm_t19$gain)\nmean_diff  - (mean_t19 - mean_t08)\n\n# Calculate degrees of freedom\nn_t08  - dim(wm_t08)[1]\nn_t19  - dim(wm_t19)[1]\ndf  - n_t08 + n_t19 - 2\n\n# Calculate the pooled standard error\nvar_t08  - (sum((wm_t08$gain - mean_t08)^2))/(n_t08 - 1)\nvar_t19  - (sum((wm_t19$gain - mean_t19)^2))/(n_t19 - 1)\nse_pooled  - sqrt((var_t08/n_t08) + (var_t19/n_t19))  Conducting an independent t-test manually (2)  Compute the observed t-value. Then we will determine the p-value using \nthe relevant t-distribution. If you recall, in the last chapter we \ncalculated the critical value. The p-value is simply an alternative \napproach to hypothesis testing and determining the significance of your \nresults. It s good to practice both! Finally, we will finish by \ncalculating effect size via Cohen s d.  # All variables from the previous exercises are preloaded in your workspace\n# Type ls() to see them\n# Calculate the t-value\nt_value  - mean_diff/se_pooled\n\n# Calculate p-value\n#two-tail test, 0.05/2 = 0.025\np_value  - 2*(1-pt(t_value,df = df))\n\n# Calculate Cohen's d\nsd_t08  - sd(wm_t08$gain)\nsd_t19  - sd(wm_t19$gain)\npooled_sd  - (sd_t08 + sd_t19) / 2\ncohens_d  - mean_diff/pooled_sd  Letting R do all the dirty work: Independent t-tests  # Conduct an independent t-test \nt.test(wm_t19$gain, wm_t08$gain,var.equal = TRUE)\n\n# Calculate Cohen's d\ncohensD(wm_t19$gain, wm_t08$gain, method = 'pooled')", 
            "title": "2, Independent t-tests"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/", 
            "text": "1, An introduction to ANOVA\n\n\n2, Post-hoc analysis\n\n\n3, Between groups factorialANOVA\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, An introduction to ANOVA\n\n\nWorking memory experiment\n\n\nWe\nll use data from the working memory experiment, which investigates\n\nthe relationship between the number of training days and a change in IQ.\n\nThere are four independent groups, each of which trained for a different\n\nperiod of time: 8, 12, 17, or 19 days. The independent variable is the\n\nnumber of training days and the dependent variable is the IQ gain.\n\n\nlibrary(XLConnectJars)\nlibrary(XLConnect)\n\n# Read in the data set and assign to the object\nwm \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'w_m', header = TRUE, startCol = 1, startRow = 1)\n\n# Check it out\nhead(wm)\n\n\n\n\nlibrary(psych)\n\n# Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions)\ndescribeBy(wm, wm$condition)\n\n# Boxplot IQ versus cond\nboxplot(wm$iq ~ wm$condition, main=\nBoxplot\n, xlab=\nGroup (cond)\n, ylab=\nIQ\n)\n\n\n\n\nNotice that the IQ increases as the amount of training sessions\n\nincreases.\n\n\nt-test vs ANOVA\n\n\nANOVA is used when more than two group means are compared, whereas a\n\nt-test can only compare two group means.\n\n\nGenerate density plot of the F-distribution\n\n\nThe test statistic associated with ANOVA is the F-test (or F-ratio).\n\nRecall that when carrying out a t-test, you computed an observed\n\nt-value, then compared that with a critical value derived from the\n\nrelevant t-distribution. That t-distribution came from a family of\n\nt-distributions, each of which was defined entirely by its degrees of\n\nfreedom.\n\n\nANOVA uses the same principle, but instead an observed F-value is\n\ncomputed and compared to the relevant F-distribution. That\n\nF-distribution comes from a family of F-distributions, each of which is\n\ndefined by two numbers (i.e. degrees of freedom).\n\n\nF-distribution has a different shape than the t-distribution.\n\n\n# Create the vector x\nx \n- seq(from = 0, to = 2, length = 100)\n\n# Simulate the F-distributions\ny_1 \n- df(x, 1, 1)\ny_2 \n- df(x, 3, 1)\ny_3 \n- df(x, 6, 1)\ny_4 \n- df(x, 3, 3)\ny_5 \n- df(x, 6, 3)\ny_6 \n- df(x, 3, 6)\ny_7 \n- df(x, 6, 6)\n\n# Plot the F-distributions\nplot(x, y_1, col = 1, 'l')\nlines(x,y_2, col = 2, 'l')\nlines(x,y_3, col = 3, 'l')\nlines(x,y_4, col = 4, 'l')\nlines(x,y_5, col = 5, 'l')\nlines(x,y_6, col = 6, 'l')\nlines(x,y_7, col = 7, 'l')\n\n# Add the legend in the top right corner and with the title 'F distributions'\nlegend('topright', title = 'F distributions', c('df = (1,1)', 'df = (3,1)', 'df = (6,1)', 'df = (3,3)', 'df = (6,3)', 'df = (3,6)', 'df = (6,6)'), col = c(1, 2, 3, 4, 5, 6, 7), lty = 1)\n\n\n\n\nThe F-distribution cannot take negative values, because it is a ratio of\n\nvariances and variances are always non-negative numbers. The\n\ndistribution represents the ratio between the variance between groups\n\nand the variance within groups.\n\n\nBetween group sum of squares\n\n\nTo calculate the F-value, you need to calculate the ratio between the\n\nvariance between groups and the variance within groups. Furthermore, to\n\ncalculate the variance (i.e. mean of squares), you first have to\n\ncalculate the sum of squares.\n\n\nNow, remember that the working memory experiment investigates the\n\nrelationship between the change in IQ and the number of training\n\nsessions. Calculate the between group sum of squares for the data from\n\nthis experiment.\n\n\n# Define number of subjects in each group\nn \n- 20\n\n# Calculate group means\nY_j \n- as.numeric(tapply(wm$iq, wm$condition, mean))\nY_j\n\n# Calculate the grand mean\nY_T \n- mean(wm$iq)\nY_T\n\n# Calculate the sum of squares\nSS_A \n- sum((Y_j - Y_T)^2) * n\nSS_A\n\n\n\n\nWithin groups sum of squares\n\n\nTo calculate the F-value, you also need the variance within groups.\n\nSimilar to the last exercise, we\nll start by computing the within groups\n\nsum of squares.\n\n\n# Create four subsets of the four groups, containing the IQ results\n\n# Make the subset for the group cond = '8 days'\nY_i1 \n- subset(wm$iq, wm$condition == '8 days')\n\n# Make the subset for the group cond = '12 days'\nY_i2 \n- subset(wm$iq, wm$condition == '12 days')\n\n# Make the subset for the group cond = '17 days'\nY_i3 \n- subset(wm$iq, wm$condition == '17 days')\n\n# Make the subset for the group cond = '19 days'\nY_i4 \n- subset(wm$iq, wm$condition == '19 days')\n\n# subtract the individual values by their group means\n# You have already calculated the group means in the previous exercise so use this result, the vector that contains these group means was called Y_j\nS_1 \n- Y_i1 - Y_j[1]\nS_2 \n- Y_i2 - Y_j[2]\n\n# Do it without the vector Y_j, so calculate the group means again.\nS_3 \n- Y_i3 - mean(Y_i3)\nS_4 \n- Y_i4 - mean(Y_i4)\n\n#Put everything back in one vector\nS_T \n- c(S_1,S_2,S_3,S_4)\n\n#Calculate the sum of squares by using the vector S_T\nSS_SA \n- sum(S_T^2)\n\n\n\n\nCalculating the F-ratio\n\n\nCalculate the F-ratio.\n\n\n# Number of groups\na \n- 4\n\n# Number of subject in each group\nn \n- 20\n\n# Define the degrees of freedom\ndf_A \n- a - 1\ndf_SA \n- a*(n - 1)\n\n# Calculate the mean squares (variances) by using the sum of squares SS_A and SS_SA\nMS_A \n- SS_A/df_A\nMS_SA \n- SS_SA/df_SA\n\n# Calculate the F-ratio\nF \n- MS_A/MS_SA\n\n\n\n\nA faster way: ANOVA in R\n\n\nNormally, we do not have to do all calculations.\n\n\n# Apply the aov function\nanova.wm \n- aov(wm$iq ~ wm$condition)\nanova.wm\n\n# Look at the summary table of the result\nsummary(anova.wm)\n\n\n\n\nF-value is significant.\n\n\nLevene\ns test\n\n\nThe assumptions of ANOVA are relatively simple. Similar to an\n\nindependent t-test, we have a continuous dependent variable, which we\n\nassume to be normally distributed. Furthermore, we assume homogeneity of\n\nvariance, which can be tested with Levene\ns test.\n\n\nlibrary(car)\n\n# Levene's test\nleveneTest(wm$iq ~ wm$condition)\n\n# Levene's test with the change for the default, namely center = mean\nleveneTest(wm$iq ~ wm$condition, center = mean)\n\n\n\n\nThe assumption of homogeneity of variance hold: the within group\n\nvariance equivalent for all groups.\n\n\n2, Post-hoc analysis\n\n\nPost-hoc tests help finding out which groups differ significantly from\n\none other and which do not. More formally, post-hoc tests allow for\n\nmultiple pairwise comparisons without inflating the type I error.\n\n\nWhat does it mean to inflate the type I error?\n\n\nSuppose the post-hoc test involves performing three pairwise\n\ncomparisons, each with the probability of a type I error set at 5%. The\n\nprobability of making at least one type I error is then equal to:\n\n\n1\u2005\u2212\u2005(\nn\no\n\u2004:\u2004\nt\ny\np\ne\n\u2004:\u2004\nI\n\u2004:\u2004\ne\nr\nr\no\nr\n\u2004:\u2004\u00d7\u2004:\u2004\nn\no\n\u2004:\u2004\nt\ny\np\ne\n\u2004:\u2004\nI\n\u2004:\u2004\ne\nr\nr\no\nr\n\u2004:\u2004\u00d7\u2004:\u2004\nn\no\n\u2004:\u2004\nt\ny\np\ne\n\u2004:\u2004\nI\n\u2004:\u2004\ne\nr\nr\no\nr\n)\n\n\nIf, for simplicity, you assume independence of these three events, the\n\nmaximum familywise error rate becomes:\n\n\n1\u2005\u2212\u2005(0.95\u2005\u00d7\u20050.95\u2005\u00d7\u20050.95)=14.26%\n\n\nIn other words, the probability of having at least one false alarm (i.e.\n\ntype I error) is 14.26%.\n\n\nWhat is the maximum familywise error rate for the working memory\n\nexperiment, assuming that you do all possible pairwise comparisons with\n\na type I error of 5%? 26.49%.\n\n\nNull Hypothesis Significance Testing (NHST) is a statistical method used\n\nto test whether or not you are able to reject or retain the null\n\nhypothesis. This type of test can confront you with a type I error. This\n\nhappens when the test rejects the null hypothesis, while it is actually\n\ntrue in reality. Furthermore, the test can also deliver a type II error.\n\nThis is the failure to reject a null hypothesis when it is false. All\n\nhypothesis tests have a probability of making type I and II errors.\n\n\nSensitivity and specificity are two concepts that statisticians use to\n\nmeasure the performance of a statistical test. The sensitivity of a test\n\nis its true positive rate:\n\n\n\n\n\\\\mathrm{sensitivity} = \\\\frac{\\\\mathrm{number\\\\ of\\\\ true\\\\ positives}}{\\\\mathrm{number\\\\ of\\\\ true\\\\ positives} + \\\\mathrm{number\\\\ of\\\\ false\\\\ negatives}}\n\n\n\n\nThe specificity of a test is its true negative rate:\n\n\n\n\n\\\\mathrm{specificity} = \\\\frac{\\\\mathrm{number\\\\ of\\\\ true\\\\ negatives}}{\\\\mathrm{number\\\\ of\\\\ true\\\\ negatives} + \\\\mathrm{number\\\\ of\\\\ false\\\\ positives}}\n\n\n\n\nCalculate both the sensitivity and specificity of the test based on\n\nnumbers displayed in the NHST table?\n\n\n\n\nThe sensitivity is 0.89 and the specificity is 0.85.\n\n\nCalculate and interpret the results of Tukey\n\n\nIn a situation were you do multiple pairwise comparisons, the\n\nprobability of type I errors in the process inflates substantially.\n\nTherefore, it is better to build in adjustments to take this into\n\naccount. This is what Tukey tests and other post-hoc procedures do. They\n\nadjust the p-value to prevent inflation of the type I error rate.Use\n\nTukey\ns procedure.\n\n\n# Read in the data set and assign to the object\nwm \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(wm)\n\n\n\n\n# Revision: Analysis of variance\nanova_wm \n- aov(wm$gain ~ wm$cond)\n\n# Summary Analysis of Variance\nsummary(anova_wm)\n\n# Post-hoc (Tukey)\nTukeyHSD(anova_wm)\n\n# Plot confidence intervals\n#plot(c(TukeyHSD(anova_wm)$lwr,TukeyHSD(anova_wm)$upr))\nplot(TukeyHSD(anova_wm))\n\n\n\n\nBonferroni adjusted p-values\n\n\nJust like Tukey\ns procedure, the Bonferroni correction is a method that\n\nis used to counteract the problem of inflated type I errors while\n\nengaging in multiple pairwise comparisons between subgroups. Bonferroni\n\nis generally known as the most conservative method to control the\n\nfamilywise error rate.\n\n\nBonferroni is based on the idea that if you test \nN\n dependent or\n\nindependent hypotheses, one way of maintaining the familywise error rate\n\nis to test each individual hypothesis at a statistical significance\n\nlevel that is deflated by a factor of \n\\\\frac{1}{n}\n. So, for a\n\nsignificance level for the whole family of tests of \n\u03b1\n, the Bonferroni\n\ncorrection would be to test each of the individual tests at a\n\nsignificance level of \n\\\\frac{\\\\alpha}{n}\n.\n\n\nThe Bonferroni correction is controversial. It is a strict measure to\n\nlimit false positives and generates conservative p-value. Alternative:\n\nincrease the sample size, compute the false discovery rate (the expected\n\npercent of false predictions in the set of predictions. For example if\n\nthe algorithm returns 100 results with a false discovery rate of .3 then\n\nwe should expect 70 of them to be correct.), and the Holm-Bonferroni\n\nmethod.\n\n\n# Use `p.adjust` \nbonferroni_ex \n- p.adjust(.005, method='bonferroni', n = 8)\n\nbonferroni_ex\n\n# Pairwise T-test\npairwise.t.test(wm$gain,wm$cond, p.adjust = 'bonferroni')\n\n\n\n\n3, Between groups factorial ANOVA\n\n\nData exploration with a barplot\n\n\nWe\nll use in this chapter is a randomized controlled experiment\n\ninvestigating the effects of talking on a cell phone while driving. The\n\ndependent variable in the experiment is the number of driving errors\n\nthat subjects made in a driving simulator. There are two independent\n\nvariables:\n\n\n\n\nConversation difficulty: None, Low, High\n\n\nDriving difficulty: Easy, Difficult\n\n\n\n\n\n\n\n# Read in the data set and assign to the object\nab \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'ab', header = TRUE, startCol = 1, startRow = 1)\n\nab$subject \n- as.integer(ab$subject)\nab$conversation \n- as.factor(ab$conversation)\nab$driving \n- as.factor(ab$driving)\nab$error \n- as.integer(ab$error)\n\n# This will print the data set in the console\nhead(ab)\n\n\n\n\nEach of the subjects was randomly assigned to one of six conditions\n\nformed by combining different values of the independent variables.\n\n\n\n\nsubject\n: unique identifier for each subject conversation: level of\n\n    conversation difficulty.\n\n\ndriving\n : level of driving difficulty in the simulator.\n\n\nerrors\n: number of driving errors made.\n\n\n\n\n\n\n\n# Use the tapply function to create your groups\nab_groups \n- tapply(ab$errors, list(ab$driving, ab$conversation), sum)\n\n# Make the required barplot\nbarplot(ab_groups, beside = TRUE, col = c('orange','blue'), main = 'Driving Errors', xlab = 'Conversation Demands', ylab = 'Errors')\n\n# Add the legend\nlegend('topright', c('Difficult','Easy'), title = 'Driving', fill = c('orange','blue'))\n\n\n\n\nThe driving errors made during different driving conditions are\n\ninfluenced by the level of conversation demand. In other words, the\n\ndriving conditions have a different effect on the number of errors made,\n\ndepending on the level of conversation demand.\n\n\nThe homogeneity of variance assumption\n\n\nBefore we do factorial ANOVA, we need to test the homogeneity of the\n\nvariance assumption. When studying one-way ANOVA, we tested this\n\nassumption with the \nleveneTest\n function.\n\n\nWe now have two independent variables instead of just one.\n\n\n# Test the homogeneity of variance assumption\nleveneTest(ab$errors ~ ab$conversation * ab$driving)\n\n\n\n\nThe homogeneity of variance assumption holds.\n\n\nBy performing a \nleveneTest\n, we can check whether or not the\n\nhomogeneity of variance assumption holds for a given dataset. The\n\nassumption must hold for the results of an ANOVA analysis to be valid.\n\n\nRecall from the first chapter that ANOVA makes use of F-statistics, or\n\nF-ratios, in which two types of degrees of freedom are involved.\n\n\ndim(ab)\nstr(ab$conversation)\nstr(ab$driving)\n\n\n\n\nThere are 120 subjets and 2 (Easy, Difficult) * 3 (High Demand, Low\n\nDemand, None) = 6 groups.\n\n\nThe factorial ANOVA\n\n\n# Factorial ANOVA\nab_model \n- aov(ab$errors ~ ab$conversation * ab$driving)\n\n# Get the summary table\nsummary(ab_model)\n\n\n\n\nBased on the summary table of the factorial ANOVA, the main effect for\n\ndriving difficulty, the main effect for conversation difficulty and the\n\ninteraction effect are all significant.\n\n\nThe interaction effect\n\n\nNow it\ns time to explore the interaction effect. You will do this with\n\nthe help of a simple effects analysis.\n\n\nWhy a simple effects analysis? Well, remember what we had to do when you\n\nhad a significant main effect in a one-way ANOVA? There, you just had to\n\nperform some post-hoc tests to see from which level of the categorical\n\nvariable the main effect was coming. With an interaction effect, it is\n\nquite similar. Conduct a simple effects analysis of the variable\n\n\nconversation\n on the outcome variable \nerrors\n at each level of\n\n\ndriving\n.\n\n\n# Create the two subsets\nab_1 \n- subset(ab, ab$driving == 'Easy')  \nab_2 \n- subset(ab, ab$driving == 'Difficult')\n\n# Perform the one-way ANOVA for both subsets\naov_ab_1 \n- aov(ab_1$errors ~ ab_1$conversation)\naov_ab_2 \n- aov(ab_2$errors ~ ab_2$conversation)\n\n# Get the summary tables for both aov_ab_1 and aov_ab_2\nsummary(aov_ab_1)\nsummary(aov_ab_2)\n\n\n\n\nThere a significant simple effect for the easy driving condition based\n\non the summary table of \naov_ab_1\n.\n\n\nThere a significant simple effect for the difficult driving condition\n\nbased on the summary table of \naov_ab_2\n.\n\n\nThe effect sizes\n\n\nThe definition of an interaction effect states that the effect of one\n\nvariable changes across levels of the other variable. For example, we\n\nmight expect the effect of conversation to be greater when driving\n\nconditions are difficult than when they are relatively easy.\n\n\nUnfortunately, it is not quite that simple. In order to really\n\nunderstand the different effect sizes, you should make use of the\n\n\netaSquared\n function.\n\n\nlibrary(lsr)\n\n# Calculate the etaSquared for the easy driving case\n#easy is ab_1\n#difficult is ab_2\netaSquared(aov_ab_1, anova = TRUE)\n\n# Calculate the etaSquared for the difficult driving case\netaSquared(aov_ab_2, anova = TRUE)\n\n\n\n\nBased on the output of the \netaSquared\n function for the easy driving\n\ncondition, the percentage of variance explained by the conversation\n\nvariable is 14.7%; the percentage of variance explained by the\n\nconversation variable is 57.8%.\n\n\nPairwise comparisons\n\n\nFinally, let us look at pairwise comparisons for the simple effects. You\n\ncan do this with the Tukey post-hoc test.\n\n\n# Tukey for easy driving\nTukeyHSD(aov_ab_1)\n\n# Tukey for difficult driving\nTukeyHSD(aov_ab_2)\n\n\n\n\nFor \nEasy Driving\n, two mean differences in terms of number of errors\n\nare significant (below 0.05).\n\n\nFor \nDifficult Driving\n, three mean differences in terms of number of\n\nerrors are significant (below 0.05).", 
            "title": "Statistics with R, Course Three, Analysis of Variance"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/#2-post-hoc-analysis", 
            "text": "Post-hoc tests help finding out which groups differ significantly from \none other and which do not. More formally, post-hoc tests allow for \nmultiple pairwise comparisons without inflating the type I error.  What does it mean to inflate the type I error?  Suppose the post-hoc test involves performing three pairwise \ncomparisons, each with the probability of a type I error set at 5%. The \nprobability of making at least one type I error is then equal to:  1\u2005\u2212\u2005( n o \u2004:\u2004 t y p e \u2004:\u2004 I \u2004:\u2004 e r r o r \u2004:\u2004\u00d7\u2004:\u2004 n o \u2004:\u2004 t y p e \u2004:\u2004 I \u2004:\u2004 e r r o r \u2004:\u2004\u00d7\u2004:\u2004 n o \u2004:\u2004 t y p e \u2004:\u2004 I \u2004:\u2004 e r r o r )  If, for simplicity, you assume independence of these three events, the \nmaximum familywise error rate becomes:  1\u2005\u2212\u2005(0.95\u2005\u00d7\u20050.95\u2005\u00d7\u20050.95)=14.26%  In other words, the probability of having at least one false alarm (i.e. \ntype I error) is 14.26%.  What is the maximum familywise error rate for the working memory \nexperiment, assuming that you do all possible pairwise comparisons with \na type I error of 5%? 26.49%.  Null Hypothesis Significance Testing (NHST) is a statistical method used \nto test whether or not you are able to reject or retain the null \nhypothesis. This type of test can confront you with a type I error. This \nhappens when the test rejects the null hypothesis, while it is actually \ntrue in reality. Furthermore, the test can also deliver a type II error. \nThis is the failure to reject a null hypothesis when it is false. All \nhypothesis tests have a probability of making type I and II errors.  Sensitivity and specificity are two concepts that statisticians use to \nmeasure the performance of a statistical test. The sensitivity of a test \nis its true positive rate:   \\\\mathrm{sensitivity} = \\\\frac{\\\\mathrm{number\\\\ of\\\\ true\\\\ positives}}{\\\\mathrm{number\\\\ of\\\\ true\\\\ positives} + \\\\mathrm{number\\\\ of\\\\ false\\\\ negatives}}   The specificity of a test is its true negative rate:   \\\\mathrm{specificity} = \\\\frac{\\\\mathrm{number\\\\ of\\\\ true\\\\ negatives}}{\\\\mathrm{number\\\\ of\\\\ true\\\\ negatives} + \\\\mathrm{number\\\\ of\\\\ false\\\\ positives}}   Calculate both the sensitivity and specificity of the test based on \nnumbers displayed in the NHST table?   The sensitivity is 0.89 and the specificity is 0.85.  Calculate and interpret the results of Tukey  In a situation were you do multiple pairwise comparisons, the \nprobability of type I errors in the process inflates substantially. \nTherefore, it is better to build in adjustments to take this into \naccount. This is what Tukey tests and other post-hoc procedures do. They \nadjust the p-value to prevent inflation of the type I error rate.Use \nTukey s procedure.  # Read in the data set and assign to the object\nwm  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm', header = TRUE, startCol = 1, startRow = 1)\n\n# This will print the data set in the console\nhead(wm)  # Revision: Analysis of variance\nanova_wm  - aov(wm$gain ~ wm$cond)\n\n# Summary Analysis of Variance\nsummary(anova_wm)\n\n# Post-hoc (Tukey)\nTukeyHSD(anova_wm)\n\n# Plot confidence intervals\n#plot(c(TukeyHSD(anova_wm)$lwr,TukeyHSD(anova_wm)$upr))\nplot(TukeyHSD(anova_wm))  Bonferroni adjusted p-values  Just like Tukey s procedure, the Bonferroni correction is a method that \nis used to counteract the problem of inflated type I errors while \nengaging in multiple pairwise comparisons between subgroups. Bonferroni \nis generally known as the most conservative method to control the \nfamilywise error rate.  Bonferroni is based on the idea that if you test  N  dependent or \nindependent hypotheses, one way of maintaining the familywise error rate \nis to test each individual hypothesis at a statistical significance \nlevel that is deflated by a factor of  \\\\frac{1}{n} . So, for a \nsignificance level for the whole family of tests of  \u03b1 , the Bonferroni \ncorrection would be to test each of the individual tests at a \nsignificance level of  \\\\frac{\\\\alpha}{n} .  The Bonferroni correction is controversial. It is a strict measure to \nlimit false positives and generates conservative p-value. Alternative: \nincrease the sample size, compute the false discovery rate (the expected \npercent of false predictions in the set of predictions. For example if \nthe algorithm returns 100 results with a false discovery rate of .3 then \nwe should expect 70 of them to be correct.), and the Holm-Bonferroni \nmethod.  # Use `p.adjust` \nbonferroni_ex  - p.adjust(.005, method='bonferroni', n = 8)\n\nbonferroni_ex\n\n# Pairwise T-test\npairwise.t.test(wm$gain,wm$cond, p.adjust = 'bonferroni')", 
            "title": "2, Post-hoc analysis"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/#3-between-groups-factorial-anova", 
            "text": "Data exploration with a barplot  We ll use in this chapter is a randomized controlled experiment \ninvestigating the effects of talking on a cell phone while driving. The \ndependent variable in the experiment is the number of driving errors \nthat subjects made in a driving simulator. There are two independent \nvariables:   Conversation difficulty: None, Low, High  Driving difficulty: Easy, Difficult    # Read in the data set and assign to the object\nab  - readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'ab', header = TRUE, startCol = 1, startRow = 1)\n\nab$subject  - as.integer(ab$subject)\nab$conversation  - as.factor(ab$conversation)\nab$driving  - as.factor(ab$driving)\nab$error  - as.integer(ab$error)\n\n# This will print the data set in the console\nhead(ab)  Each of the subjects was randomly assigned to one of six conditions \nformed by combining different values of the independent variables.   subject : unique identifier for each subject conversation: level of \n    conversation difficulty.  driving  : level of driving difficulty in the simulator.  errors : number of driving errors made.    # Use the tapply function to create your groups\nab_groups  - tapply(ab$errors, list(ab$driving, ab$conversation), sum)\n\n# Make the required barplot\nbarplot(ab_groups, beside = TRUE, col = c('orange','blue'), main = 'Driving Errors', xlab = 'Conversation Demands', ylab = 'Errors')\n\n# Add the legend\nlegend('topright', c('Difficult','Easy'), title = 'Driving', fill = c('orange','blue'))  The driving errors made during different driving conditions are \ninfluenced by the level of conversation demand. In other words, the \ndriving conditions have a different effect on the number of errors made, \ndepending on the level of conversation demand.  The homogeneity of variance assumption  Before we do factorial ANOVA, we need to test the homogeneity of the \nvariance assumption. When studying one-way ANOVA, we tested this \nassumption with the  leveneTest  function.  We now have two independent variables instead of just one.  # Test the homogeneity of variance assumption\nleveneTest(ab$errors ~ ab$conversation * ab$driving)  The homogeneity of variance assumption holds.  By performing a  leveneTest , we can check whether or not the \nhomogeneity of variance assumption holds for a given dataset. The \nassumption must hold for the results of an ANOVA analysis to be valid.  Recall from the first chapter that ANOVA makes use of F-statistics, or \nF-ratios, in which two types of degrees of freedom are involved.  dim(ab)\nstr(ab$conversation)\nstr(ab$driving)  There are 120 subjets and 2 (Easy, Difficult) * 3 (High Demand, Low \nDemand, None) = 6 groups.  The factorial ANOVA  # Factorial ANOVA\nab_model  - aov(ab$errors ~ ab$conversation * ab$driving)\n\n# Get the summary table\nsummary(ab_model)  Based on the summary table of the factorial ANOVA, the main effect for \ndriving difficulty, the main effect for conversation difficulty and the \ninteraction effect are all significant.  The interaction effect  Now it s time to explore the interaction effect. You will do this with \nthe help of a simple effects analysis.  Why a simple effects analysis? Well, remember what we had to do when you \nhad a significant main effect in a one-way ANOVA? There, you just had to \nperform some post-hoc tests to see from which level of the categorical \nvariable the main effect was coming. With an interaction effect, it is \nquite similar. Conduct a simple effects analysis of the variable  conversation  on the outcome variable  errors  at each level of  driving .  # Create the two subsets\nab_1  - subset(ab, ab$driving == 'Easy')  \nab_2  - subset(ab, ab$driving == 'Difficult')\n\n# Perform the one-way ANOVA for both subsets\naov_ab_1  - aov(ab_1$errors ~ ab_1$conversation)\naov_ab_2  - aov(ab_2$errors ~ ab_2$conversation)\n\n# Get the summary tables for both aov_ab_1 and aov_ab_2\nsummary(aov_ab_1)\nsummary(aov_ab_2)  There a significant simple effect for the easy driving condition based \non the summary table of  aov_ab_1 .  There a significant simple effect for the difficult driving condition \nbased on the summary table of  aov_ab_2 .  The effect sizes  The definition of an interaction effect states that the effect of one \nvariable changes across levels of the other variable. For example, we \nmight expect the effect of conversation to be greater when driving \nconditions are difficult than when they are relatively easy.  Unfortunately, it is not quite that simple. In order to really \nunderstand the different effect sizes, you should make use of the  etaSquared  function.  library(lsr)\n\n# Calculate the etaSquared for the easy driving case\n#easy is ab_1\n#difficult is ab_2\netaSquared(aov_ab_1, anova = TRUE)\n\n# Calculate the etaSquared for the difficult driving case\netaSquared(aov_ab_2, anova = TRUE)  Based on the output of the  etaSquared  function for the easy driving \ncondition, the percentage of variance explained by the conversation \nvariable is 14.7%; the percentage of variance explained by the \nconversation variable is 57.8%.  Pairwise comparisons  Finally, let us look at pairwise comparisons for the simple effects. You \ncan do this with the Tukey post-hoc test.  # Tukey for easy driving\nTukeyHSD(aov_ab_1)\n\n# Tukey for difficult driving\nTukeyHSD(aov_ab_2)  For  Easy Driving , two mean differences in terms of number of errors \nare significant (below 0.05).  For  Difficult Driving , three mean differences in terms of number of \nerrors are significant (below 0.05).", 
            "title": "3, Between groups factorial ANOVA"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Four,_Repeated_Measures_Anova/", 
            "text": "1, An introduction to repeated measures\n\n\n2, Repeated measures ANOVA\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n1, An introduction to repeated measures\n\n\nThe independent t-test is analogous to between-groups ANOVA and the\n\npaired-sample t-test is analogous to repeated measures ANOVA.\n\n\nIn a between-groups design, each subject is exposed to two or more\n\ntreatments or conditions over time. In a within-subjects design, each\n\nsubject is allocated to exactly one treatment or condition.\n\n\nBetween:\n\n\n\n\n\n\nExperiment 1: You want to test the effect of alcohol on test scores\n\n    of students. There are three conditions: the student consumed no\n\n    alcohol, two glasses of beer, or five glasses of beer. Alcohol\n\n    tolerance and time spent studying should also be considered somehow.\n\n\n\n\n\n\nExperiment 2: You want to investigate the effects of certain\n\n    fertilizers on plant growth. Assume you have two different\n\n    fertilizers, A and B. Consider three conditions: you give the plant\n\n    no fertilizer, fertilizer A, or fertilizer B. You measure the height\n\n    of the plant after a specific period of time to see whether the\n\n    fertilizers had an effect.\n\n\n\n\n\n\nUse a within-subjects design for for Experiment 1 and between-groups\n\ndesign for Experiment 2.\n\n\nIs it always either manipulation between-groups or manipulation\n\nwithin-groups, or are there experiments where you could use either\n\napproach? In some cases, either approach is possible.\n\n\nExplore the working memory data\n\n\nlibrary(XLConnectJars)\nlibrary(XLConnect)\n\n# Read in the data set and assign to the object\nwm \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'w__m', header = TRUE, startCol = 1, startRow = 1)\n\nwm$condition \n- as.factor(wm$condition)\n\n# Check it out\nstr(wm)\n\n\n\n\nlibrary(psych)\nlibrary(ggplot2)\n\n# Define the variable subject as a categorical variable\nwm$subject \n- factor(wm$subject)\n\n# Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions)\ndescribeBy(wm, wm$condition)\n\n# Boxplot IQ versus condition\nboxplot(wm$iq ~ wm$condition, main = 'Boxplot', xlab = 'Training sessions', ylab = 'IQ')\n\n# Illustration data, each line represents the development of each subject by number of trainings\nggplot(data = wm, aes(x = wm$condition, y = wm$iq, group = wm$subject, colour = wm$subject)) + geom_line() + geom_point()\n\n\n\n\nReduced cost\n\n\nThe cost advantage of using manipulation within groups verses\n\nmanipulation between groups for the working memory experiment is you\n\nneed 60 subjects fewer.\n\n\nStatistically more powerful\n\n\nRepeated measures analysis accounts for individual differences across\n\nthe experiment. This reduces the error term, which increases statistical\n\npower.\n\n\nCounterbalancing\n\n\nSuppose you have three levels of an independent variable A (i.e. A1, A2,\n\nA3) and a blocked design. You want to use full counterbalancing to take\n\ninto account order effects.\n\n\nWhat are all the possible orders that you need to use? In other words,\n\nwhat are the order conditions?\n\n\n(A1, A2, A3), (A1, A3, A2) , (A2, A1, A3), (A2, A3, A1) , (A3, A2, A1),\n\n(A3, A1, A2)\n\n\nNumber of order conditions?\n\n\nAssume the number of levels of the independent variable goes up and you\n\nwant to completely counterbalance. What will happen to the number of\n\norder conditions you\nll need?\n\n\nThe number becomes really large. An independent variable with n levels\n\nwill have \nn!=n*(n???1)*(n???2)*...\n order conditions.\n\n\nLatin Squares design\n\n\nAs you hopefully realized in the previous exercise, completely\n\ncounterbalancing is not always a practical solution for taking into\n\naccount order effects. This is because the number of different orders\n\nrequired gets really large as the number of possible conditions\n\nincreases.\n\n\nThe most common workaround to this problem is the Latin Squares design,\n\nin which you do not completely counterbalance, but instead put each\n\ncondition at every position (at least) once.\n\n\nWhich of the following examples has been constructed according to the\n\nLatin Squares design?\n\n\n(A1, A2, A3), (A2, A3, A1), (A3, A1, A2)\n\n\nMore on Latin Squares\n\n\nThe number of order conditions is always equal to the number of levels\n\nof your independent variable.\n\n\nWhy is missing data a problem?\n\n\nIn a between-groups design, it is okay to have a slightly different\n\nnumber of subjects in each group, so if one subject drops out in one of\n\nthe conditions then that group has just one less subject. Now you want\n\nto look at how subjects change over time and the different scores\n\nbetween two or more conditions for each subject.\n\n\nUnderstanding sphericity\n\n\nThe variances of the differences between all possible pairs of groups\n\n(i.e. levels of the independent variable) are equal.\n\n\nMauchly\ns test\n\n\n# Define iq as a data frame where each column represents a condition\niq \n- cbind(wm$iq[wm$condition == '8 days'], wm$iq[wm$condition == '12 days'], wm$iq[wm$condition == '17 days'], wm$iq[wm$condition == '19 days'])\n\n# Make an mlm object\nmlm \n- lm(iq ~ 1)\n\n# Mauchly's test\nmauchly.test(mlm, x = ~ 1)\n\n\n\n\nBased on the results, the sphericity assumption holds.\n\n\nPros of repeated measures\n\n\nLess cost and statistically more powerful\n\n\nCons of repeated measures\n\n\nOrder effects, counterbalancing, missing data, and an extra assumption\n\n\n2, Repeated measures ANOVA\n\n\nThe systematic between groups variance\n\n\nTo understand everything a bit better, we will calculate the F-ratio for\n\na repeated measures design by ourself in the next exercises.\n\n\nFirst, we will need the systematic between-groups variance. This is the\n\nsame as in the between-groups design\nthe variance due to grouping by\n\ncondition.\n\n\n# Define number of subjects for each condition\nn \n- 20\n\n# Calculate group means\ny_j \n- tapply(wm$iq, wm$condition, mean)\n\n# Calculate the grand mean\ny_t \n- mean(y_j)\n\n# Calculate the sum of squares\nss_cond \n- sum((y_j - y_t)^2)*n\n\n# Define the degrees of freedom for conditions\ndf \n- (4 - 1)\n\n# Calculate the mean squares (variance)\nms_cond \n- ss_cond/df\n\n\n\n\nThe subject variance\n\n\nWe will also need the error term of the repeated measures design. This\n\ncan be calculated in a few steps.\n\n\n\n\nFirst calculate the systematic variance due to subjects.\n\n\nBelow, we will calculate the unsystematic variance, like we did with\n\n    the between-groups design.\n\n\nIf we subtract these two results, we will get the error term of the\n\n    repeated measures design.\n\n\n\n\nThe systematic (stable) subject variance will be taken out of the error\n\nterm, so the error term is reduced in comparison with the between-groups\n\ndesign.\n\n\n# Define number of conditions for each subject\nn \n- 4\n\n# Calculate subject means\ny_j \n- tapply(wm$iq, wm$subject, mean)\n\n# Calculate the grand mean\ny_t \n- mean(y_j)\n\n# Calculate the sum of squares\nss_subjects \n- sum((y_j - y_t)^2)*n\n\n# Define the degrees of freedom for subjects\ndf \n- (20 - 1)\n\n# Calculate the mean squares (variance)\nms_subjects \n- ss_subjects/df\n\n\n\n\nThe unsystematic within groups variance\n\n\nTo calculate the error term of the repeated measures design, we need the\n\nunsystematic within-groups variance: the unsystematic variance or the\n\nerror term of the between-groups design.\n\n\n# Create four subsets of the four groups, containing the IQ results\n    # Make the subset for the group condition = \n8 days\n\ny_i1 \n- subset(wm$iq, wm$condition == \n8 days\n)\n    # Make the subset for the group condition = \n12 days\n\ny_i2 \n- subset(wm$iq, wm$condition == \n12 days\n)\n    # Make the subset for the group condition = \n17 days\n\ny_i3 \n- subset(wm$iq, wm$condition == \n17 days\n)\n    # Make the subset for the group condition = \n19 days\n\ny_i4 \n- subset(wm$iq, wm$condition == \n19 days\n)\n\n# Subtract the individual values by their group means\ns_1 \n- y_i1 - mean(y_i1)\ns_2 \n- y_i2 - mean(y_i2)\ns_3 \n- y_i3 - mean(y_i3)\ns_4 \n- y_i4 - mean(y_i4)\n\n# Put everything back into one vector\ns_t \n- c(s_1, s_2, s_3, s_4)\n\n# Calculate the within sum of squares by using the vector s_t\nss_sa \n- sum(s_t^2)\n\n# Define the degrees of freedom\ndf \n- 4*(20-1)\n\n# Calculate the mean squares (variances)\nms_sa \n- ss_sa/df\n\n\n\n\nThe unsystematic variance for the repeated measures design\n\n\nNow we can easily calculate the unsystematic variance for the repeated\n\nmeasures design, also called the error term.\n\n\n# ss_sa = ss_subjects + ss_rm\nss_rm \n- ss_sa - ss_subjects\n\n# Define the degrees of freedom\ndf \n- (20 - 1)*(4 - 1)\n\n# Calculate the mean squares (variances)\nms_rm \n- ss_rm/df\n\n\n\n\nNow we\nve calculated the error term of the repeated measures design,\n\nwhich is clearly smaller than the error term of the between-groups\n\ndesign because you reduced this one. To complete you can now calculate\n\nthe F-ratio and the corresponding p-value.\n\n\nF-ratio and p-value\n\n\nTo do the ANOVA analysis we actually need the F-ratio and the\n\ncorresponding p-value.\n\n\n# Calculate the F-ratio\nf_rat \n- ms_cond/ms_rm\n\n# Define the degrees of freedom of the F-distribution\n# df1 for freedom of conditions (ss_cond)\n# df2 for (freedom of) conditions and subjects (sa_sa)\ndf1 \n- (4 - 1)\ndf2 \n- (4 - 1)*(20 - 1)\n\n# Calculate the p-value\np \n- 1 - pf(f_rat, df1, df2)\n\n\n\n\nError term in a repeated measures design?\n\n\nThe inconsistent individual differences across conditions, the effect of\n\nsubjects that differ across conditions. So it is an interaction between\n\nsubjects and condition.\n\n\nAnova in R\n\n\nANOVA in R is usually done with the \naov\n function.\n\n\n# anova model\nmodel \n- aov(wm$iq ~ wm$condition + Error(wm$subject / wm$condition))\n\n# summary model\nsummary(model)\n\n\n\n\nThe F-ratio is significant. Therefore, the number of training days does\n\naffect the IQ scores.\n\n\nEffect size\n\n\nCalculate eta-squared, which helps you estimate effect size.\n\n\n# Define the total sum of squares\n\n# ss_cond (syst. between groups or of the effect), \n# ss_sa (unsyst. within groups) =\n# ss_rm (unsyst. for repeated measures design) + # ss_subjects (ok)\nss_total \n- ss_cond + ss_rm\n\n# Calculate the effect size\neta_sq \n- ss_cond / ss_total\n\n\n\n\nPost-hoc test one\n\n\nWe will now look at a few different procedures for the post-hoc test.\n\nLet\ns start with the Holm procedure.\n\n\n# Post-hoc test: default procedure\nwith(wm, pairwise.t.test(iq, condition, paired = T))\n\n\n\n\nWe get a table with some values as the output for the post-hoc test and\n\na line saying that we have used the Holm procedure. What are the values\n\nin the table of the output? p-values.\n\n\nRecall that the hypotheses are tested at a 5% significance level.\n\n\nWe can conclude that all pairwise comparisons are significant, except\n\nfor the comparison between 19 and 17 and the comparison between 12 and\n\n8.\n\n\nPost-hoc test: Bonferroni\n\n\nNow we\nll take a look at the most conservative procedure, Bonferroni.\n\nThis procedure will apply the most extreme adjustments to the p-values.\n\n\n# Post-hoc test: Bonferroni procedure\nwith(wm, pairwise.t.test(iq, condition, paired = TRUE, p.adjust.method = 'bonferroni'))\n\n\n\n\nNotice the change in p-values in comparison with the previous procedure.\n\n\nPaired t-test\n\n\nAssume that you do not know how to perform an analysis of variance\n\n(ANOVA), we may do a number of paired t-tests instead.\n\n\nHave a look at just one paired t-test and take, for example, the\n\ncomparison between 12 days and 17 days.\n\n\n# Define two subsets containing the IQ scores for the condition group '12 days' and '17 days'\ncond_12days \n- subset(wm, condition == '12 days')$iq\ncond_17days \n- subset(wm, condition == '17 days')$iq\ncond_12days\n\n# t-test\nt.test(cond_12days, cond_17days, paired = TRUE)\n\n\n\n\nIt is clear that we can apply different procedures for post-hoc tests.\n\nThese procedures differ with respect to how they handle inflation of the\n\npossibility of a type I error and will therefore give us different\n\np-values. However, these p-values will always be in a certain range.\n\n\nWhat is the (smallest) range of p-values for the comparison between 12\n\ndays and 17 days? Look at the results from applying the Bonferroni\n\nprocedure as well as the paired t-test.\n\n\np-values:\n\n\n\n\nBonferroni 12 days-17 days: 0.03910\n\n\nPaired t-test (cond_12days, cond_17days): 0.006517\n\n\n\n\nTherefore: \n0.0065, 0.0391\n.", 
            "title": "Statistics with R, Course Four, Repeated Measures ANOVA"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Four,_Repeated_Measures_Anova/#2-repeated-measures-anova", 
            "text": "The systematic between groups variance  To understand everything a bit better, we will calculate the F-ratio for \na repeated measures design by ourself in the next exercises.  First, we will need the systematic between-groups variance. This is the \nsame as in the between-groups design the variance due to grouping by \ncondition.  # Define number of subjects for each condition\nn  - 20\n\n# Calculate group means\ny_j  - tapply(wm$iq, wm$condition, mean)\n\n# Calculate the grand mean\ny_t  - mean(y_j)\n\n# Calculate the sum of squares\nss_cond  - sum((y_j - y_t)^2)*n\n\n# Define the degrees of freedom for conditions\ndf  - (4 - 1)\n\n# Calculate the mean squares (variance)\nms_cond  - ss_cond/df  The subject variance  We will also need the error term of the repeated measures design. This \ncan be calculated in a few steps.   First calculate the systematic variance due to subjects.  Below, we will calculate the unsystematic variance, like we did with \n    the between-groups design.  If we subtract these two results, we will get the error term of the \n    repeated measures design.   The systematic (stable) subject variance will be taken out of the error \nterm, so the error term is reduced in comparison with the between-groups \ndesign.  # Define number of conditions for each subject\nn  - 4\n\n# Calculate subject means\ny_j  - tapply(wm$iq, wm$subject, mean)\n\n# Calculate the grand mean\ny_t  - mean(y_j)\n\n# Calculate the sum of squares\nss_subjects  - sum((y_j - y_t)^2)*n\n\n# Define the degrees of freedom for subjects\ndf  - (20 - 1)\n\n# Calculate the mean squares (variance)\nms_subjects  - ss_subjects/df  The unsystematic within groups variance  To calculate the error term of the repeated measures design, we need the \nunsystematic within-groups variance: the unsystematic variance or the \nerror term of the between-groups design.  # Create four subsets of the four groups, containing the IQ results\n    # Make the subset for the group condition =  8 days \ny_i1  - subset(wm$iq, wm$condition ==  8 days )\n    # Make the subset for the group condition =  12 days \ny_i2  - subset(wm$iq, wm$condition ==  12 days )\n    # Make the subset for the group condition =  17 days \ny_i3  - subset(wm$iq, wm$condition ==  17 days )\n    # Make the subset for the group condition =  19 days \ny_i4  - subset(wm$iq, wm$condition ==  19 days )\n\n# Subtract the individual values by their group means\ns_1  - y_i1 - mean(y_i1)\ns_2  - y_i2 - mean(y_i2)\ns_3  - y_i3 - mean(y_i3)\ns_4  - y_i4 - mean(y_i4)\n\n# Put everything back into one vector\ns_t  - c(s_1, s_2, s_3, s_4)\n\n# Calculate the within sum of squares by using the vector s_t\nss_sa  - sum(s_t^2)\n\n# Define the degrees of freedom\ndf  - 4*(20-1)\n\n# Calculate the mean squares (variances)\nms_sa  - ss_sa/df  The unsystematic variance for the repeated measures design  Now we can easily calculate the unsystematic variance for the repeated \nmeasures design, also called the error term.  # ss_sa = ss_subjects + ss_rm\nss_rm  - ss_sa - ss_subjects\n\n# Define the degrees of freedom\ndf  - (20 - 1)*(4 - 1)\n\n# Calculate the mean squares (variances)\nms_rm  - ss_rm/df  Now we ve calculated the error term of the repeated measures design, \nwhich is clearly smaller than the error term of the between-groups \ndesign because you reduced this one. To complete you can now calculate \nthe F-ratio and the corresponding p-value.  F-ratio and p-value  To do the ANOVA analysis we actually need the F-ratio and the \ncorresponding p-value.  # Calculate the F-ratio\nf_rat  - ms_cond/ms_rm\n\n# Define the degrees of freedom of the F-distribution\n# df1 for freedom of conditions (ss_cond)\n# df2 for (freedom of) conditions and subjects (sa_sa)\ndf1  - (4 - 1)\ndf2  - (4 - 1)*(20 - 1)\n\n# Calculate the p-value\np  - 1 - pf(f_rat, df1, df2)  Error term in a repeated measures design?  The inconsistent individual differences across conditions, the effect of \nsubjects that differ across conditions. So it is an interaction between \nsubjects and condition.  Anova in R  ANOVA in R is usually done with the  aov  function.  # anova model\nmodel  - aov(wm$iq ~ wm$condition + Error(wm$subject / wm$condition))\n\n# summary model\nsummary(model)  The F-ratio is significant. Therefore, the number of training days does \naffect the IQ scores.  Effect size  Calculate eta-squared, which helps you estimate effect size.  # Define the total sum of squares\n\n# ss_cond (syst. between groups or of the effect), \n# ss_sa (unsyst. within groups) =\n# ss_rm (unsyst. for repeated measures design) + # ss_subjects (ok)\nss_total  - ss_cond + ss_rm\n\n# Calculate the effect size\neta_sq  - ss_cond / ss_total  Post-hoc test one  We will now look at a few different procedures for the post-hoc test. \nLet s start with the Holm procedure.  # Post-hoc test: default procedure\nwith(wm, pairwise.t.test(iq, condition, paired = T))  We get a table with some values as the output for the post-hoc test and \na line saying that we have used the Holm procedure. What are the values \nin the table of the output? p-values.  Recall that the hypotheses are tested at a 5% significance level.  We can conclude that all pairwise comparisons are significant, except \nfor the comparison between 19 and 17 and the comparison between 12 and \n8.  Post-hoc test: Bonferroni  Now we ll take a look at the most conservative procedure, Bonferroni. \nThis procedure will apply the most extreme adjustments to the p-values.  # Post-hoc test: Bonferroni procedure\nwith(wm, pairwise.t.test(iq, condition, paired = TRUE, p.adjust.method = 'bonferroni'))  Notice the change in p-values in comparison with the previous procedure.  Paired t-test  Assume that you do not know how to perform an analysis of variance \n(ANOVA), we may do a number of paired t-tests instead.  Have a look at just one paired t-test and take, for example, the \ncomparison between 12 days and 17 days.  # Define two subsets containing the IQ scores for the condition group '12 days' and '17 days'\ncond_12days  - subset(wm, condition == '12 days')$iq\ncond_17days  - subset(wm, condition == '17 days')$iq\ncond_12days\n\n# t-test\nt.test(cond_12days, cond_17days, paired = TRUE)  It is clear that we can apply different procedures for post-hoc tests. \nThese procedures differ with respect to how they handle inflation of the \npossibility of a type I error and will therefore give us different \np-values. However, these p-values will always be in a certain range.  What is the (smallest) range of p-values for the comparison between 12 \ndays and 17 days? Look at the results from applying the Bonferroni \nprocedure as well as the paired t-test.  p-values:   Bonferroni 12 days-17 days: 0.03910  Paired t-test (cond_12days, cond_17days): 0.006517   Therefore:  0.0065, 0.0391 .", 
            "title": "2, Repeated measures ANOVA"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Five,_Correlation_and_Regression/", 
            "text": "1, An introduction to Correlation\n\n\n2, An introduction to Linear Regression Models\n\n\n3, Linear Regression Models continued\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets only.\n\n\n\n\n\n\n1, An introduction to Correlation\n\n\nManual computation of correlation coefficients - Part 1\n\n\nlibrary(XLConnectJars)\nlibrary(XLConnect)\n\n# Read in the data set and assign to the object\nPE \n- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'PE', header = TRUE, startCol = 1, startRow = 1)\n\n# Check it out\nstr(PE)\n\n\n\n\n# Take a quick peek at both vectors\nA \n- PE$activeyears\nB \n- PE$endurance\n\n# Save the differences of each vector element with the mean in a new variable\ndiff_A \n- A - mean(A)\ndiff_B \n- B - mean(B)\n\n# Do the summation of the elements of the vectors and divide by N-1 in order to acquire the covariance between the two vectors\ncov \n- sum(diff_A*diff_B) / (length(A) - 1)\ncov\n\n\n\n\nManual computation of correlation coefficients - Part 2\n\n\n# Square the differences that were found in the previous step\nsq_diff_A \n- diff_A^2\nsq_diff_B \n- diff_B^2\n\n# Take the sum of the elements, divide them by N-1 and consequently take the square root to acquire the sample standard deviations\nsd_A \n- sqrt(sum(sq_diff_A)/(length(A) - 1))\nsd_B \n- sqrt(sum(sq_diff_B)/(length(B) - 1))\nsd_A\nsd_B\n\n\n\n\nManual computation of correlation coefficients - part 3\n\n\n# Combine all the pieces of the puzzle\ncorrelation \n- cov/(sd_A*sd_B)\ncorrelation\n\n# Check the validity of your result with the cor() command\ncor(A,B)\n\n\n\n\nCreating scatterplots\n\n\nlibrary(psych)\n\n# Summary statistics\ndescribe(PE)\n\n# Scatter plots\npar(mfrow = c(1, 3))\n\nplot(PE$age ~ PE$activeyears)\nplot(PE$endurance ~ PE$activeyears)\nplot(PE$endurance ~ PE$age)\n\npar(mfrow = c(1, 1))\n\n\n\n\nCorrelation matrix\n\n\n# Correlation Analysis \nround(cor(PE[2:4], use = 'pairwise.complete.obs', method = 'pearson'), 2)  \n# Do some correlation tests. If the null hypothesis of no correlation can be rejected on a significance level of 5%, then the relationship between variables is  significantly different from zero at the 95% confidence level\n\ncor.test(PE$age, PE$activeyears)\ncor.test(PE$age, PE$endurance)\ncor.test(PE$endurance, PE$activeyears)\n\n\n\n\nCAUTION:\n\n\n\n\nThe magnitude of a correlation depends upon many factors,\n\n    including sampling.\n\n\nThe magnitude of a correlation is also influenced by measurement of\n\n    X \n Y.\n\n\nThe correlation coefficient is a sample statistic, just like\n\n    the mean.\n\n\n\n\nNon-representative data samples\n\n\n# Summary statistics entire dataset\ndescribe(impact)\n\n# Calculate correlation coefficient \nentirecorr \n- round(cor(impact$vismem2, impact$vermem2),2)\n\n# Summary statistics subsets\ndescribeBy(impact, group = impact$condition)\n\n# Create 2 subsets: control and concussed\ncontrol \n- subset(impact, impact$condition == 'control')\nconcussed \n- subset(impact, impact$condition == 'concussed')\n\n# Calculate correlation coefficients for each subset\ncontrolcorr \n- round(cor(control$vismem2, control$vermem2),2)\nconcussedcorr \n- round(cor(concussed$vismem2, concussed$vermem2),2)\n\n# Display all values at the same time\ncorrelations \n- cbind(entirecorr, controlcorr, concussedcorr)\ncorrelations\n\n\n\n\n2, An introduction to Linear Regression Models\n\n\nImpact experiment\n\n\n# Create a correlation matrix for the dataset (9-14 are the '2' variables only)\ncorrelations \n- cor(PE[9:14,])\n\n# Create the scatterplot matrix for the dataset\nlibrary(corrplot)\n\ncorrplot(correlations)\n\n\n\n\nManual computation of a simple linear regression\n\n\n# Calculate the required means, standard deviations and correlation coefficient\nmean_ay \n- mean(PE$activeyears)\nmean_e \n- mean(PE$endurance)\nsd_ay \n- sd(PE$activeyears)\nsd_e \n- sd(PE$endurance)\nr \n- cor(PE$activeyears, PE$endurance)\n\n# Calculate the slope\nB_1 \n- r * (sd_e)/(sd_ay)\n\n# Calculate the intercept\nB_0 \n- mean_e - B_1 * mean_ay\n\n# Plot of ic2 against sym2\nplot(PE$activeyear, PE$endurance, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\n\n# Add the regression line\nabline(B_0, B_1, col = 'red')\n\n\n\n\nExecuting a simple linear regression using R\n\n\n# Construct the regression model\nmodel_1 \n- lm(PE$endurance ~ PE$activeyear)\n\n# Look at the results of the regression by using the summary function\nsummary(model_1)\n\n# Extract the predicted values\npredicted \n- fitted(model_1)\n\n# Create a scatter plot of Impulse Control against Symptom Score\nplot(PE$endurance ~ PE$activeyear, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\n\n# Add a regression line\nabline(model_1, col = 'red')\nabline(lm(predicted ~ PE$activeyears), col = 'green', lty = 2)\n\n\n\n\nExecuting a multiple regression in R\n\n\n# Multiple Regression\nmodel_2 \n- lm(PE$endurance ~ PE$activeyear + PE$age)\n\n# Examine the results of the regression\nsummary(model_2)\n\n# Extract the predicted values\npredicted \n- fitted(model_2)\n\n# Plotting predicted scores against observed scores\nplot(predicted ~ PE$activeyears, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\nabline(lm(predicted ~ PE$activeyears), col = 'green')\n\n\n\n\n3, Linear Regression Models continued\n\n\nCalculating the sum of squared residuals\n\n\n# Create a linear regression with `ic2` and `vismem2` as regressors\nmodel_1 \n- lm(PE$endurance ~ PE$activeyear + PE$age)\n\n# Extract the predicted values\npredicted_1 \n- fitted(model_1)\n\n# Calculate the squared deviation of the predicted values from the observed values \ndeviation_1 \n- (predicted_1 - PE$endurance)^2\n\n# Sum the squared deviations\nSSR_1 \n- sum(deviation_1)\nSSR_1\n\n\n\n\nStandardized linear regression\n\n\n# Create a standardized simple linear regression\nmodel_1_z \n- lm(scale(PE$endurance) ~ scale(PE$activeyear))\n\n# Look at the output of this regression model\nsummary(model_1_z)\n\n# Extract the R-Squared value for this regression\nr_square_1 \n- summary(model_1_z)$r.squared\nr_square_1\n\n# Calculate the correlation coefficient\ncorr_coef_1 \n- sqrt(r_square_1)\ncorr_coef_1\n\n# Create a standardized multiple linear regression\nmodel_2_z \n- lm(scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age))\n\n# Look at the output of this regression model\nsummary(model_2_z)\n\n# Extract the R-Squared value for this regression\nr_square_2 \n- summary(model_2_z)$r.squared\nr_square_2\n\n# Calculate the correlation coefficient\ncorr_coef_2 \n- sqrt(r_square_2)\ncorr_coef_2\n\n\n\n\nAssumptions of linear regression:\n\n\n\n\nNormal distribution for Y.\n\n\nLinear relationship between X and Y.\n\n\nHomoscedasticity.\n\n\nReliability of X and Y.\n\n\nValidity of X and Y.\n\n\nRandom and representative sampling.\n\n\n\n\nCheck it out with Anscombe\ns quartet plots.\n\n\nPlotting residuals\n\n\n# Extract the residuals from the model\nresidual \n- resid(model_2)\n\n# Draw a histogram of the residuals\nhist(residual)\n\n# Extract the predicted symptom scores from the model\npredicted \n- fitted(model_2)\n\n# Plot the residuals against the predicted symptom scores\nplot(residual ~ predicted, main = 'Scatterplot', ylab = 'Model 2 Residuals', xlab = 'Model 2 Predicted Scores')\nabline(lm(residual ~ predicted), col = 'red')", 
            "title": "Statistics with R, Course Five, Correlation and Regression"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Five,_Correlation_and_Regression/#2-an-introduction-to-linear-regression-models", 
            "text": "Impact experiment  # Create a correlation matrix for the dataset (9-14 are the '2' variables only)\ncorrelations  - cor(PE[9:14,])\n\n# Create the scatterplot matrix for the dataset\nlibrary(corrplot)\n\ncorrplot(correlations)  Manual computation of a simple linear regression  # Calculate the required means, standard deviations and correlation coefficient\nmean_ay  - mean(PE$activeyears)\nmean_e  - mean(PE$endurance)\nsd_ay  - sd(PE$activeyears)\nsd_e  - sd(PE$endurance)\nr  - cor(PE$activeyears, PE$endurance)\n\n# Calculate the slope\nB_1  - r * (sd_e)/(sd_ay)\n\n# Calculate the intercept\nB_0  - mean_e - B_1 * mean_ay\n\n# Plot of ic2 against sym2\nplot(PE$activeyear, PE$endurance, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\n\n# Add the regression line\nabline(B_0, B_1, col = 'red')  Executing a simple linear regression using R  # Construct the regression model\nmodel_1  - lm(PE$endurance ~ PE$activeyear)\n\n# Look at the results of the regression by using the summary function\nsummary(model_1)\n\n# Extract the predicted values\npredicted  - fitted(model_1)\n\n# Create a scatter plot of Impulse Control against Symptom Score\nplot(PE$endurance ~ PE$activeyear, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\n\n# Add a regression line\nabline(model_1, col = 'red')\nabline(lm(predicted ~ PE$activeyears), col = 'green', lty = 2)  Executing a multiple regression in R  # Multiple Regression\nmodel_2  - lm(PE$endurance ~ PE$activeyear + PE$age)\n\n# Examine the results of the regression\nsummary(model_2)\n\n# Extract the predicted values\npredicted  - fitted(model_2)\n\n# Plotting predicted scores against observed scores\nplot(predicted ~ PE$activeyears, main = 'Scatterplot', ylab = 'Endurance', xlab = 'Active Years')\nabline(lm(predicted ~ PE$activeyears), col = 'green')", 
            "title": "2, An introduction to Linear Regression Models"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Five,_Correlation_and_Regression/#3-linear-regression-models-continued", 
            "text": "Calculating the sum of squared residuals  # Create a linear regression with `ic2` and `vismem2` as regressors\nmodel_1  - lm(PE$endurance ~ PE$activeyear + PE$age)\n\n# Extract the predicted values\npredicted_1  - fitted(model_1)\n\n# Calculate the squared deviation of the predicted values from the observed values \ndeviation_1  - (predicted_1 - PE$endurance)^2\n\n# Sum the squared deviations\nSSR_1  - sum(deviation_1)\nSSR_1  Standardized linear regression  # Create a standardized simple linear regression\nmodel_1_z  - lm(scale(PE$endurance) ~ scale(PE$activeyear))\n\n# Look at the output of this regression model\nsummary(model_1_z)\n\n# Extract the R-Squared value for this regression\nr_square_1  - summary(model_1_z)$r.squared\nr_square_1\n\n# Calculate the correlation coefficient\ncorr_coef_1  - sqrt(r_square_1)\ncorr_coef_1\n\n# Create a standardized multiple linear regression\nmodel_2_z  - lm(scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age))\n\n# Look at the output of this regression model\nsummary(model_2_z)\n\n# Extract the R-Squared value for this regression\nr_square_2  - summary(model_2_z)$r.squared\nr_square_2\n\n# Calculate the correlation coefficient\ncorr_coef_2  - sqrt(r_square_2)\ncorr_coef_2  Assumptions of linear regression:   Normal distribution for Y.  Linear relationship between X and Y.  Homoscedasticity.  Reliability of X and Y.  Validity of X and Y.  Random and representative sampling.   Check it out with Anscombe s quartet plots.  Plotting residuals  # Extract the residuals from the model\nresidual  - resid(model_2)\n\n# Draw a histogram of the residuals\nhist(residual)\n\n# Extract the predicted symptom scores from the model\npredicted  - fitted(model_2)\n\n# Plot the residuals against the predicted symptom scores\nplot(residual ~ predicted, main = 'Scatterplot', ylab = 'Model 2 Residuals', xlab = 'Model 2 Predicted Scores')\nabline(lm(residual ~ predicted), col = 'red')", 
            "title": "3, Linear Regression Models continued"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Six,_Multiple_Regression/", 
            "text": "1, A gentle introduction to the principles of multiple regression\n\n\n2, Intuition behind estimation of multiple regression coefficients\n\n\n3, Dummy coding\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets only.\n\n\n\n\n\n\n1, A gentle introduction to the principles of multiple regression\n\n\nMultiple regression: visualization of the relationships\n\n\n# Perform the two single regressions and save them in a variable\n#model_years \n- lm(salary ~ years, data = fs)\n#model_pubs \n-  lm(salary ~ pubs, data = fs)\nmodel_years \n- lm(fs$salary ~ fs$years)\nmodel_pubs \n- lm(fs$salary ~ fs$pubs)\n\n# Plot both enhanced scatter plots in one plot matrix of 1 by 2\npar(mfrow = c(1, 2))\n\n#plot(fs$years, fs$salary, main = 'plot_years', xlab = 'years', ylab = 'salary')\nplot(fs$salary ~ fs$years, main = 'plot_years', xlab = 'years', ylab = 'salary')\nabline(model_years)\n\n#plot(fs$pubs, fs$salary, main = 'plot_pubs', xlab = 'pubs', ylab = 'salary')\nplot(fs$salary ~ fs$pubs, main = 'plot_pubs', xlab = 'pubs', ylab = 'salary')\nabline(model_pubs)\n\n\n\n\nMultiple regression: model selection\n\n\n# Do a single regression of salary onto years of experience and check the output\nmodel_1 \n- lm(fs$salary ~ fs$years)\nsummary(model_1)\n\n# Do a multiple regression of salary onto years of experience and numbers of publications and check the output\nmodel_2 \n- lm(fs$salary ~ fs$years + fs$pubs)\nsummary(model_2)\n\n# Save the R squared of both models in preliminary variables\npreliminary_model_1 \n- summary(model_1)$r.squared\npreliminary_model_2 \n- summary(model_2)$r.squared\n\n# Round them off while you save them in new variables\nr_squared \n- c()\nr_squared[1] \n- round(preliminary_model_1, 3)\nr_squared[2] \n- round(preliminary_model_2, 3)\n\n# Print out the vector to see both R squared coefficients\nr_squared\n\n\n\n\nMultiple regression: beware of redundancy\n\n\n# Do multiple regression and check the regression output\nmodel_3 \n- lm(fs$salary ~ fs$years + fs$pubs + fs$age)\nsummary(model_3)\n\n# Round off the R squared coefficients and save the result in the vector (in one step!)\nr_squared[3] \n- round(summary(model_3)$r.squared,3)\n\n# Print out the vector in order to display all R squared coefficients simultaneously\nr_squared\n\n\n\n\n2, Intuition behind estimation of multiple regression coefficients\n\n\nDefinition of matrices\n\n\n# Construction of 3 by 8 matrix r that contains the numbers 1 up to 24\nr \n- matrix(seq(1,24), 3)\n\n# Construction of 3 by 8 matrix s that contains the numbers 21 up to 44\ns \n- matrix(seq(21,44), 3)\n\n# Take the transpose t of matrix r\nt \n- t(r)\n\n\n\n\nAddition, subtraction and multiplication of matrices\n\n\n# Compute the sum of matrices r and s\noperation_1 \n- r + s\n\n# Compute the difference between matrices r and s\noperation_2 \n- r - s\n\n# Multiply matrices t and s\noperation_3 \n- t %*% s\n\n\n\n\nRow vector of sums\n\n\n# The raw dataframe `X` is already loaded in.\nX\n\n# Construction of 1 by 10 matrix I of which the elements are all 1\nI \n- matrix(rep(1,10), 1, 10)\n\n# Compute the row vector of sums\nt_mat \n- I %*% X\n\n\n\n\nRow vector of means and matrix of means\n\n\n# The data matrix `X` and the row vector of sums (`T`) are saved and can be used.\n# Number of observations\nn = 10\n\n# Compute the row vector of means\n# you summed up the row, you divide by the nrow to compute the average\nM \n- t_mat / 10\n\n# Construction of 10 by 1 matrix J of which the elements are all 1\nJ \n- matrix(rep(1,10), 10, 1)\n\n# Compute the matrix of means \nMM \n- J %*% M\n\n\n\n\nMatrix of deviation scores\n\n\n# The previously generated matrices X, M and MM do not need to be constructed again but are saved and can be used.\n\n# Matrix of deviation scores D \nD \n- X - MM\n\n\n\n\nSum of squares and sum of cross products matrix\n\n\n# The previously generated matrices X, M, MM and D do not need to be constructed again but are saved and can be used.\n\n# Sum of squares and sum of cross products matrix\nS \n- t(D) %*% D\n\n\n\n\nCalculating the correlation matrix\n\n\n# The previously generated matrices X, M, MM, D and S do not need to be constructed again but are saved and can be used.\nn = 10\n\n# Construct the variance-covariance matrix \nC \n-  S * 1/n\n\n# Generate the standard deviations matrix \nSD \n- diag(x = diag(C)^(1/2), nrow = 3, ncol = 3)\n\n# Compute the correlation matrix\nR \n- solve(SD) %*% C %*% solve(SD)\n\n\n\n\n3, Dummy coding\n\n\nStarting off\n\n\n# Summary statistics\ndescribeBy(fs, fs$dept)\n\n\n\n\nA system to code categorical predictors in a regression analysis\n\n\nSuppose we have a categorical vector of observations. The vector counts\n\n4 distinct groups. Here is how to assign the dummy variables:\n\n\n\n\n\n\n\n\nProfID\n\n\nGroup\n\n\nPubs\n\n\nD1\n\n\nD2\n\n\nD3\n\n\n\n\n\n\n\n\n\n\nNU\n\n\nCognitive\n\n\n83\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\nZH\n\n\nClinical\n\n\n74\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n\n\nMK\n\n\nDevelopmental\n\n\n80\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\nRH\n\n\nSocial\n\n\n68\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n\n\n\n\n\n\nGroup\n\n\nM\n\n\nSD\n\n\nN\n\n\n\n\n\n\n\n\n\n\nCognitive\n\n\n93.31\n\n\n29.48\n\n\n13\n\n\n\n\n\n\nClinical\n\n\n60.67\n\n\n11.12\n\n\n8\n\n\n\n\n\n\nDevelopmental\n\n\n103.5\n\n\n23.64\n\n\n6\n\n\n\n\n\n\nSocial\n\n\n70.13\n\n\n21.82\n\n\n9\n\n\n\n\n\n\n\n\n\nModel:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\n(\nD\n1\n)+\n\u03b2\n2\n(\nD\n2\n)+\n\u03b2\n3\n(\nD\n3\n)+\n\u03f5\n\n\nCoefficient:\n\n\n\n\n\n\n\n\n\n\nB\n\n\nSE\n\n\nB\n\n\nt\n\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n93.31\n\n\n6.5\n\n\n0\n\n\n14.37\n\n\n.001\n\n\n\n\n\n\nD1 (Clinical)\n\n\n-32.64\n\n\n10.16\n\n\n-0.51\n\n\n-3.21\n\n\n0.003\n\n\n\n\n\n\nD2 (Devel)\n\n\n10.19\n\n\n11.56\n\n\n0.14\n\n\n0.88\n\n\n0.384\n\n\n\n\n\n\nD3 (Social)\n\n\n-23.18\n\n\n10.52\n\n\n-0.35\n\n\n-2.2\n\n\n0.035\n\n\n\n\n\n\n\n\n\nCreating dummy variables (1)\n\n\n# Create the dummy variables\ndept_code \n- dummy.code(fs$dept)\ndept_code\n\n# Merge the dataset in an extended dataframe\nextended_fs \n- cbind(fs, dept_code)\n\n# Look at the extended dataframe\nextended_fs\n\n# Provide summary statistics\nsummary(extended_fs)\n\n\n\n\nCreating dummy variables (2)\n\n\n# Regress salary against years and publications\nmodel \n- lm(fs$salary ~ fs$years + fs$pubs)\n\n# Apply the summary function to get summarized results for model\nsummary(model)\n\n# Compute the confidence intervals for model\nconfint(model) \n\n# Create dummies for the categorical variable fs$dept by using the C() function\ndept_code \n- C(fs$dept, treatment)\n\n# Regress salary against years, publications and department \nmodel_dummy \n- lm(fs$salary ~ fs$years + fs$pubs + dept_code)\n\n# Apply the summary function to get summarized results for model_dummy\nsummary(model_dummy)\n\n# Compute the confidence intervals for model_dummy\nconfint(model_dummy)\n\n\n\n\nModel selection: ANOVA\n\n\n# Compare model 4 with model3\nanova(model, model_dummy)\n\n\n\n\nDiscrepancy between actual and predicted means\n\n\n# Actual means of fs$salary\ntapply(fs$salary, fs$dept, mean)\n\n\n\n\nUnweighted effects coding\n\n\nConsult the PDF for \nUnweighted Effects Coding\n.\n\n\n# Number of levels\nfs$dept\n\n# Factorize the categorical variable fs$dept and name the factorized variable dept.f\ndept.f \n- factor(fs$dept)\n\n# Assign the 3 levels generated in step 2 to dept.f\ncontrasts(dept.f) \n- contr.sum(3)\n\n# Regress salary against dept.f\nmodel_unweighted \n- lm(fs$salary ~ dept.f)\n\n# Apply the summary() function\nsummary(model_unweighted)\n\n\n\n\nWeighted effects coding\n\n\nConsult \nWeighted Effects Coding\n.\n\n\n# Factorize the categorical variable fs$dept and name the factorized variable dept.g\ndept.g \n- factor(fs$dept)\n\n# Assign the weights matrix to dept.g \ncontrasts(dept.g) \n- weights\n\n# Regress salary against dept.f and apply the summary() function\nmodel_weighted \n- lm(fs$salary ~ dept.g)\n\n# Apply the summary() function\nsummary(model_weighted)", 
            "title": "Statistics with R, Course Six, Multiple Regression"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Six,_Multiple_Regression/#2-intuition-behind-estimation-of-multiple-regression-coefficients", 
            "text": "Definition of matrices  # Construction of 3 by 8 matrix r that contains the numbers 1 up to 24\nr  - matrix(seq(1,24), 3)\n\n# Construction of 3 by 8 matrix s that contains the numbers 21 up to 44\ns  - matrix(seq(21,44), 3)\n\n# Take the transpose t of matrix r\nt  - t(r)  Addition, subtraction and multiplication of matrices  # Compute the sum of matrices r and s\noperation_1  - r + s\n\n# Compute the difference between matrices r and s\noperation_2  - r - s\n\n# Multiply matrices t and s\noperation_3  - t %*% s  Row vector of sums  # The raw dataframe `X` is already loaded in.\nX\n\n# Construction of 1 by 10 matrix I of which the elements are all 1\nI  - matrix(rep(1,10), 1, 10)\n\n# Compute the row vector of sums\nt_mat  - I %*% X  Row vector of means and matrix of means  # The data matrix `X` and the row vector of sums (`T`) are saved and can be used.\n# Number of observations\nn = 10\n\n# Compute the row vector of means\n# you summed up the row, you divide by the nrow to compute the average\nM  - t_mat / 10\n\n# Construction of 10 by 1 matrix J of which the elements are all 1\nJ  - matrix(rep(1,10), 10, 1)\n\n# Compute the matrix of means \nMM  - J %*% M  Matrix of deviation scores  # The previously generated matrices X, M and MM do not need to be constructed again but are saved and can be used.\n\n# Matrix of deviation scores D \nD  - X - MM  Sum of squares and sum of cross products matrix  # The previously generated matrices X, M, MM and D do not need to be constructed again but are saved and can be used.\n\n# Sum of squares and sum of cross products matrix\nS  - t(D) %*% D  Calculating the correlation matrix  # The previously generated matrices X, M, MM, D and S do not need to be constructed again but are saved and can be used.\nn = 10\n\n# Construct the variance-covariance matrix \nC  -  S * 1/n\n\n# Generate the standard deviations matrix \nSD  - diag(x = diag(C)^(1/2), nrow = 3, ncol = 3)\n\n# Compute the correlation matrix\nR  - solve(SD) %*% C %*% solve(SD)", 
            "title": "2, Intuition behind estimation of multiple regression coefficients"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Six,_Multiple_Regression/#3-dummy-coding", 
            "text": "Starting off  # Summary statistics\ndescribeBy(fs, fs$dept)  A system to code categorical predictors in a regression analysis  Suppose we have a categorical vector of observations. The vector counts \n4 distinct groups. Here is how to assign the dummy variables:     ProfID  Group  Pubs  D1  D2  D3      NU  Cognitive  83  0  0  0    ZH  Clinical  74  1  0  0    MK  Developmental  80  0  1  0    RH  Social  68  0  0  1     Summary statistics:     Group  M  SD  N      Cognitive  93.31  29.48  13    Clinical  60.67  11.12  8    Developmental  103.5  23.64  6    Social  70.13  21.82  9     Model:  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 ( D 1 )+ \u03b2 2 ( D 2 )+ \u03b2 3 ( D 3 )+ \u03f5  Coefficient:      B  SE  B  t  p       93.31  6.5  0  14.37  .001    D1 (Clinical)  -32.64  10.16  -0.51  -3.21  0.003    D2 (Devel)  10.19  11.56  0.14  0.88  0.384    D3 (Social)  -23.18  10.52  -0.35  -2.2  0.035     Creating dummy variables (1)  # Create the dummy variables\ndept_code  - dummy.code(fs$dept)\ndept_code\n\n# Merge the dataset in an extended dataframe\nextended_fs  - cbind(fs, dept_code)\n\n# Look at the extended dataframe\nextended_fs\n\n# Provide summary statistics\nsummary(extended_fs)  Creating dummy variables (2)  # Regress salary against years and publications\nmodel  - lm(fs$salary ~ fs$years + fs$pubs)\n\n# Apply the summary function to get summarized results for model\nsummary(model)\n\n# Compute the confidence intervals for model\nconfint(model) \n\n# Create dummies for the categorical variable fs$dept by using the C() function\ndept_code  - C(fs$dept, treatment)\n\n# Regress salary against years, publications and department \nmodel_dummy  - lm(fs$salary ~ fs$years + fs$pubs + dept_code)\n\n# Apply the summary function to get summarized results for model_dummy\nsummary(model_dummy)\n\n# Compute the confidence intervals for model_dummy\nconfint(model_dummy)  Model selection: ANOVA  # Compare model 4 with model3\nanova(model, model_dummy)  Discrepancy between actual and predicted means  # Actual means of fs$salary\ntapply(fs$salary, fs$dept, mean)  Unweighted effects coding  Consult the PDF for  Unweighted Effects Coding .  # Number of levels\nfs$dept\n\n# Factorize the categorical variable fs$dept and name the factorized variable dept.f\ndept.f  - factor(fs$dept)\n\n# Assign the 3 levels generated in step 2 to dept.f\ncontrasts(dept.f)  - contr.sum(3)\n\n# Regress salary against dept.f\nmodel_unweighted  - lm(fs$salary ~ dept.f)\n\n# Apply the summary() function\nsummary(model_unweighted)  Weighted effects coding  Consult  Weighted Effects Coding .  # Factorize the categorical variable fs$dept and name the factorized variable dept.g\ndept.g  - factor(fs$dept)\n\n# Assign the weights matrix to dept.g \ncontrasts(dept.g)  - weights\n\n# Regress salary against dept.f and apply the summary() function\nmodel_weighted  - lm(fs$salary ~ dept.g)\n\n# Apply the summary() function\nsummary(model_weighted)", 
            "title": "3, Dummy coding"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Seven,_Moderation_and_Mediation/", 
            "text": "1, An introduction to moderation\n\n\n2, An introduction to centeringpredictors\n\n\n3, An introduction to mediation\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets only.\n\n\n\n\n\n\n1, An introduction to moderation\n\n\nData exploration\n\n\nlibrary(psych)\n\n# Summary statistics\ndescribeBy(mod, mod$condition)\n\n# Create a boxplot of the data\nboxplot(mod$iq ~ mod$condition, main = 'Boxplot', ylab = 'IQ', xlab = 'Group condition')\n\n\n\n\nCalculate correlations\n\n\n# Create subsets of the three groups\n\n# Make the subset for the group condition = 'control'\nmod_control \n- subset(mod, condition == 'control')\n\n# Make the subset for the group condition = 'threat1'\nmod_threat1 \n- subset(mod, condition == 'threat1')\n\n# Make the subset for the group condition = 'threat2'\nmod_threat2 \n- subset(mod, condition == 'threat2')\n\n# Calculate the correlations\ncor(mod_control$iq, mod_control$wm, method = 'pearson')\n\ncor(mod_threat1$iq, mod_threat1$wm, method = 'pearson')\n\ncor(mod_threat2$iq, mod_threat2$wm, method = 'pearson')\n\n\n\n\nModel with and without moderation\n\n\nA moderator variable (Z) will enhance a regression model if the\n\nrelationship between X and Y varies as a function of Z.\n\n\nExperimental research\n\n\n\n\nThe manipulation of an X causes change in a Y.\n\n\nA moderator variable (Z) implies that the effect of the X on the Y\n\n    is NOT consistent across the distribution of Z.\n\n\n\n\nCorrelational research\n\n\n\n\nAssume a correlation between X and Y.\n\n\nA moderator variable (Z) implies that the correlation between X and\n\n    Y is NOT consistent across the distribution of Z.\n\n\n\n\nIf both X and Z are continuous:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nX\n\u2005+\u2005\n\u03b2\n2\nZ\n\u2005+\u2005\n\u03b2\n3\n(\nX\n\u2005*\u2005\nZ\n)+\n\u03f5\n\n\nIf X is categorical and Z is continuous:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\n(\nD\n1\n)+\n\u03b2\n2\n(\nD\n2\n)+\n\u03b2\n3\nZ\n\u2005+\u2005\n\u03b2\n4\n(\nD\n1\n\u2005*\u2005\nZ\n)+\n\u03b2\n5\n(\nD\n2\n\u2005*\u2005\nZ\n)+\n\u03f5\n\n\nConsult the PDF for performing tests.\n\n\n# Model without moderation (tests for 'first-order effects')\nmodel_1 \n- lm(mod$iq ~ mod$wm + mod$d1 + mod$d2)\n\n # Make a summary of model_1\nsummary(model_1)\n\n# Create new predictor variables\nwm_d1 \n- mod$wm * mod$d1\nwm_d2 \n- mod$wm * mod$d2\n\n# Model with moderation\nmodel_2 \n- lm(mod$iq ~ mod$wm + mod$d1 + mod$d2 + wm_d1 + wm_d2)\n\n# Make a summary of model_2\nsummary(model_2)\n\n\n\n\nModel comparison\n\n\n# Compare model_1 and model_2\nanova(model_1, model_2)\n\n\n\n\nScatterplot\n\n\n# Choose colors to represent the points by group\ncolor \n- c('red', 'green', 'blue')\n\n# Illustration of the first-order effects of working memory on IQ\nggplot(mod, aes(x = wm, y = iq)) + geom_smooth(method = 'lm', color = 'black') + \n  geom_point(aes(color = condition))\n\n# Illustration of the moderation effect of working memory on IQ\nggplot(mod, aes(x = wm, y = iq)) + geom_smooth(aes(group = condition), method = 'lm', se = T, color = 'black', fullrange = T) + geom_point(aes(color = condition))\n\n\n\n\nCentering data\n\n\n# Define wm_center\nwm_center \n- mod$wm - mean(mod$wm)\n\n# Compare with the variable wm.centered\nall.equal(wm_center, mod$wm.centered)\n\n\n\n\n2, An introduction to centering predictors\n\n\nCentering versus no centering\n\n\nTo center means to put in deviation form: \nX\nC\n\u2004=\u2004\nX\n\u2005\u2212\u2005\nM\n.\n\nConvert raw scores to deviation scores.\n\n\nTwo reason:\n\n\n\n\nConceptual: regression constant will be more meaningful.\n\n\nStatistical: avoid multicolinearity.\n\n\n\n\nConceptual.\n\n\n\n\nThe intercept, \n\u03b2\n0\n, is the predicted score on Y (child\ns\n\n    verbal ability) when all predictors (X = mother\ns vocabulary, Z =\n\n    child\ns age) are zero.\n\n\nIf X = zero or Z = zero is meaningless, or impossible, then\n\n\n\u03b2\n0\n will be difficult to interpret.\n\n\nIn contrast, if X = zero and Z = zero, are the average then\n\n\n\u03b2\n0\n is easy to interpret.\n\n\nThe regression coefficient \n\u03b2\n1\n is the slope for X\n\n    assuming an average score on Z.\n\n\nNo moderation effect implies that \n\u03b2\n1\n is consistent\n\n    across the entire distribution of Z.\n\n\nIn contrast, a moderation effect implies that \n\u03b2\n1\n is NOT\n\n    consistent across the entire distribution of Z.\n\n\nWhere in the distribution of Z is \n\u03b2\n1\n most\n\n    representative of the relationship between X \n Y?\n\n\n\n\nStatistical\n\n\n\n\nThe predictors, X and Z, can become highly correlated with the\n\n    product, (X*Z).\n\n\nMulticolinearity: when two predictor variables in a GLM are so\n\n    highly correlated that they are essentially redundant and it becomes\n\n    difficult to estimate \n\u03b2\n values associated with each predictor.\n\n\n\n\n\n\n\n# Model without moderation and with centered data\nmodel_1_centered \n- lm(mod$iq ~ mod$wm.centered + mod$d1 + mod$d2)\n\n# Make a summary of model_1_centered\nsummary(model_1_centered)\n\n\n\n\nCentering versus no centering with moderation\n\n\n# Create new predictor variables\nwm_d1_centered \n- mod$wm.centered * mod$d1\nwm_d2_centered \n- mod$wm.centered * mod$d2\n\n# Define model_2_centered\nmodel_2_centered \n- lm(mod$iq ~ mod$wm.centered + mod$d1 + mod$d2 + wm_d1_centered + wm_d2_centered)\n\n# Make a summary of model_2_centered\nsummary(model_2_centered)\n\n\n\n\nModel comparison\n\n\n# Compare model_1_centered and model_2_centered\nanova(model_1_centered, model_2_centered)\n\n# Compare model_1 and model_2\nanova(model_1, model_2)\n\n\n\n\nSome correlations\n\n\n# Calculate the correlations between working memory capacity and the product terms\n cor_wmd1 \n- cor(mod$wm, wm_d1)\n cor_wmd2 \n- cor(mod$wm, wm_d2)\n cor_wmd1_centered \n- cor(mod$wm.centered, wm_d1_centered)\n cor_wmd2_centered \n- cor(mod$wm.centered, wm_d2_centered)\n\n# Calculate the correlations between the dummy variables and the product terms\n cor_d1d1\n- cor(mod$d1, wm_d1)\n cor_d2d2 \n- cor(mod$d2, wm_d2)\n cor_d1d1_centered \n- cor(mod$d1, wm_d1_centered)\n cor_d2d2_centered \n- cor(mod$d2, wm_d2_centered)\n\n# correlations\n rbind(c(cor_wmd1, cor_wmd2), c(cor_wmd1_centered, cor_wmd2_centered))\n rbind(c(cor_d1d1, cor_d2d2), c(cor_d1d1_centered, cor_d2d2_centered))\n\n\n\n\n3, An introduction to mediation\n\n\nModel with and without mediation\n\n\n\n\nX: Experimental manipulation (Stereotype threat).\n\n\nY: Behavioral outcome (IQ score).\n\n\nM: Mediator (Mechanism = Working memory capacity).\n\n\n\n\nA mediation analysis is typically conducted to better understand an\n\nobserved effect of a correlation between X and Y. Why, and how, does\n\nstereotype threat influence IQ test performance?\n\n\nIf X and Y are correlated then we can use regression to predict Y from X\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nX\n\u2005+\u2005\n\u03f5\n\n\nIf X and Y are correlated BECAUSE of the mediator M, then (X \u00e0 M \u00e0 Y):\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nM\n\u2005+\u2005\n\u03f5\n\n\nM\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nX\n\u2005+\u2005\n\u03f5\n\n\nIf X and Y are correlated BECAUSE of the mediator M, and:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nM\n\u2005+\u2005\n\u03b2\n2\nX\n\u2005+\u2005\n\u03f5\n\n\nA mediator variable (M) accounts for some (partial) or all of the\n\nrelationship between X and Y.\n\n\nCAUTION:\n\n\n\n\nCorrelation does not imply causation!\n\n\nIn other words, there is a BIG difference between statistical\n\n    mediation and true causal mediation.\n\n\n\n\nData exploration\n\n\n# Summary statistics\ndescribeBy(med, med$condition)\n\n# Create a boxplot of the data\nboxplot(med$iq ~ med$condition, main = 'Boxplot', xlab = 'Group condition', ylab = 'IQ')\n\n\n\n\nRun 3 regression models on the data\n\n\n# Run the three regression models\n# outcome ~ predictor\nmodel_yx \n- lm(med$iq ~ med$condition)\n\n# mediator ~ predictor\nmodel_mx \n- lm(med$wm ~ med$condition)\n\n# outcome ~ predictor + mediator\nmodel_yxm \n- lm(med$iq ~ med$condition + med$wm)\n\n# Make a summary of the three models\nsummary(model_yx)\nsummary(model_mx)\nsummary(model_yxm)\n\n\n\n\nSobel test\n\n\n# Compare the previous results to the output of the sobel function\n\n# sobel(pred,med,out)\nmodel_all \n- sobel(med$condition, med$wm, med$iq)\nmodel_all\n\n\n\n\nThe Sobel test is a method of testing the significance of a mediation\n\neffect. Consult the PDF.", 
            "title": "Statistics with R, Course Seven, Moderation and Mediation"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Seven,_Moderation_and_Mediation/#2-an-introduction-to-centering-predictors", 
            "text": "Centering versus no centering  To center means to put in deviation form:  X C \u2004=\u2004 X \u2005\u2212\u2005 M . \nConvert raw scores to deviation scores.  Two reason:   Conceptual: regression constant will be more meaningful.  Statistical: avoid multicolinearity.   Conceptual.   The intercept,  \u03b2 0 , is the predicted score on Y (child s \n    verbal ability) when all predictors (X = mother s vocabulary, Z = \n    child s age) are zero.  If X = zero or Z = zero is meaningless, or impossible, then  \u03b2 0  will be difficult to interpret.  In contrast, if X = zero and Z = zero, are the average then  \u03b2 0  is easy to interpret.  The regression coefficient  \u03b2 1  is the slope for X \n    assuming an average score on Z.  No moderation effect implies that  \u03b2 1  is consistent \n    across the entire distribution of Z.  In contrast, a moderation effect implies that  \u03b2 1  is NOT \n    consistent across the entire distribution of Z.  Where in the distribution of Z is  \u03b2 1  most \n    representative of the relationship between X   Y?   Statistical   The predictors, X and Z, can become highly correlated with the \n    product, (X*Z).  Multicolinearity: when two predictor variables in a GLM are so \n    highly correlated that they are essentially redundant and it becomes \n    difficult to estimate  \u03b2  values associated with each predictor.    # Model without moderation and with centered data\nmodel_1_centered  - lm(mod$iq ~ mod$wm.centered + mod$d1 + mod$d2)\n\n# Make a summary of model_1_centered\nsummary(model_1_centered)  Centering versus no centering with moderation  # Create new predictor variables\nwm_d1_centered  - mod$wm.centered * mod$d1\nwm_d2_centered  - mod$wm.centered * mod$d2\n\n# Define model_2_centered\nmodel_2_centered  - lm(mod$iq ~ mod$wm.centered + mod$d1 + mod$d2 + wm_d1_centered + wm_d2_centered)\n\n# Make a summary of model_2_centered\nsummary(model_2_centered)  Model comparison  # Compare model_1_centered and model_2_centered\nanova(model_1_centered, model_2_centered)\n\n# Compare model_1 and model_2\nanova(model_1, model_2)  Some correlations  # Calculate the correlations between working memory capacity and the product terms\n cor_wmd1  - cor(mod$wm, wm_d1)\n cor_wmd2  - cor(mod$wm, wm_d2)\n cor_wmd1_centered  - cor(mod$wm.centered, wm_d1_centered)\n cor_wmd2_centered  - cor(mod$wm.centered, wm_d2_centered)\n\n# Calculate the correlations between the dummy variables and the product terms\n cor_d1d1 - cor(mod$d1, wm_d1)\n cor_d2d2  - cor(mod$d2, wm_d2)\n cor_d1d1_centered  - cor(mod$d1, wm_d1_centered)\n cor_d2d2_centered  - cor(mod$d2, wm_d2_centered)\n\n# correlations\n rbind(c(cor_wmd1, cor_wmd2), c(cor_wmd1_centered, cor_wmd2_centered))\n rbind(c(cor_d1d1, cor_d2d2), c(cor_d1d1_centered, cor_d2d2_centered))", 
            "title": "2, An introduction to centering predictors"
        }, 
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Seven,_Moderation_and_Mediation/#3-an-introduction-to-mediation", 
            "text": "Model with and without mediation   X: Experimental manipulation (Stereotype threat).  Y: Behavioral outcome (IQ score).  M: Mediator (Mechanism = Working memory capacity).   A mediation analysis is typically conducted to better understand an \nobserved effect of a correlation between X and Y. Why, and how, does \nstereotype threat influence IQ test performance?  If X and Y are correlated then we can use regression to predict Y from X  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5  If X and Y are correlated BECAUSE of the mediator M, then (X \u00e0 M \u00e0 Y):  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03f5  M \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5  If X and Y are correlated BECAUSE of the mediator M, and:  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03b2 2 X \u2005+\u2005 \u03f5  A mediator variable (M) accounts for some (partial) or all of the \nrelationship between X and Y.  CAUTION:   Correlation does not imply causation!  In other words, there is a BIG difference between statistical \n    mediation and true causal mediation.   Data exploration  # Summary statistics\ndescribeBy(med, med$condition)\n\n# Create a boxplot of the data\nboxplot(med$iq ~ med$condition, main = 'Boxplot', xlab = 'Group condition', ylab = 'IQ')  Run 3 regression models on the data  # Run the three regression models\n# outcome ~ predictor\nmodel_yx  - lm(med$iq ~ med$condition)\n\n# mediator ~ predictor\nmodel_mx  - lm(med$wm ~ med$condition)\n\n# outcome ~ predictor + mediator\nmodel_yxm  - lm(med$iq ~ med$condition + med$wm)\n\n# Make a summary of the three models\nsummary(model_yx)\nsummary(model_mx)\nsummary(model_yxm)  Sobel test  # Compare the previous results to the output of the sobel function\n\n# sobel(pred,med,out)\nmodel_all  - sobel(med$condition, med$wm, med$iq)\nmodel_all  The Sobel test is a method of testing the significance of a mediation \neffect. Consult the PDF.", 
            "title": "3, An introduction to mediation"
        }, 
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/", 
            "text": "Documentation\n\n\n1, Introduction\n\n\n2, Data Exploration\n\n\n3, Data Manipulation\n\n\n4, Data Analysis\n\n\n\n\n\n\nForeword\n\n\n\n\nOutput options: the \ntango\n syntax and the \nreadable\n theme.\n\n\nSnippets only.\n\n\n\n\n\n\nDocumentation\n\n\n\n\nMicrosoft R Server: previously called Revolution R Enterprise for\n\n    Hadoop, Linux and Teradata and included new Microsoft enterprise\n\n    support and purchasing options. Microsoft R Server was further made\n\n    available to students through the Microsoft DreamSpark programme.\n\n\nMicrosoft R Server Developer Edition: a gratis version for\n\n    developers that with a feature set akin to the commercial edition.\n\n\nMicrosoft Data Science Virtual Machine: an analytics tool developed\n\n    by the Revolution Analytics division premiered in January 2015.\n\n\nMicrosoft R Open: a rebranded version of Revolution R Open.\n\n\n\n\n1, Introduction\n\n\nImporting data with \nrxImport\n function\n\n\n# Declare the file paths for the csv and xdf files\n# find the path or directory where the file is, load the path variable\nmyAirlineCsv \n- file.path(rxGetOption('sampleDataDir'), '2007_subset.csv')\n\n# fin the data in this directory and load the data variable\nmyAirlineXdf \n- '2007_subset.xdf'\n\n# Use rxImport to import the data into xdf format\n# rxImport(inData = myAirlineCsv, outFile = myAirlineXdf, overwrite = TRUE)\n# or\n# function within a function for more stats\nsystem.time(rxImport(inData = myAirlineCsv, \n                     outFile = myAirlineXdf, \n                     overwrite = TRUE))\nlist.files()\n\n\n\n\nFunctions for summarizing data\n\n\n# Get basic information about your data\nrxGetInfo(data = myAirlineXdf, \n          getVarInfo = TRUE, \n          numRows = 10)\n\n# Summarize the variables corresponding to actual elapsed time, time in the air, departure delay, flight Distance\nrxSummary(formula = ~ ActualElapsedTime + AirTime + DepDelay + Distance, \n          data = myAirlineXdf)\n\n# Histogram of departure delays\nrxHistogram(formula = ~DepDelay, \n            data = myAirlineXdf)\n\n# Use parameters similar to a regular histogram to zero in on the interesting area\nrxHistogram(formula = ~DepDelay, \n            data = myAirlineXdf, \n            xAxisMinMax = c(-100, 400), \n            numBreaks = 500,\n            xNumTicks = 10)\n\n\n\n\nCreating new variables using \nrxDataStep\n\n\n# Calculate an additional variable: airspeed (distance traveled / time in the air)\nrxDataStep(inData = myAirlineXdf, \n           outFile = myAirlineXdf, \n           varsToKeep = c('Distance', 'AirTime'),\n           transforms = list(airSpeed = Distance / Airtime),\n           append = 'cols',\n           overwrite = TRUE)\n\n# Get Variable Information for airspeed\nrxGetInfo(data = myAirlineXdf, \n          getVarInfo = TRUE,\n          varsToKeep = 'airSpeed')\n\n# Summary for the airspeed variable\nrxSummary(~airSpeed, \n          data = myAirlineXdf)\n\n# Construct a histogtam for airspeed\n# We can use the xAxisMinMax argument to limit the X-axis\nrxHistogram(~airSpeed, \n            data = myAirlineXdf)\n\nrxHistogram(~airSpeed, \n            data = myAirlineXdf,\n            xNumTicks = 10,\n            numBreaks = 1500,\n            xAxisMinMax = c(0,12))\n\n\n\n\nTransforming variables using \nrxDataStep\n\n\n# Conversion to miles per hour\nrxDataStep(inData = myAirlineXdf, \n         outFile = myAirlineXdf, \n         varsToKeep = c('airSpeed'),\n           transforms = list(airSpeed = airSpeed * 60),\n         overwrite = TRUE)\n\n# Histogram for airspeed after conversion\nrxHistogram(~airSpeed, \n            data = myAirlineXdf)\n\n\n\n\nCorrelations\n\n\n# Correlation for departure delay, arrival delay, and air speed\nrxCor(formula = ~ DepDelay + ArrDelay + airSpeed,\n      data = myAirlineXdf,\n      rowSelection = (airSpeed \n 50) \n (airSpeed \n 800))\n\n\n\n\nLinear regression\n\n\n# Regression for airSpeed based on departure delay\nmyLMobj \n- rxLinMod(formula = airSpeed ~ DepDelay, \n         data = myAirlineXdf,\n         rowSelection = (airSpeed \n 50) \n (airSpeed \n 800))\n\nsummary(myLMobj)\n\n\n\n\n2, Data Exploration\n\n\nRevoScaleR\n options\n\n\n# Extract the names of the possible options\nnames(rxOptions())\n\n# Extract the sample data directory\nrxGetOption('sampleDataDir')\n\n# View the current value of the reportProgress option\nrxGetOption('reportProgress')\n\n# Set the value of the reportProgress option to 0\nrxOptions(reportProgress = 0)\n\n\n\n\nImport and explore Dow Jones data\n\n\n# Set up the variable that has the address of the relevant data file\ndjiXdf \n- file.path(rxGetOption('sampleDataDir'), 'DJIAdaily.xdf')\n\n# Get information about that dataset\nrxGetInfo(djiXdf, getVarInfo = TRUE)\n\n\n\n\nExtracting meta data about a variable using \nrxGetVarInfo\n\n\n# Get variable information for the dataset\ndjiVarInfo \n- rxGetVarInfo(djiXdf)\nnames(djiVarInfo)\n\n# Extract information about the closing cost variable\n(closeVarInfo \n- djiVarInfo[['Close']])\n\n# Get the class of the closeVarInfo object\nclass(closeVarInfo)\n\n# Examine the structure of the closeVarInfo object\nstr(closeVarInfo)\n\n# Extract the global maximum of the closing cost variable\ncloseMax \n- closeVarInfo[['high']]\n\n\n\n\nSummarizing variables with \nrxSummary\n\n\n# Basic summary statistics\nrxSummary(~ DayOfWeek + Close + Volume, \n          data = djiXdf)\n\n# Frequency weighted\nrxSummary(~ DayOfWeek + Close, \n          data = djiXdf, \n          fweights = 'Volume')\n\n# Basic frequency count\nrxCrossTabs(~ DayOfWeek, \n            data = djiXdf)\n\n\n\n\nExploring a distribution with \nrxHistogram\n\n\n# Numeric Variables\nrxHistogram(~ Close, \n            data = djiXdf)\n\n# Categorical Variable\nrxHistogram(~ DayOfWeek, \n            data = djiXdf)\n\n# Different panels for different days of the week\nrxHistogram(~ Close | DayOfWeek, \n            data = djiXdf)\n\n# Numeric Variables with a frequency weighting\nrxHistogram(~ Close, data = djiXdf, \n            fweights = 'Volume')\n\n\n\n\nPlotting bivariate relationships with \nrxLinePlot\n\n\n# Simple bivariate line plot\nrxLinePlot(Close ~ DaysSince1928, \n           data = djiXdf)\n\n# Using different panels for different days of the week\nrxLinePlot(Close ~ DaysSince1928 | DayOfWeek, \n           data = djiXdf)\n\n# Using different groups\nrxLinePlot(Close ~ DaysSince1928, \n           groups = DayOfWeek, \n           data = djiXdf)\n\n# Simple bivariate line plot, after taking the log() of the ordinate (y) variable\nrxLinePlot(log(Close) ~ DaysSince1928, \n           data = djiXdf)\n\n\n\n\nSummarzing variables with \nrxCrossTabs\n\n\n# Compute the the summed volume for each day of the week\nrxCrossTabs(formula = Volume ~ DayOfWeek, \n            data = djiXdf)\n\n# Compute the the summed volume for each day of the week for each month\nrxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, \n            data = djiXdf)\n\n# Compute the the average volume for each day of the week for each month\nrxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, \n            data = djiXdf, \n            means = TRUE)\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\nrxCrossTabs(formula = Close ~ F(Month):DayOfWeek, \n            data = djiXdf, \n            means = TRUE, \n            fweights = 'Volume')\n\n\n\n\nSummarzing variables with \nrxCube\n\n\n# Compute the the summed volume for each day of the week\nrxCrossTabs(Volume ~ DayOfWeek, \n            data = djiXdf)\n\nrxCube(Volume ~ DayOfWeek, \n       data = djiXdf, \n       means = FALSE)\n\n# Compute the the summed volume for each day of the week for each month\nrxCrossTabs(Volume ~ F(Month):DayOfWeek, \n            data = djiXdf)\n\nrxCube(Volume ~ F(Month):DayOfWeek, \n       data = djiXdf, \n       means = FALSE)\n\n# Compute the the average volume for each day of the week for each month\nrxCube(Volume ~ F(Month):DayOfWeek, \n       data = djiXdf)\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\nrxCube(Close ~ DayOfWeek, \n       data = djiXdf, \n       fweights = 'Volume')\n\n\n\n\n3, Data Manipulation\n\n\nUsing \nrxDataStep\n to transform data\n\n\n# Get information on mortData\nrxGetInfo(mortData)\n\n## Set up my personal copy of the data\nmyMortData \n- 'myMD.xdf'\n\n# Create the transform\nrxDataStep(inData = mortData, \n           outFile = myMortData, \n           transforms = list(highDebtRow = ccDebt \n 8000), \n           overwrite = TRUE)\n\n#rxDataStep(inData = mortData, outFile = myMortData, transforms = list(highDebtRow = ccDebt \n 8000))\n# Get the variable information\nrxGetVarInfo(myMortData)\n\n# Get the proportion of values that are 1\nrxSummary( ~ highDebtRow, \n           data = myMortData)\n\n# Compute multiple transforms!\nrxDataStep(inData = myMortData, outFile = myMortData,\n           transforms = list(\n             newHouse = houseAge \n 10,\n             ccsXhd = creditScore * highDebtRow),\n           append = 'cols',\n           overwrite = TRUE)\n\n\n\n\nMore complex transforms using \ntransformFuncs\n\n\n# Compute the summary statistics\n(csSummary \n- rxSummary(~ creditScore, data = mortData))\n\n# Extract the mean and std. deviation\nmeanCS \n- csSummary$sDataFrame$Mean[1]\nsdCS \n- csSummary$sDataFrame$StdDev[1]\n\n# Create a function to compute the scaled variable\nscaleCS \n- function(mylist){\n  mylist[['scaledCreditScore']] \n- (mylist[['creditScore']] - myCenter) / myScale\n  return(mylist)\n}\n\n# Run it with rxDataStep (A above in B below)\nmyMortData \n- 'myMD.xdf'\nrxDataStep(inData = mortData, outFile = myMortData,\n           transformFunc = scaleCS,\n           transformObjects = list(myCenter = meanCS, myScale = sdCS))\n\n# Check the new variable\nrxGetVarInfo(myMortData)\nrxSummary(~ scaledCreditScore, \n          data = myMortData)\n\n\n\n\n4, Data Analysis\n\n\nPreparing data for analysis: import\n\n\n# Declare the file paths for the csv and xdf files\nmyAirlineCsv \n- file.path(rxGetOption('sampleDataDir'), 'AirlineDemoSmall.csv')\nmyAirlineXdf \n- 'ADS.xdf'\n\n# Use rxImport to import the data into xdf format\nrxImport(inData = myAirlineCsv, \n         outFile = myAirlineXdf, \n         overwrite = TRUE,\n         colInfo = list( \n           DayOfWeek = list(\n            type = 'factor', \n            levels = c('Monday', 'Tuesday', 'Wednesday', \n                       'Thursday', 'Friday', 'Saturday', 'Sunday'))))\n\n\n\n\nPreparing data Ffor analysis: exploration\n\n\n# Summarize arrival delay for each day of the week\nrxSummary(formula = ArrDelay ~ DayOfWeek, \n          data = myAirlineXdf)\n\n# Vizualize the arrival delay histogram\nrxHistogram(formula = ~ArrDelay, \n            data = myAirlineXdf)\n\n\n\n\nConstruct a linear model\n\n\n# predict arrival delay by day of the week\nmyLM1 \n- rxLinMod(ArrDelay ~ DayOfWeek, \n                  data = myAirlineXdf)\n\n# summarize the model\nsmmary(myLM1)\n\n# Use the transforms argument to create a factor variable associated with departure time 'on the fly,'\n# predict Arrival Delay by the interaction between Day of the week and that new factor variable\nmyLM2 \n- rxLinMod(ArrDelay ~ DayOfWeek, \n                  data = myAirlineXdf,\n                  transforms = list(\n                    catDepTime = cut(CRSDepTime, breaks = seq(from = 5, to = 23, by = 2))),\n                    cube = TRUE)\n\n# summarize the model\nsummary(myLM2)\n\n\n\n\nGenerating predictions and residuals\n\n\n# Summarize model first\nsummary(myLM2)\n\n# Path to new dataset storing predictions\nmyNewADS \n- 'myNEWADS.xdf'\n\n# Generate predictions\nrxPredict(modelObject = myLM2, \n          data = myAirlineXdf, \n          outData = myNewADS, \n          writeModelVars = TRUE)\n\n# Get information on the new dataset\nrxGetInfo(myNewADS, getVarInfo = TRUE)\n\n# Generate residuals.\nrxPredict(modelObject = myLM2, \n          data = myAirlineXdf, \n          outData = myNewADS, \n          writeModelVars = TRUE, \n          computeResiduals = TRUE, \n          overwrite = TRUE)\n\n# Get information on the new dataset\nrxGetInfo(myNewADS, getVarInfo = TRUE)\n\n\n\n\nLogistic regression\n\n\n# look at the meta data\nls()\nrxGetInfo(data = mortData, getVarInfo = TRUE)\n\n# Construct the logit model\nlogitModel \n- rxLogit(formula = default ~ houseAge + F(year) + ccDebt + creditScore + yearsEmploy, \n                      data = mortData)\n\n# Summarize the result contained in logitModel\nsummary(logitModel\n\n\n\n\nIndividual mortgage information\n\n\n# Summarize the model\nsummary(logitModel)\n\n# view the first few rows\nhead(newData)\n\n# Make predictions\ndataWithPredictions \n- rxPredict(modelObject = logitModel, \n                                 data = newData, \n                                 outData = newData, \n                                 type = 'response')\n\n# view the predictions\ndataWithPredictions\n\n\n\n\nComputing k-means with \nrxKmeans\n\n\n# Examine the mortData dataset\nrxGetInfo(mortData, getVarInfo = TRUE)\n\n# Set up a path to a new xdf file\nmyNewMortData = 'myMDwithKMeans.xdf'\n\n# Run k-means\nKMout \n- rxKmeans(formula = ~ ccDebt + creditScore + houseAge, \n         data = mortData,\n         outFile = myNewMortData,\n         rowSelection = year == 2000,\n         numClusters = 4,\n         writeModelVars = TRUE)\n\nprint(KMout)\n\n# Examine the variables in the new dataset:\nrxGetInfo(myNewMortData, getVarInfo = TRUE)\n\n# Summarize the cluster variable:\nrxSummary(~ F(.rxCluster), data = myNewMortData)\n\n# Read into memory 10% of the data:\nmydf \n- rxXdfToDataFrame(myNewMortData,\n                         rowSelection = randSamp == 1,\n                         varsToDrop = 'year',\n                         transforms = list(randSamp = sample(10, size = .rxNumRows, replace = TRUE)))\n\n## Visualize the clusters\nplot(mydf[-1], col = mydf$.rxCluster)\n\n\n\n\nCreate some decision trees\n\n\n# regression tree\nregTreeOut \n- rxDTree(default ~ creditScore + ccDebt + yearsEmploy + houseAge, \n                      rowSelection = year == 2000, \n                      data = mortData, maxdepth = 5)\n\n# print out the object\nprint(regTreeOut)\n\n# plot a dendrogram, and add node labels\nplot(rxAddInheritance(regTreeOut))\ntext(rxAddInheritance(regTreeOut))\n\n# Another visualization\n#library(RevoTreeView)\n#createTreeView(regTreeOut)\n# predict values\nmyNewData = 'myNewMortData.xdf'\n\nrxPredict(regTreeOut,\n          data = mortData,\n          outData = myNewData,\n          writeModelVars = TRUE,\n          predVarNames = 'default_RegPred')\n\n# visualize ROC curve\nrxRocCurve(actualVarName = 'default', \n           predVarNames = 'default_RegPred', \n           data = myNewData)", 
            "title": "Big Data Analysis with Revolution R Enterprise"
        }, 
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#1-introduction", 
            "text": "Importing data with  rxImport  function  # Declare the file paths for the csv and xdf files\n# find the path or directory where the file is, load the path variable\nmyAirlineCsv  - file.path(rxGetOption('sampleDataDir'), '2007_subset.csv')\n\n# fin the data in this directory and load the data variable\nmyAirlineXdf  - '2007_subset.xdf'\n\n# Use rxImport to import the data into xdf format\n# rxImport(inData = myAirlineCsv, outFile = myAirlineXdf, overwrite = TRUE)\n# or\n# function within a function for more stats\nsystem.time(rxImport(inData = myAirlineCsv, \n                     outFile = myAirlineXdf, \n                     overwrite = TRUE))\nlist.files()  Functions for summarizing data  # Get basic information about your data\nrxGetInfo(data = myAirlineXdf, \n          getVarInfo = TRUE, \n          numRows = 10)\n\n# Summarize the variables corresponding to actual elapsed time, time in the air, departure delay, flight Distance\nrxSummary(formula = ~ ActualElapsedTime + AirTime + DepDelay + Distance, \n          data = myAirlineXdf)\n\n# Histogram of departure delays\nrxHistogram(formula = ~DepDelay, \n            data = myAirlineXdf)\n\n# Use parameters similar to a regular histogram to zero in on the interesting area\nrxHistogram(formula = ~DepDelay, \n            data = myAirlineXdf, \n            xAxisMinMax = c(-100, 400), \n            numBreaks = 500,\n            xNumTicks = 10)  Creating new variables using  rxDataStep  # Calculate an additional variable: airspeed (distance traveled / time in the air)\nrxDataStep(inData = myAirlineXdf, \n           outFile = myAirlineXdf, \n           varsToKeep = c('Distance', 'AirTime'),\n           transforms = list(airSpeed = Distance / Airtime),\n           append = 'cols',\n           overwrite = TRUE)\n\n# Get Variable Information for airspeed\nrxGetInfo(data = myAirlineXdf, \n          getVarInfo = TRUE,\n          varsToKeep = 'airSpeed')\n\n# Summary for the airspeed variable\nrxSummary(~airSpeed, \n          data = myAirlineXdf)\n\n# Construct a histogtam for airspeed\n# We can use the xAxisMinMax argument to limit the X-axis\nrxHistogram(~airSpeed, \n            data = myAirlineXdf)\n\nrxHistogram(~airSpeed, \n            data = myAirlineXdf,\n            xNumTicks = 10,\n            numBreaks = 1500,\n            xAxisMinMax = c(0,12))  Transforming variables using  rxDataStep  # Conversion to miles per hour\nrxDataStep(inData = myAirlineXdf, \n         outFile = myAirlineXdf, \n         varsToKeep = c('airSpeed'),\n           transforms = list(airSpeed = airSpeed * 60),\n         overwrite = TRUE)\n\n# Histogram for airspeed after conversion\nrxHistogram(~airSpeed, \n            data = myAirlineXdf)  Correlations  # Correlation for departure delay, arrival delay, and air speed\nrxCor(formula = ~ DepDelay + ArrDelay + airSpeed,\n      data = myAirlineXdf,\n      rowSelection = (airSpeed   50)   (airSpeed   800))  Linear regression  # Regression for airSpeed based on departure delay\nmyLMobj  - rxLinMod(formula = airSpeed ~ DepDelay, \n         data = myAirlineXdf,\n         rowSelection = (airSpeed   50)   (airSpeed   800))\n\nsummary(myLMobj)", 
            "title": "1, Introduction"
        }, 
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#2-data-exploration", 
            "text": "RevoScaleR  options  # Extract the names of the possible options\nnames(rxOptions())\n\n# Extract the sample data directory\nrxGetOption('sampleDataDir')\n\n# View the current value of the reportProgress option\nrxGetOption('reportProgress')\n\n# Set the value of the reportProgress option to 0\nrxOptions(reportProgress = 0)  Import and explore Dow Jones data  # Set up the variable that has the address of the relevant data file\ndjiXdf  - file.path(rxGetOption('sampleDataDir'), 'DJIAdaily.xdf')\n\n# Get information about that dataset\nrxGetInfo(djiXdf, getVarInfo = TRUE)  Extracting meta data about a variable using  rxGetVarInfo  # Get variable information for the dataset\ndjiVarInfo  - rxGetVarInfo(djiXdf)\nnames(djiVarInfo)\n\n# Extract information about the closing cost variable\n(closeVarInfo  - djiVarInfo[['Close']])\n\n# Get the class of the closeVarInfo object\nclass(closeVarInfo)\n\n# Examine the structure of the closeVarInfo object\nstr(closeVarInfo)\n\n# Extract the global maximum of the closing cost variable\ncloseMax  - closeVarInfo[['high']]  Summarizing variables with  rxSummary  # Basic summary statistics\nrxSummary(~ DayOfWeek + Close + Volume, \n          data = djiXdf)\n\n# Frequency weighted\nrxSummary(~ DayOfWeek + Close, \n          data = djiXdf, \n          fweights = 'Volume')\n\n# Basic frequency count\nrxCrossTabs(~ DayOfWeek, \n            data = djiXdf)  Exploring a distribution with  rxHistogram  # Numeric Variables\nrxHistogram(~ Close, \n            data = djiXdf)\n\n# Categorical Variable\nrxHistogram(~ DayOfWeek, \n            data = djiXdf)\n\n# Different panels for different days of the week\nrxHistogram(~ Close | DayOfWeek, \n            data = djiXdf)\n\n# Numeric Variables with a frequency weighting\nrxHistogram(~ Close, data = djiXdf, \n            fweights = 'Volume')  Plotting bivariate relationships with  rxLinePlot  # Simple bivariate line plot\nrxLinePlot(Close ~ DaysSince1928, \n           data = djiXdf)\n\n# Using different panels for different days of the week\nrxLinePlot(Close ~ DaysSince1928 | DayOfWeek, \n           data = djiXdf)\n\n# Using different groups\nrxLinePlot(Close ~ DaysSince1928, \n           groups = DayOfWeek, \n           data = djiXdf)\n\n# Simple bivariate line plot, after taking the log() of the ordinate (y) variable\nrxLinePlot(log(Close) ~ DaysSince1928, \n           data = djiXdf)  Summarzing variables with  rxCrossTabs  # Compute the the summed volume for each day of the week\nrxCrossTabs(formula = Volume ~ DayOfWeek, \n            data = djiXdf)\n\n# Compute the the summed volume for each day of the week for each month\nrxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, \n            data = djiXdf)\n\n# Compute the the average volume for each day of the week for each month\nrxCrossTabs(formula = Volume ~ F(Month):DayOfWeek, \n            data = djiXdf, \n            means = TRUE)\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\nrxCrossTabs(formula = Close ~ F(Month):DayOfWeek, \n            data = djiXdf, \n            means = TRUE, \n            fweights = 'Volume')  Summarzing variables with  rxCube  # Compute the the summed volume for each day of the week\nrxCrossTabs(Volume ~ DayOfWeek, \n            data = djiXdf)\n\nrxCube(Volume ~ DayOfWeek, \n       data = djiXdf, \n       means = FALSE)\n\n# Compute the the summed volume for each day of the week for each month\nrxCrossTabs(Volume ~ F(Month):DayOfWeek, \n            data = djiXdf)\n\nrxCube(Volume ~ F(Month):DayOfWeek, \n       data = djiXdf, \n       means = FALSE)\n\n# Compute the the average volume for each day of the week for each month\nrxCube(Volume ~ F(Month):DayOfWeek, \n       data = djiXdf)\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\nrxCube(Close ~ DayOfWeek, \n       data = djiXdf, \n       fweights = 'Volume')", 
            "title": "2, Data Exploration"
        }, 
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#3-data-manipulation", 
            "text": "Using  rxDataStep  to transform data  # Get information on mortData\nrxGetInfo(mortData)\n\n## Set up my personal copy of the data\nmyMortData  - 'myMD.xdf'\n\n# Create the transform\nrxDataStep(inData = mortData, \n           outFile = myMortData, \n           transforms = list(highDebtRow = ccDebt   8000), \n           overwrite = TRUE)\n\n#rxDataStep(inData = mortData, outFile = myMortData, transforms = list(highDebtRow = ccDebt   8000))\n# Get the variable information\nrxGetVarInfo(myMortData)\n\n# Get the proportion of values that are 1\nrxSummary( ~ highDebtRow, \n           data = myMortData)\n\n# Compute multiple transforms!\nrxDataStep(inData = myMortData, outFile = myMortData,\n           transforms = list(\n             newHouse = houseAge   10,\n             ccsXhd = creditScore * highDebtRow),\n           append = 'cols',\n           overwrite = TRUE)  More complex transforms using  transformFuncs  # Compute the summary statistics\n(csSummary  - rxSummary(~ creditScore, data = mortData))\n\n# Extract the mean and std. deviation\nmeanCS  - csSummary$sDataFrame$Mean[1]\nsdCS  - csSummary$sDataFrame$StdDev[1]\n\n# Create a function to compute the scaled variable\nscaleCS  - function(mylist){\n  mylist[['scaledCreditScore']]  - (mylist[['creditScore']] - myCenter) / myScale\n  return(mylist)\n}\n\n# Run it with rxDataStep (A above in B below)\nmyMortData  - 'myMD.xdf'\nrxDataStep(inData = mortData, outFile = myMortData,\n           transformFunc = scaleCS,\n           transformObjects = list(myCenter = meanCS, myScale = sdCS))\n\n# Check the new variable\nrxGetVarInfo(myMortData)\nrxSummary(~ scaledCreditScore, \n          data = myMortData)", 
            "title": "3, Data Manipulation"
        }, 
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#4-data-analysis", 
            "text": "Preparing data for analysis: import  # Declare the file paths for the csv and xdf files\nmyAirlineCsv  - file.path(rxGetOption('sampleDataDir'), 'AirlineDemoSmall.csv')\nmyAirlineXdf  - 'ADS.xdf'\n\n# Use rxImport to import the data into xdf format\nrxImport(inData = myAirlineCsv, \n         outFile = myAirlineXdf, \n         overwrite = TRUE,\n         colInfo = list( \n           DayOfWeek = list(\n            type = 'factor', \n            levels = c('Monday', 'Tuesday', 'Wednesday', \n                       'Thursday', 'Friday', 'Saturday', 'Sunday'))))  Preparing data Ffor analysis: exploration  # Summarize arrival delay for each day of the week\nrxSummary(formula = ArrDelay ~ DayOfWeek, \n          data = myAirlineXdf)\n\n# Vizualize the arrival delay histogram\nrxHistogram(formula = ~ArrDelay, \n            data = myAirlineXdf)  Construct a linear model  # predict arrival delay by day of the week\nmyLM1  - rxLinMod(ArrDelay ~ DayOfWeek, \n                  data = myAirlineXdf)\n\n# summarize the model\nsmmary(myLM1)\n\n# Use the transforms argument to create a factor variable associated with departure time 'on the fly,'\n# predict Arrival Delay by the interaction between Day of the week and that new factor variable\nmyLM2  - rxLinMod(ArrDelay ~ DayOfWeek, \n                  data = myAirlineXdf,\n                  transforms = list(\n                    catDepTime = cut(CRSDepTime, breaks = seq(from = 5, to = 23, by = 2))),\n                    cube = TRUE)\n\n# summarize the model\nsummary(myLM2)  Generating predictions and residuals  # Summarize model first\nsummary(myLM2)\n\n# Path to new dataset storing predictions\nmyNewADS  - 'myNEWADS.xdf'\n\n# Generate predictions\nrxPredict(modelObject = myLM2, \n          data = myAirlineXdf, \n          outData = myNewADS, \n          writeModelVars = TRUE)\n\n# Get information on the new dataset\nrxGetInfo(myNewADS, getVarInfo = TRUE)\n\n# Generate residuals.\nrxPredict(modelObject = myLM2, \n          data = myAirlineXdf, \n          outData = myNewADS, \n          writeModelVars = TRUE, \n          computeResiduals = TRUE, \n          overwrite = TRUE)\n\n# Get information on the new dataset\nrxGetInfo(myNewADS, getVarInfo = TRUE)  Logistic regression  # look at the meta data\nls()\nrxGetInfo(data = mortData, getVarInfo = TRUE)\n\n# Construct the logit model\nlogitModel  - rxLogit(formula = default ~ houseAge + F(year) + ccDebt + creditScore + yearsEmploy, \n                      data = mortData)\n\n# Summarize the result contained in logitModel\nsummary(logitModel  Individual mortgage information  # Summarize the model\nsummary(logitModel)\n\n# view the first few rows\nhead(newData)\n\n# Make predictions\ndataWithPredictions  - rxPredict(modelObject = logitModel, \n                                 data = newData, \n                                 outData = newData, \n                                 type = 'response')\n\n# view the predictions\ndataWithPredictions  Computing k-means with  rxKmeans  # Examine the mortData dataset\nrxGetInfo(mortData, getVarInfo = TRUE)\n\n# Set up a path to a new xdf file\nmyNewMortData = 'myMDwithKMeans.xdf'\n\n# Run k-means\nKMout  - rxKmeans(formula = ~ ccDebt + creditScore + houseAge, \n         data = mortData,\n         outFile = myNewMortData,\n         rowSelection = year == 2000,\n         numClusters = 4,\n         writeModelVars = TRUE)\n\nprint(KMout)\n\n# Examine the variables in the new dataset:\nrxGetInfo(myNewMortData, getVarInfo = TRUE)\n\n# Summarize the cluster variable:\nrxSummary(~ F(.rxCluster), data = myNewMortData)\n\n# Read into memory 10% of the data:\nmydf  - rxXdfToDataFrame(myNewMortData,\n                         rowSelection = randSamp == 1,\n                         varsToDrop = 'year',\n                         transforms = list(randSamp = sample(10, size = .rxNumRows, replace = TRUE)))\n\n## Visualize the clusters\nplot(mydf[-1], col = mydf$.rxCluster)  Create some decision trees  # regression tree\nregTreeOut  - rxDTree(default ~ creditScore + ccDebt + yearsEmploy + houseAge, \n                      rowSelection = year == 2000, \n                      data = mortData, maxdepth = 5)\n\n# print out the object\nprint(regTreeOut)\n\n# plot a dendrogram, and add node labels\nplot(rxAddInheritance(regTreeOut))\ntext(rxAddInheritance(regTreeOut))\n\n# Another visualization\n#library(RevoTreeView)\n#createTreeView(regTreeOut)\n# predict values\nmyNewData = 'myNewMortData.xdf'\n\nrxPredict(regTreeOut,\n          data = mortData,\n          outData = myNewData,\n          writeModelVars = TRUE,\n          predVarNames = 'default_RegPred')\n\n# visualize ROC curve\nrxRocCurve(actualVarName = 'default', \n           predVarNames = 'default_RegPred', \n           data = myNewData)", 
            "title": "4, Data Analysis"
        }, 
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/", 
            "text": "1, Introduction to eXtensible Time\n\n\nWhat is an \nxts\n object\n\n\nxts\n, a constructor or a subclass that inherits behavior from parents. \nxts\n (as a subclass) extends the popular \nzoo\n class (as a parent). Most \nzoo\n methods work for \nxts\n. \n\n\nxts\n is a matrix objects; subsets always preserve the matrix form. \n\n\nxts\n are indexed by a formal time object. You can time-stamp the data.\n\n\nThe main \nxts\n constructor two most important arguments are \nx\n for the data and \norder.by\n for the index. \nx\n must be a vector or matrix. \norder.by\n is a vector of the same length or number of rows of \nx\n; it must be a proper time or date object and it must be in increasing order.\n\n\nxts\n also allows you to bind arbitrary key-value attributes to your data. This lets you keep metadata about your object inside your object.\n\n\n# Load xts\nlibrary(xts)\n\n\n\n\nA first \nxts\n object\n\n\ncore \n- matrix(c(rep(1,3), rep(2,3)), nrow = 3, ncol = 2)\ncore\n\nidx \n- as.Date(c(\n2016-06-01\n,\n2016-06-02\n, \n2016-06-03\n))\nidx\n\nex_matrix \n- xts(core, order.by = idx)\nex_matrix\n\n\n\n\n\n\n\n    \n1\n2\n\n    \n1\n2\n\n    \n1\n2\n\n\n\n\n\n\n\n[1] \"2016-06-01\" \"2016-06-02\" \"2016-06-03\"\n\n\n\n           [,1] [,2]\n2016-06-01    1    2\n2016-06-02    1    2\n2016-06-03    1    2\n\n\n\nMore than a matrix\n\n\nxts\n objects are normal R matrices, but with special powers.\n\n\n# View the structure of ex_matrix\nstr(ex_matrix)\n\n\n\n\nAn 'xts' object on 2016-06-01/2016-06-03 containing:\n  Data: num [1:3, 1:2] 1 1 1 2 2 2\n  Indexed by objects of class: [Date] TZ: UTC\n  xts Attributes:  \n NULL\n\n\n\n# Extract the 3rd observation of the 2nd column of ex_matrix\nex_matrix[3, 2]\n\n# Extract the 3rd observation of the 2nd column of core \ncore[3, 2]\n\n\n\n\n           [,1]\n2016-06-03    2\n\n\n\n2\n\n\nYour first xts object\n\n\nVector must be of the same length or number of rows as x and, this is very important: it must be a proper time or date object and it must be in increasing order.\n\n\n# Create the object data using 5 random numbers\ndata \n- rnorm(5)\ndata\n\n# Create dates as a Date class object starting from 2016-01-01\ndates \n- seq(as.Date(\n2016-01-01\n), length = 5, by = \ndays\n)\ndates\n\n# Use xts() to create smith\nsmith \n- xts(x = data, order.by = dates)\nsmith\n\n# Create bday (1899-05-08) using a POSIXct date class object\nbday \n- as.POSIXct(\n1899-05-08\n)\nbday\n\n# Create hayek and add a new attribute called born\nhayek \n- xts(x = data, order.by = dates, born = bday)\nhayek\n\n\n\n\n\n    \n0.384581498733952\n\n    \n-0.423869463715667\n\n    \n-0.49549963387221\n\n    \n1.49946058244083\n\n    \n-0.92906780054414\n\n\n\n\n\n[1] \"2016-01-01\" \"2016-01-02\" \"2016-01-03\" \"2016-01-04\" \"2016-01-05\"\n\n\n\n                 [,1]\n2016-01-01  0.3845815\n2016-01-02 -0.4238695\n2016-01-03 -0.4954996\n2016-01-04  1.4994606\n2016-01-05 -0.9290678\n\n\n\n[1] \"1899-05-08 EST\"\n\n\n\n                 [,1]\n2016-01-01  0.3845815\n2016-01-02 -0.4238695\n2016-01-03 -0.4954996\n2016-01-04  1.4994606\n2016-01-05 -0.9290678\n\n\n\nDeconstructing xts\n\n\nSeparate a time series into its core data and index attributes for additional analysis and manipulation.\n\n\nFunctions are methods from the \nzoo\n class.\n\n\n# Extract the core data of hayek\nhayek_core \n- coredata(hayek)\n\n# View the class of hayek_core\nclass(hayek_core)\n\n# Extract the index of hayek\nhayek_index \n- index(hayek)\n\n# View the class of hayek_index\nclass(hayek_index)\n\n\n\n\nmatrix\n\n\nDate\n\n\nTime-based indices\n\n\nxts\n objects get their power from the index attribute that holds the time dimension. One major difference between \nxts\n and most other time series objects in R is the ability to use any one of various classes that are used to represent time. Whether \nPOSIXct\n, \nDate\n, or some other class, xts will convert this into an internal form to make subsetting as natural to the user as possible.\n\n\n# Create dates\ndates \n- as.Date(\n2016-01-01\n) + 0:4\n\n# Create ts_a\nts_a \n- xts(x = 1:5, order.by = dates)\n\n# Create ts_b\nts_b \n- xts(x = 1:5, order.by = as.POSIXct(dates))\n\n# Extract the rows of ts_a using the index of ts_b\nts_a[index(ts_a)]\n\n# Extract the rows of ts_b using the index of ts_a\nts_b[index(ts_a)]\n\n\n\n\n           [,1]\n2016-01-01    1\n2016-01-02    2\n2016-01-03    3\n2016-01-04    4\n2016-01-05    5\n\n\n\n     [,1]\n\n\n\n2, First Order of Business - Basic Manipulations\n\n\nConverting xts objects\n\n\nxts\n provides methods to convert all of the major objects you are likely to come across - suitable native R types like matrix, data.frame, and ts are supported, as well as contributed ones such as \ntimeSeries\n, \nfts\n and of course \nzoo\n. \nas.xts\n is the workhorse function to do the conversions to \nxts\n, and similar functions will provide the reverse behavior.\n\n\n# using a R dataset\nhead(austres)\n\n\n\n\n\n    \n13067.3\n\n    \n13130.5\n\n    \n13198.4\n\n    \n13254.2\n\n    \n13303.7\n\n    \n13353.9\n\n\n\n\n\n# Convert austres to an xts object called au\nau \n- as.xts(austres)\nhead(au, 5)\n\n# Convert your xts object (au) into a matrix am\nam \n- as.matrix(au)\nhead(am, 5)\n\n# Convert the original austres into a matrix am2\nam2 \n- as.matrix(austres)\nhead(am2, 5)\n\n\n\n\n           [,1]\n1971 Q2 13067.3\n1971 Q3 13130.5\n1971 Q4 13198.4\n1972 Q1 13254.2\n1972 Q2 13303.7\n\n\n\n\n\n\n    \n1971 Q2\n13067.3\n\n    \n1971 Q3\n13130.5\n\n    \n1971 Q4\n13198.4\n\n    \n1972 Q1\n13254.2\n\n    \n1972 Q2\n13303.7\n\n\n\n\n\n\n\n\n\n\n    \n13067.3\n\n    \n13130.5\n\n    \n13198.4\n\n    \n13254.2\n\n    \n13303.7\n\n\n\n\n\n\n\nImporting data\n\n\nRead raw data from files on disk or the web and convert data to \nxts\n.\n\n\nRead the same data into a \nzoo\n object using read.zoo and convert the \nzoo\n into an \nxts\n object.\n\n\n# Create dat by reading tmp_file\ndat \n- as.matrix(read.csv('tmp_file.csv', sep = ',', header = TRUE))\ndat\n\n\n\n\n\n\na\nb\n\n\n\n    \n1/02/2015\n1\n3\n\n    \n2/03/2015\n2\n4\n\n\n\n\n\n\n\n# Convert dat into xts\nxts(dat, order.by = as.Date(rownames(dat), \n%m%d%Y\n))\n\n\n\n\n     a b\n\nNA\n 1 3\n\nNA\n 2 4\n\n\n\n# Read tmp_file using read.zoo and as.xts\ndat_xts \n- as.xts(read.zoo('tmp_file.csv', index.column = 0, sep = \n,\n, format = \n%m/%d/%Y\n))\ndat_xts\n\n\n\n\n           a b\n2015-01-02 1 3\n2015-02-03 2 4\n\n\n\nExporting xts objects\n\n\nhead(sunspots, 5)\n\n# Convert sunspots to xts\nsunspots_xts \n- as.xts(sunspots)\n\n\n\n\n\n    \n58\n\n    \n62.6\n\n    \n70\n\n    \n55.7\n\n    \n85\n\n\n\n\n\n# Get the temporary file name\ntmp \n- tempfile()\n\n# Write the xts object using zoo to tmp \nwrite.zoo(sunspots, sep = \n,\n, file = tmp)\n\ntmp\n\n\n\n\nC:\\Users\\Dell\\AppData\\Local\\Temp\\RtmpEdYcAQ\\file1a706c8c732b\n\n\n# Read the tmp file\n# FUN=as.yearmon will convert strings such as Jan 1749 into a proper time class\nsun \n- read.zoo(tmp, sep = \n,\n, FUN = as.yearmon)\n\n# Convert sun into xts\nsun_xts \n- as.xts(sun)\nhead(sun_xts, 5)\n\n\n\n\nError in eval(expr, envir, enclos): impossible de trouver la fonction \"read.zoo\"\nTraceback:\n\n\n\nThe ISO8601 Standard\n\n\nThe ISO8601 standard is the internationally recognized and accepted way to represent dates and times. From the biggest to the smallest, left to right, YYYY-MM-DDTHH:MM:SS.\n\n\nThe standard allows for a common format to not only describe dates, but also a way to represent ranges and repeating intervals:\n\n\n\n\n2004 or 2001/2015\n\n\n201402/03\n\n\n2014-02-22 08:30:00\n\n\nT08:00/T09:00\n\n\n\n\nQuerying for dates\n\n\nDate ranges can be extracted from xts objects by simply specifying the period(s) you want using special character strings in your subset.\n\n\nCode:\n\n\nA[\n201601\n]         ## Jan 2016\nA[\n20160125\n]       ## Jan 25, 2016\nA[\n201203/201212\n]  ## Feb to Dec 2012\n\n# Select all of 2016 from x\nx_2016 \n- x[\n2016\n]\n\n# Select January 2016 to March 22\njan_march \n- x[\n201601/20160322\n]\n\n\n\n\nExtracting recurring intraday intervals\n\n\nMost common data \nin the wild\n is daily. On ocassion you may find yourself working with intraday data, that is date plus times.\n\n\nYou can slice days easily by using special notation in the \ni =\n argument to the single bracket extraction (i.e. \n[i,j]\n).\n\n\nCode:\n\n\n# Intraday times for all days\nNYSE[\nT09:30/T16:00\n]\n\n# Extract all data between 8AM and 10AM\nmorn_2010 \n- irreg[\nT8:00/T10:00\n]\n\n# Extract the observations for January 13th\nmorn_2010[\n2010-01-13\n]\n\n\n\n\nAlternating extraction techniques\n\n\nOften times you may need to subset an existing time series with a set of Dates, or time-based objects. These might be from \nas.Date()\n, or \nas.POSIXct()\n, or a variety of other classes.\n\n\nCode:\n\n\n# Subset x using the vector dates\nx[dates]\n\n# Subset x using dates as POSIXct\nx[as.POSIXct(dates)]\n\n\n\n\nExtracting recurring intraday intervals\n\n\nThe most common time series data is daily, intraday data, which contains both dates and times. \n\n\n# Extract all data between 8AM and 10AM\nmorn_2010 \n- irreg[\nT8:00/T10:00\n]\n\n# Extract the observations for January 13th\nmorn_2010[\n2010-01-13\n]\n\n\n\n\nRow selection with time objects\n\n\nSubset an existing time series with a set of Dates, or time-based objects.\n\n\n# Subset x using the vector dates\nx[dates]\n\n# Subset x using dates as POSIXct\nx[as.POSIXct(dates)]\n\n\n\n\nUpdate and replace elements\n\n\nReplace known intervals or observations with an NA, say due to a malfunctioning sensor on a particular day or a set of outliers given a holiday.\n\n\nCode:\n\n\n# Replace all values in x on dates with NA\nx[dates] \n- NA\n\n# Replace dates from 2016-06-09 and on with 0\nx[\n20160609/\n] \n- 0\n\n\n\n\nFind the \nfirst\n or \nlast\n period of time\n\n\nSometimes you need to locate data by relative time. Instead of using an absolute offset, you describe a position relative in time. A simple example would be something like the \nlast 3 weeks\n of a series, or the \nfirst day of current month\n.\n\n\nCode:\n\n\n# Create lastweek with using the last 1 week of temps\nlastweek \n- last(temps, \n1 week\n)\n\n# Print the last 2 observations in lastweek\nlast(lastweek, 2)\n\n# Extract all but the last two days of lastweek\nlast(lastweek, \n-2 day\n)\n\n\n\n\nCombining \nfirst\n and \nlast\n\n\nCode:\n\n\n# Last 3 days of first week\nlast(first(Temps, '1 week'),'3 days') \n\n# Extract the first three days of the second week of temps\nfirst(last(first(temps, \n2 week\n), \n1 week\n), \n3 day\n)\n\n\n\n\nMatrix arithmetic - add, subtract, multiply and divide in time!\n\n\nWhen you perform any binary operation using two xts objects, these objects are first aligned using the intersection of the indexes to preserve the point-in-time aspect of your data, assuring that you don\nt introduce accidental look ahead (or look behind!) bias into your calculations.\n\n\nYour options include:\n\n\n\n\ncoredata() or as.numeric() (drop one to a matrix or vector).\n\n\nManually shift index values - i.e. use lag().\n\n\nReindex your data (before or after the calculation).\n\n\n\n\nxts\n respects time and will only return the intersection of times when doing various mathematical operations. First option:\n\n\n# Add a and b\na + b\n\n# Add a with the numeric value of b\na + as.numeric(b)\n\n\n\n\nMath with non-overlapping indexes\n\n\nThis third way involves modifying the two series you want by aligning assuring you have some union of dates - the dates you require in your final output. This makes it possibly to preserve dimensionality of the data:\n\n\nmerge(b, index(a))\n\n# Add a to b, and fill all missing rows of b with 0\na + merge(b, index(a), fill = 0)\n\n# Add a to b and fill NAs with the last observation\na + merge(b, index(a), fill = na.locf)\n\n\n\n\n3, Merging and modifying time series\n\n\nCombining \nxts\n by column with \nmerge\n\n\nxts\n makes it easy to join data by column and row using a few different functions.  \n\n\nxts\n objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types.\n\n\nOne of the most important functions to accomplish this is \nmerge\n. It works like a \ncbind\n or and SQL \njoin\n:\n\n\n\n\ninner join (and)\n\n\nouter join (or)\n\n\nleft join\n\n\nright join\n\n\n\n\n# Basic argument use\nmerge(a, b, join = \nright\n, fill = 9999)\n\n# Perform an inner join of a and b\nmerge(a, b, join = \ninner\n)\n\n# Perform of a left-join of a and b, fill mising values with 0\nmerge(a, b, join = \nleft\n, fill = 0)\n\n# fill = na.locf, fill = NA\n\n\n\n\nCombining \nxts\n by row with \nrbind\n\n\nEasy to add new rows to your data.\n\n\n# Row bind temps_june30 to temps, assign this to temps2\ntemps2 \n- rbind(temps, temps_june30)\n\n# Row bind temps_july17 and temps_july18 to temps2, call this temps3\ntemps3 \n- rbind(temps2, temps_july17, temps_july18)\n\n\n\n\nFill missing values using last or previous observation\n\n\nThe \nna.locf\n function takes the last-observation-carried-forward approach.\n\n\nThe \nxts\n package leverages the power of \nzoo\n for help with this. \nzoo\n provides a variety of missing data handling functions which are usable by \nxts\n, and very handy.\n\n\n# Last obs. carried forward\nna.locf(x)                \n\n# Next obs. carried backward\nna.locf(x, fromLast = TRUE)\n\n# Fill missing values using the last observation\nna.locf(temps, na.rm = TRUE,\n                fromLast = TRUE)\n\n# Fill missing values using the next observation\nna.locf(temps, na.rm = TRUE)\n\n# maxgap = Inf\n# rule = 2,\n\n\n\n\nNA interpolation\n\n\nFrom \nzoo\n.\n\n\nOn occassion, a simple carry forward approach to missingness isn\nt appropriate. It may be that a series is missing an observation due to a higher frequency sampling than the generating process. You might also encounter an observation that is in error, yet expected to be somewhere between the values of its neighboring observations.\n\n\nBoth of these cases are examples of where interpolation is useful.\n\n\n# Interpolate NAs using linear approximation\nna.approx(AirPass)\n\n\n\n\nCombine a leading and lagging time series\n\n\nAnother common modification for time series is the ability to lag a series.\n\n\n# Your final object\nlag(x, k = 1, na.pad = TRUE)\n\n# k = -1  leading\n# k = 1   lagging\n# zoo uses the other way around\n\n# k = c(0, 1, 5)  multiple leads or lags\n\n# Create a leading object called lead_x\nlead_x \n- lag(x, k = -1)\n\n# Create a lagging object called lag_x\nlag_x \n- lag(x, k = 1)\n\n# Merge your three series together and assign to z\nz \n- merge(lead_x, x, lag_x)\n\n\n\n\nCalculate a difference of a series with \ndiff\n\n\nAnother common operation on time series, typically on those that are non-stationary, is to take a difference of the series.\n\n\ndifferences\n is the order of the difference (how many times \ndiff\n is called). A simple way to view a single (or \nfirst order\n) difference is to see it as \nx(t) - x(t-k)\n where \nk\n is the number of lags to go back. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference).\n\n\ndiff(x, lag = 1, difference = 1, log = FALSE, na.pad = TRUE)\n\n# calculate the first difference of AirPass using lag and subtraction\nAirPass - lag(AirPass, k = 1)\n\n# calculate the first order 12-month difference if AirPass\ndiff(AirPass, lag = 12, differences = 1)\n\n\n\n\nWhat is the key difference in lag between xts and zoo? The \nk\n argument in \nzoo\n uses positive values for shifting past observations forward.\n\n\n4, Apply and aggregate by time\n\n\nFind intervals by time in \nxts\n\n\nThe main function in xts to facilitate this is endpoints. It takes a time series (or a vector of times) and returns the locations of the last observations in each interval.\n\n\nFor example, the code below locates the last observation of each year for the \nAirPass\n dataset:\n\n\nendpoints(AirPass, on = \nyears\n)\n\n# The argument on supports a variety of periods, including \nyears\n, \nquarters\n, \nmonths\n, as well as intraday intervals such as \nhours\n, and \nminutes\n\n\n# on = \nweeks\n, k = 2, the result would be the final day of every other week in your data\n\n# Locate the final day of every week\nendpoints(temps, on = \nweeks\n)\n\n# Locate the final day of every two weeks\nendpoints(temps, on = \nweeks\n, k = 2)\n\n\n\n\nApply a function by time period(s)\n\n\nxts\n provides \nperiod.apply\n, which takes a time series, an \nINDEX\n of endpoints, and a function to \napply\n:\n\n\nperiod.apply(x, INDEX, FUN, ...)\n\n# Calculate the weekly endpoints\nep \n- endpoints(temps, on = \nweeks\n)\n\n# Now calculate the weekly mean and display the results\nperiod.apply(temps[, \nTemp.Mean\n], INDEX = ep, FUN = mean)\n\n\n\n\nUsing \nlapply\n and \nsplit\n to apply functions on intervals\n\n\nOften it is useful to physically split your data into disjoint chunks by time and perform some calculation on these periods.\n\n\n# Split temps by week\ntemps_weekly \n- split(temps, f = \nweeks\n)\n\n# Create a list of weekly means\nlapply(X = temps_weekly, FUN = mean)\n\n\n\n\nSelection by \nendpoints\n vs. \nsplit-lapply-rbind\n\n\n# use the proper combination of split, lapply and rbind.\n#T1 \n- do.call(rbind, ___(___(Temps, \nweeks\n), function(w) last(w, n=\n___\n)))\nT1 \n- do.call(rbind, lapply(split(Temps, \nweeks\n), function(w) last(w, n= \n1 day\n)))\n\n# now subset Temps using the results of 'endpoints'\nlast_day_of_weeks \n- endpoints(Temps, on = \nweeks\n)\nT2 \n- Temps[last_day_of_weeks]\n\n\n\n\nConvert univariate series to OHLC data\n\n\nIn financial series it is common to find Open-High-Low-Close data calculated over some repeating and regular interval.\n\n\nAlso known as range bars, aggregating a series based on some regular window can make analysis easier amongst series that have varying frequencies. For example, a weekly economic series and a daily stock series can be compared more easily if the daily is converted to weekly.\n\n\nOHLC means Opening, High, Low, Closing values.\n\n\nto.period(x,\n          period = \nmonths\n, \n          k = 1, \n          indexAt, \n          name=NULL,\n          OHLC = TRUE,\n          ...)\n\n# Convert USDEUR to weekly\n# OHLC = FALSE by default\nUSDEUR_weekly \n- to.period(USDEUR, period = \nweeks\n)\nhead(USDEUR)\nhead(USDEUR_weekly)\n\n# Convert USDEUR_weekly to monthly\nUSDEUR_monthly \n- to.period(USDEUR_weekly, period = \nmonths\n)\n\n# Convert USDEUR_monthly to yearly univariate\nUSDEUR_yearly \n- to.period(USDEUR_monthly, period = \nyears\n, OHLC = FALSE)\n\n\n\n\nConvert a series to a lower frequency\n\n\nBesides converting univariate time series to OHLC series, to.period() also lets you convert OHLC to lower regularized frequency - something like subsampling your data.\n\n\nFor example, when using the shortcut function \nto.quarterly()\n xts will convert your index to the \nyearqtr\n class to make periods more obvious.\n\n\n# Convert EqMktNeutral to quarterly OHLC\nmkt_quarterly \n- to.period(EqMktNeutral, period = \nquarters\n)\n\n# Convert EqMktNeutral to quarterly using shortcut function\n# Change the base name of each OHLC column to EDHEC.Equity and change the index to \nfirstof\n\nmkt_quarterly2 \n- to.quarterly(EqMktNeutral, name = \nEDHEC.Equity\n, indexAt = \nfirst\n)\n\n\n\n\nCalculate basic rolling value of series by month\n\n\nRolling windows can be discrete: use \nlapply\n with (\ncumsum, cumprod, cummax, cummin\n) and \nsplit\n:\n\n\n# split-lapply-rbind pattern\n\nx_split \n- split(x, f = \nmonths\n)\nx_list \n- lapply(x_split, cummax)\nx_list_rbind \n- do.call(rbind, x_list)\n\n# Split edhec into years\nedhec_years \n- split(edhec , f = \nyears\n)\n\n# Use lapply to calculate the cumsum for each year\nedhec_ytd \n- lapply(edhec_years, FUN = cumsum)\n\n# Use do.call to rbind the results\nedhec_xts \n- do.call(rbind, edhec_ytd)\n\n\n\n\nCalculate the rolling standard deviation of a time series\n\n\nRolling windows can be continuous: use \nrollapply\n:\n\n\nrollapply(x, 10, FUN = max, na.rm = TRUE)\n\n# Use rollapply to calculate the rolling 3 period sd of EqMktNeutral\neq_monthly \n- rollapply(EqMktNeutral, 3, FUN = sd)\n\n\n\n\n5, Extra features of xts\n\n\nIndex, attributes, and time zones\n\n\nAll time is stored in seconds since 1970-01-01. Time via \n.index()\n () and \nindex()\n (raw seconds since 1970-01-01).\n\n\nindex()\n returns the time of an \nxts\n object, as a vector of class \nindexClass\n.\n\n\nSys.setenv()\n.\n\n\nClass attributes - \ntclass\n, \ntzone\n and \ntformat\n\n\nxts\n objects are somewhat tricky when it comes to times. Internally, we have now seen that the index attribute is really a vector of numeric values corresponding to the seconds since the UNIX epoch (1970-01-01).\n\n\nHow these values are displayed on printing and how they are returned to the user when using the \nindex()\n function is dependent on a few key internal attributes.\n\n\nThe information that controls this behavior can be viewed and even changed through a set of accessor functions detailed below:\n\n\n\n\nThe index class using indexClass (e.g. from Date to chron)\n\n\nThe time zone using indexTZ (e.g. from America/Chicago to Europe/London)\n\n\nThe time format to be displayed via indexFormat (e.g. YYYY/MM/DD)\n\n\n\n\n# Get the index class of temps\nindexClass(temps)\n\n# Get the timezone of temps\nindexTZ(temps)\n\n# Change the format of the time display\nindexFormat(temps) \n- \nM/DD/YYYY\n\n\n# Extract the new index\ntime(temps)\n\n\n\n\nTime zones - (and why you should care!)\n\n\nR provides time zone support in native classes \nPOSIXct\n and \nPOSIXlt\n. xts extends this power to the entire object, allowing you to have multiple timezones across various objects. One very important thing to remember is that some internal operation system functions require a timezone to do date math, and if not set one is chosen for you! Be careful to always set a timezone in your environment to prevent very hard to debug errors when working with dates and times. \n\n\nxts\n provides the function \ntzone\n. This function allows to you to extract, or set timezones. \ntzone(x) \n- \"Time_Zone\"\n In this exercise you will work with an object called times to practice constructing your own xts objects with custom time zones.\n\n\n# Construct times_xts with TZ set to America/Chicago\ntimes_xts \n- xts(1:10, order.by = times, tzone = \nAmerica/Chicago\n)\n\n# Change the time zone to Asia/Hong_Kong\ntzone(times_xts) \n- \nAsia/Hong_Kong\n\n\n# Extract the current timezone \nindexTZ(times_xts)\n\n\n\n\nPeriods, periodicity, and timestamps\n\n\nPeriods (yearly or intradays time index?): \nperiodicity()\n\n\nBroken downtime with \n.index*\n, \n.index()\n, \n.indexmday()\n, \nindexyday()\n, \nindexyear()\n.\n\n\nTimestamps.\n\n\nalign.time()\n round time stamps to another period.\n\n\nmake.index.unique()\n removes observations from duplicate time stamps.\n\n\nDetermining periodicity\n\n\nThe idea of periodicity is pretty simple; with what regularity does your data repeat? For stock market data, you might have hourly prices or maybe daily open-high-low-close bars. For macro economic series, it might be monthly or weekly survey numbers.\n\n\n# Calculate the periodicity of temps\nperiodicity(temps)\n\n# Calculate the periodicity of edhec\nperiodicity(edhec)\n\n# Convert edhec to yearly\nedhec_yearly \n- to.yearly(edhec, 12)\n\n# Calculate the periodicity of edhec_yearly\nperiodicity(edhec_yearly)\n\n\n\n\nFind the number of periods in your data\n\n\nOften times it is handy to know not just the range of your time series index, but also have an idea of how many discrete irregular periods this covers. xts provides a set of functions to do just that. If you have a time series, it is now easy to see how many days, weeks or years your data contains.\n\n\nCount: \nnseconds()\n, \nnminutes()\n, \nnhours()\n, etc.\n\n\n`# Count the months\nnmonths(edhec)\n\n# Count the quarters\nnquarters(edhec)\n\n# Count the years\nnyears(edhec)  \n\n\n\n\nSecret index tools\n\n\nNormally you want to access the times you stored. \nindex()\n does this magically for you by using your \nindexClass\n. To get to the raw vector another function is provided, \n.index()\n. Note the critical dot before the function name.\n\n\nMore useful than extracting raw seconds is the ability to extract time components similar to the \nPOSIXlt\n class, which mirrors closely the underlying POSIX internal compiled structure \ntm\n. This is provided by a handful of functions such as \n.indexday()\n, \n.indexmon()\n, \n.indexyear()\n and more.\n\n\n# Explore underlying units of temps\n.index(temps) # in seconds\n.indexwday(temps) # (0 for Sun, 1, 2,..., 6 for Sat) day of the week\n\n# Create an index using which (Sunday has a value of 0, and Saturday has a value of 6)\nindex \n- which(.indexwday(temps) == 0 | .indexwday(temps) == 6)\n\n# Select the index\ntemps[index]\n\n\n\n\nModifying timestamps\n\n\nDepending on your field you might encounter higher frequency data - think intraday trading intervals, or sensor data from medical equipment.\n\n\nIf you find that you have observations with identical timestamps, it might be useful to perturb or remove these times to allow for uniqueness.\n\n\nOn other ocassions you might find your timestamps a bit too precise. In these instances it might be better to round up to some fixed interval, for example an observation may occur at any point in an hour, but you want to record the latest as of beginning of the next hour.\n\n\nmake.index.unique(x, eps = 1e-4)  # Perturb\nmake.index.unique(x, drop = TRUE) # Drop duplicates\nalign.time(x, n = 60) # Round to the minute\n\n# Make z have unique timestamps\nmake.index.unique(z, eps = 1e-4)\n\n# Remove duplicate times in z\nmake.index.unique(z, drop = TRUE)\n\n# Round observations to the next time\nalign.time(z, n = 3600) # next hour", 
            "title": "Time Series in R, The Power of xts and zoo"
        }, 
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#2-first-order-of-business-basic-manipulations", 
            "text": "Converting xts objects  xts  provides methods to convert all of the major objects you are likely to come across - suitable native R types like matrix, data.frame, and ts are supported, as well as contributed ones such as  timeSeries ,  fts  and of course  zoo .  as.xts  is the workhorse function to do the conversions to  xts , and similar functions will provide the reverse behavior.  # using a R dataset\nhead(austres)  \n     13067.3 \n     13130.5 \n     13198.4 \n     13254.2 \n     13303.7 \n     13353.9   # Convert austres to an xts object called au\nau  - as.xts(austres)\nhead(au, 5)\n\n# Convert your xts object (au) into a matrix am\nam  - as.matrix(au)\nhead(am, 5)\n\n# Convert the original austres into a matrix am2\nam2  - as.matrix(austres)\nhead(am2, 5)             [,1]\n1971 Q2 13067.3\n1971 Q3 13130.5\n1971 Q4 13198.4\n1972 Q1 13254.2\n1972 Q2 13303.7   \n     1971 Q2 13067.3 \n     1971 Q3 13130.5 \n     1971 Q4 13198.4 \n     1972 Q1 13254.2 \n     1972 Q2 13303.7     \n     13067.3 \n     13130.5 \n     13198.4 \n     13254.2 \n     13303.7    Importing data  Read raw data from files on disk or the web and convert data to  xts .  Read the same data into a  zoo  object using read.zoo and convert the  zoo  into an  xts  object.  # Create dat by reading tmp_file\ndat  - as.matrix(read.csv('tmp_file.csv', sep = ',', header = TRUE))\ndat   a b  \n     1/02/2015 1 3 \n     2/03/2015 2 4    # Convert dat into xts\nxts(dat, order.by = as.Date(rownames(dat),  %m%d%Y ))       a b NA  1 3 NA  2 4  # Read tmp_file using read.zoo and as.xts\ndat_xts  - as.xts(read.zoo('tmp_file.csv', index.column = 0, sep =  , , format =  %m/%d/%Y ))\ndat_xts             a b\n2015-01-02 1 3\n2015-02-03 2 4  Exporting xts objects  head(sunspots, 5)\n\n# Convert sunspots to xts\nsunspots_xts  - as.xts(sunspots)  \n     58 \n     62.6 \n     70 \n     55.7 \n     85   # Get the temporary file name\ntmp  - tempfile()\n\n# Write the xts object using zoo to tmp \nwrite.zoo(sunspots, sep =  , , file = tmp)\n\ntmp  C:\\Users\\Dell\\AppData\\Local\\Temp\\RtmpEdYcAQ\\file1a706c8c732b  # Read the tmp file\n# FUN=as.yearmon will convert strings such as Jan 1749 into a proper time class\nsun  - read.zoo(tmp, sep =  , , FUN = as.yearmon)\n\n# Convert sun into xts\nsun_xts  - as.xts(sun)\nhead(sun_xts, 5)  Error in eval(expr, envir, enclos): impossible de trouver la fonction \"read.zoo\"\nTraceback:  The ISO8601 Standard  The ISO8601 standard is the internationally recognized and accepted way to represent dates and times. From the biggest to the smallest, left to right, YYYY-MM-DDTHH:MM:SS.  The standard allows for a common format to not only describe dates, but also a way to represent ranges and repeating intervals:   2004 or 2001/2015  201402/03  2014-02-22 08:30:00  T08:00/T09:00   Querying for dates  Date ranges can be extracted from xts objects by simply specifying the period(s) you want using special character strings in your subset.  Code:  A[ 201601 ]         ## Jan 2016\nA[ 20160125 ]       ## Jan 25, 2016\nA[ 201203/201212 ]  ## Feb to Dec 2012\n\n# Select all of 2016 from x\nx_2016  - x[ 2016 ]\n\n# Select January 2016 to March 22\njan_march  - x[ 201601/20160322 ]  Extracting recurring intraday intervals  Most common data  in the wild  is daily. On ocassion you may find yourself working with intraday data, that is date plus times.  You can slice days easily by using special notation in the  i =  argument to the single bracket extraction (i.e.  [i,j] ).  Code:  # Intraday times for all days\nNYSE[ T09:30/T16:00 ]\n\n# Extract all data between 8AM and 10AM\nmorn_2010  - irreg[ T8:00/T10:00 ]\n\n# Extract the observations for January 13th\nmorn_2010[ 2010-01-13 ]  Alternating extraction techniques  Often times you may need to subset an existing time series with a set of Dates, or time-based objects. These might be from  as.Date() , or  as.POSIXct() , or a variety of other classes.  Code:  # Subset x using the vector dates\nx[dates]\n\n# Subset x using dates as POSIXct\nx[as.POSIXct(dates)]  Extracting recurring intraday intervals  The most common time series data is daily, intraday data, which contains both dates and times.   # Extract all data between 8AM and 10AM\nmorn_2010  - irreg[ T8:00/T10:00 ]\n\n# Extract the observations for January 13th\nmorn_2010[ 2010-01-13 ]  Row selection with time objects  Subset an existing time series with a set of Dates, or time-based objects.  # Subset x using the vector dates\nx[dates]\n\n# Subset x using dates as POSIXct\nx[as.POSIXct(dates)]  Update and replace elements  Replace known intervals or observations with an NA, say due to a malfunctioning sensor on a particular day or a set of outliers given a holiday.  Code:  # Replace all values in x on dates with NA\nx[dates]  - NA\n\n# Replace dates from 2016-06-09 and on with 0\nx[ 20160609/ ]  - 0  Find the  first  or  last  period of time  Sometimes you need to locate data by relative time. Instead of using an absolute offset, you describe a position relative in time. A simple example would be something like the  last 3 weeks  of a series, or the  first day of current month .  Code:  # Create lastweek with using the last 1 week of temps\nlastweek  - last(temps,  1 week )\n\n# Print the last 2 observations in lastweek\nlast(lastweek, 2)\n\n# Extract all but the last two days of lastweek\nlast(lastweek,  -2 day )  Combining  first  and  last  Code:  # Last 3 days of first week\nlast(first(Temps, '1 week'),'3 days') \n\n# Extract the first three days of the second week of temps\nfirst(last(first(temps,  2 week ),  1 week ),  3 day )  Matrix arithmetic - add, subtract, multiply and divide in time!  When you perform any binary operation using two xts objects, these objects are first aligned using the intersection of the indexes to preserve the point-in-time aspect of your data, assuring that you don t introduce accidental look ahead (or look behind!) bias into your calculations.  Your options include:   coredata() or as.numeric() (drop one to a matrix or vector).  Manually shift index values - i.e. use lag().  Reindex your data (before or after the calculation).   xts  respects time and will only return the intersection of times when doing various mathematical operations. First option:  # Add a and b\na + b\n\n# Add a with the numeric value of b\na + as.numeric(b)  Math with non-overlapping indexes  This third way involves modifying the two series you want by aligning assuring you have some union of dates - the dates you require in your final output. This makes it possibly to preserve dimensionality of the data:  merge(b, index(a))\n\n# Add a to b, and fill all missing rows of b with 0\na + merge(b, index(a), fill = 0)\n\n# Add a to b and fill NAs with the last observation\na + merge(b, index(a), fill = na.locf)", 
            "title": "2, First Order of Business - Basic Manipulations"
        }, 
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#3-merging-and-modifying-time-series", 
            "text": "Combining  xts  by column with  merge  xts  makes it easy to join data by column and row using a few different functions.    xts  objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types.  One of the most important functions to accomplish this is  merge . It works like a  cbind  or and SQL  join :   inner join (and)  outer join (or)  left join  right join   # Basic argument use\nmerge(a, b, join =  right , fill = 9999)\n\n# Perform an inner join of a and b\nmerge(a, b, join =  inner )\n\n# Perform of a left-join of a and b, fill mising values with 0\nmerge(a, b, join =  left , fill = 0)\n\n# fill = na.locf, fill = NA  Combining  xts  by row with  rbind  Easy to add new rows to your data.  # Row bind temps_june30 to temps, assign this to temps2\ntemps2  - rbind(temps, temps_june30)\n\n# Row bind temps_july17 and temps_july18 to temps2, call this temps3\ntemps3  - rbind(temps2, temps_july17, temps_july18)  Fill missing values using last or previous observation  The  na.locf  function takes the last-observation-carried-forward approach.  The  xts  package leverages the power of  zoo  for help with this.  zoo  provides a variety of missing data handling functions which are usable by  xts , and very handy.  # Last obs. carried forward\nna.locf(x)                \n\n# Next obs. carried backward\nna.locf(x, fromLast = TRUE)\n\n# Fill missing values using the last observation\nna.locf(temps, na.rm = TRUE,\n                fromLast = TRUE)\n\n# Fill missing values using the next observation\nna.locf(temps, na.rm = TRUE)\n\n# maxgap = Inf\n# rule = 2,  NA interpolation  From  zoo .  On occassion, a simple carry forward approach to missingness isn t appropriate. It may be that a series is missing an observation due to a higher frequency sampling than the generating process. You might also encounter an observation that is in error, yet expected to be somewhere between the values of its neighboring observations.  Both of these cases are examples of where interpolation is useful.  # Interpolate NAs using linear approximation\nna.approx(AirPass)  Combine a leading and lagging time series  Another common modification for time series is the ability to lag a series.  # Your final object\nlag(x, k = 1, na.pad = TRUE)\n\n# k = -1  leading\n# k = 1   lagging\n# zoo uses the other way around\n\n# k = c(0, 1, 5)  multiple leads or lags\n\n# Create a leading object called lead_x\nlead_x  - lag(x, k = -1)\n\n# Create a lagging object called lag_x\nlag_x  - lag(x, k = 1)\n\n# Merge your three series together and assign to z\nz  - merge(lead_x, x, lag_x)  Calculate a difference of a series with  diff  Another common operation on time series, typically on those that are non-stationary, is to take a difference of the series.  differences  is the order of the difference (how many times  diff  is called). A simple way to view a single (or  first order ) difference is to see it as  x(t) - x(t-k)  where  k  is the number of lags to go back. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference).  diff(x, lag = 1, difference = 1, log = FALSE, na.pad = TRUE)\n\n# calculate the first difference of AirPass using lag and subtraction\nAirPass - lag(AirPass, k = 1)\n\n# calculate the first order 12-month difference if AirPass\ndiff(AirPass, lag = 12, differences = 1)  What is the key difference in lag between xts and zoo? The  k  argument in  zoo  uses positive values for shifting past observations forward.", 
            "title": "3, Merging and modifying time series"
        }, 
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#4-apply-and-aggregate-by-time", 
            "text": "Find intervals by time in  xts  The main function in xts to facilitate this is endpoints. It takes a time series (or a vector of times) and returns the locations of the last observations in each interval.  For example, the code below locates the last observation of each year for the  AirPass  dataset:  endpoints(AirPass, on =  years )\n\n# The argument on supports a variety of periods, including  years ,  quarters ,  months , as well as intraday intervals such as  hours , and  minutes \n\n# on =  weeks , k = 2, the result would be the final day of every other week in your data\n\n# Locate the final day of every week\nendpoints(temps, on =  weeks )\n\n# Locate the final day of every two weeks\nendpoints(temps, on =  weeks , k = 2)  Apply a function by time period(s)  xts  provides  period.apply , which takes a time series, an  INDEX  of endpoints, and a function to  apply :  period.apply(x, INDEX, FUN, ...)\n\n# Calculate the weekly endpoints\nep  - endpoints(temps, on =  weeks )\n\n# Now calculate the weekly mean and display the results\nperiod.apply(temps[,  Temp.Mean ], INDEX = ep, FUN = mean)  Using  lapply  and  split  to apply functions on intervals  Often it is useful to physically split your data into disjoint chunks by time and perform some calculation on these periods.  # Split temps by week\ntemps_weekly  - split(temps, f =  weeks )\n\n# Create a list of weekly means\nlapply(X = temps_weekly, FUN = mean)  Selection by  endpoints  vs.  split-lapply-rbind  # use the proper combination of split, lapply and rbind.\n#T1  - do.call(rbind, ___(___(Temps,  weeks ), function(w) last(w, n= ___ )))\nT1  - do.call(rbind, lapply(split(Temps,  weeks ), function(w) last(w, n=  1 day )))\n\n# now subset Temps using the results of 'endpoints'\nlast_day_of_weeks  - endpoints(Temps, on =  weeks )\nT2  - Temps[last_day_of_weeks]  Convert univariate series to OHLC data  In financial series it is common to find Open-High-Low-Close data calculated over some repeating and regular interval.  Also known as range bars, aggregating a series based on some regular window can make analysis easier amongst series that have varying frequencies. For example, a weekly economic series and a daily stock series can be compared more easily if the daily is converted to weekly.  OHLC means Opening, High, Low, Closing values.  to.period(x,\n          period =  months , \n          k = 1, \n          indexAt, \n          name=NULL,\n          OHLC = TRUE,\n          ...)\n\n# Convert USDEUR to weekly\n# OHLC = FALSE by default\nUSDEUR_weekly  - to.period(USDEUR, period =  weeks )\nhead(USDEUR)\nhead(USDEUR_weekly)\n\n# Convert USDEUR_weekly to monthly\nUSDEUR_monthly  - to.period(USDEUR_weekly, period =  months )\n\n# Convert USDEUR_monthly to yearly univariate\nUSDEUR_yearly  - to.period(USDEUR_monthly, period =  years , OHLC = FALSE)  Convert a series to a lower frequency  Besides converting univariate time series to OHLC series, to.period() also lets you convert OHLC to lower regularized frequency - something like subsampling your data.  For example, when using the shortcut function  to.quarterly()  xts will convert your index to the  yearqtr  class to make periods more obvious.  # Convert EqMktNeutral to quarterly OHLC\nmkt_quarterly  - to.period(EqMktNeutral, period =  quarters )\n\n# Convert EqMktNeutral to quarterly using shortcut function\n# Change the base name of each OHLC column to EDHEC.Equity and change the index to  firstof \nmkt_quarterly2  - to.quarterly(EqMktNeutral, name =  EDHEC.Equity , indexAt =  first )  Calculate basic rolling value of series by month  Rolling windows can be discrete: use  lapply  with ( cumsum, cumprod, cummax, cummin ) and  split :  # split-lapply-rbind pattern\n\nx_split  - split(x, f =  months )\nx_list  - lapply(x_split, cummax)\nx_list_rbind  - do.call(rbind, x_list)\n\n# Split edhec into years\nedhec_years  - split(edhec , f =  years )\n\n# Use lapply to calculate the cumsum for each year\nedhec_ytd  - lapply(edhec_years, FUN = cumsum)\n\n# Use do.call to rbind the results\nedhec_xts  - do.call(rbind, edhec_ytd)  Calculate the rolling standard deviation of a time series  Rolling windows can be continuous: use  rollapply :  rollapply(x, 10, FUN = max, na.rm = TRUE)\n\n# Use rollapply to calculate the rolling 3 period sd of EqMktNeutral\neq_monthly  - rollapply(EqMktNeutral, 3, FUN = sd)", 
            "title": "4, Apply and aggregate by time"
        }, 
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#5-extra-features-of-xts", 
            "text": "Index, attributes, and time zones  All time is stored in seconds since 1970-01-01. Time via  .index()  () and  index()  (raw seconds since 1970-01-01).  index()  returns the time of an  xts  object, as a vector of class  indexClass .  Sys.setenv() .  Class attributes -  tclass ,  tzone  and  tformat  xts  objects are somewhat tricky when it comes to times. Internally, we have now seen that the index attribute is really a vector of numeric values corresponding to the seconds since the UNIX epoch (1970-01-01).  How these values are displayed on printing and how they are returned to the user when using the  index()  function is dependent on a few key internal attributes.  The information that controls this behavior can be viewed and even changed through a set of accessor functions detailed below:   The index class using indexClass (e.g. from Date to chron)  The time zone using indexTZ (e.g. from America/Chicago to Europe/London)  The time format to be displayed via indexFormat (e.g. YYYY/MM/DD)   # Get the index class of temps\nindexClass(temps)\n\n# Get the timezone of temps\nindexTZ(temps)\n\n# Change the format of the time display\nindexFormat(temps)  -  M/DD/YYYY \n\n# Extract the new index\ntime(temps)  Time zones - (and why you should care!)  R provides time zone support in native classes  POSIXct  and  POSIXlt . xts extends this power to the entire object, allowing you to have multiple timezones across various objects. One very important thing to remember is that some internal operation system functions require a timezone to do date math, and if not set one is chosen for you! Be careful to always set a timezone in your environment to prevent very hard to debug errors when working with dates and times.   xts  provides the function  tzone . This function allows to you to extract, or set timezones.  tzone(x)  - \"Time_Zone\"  In this exercise you will work with an object called times to practice constructing your own xts objects with custom time zones.  # Construct times_xts with TZ set to America/Chicago\ntimes_xts  - xts(1:10, order.by = times, tzone =  America/Chicago )\n\n# Change the time zone to Asia/Hong_Kong\ntzone(times_xts)  -  Asia/Hong_Kong \n\n# Extract the current timezone \nindexTZ(times_xts)  Periods, periodicity, and timestamps  Periods (yearly or intradays time index?):  periodicity()  Broken downtime with  .index* ,  .index() ,  .indexmday() ,  indexyday() ,  indexyear() .  Timestamps.  align.time()  round time stamps to another period.  make.index.unique()  removes observations from duplicate time stamps.  Determining periodicity  The idea of periodicity is pretty simple; with what regularity does your data repeat? For stock market data, you might have hourly prices or maybe daily open-high-low-close bars. For macro economic series, it might be monthly or weekly survey numbers.  # Calculate the periodicity of temps\nperiodicity(temps)\n\n# Calculate the periodicity of edhec\nperiodicity(edhec)\n\n# Convert edhec to yearly\nedhec_yearly  - to.yearly(edhec, 12)\n\n# Calculate the periodicity of edhec_yearly\nperiodicity(edhec_yearly)  Find the number of periods in your data  Often times it is handy to know not just the range of your time series index, but also have an idea of how many discrete irregular periods this covers. xts provides a set of functions to do just that. If you have a time series, it is now easy to see how many days, weeks or years your data contains.  Count:  nseconds() ,  nminutes() ,  nhours() , etc.  `# Count the months\nnmonths(edhec)\n\n# Count the quarters\nnquarters(edhec)\n\n# Count the years\nnyears(edhec)    Secret index tools  Normally you want to access the times you stored.  index()  does this magically for you by using your  indexClass . To get to the raw vector another function is provided,  .index() . Note the critical dot before the function name.  More useful than extracting raw seconds is the ability to extract time components similar to the  POSIXlt  class, which mirrors closely the underlying POSIX internal compiled structure  tm . This is provided by a handful of functions such as  .indexday() ,  .indexmon() ,  .indexyear()  and more.  # Explore underlying units of temps\n.index(temps) # in seconds\n.indexwday(temps) # (0 for Sun, 1, 2,..., 6 for Sat) day of the week\n\n# Create an index using which (Sunday has a value of 0, and Saturday has a value of 6)\nindex  - which(.indexwday(temps) == 0 | .indexwday(temps) == 6)\n\n# Select the index\ntemps[index]  Modifying timestamps  Depending on your field you might encounter higher frequency data - think intraday trading intervals, or sensor data from medical equipment.  If you find that you have observations with identical timestamps, it might be useful to perturb or remove these times to allow for uniqueness.  On other ocassions you might find your timestamps a bit too precise. In these instances it might be better to round up to some fixed interval, for example an observation may occur at any point in an hour, but you want to record the latest as of beginning of the next hour.  make.index.unique(x, eps = 1e-4)  # Perturb\nmake.index.unique(x, drop = TRUE) # Drop duplicates\nalign.time(x, n = 60) # Round to the minute\n\n# Make z have unique timestamps\nmake.index.unique(z, eps = 1e-4)\n\n# Remove duplicate times in z\nmake.index.unique(z, drop = TRUE)\n\n# Round observations to the next time\nalign.time(z, n = 3600) # next hour", 
            "title": "5, Extra features of xts"
        }, 
        {
            "location": "/Jupyter Notebook and the R Kernel/", 
            "text": "Jupyter Notebook\n\n\nForeword\n\n\nNotes.\n\n\n\n\nCONTENT\n\n\nJupyter Notebook\n\n\nDistribution\n\n\nImplementing the R Kernel (or irkernel)\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\n\n\n\n\nJupyter.org (Official Website)\n\n\nOn Windows, use the Anaconda distribution.\n\n\nThe stack includes the Jupyter (IPython) Notebook, the Cloud (online resources), the Navigator (home screen), the prompt (shell), the QTConsole, and the Spyder IDE.\n\n\n\n\n\n\nOn Linux, the Jupyter Stack can easily be installed through the bash with \npip\n. \n\n\nLaunch Jupyter Notebook (\njupyter notebook\n) or any tools through the bash or in the GUI.\n\n\n\n\n\n\nAnaconda2 has Python 2 by default (root) and Anaconda3, no surprise, has Python 3!\n\n\nHowever, other kernels can be installed on any distribution (another Python version, R, bash, etc.).\n\n\n\n\nImplementing the R Kernel (or irkernel)\n\n\n\n\nStart R (admin or sudo mode).\n\n\nEnter the commands:\n\n\n\n\ninstall.packages(c('repr','pbdZMQ', 'devtools')) # reprisalready on CRAN\n\ndevtools::install_github('IRkernel/IRdisplay')\n\ndevtools::install_github('IRkernel/IRkernel')\n\nIRkernel::installspec()\n\n\n\n\n\n\nConfigure Jupyter in R: \nIRkernel::installspec(user = FALSE)\n.\n\n\nStart Jupyter Notebook.", 
            "title": "Jupyter Notebook and the R Kernel"
        }, 
        {
            "location": "/Jupyter Notebook and the R Kernel/#distribution", 
            "text": "Jupyter.org (Official Website)  On Windows, use the Anaconda distribution.  The stack includes the Jupyter (IPython) Notebook, the Cloud (online resources), the Navigator (home screen), the prompt (shell), the QTConsole, and the Spyder IDE.    On Linux, the Jupyter Stack can easily be installed through the bash with  pip .   Launch Jupyter Notebook ( jupyter notebook ) or any tools through the bash or in the GUI.    Anaconda2 has Python 2 by default (root) and Anaconda3, no surprise, has Python 3!  However, other kernels can be installed on any distribution (another Python version, R, bash, etc.).", 
            "title": "Distribution"
        }, 
        {
            "location": "/Jupyter Notebook and the R Kernel/#implementing-the-r-kernel-or-irkernel", 
            "text": "Start R (admin or sudo mode).  Enter the commands:   install.packages(c('repr','pbdZMQ', 'devtools')) # reprisalready on CRAN\n\ndevtools::install_github('IRkernel/IRdisplay')\n\ndevtools::install_github('IRkernel/IRkernel')\n\nIRkernel::installspec()   Configure Jupyter in R:  IRkernel::installspec(user = FALSE) .  Start Jupyter Notebook.", 
            "title": "Implementing the R Kernel (or irkernel)"
        }
    ]
}