<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Ugo Spark">
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Statistics with R, Course Three, Analysis of Variance - ugo_r_doc</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Statistics with R, Course Three, Analysis of Variance";
    var mkdocs_page_input_path = "A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance.md";
    var mkdocs_page_url = "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-93008985-1', 'mkdocs.org');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> ugo_r_doc</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../An%20Introduction%20to%20R/">An Introduction to R</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../Le%20logiciel%20R%2C%20maitriser%20le%20langage%2C%20effectuer%20des%20analyses%20%28bio%29statistiques/">Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Intermediate R</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Intermediate_R/">Intermediate R</a>
                </li>
                <li class="">
                    
    <a class="" href="../Intermediate_R_-_The_apply_Family/">Intermediate R - The apply Family</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">RStudio</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Code___Plot_Chunk_Options/">Code & Plot Chunk Options</a>
                </li>
                <li class="">
                    
    <a class="" href="../Working_with_RStudio_IDE/">Working with the RStudio IDE</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Data Wrangling Snippets</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../IO_snippets___Cleaning/">I/O snippets & Cleaning</a>
                </li>
                <li class="">
                    
    <a class="" href="../Reading_Data_into_R_with_readr/">Reading Data into R with readr</a>
                </li>
                <li class="">
                    
    <a class="" href="../Data_Wrangling/">Data Wrangling</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Plot Snippets</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Plot_snippets_-_Basics/">Plot Snippets for Exploratory (and some Explanatory) Analyses</a>
                </li>
                <li class="">
                    
    <a class="" href="../Plot_snippets_-_ggplot2/">Plot Snippets - ggplot2</a>
                </li>
                <li class="">
                    
    <a class="" href="../Plot_snippets_-_ggvis/">Plot Snippets - ggvis</a>
                </li>
                <li class="">
                    
    <a class="" href="../Plot_snippets_-_Colours/">Plot Snippets - Colours</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Other Snippets</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Tables/">Tables</a>
                </li>
                <li class="">
                    
    <a class="" href="../Charts/">Charts & Colors</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">A Hands-on Introduction to Statistics with R</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_One%2C_Introduction/">Statistics with R, Course One, Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_Two%2C_Student_s_T-test/">Statistics with R, Course Two, Student's t-test</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Statistics with R, Course Three, Analysis of Variance</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#2-post-hoc-analysis">2, Post-hoc analysis</a></li>
    

    <li class="toctree-l3"><a href="#3-between-groups-factorial-anova">3, Between groups factorial ANOVA</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_Four%2C_Repeated_Measures_Anova/">Statistics with R, Course Four, Repeated Measures ANOVA</a>
                </li>
                <li class="">
                    
    <a class="" href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_Five%2C_Correlation_and_Regression/">Statistics with R, Course Five, Correlation and Regression</a>
                </li>
                <li class="">
                    
    <a class="" href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_Six%2C_Multiple_Regression/">Statistics with R, Course Six, Multiple Regression</a>
                </li>
                <li class="">
                    
    <a class="" href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_Seven%2C_Moderation_and_Mediation/">Statistics with R, Course Seven, Moderation and Mediation</a>
                </li>
                <li class="">
                    
    <a class="" href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Notes/">Statistics with R, Notes</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../Big_Data_Analysis_with_Revolution_R_Enterprise/">Big Data Analysis with Revolution R Enterprise</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../Time%2BSeries%2Bin%2BR%2BThe%2BPower%2Bof%2Bxts%2Band%2Bzoo/">Time Series in R, The Power of xts and zoo</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">ugo_r_doc</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>A Hands-on Introduction to Statistics with R &raquo;</li>
        
      
    
    <li>Statistics with R, Course Three, Analysis of Variance</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <ul>
<li><a href="#an-introduction-to-anova">1, An introduction to ANOVA</a></li>
<li><a href="#post-hoc-analysis">2, Post-hoc analysis</a></li>
<li><a href="#between-groups-factorial-anova">3, Between groups factorial ANOVA</a></li>
</ul>
<hr />
<p><strong>Foreword</strong></p>
<ul>
<li>Output options: the &lsquo;tango&rsquo; syntax and the &lsquo;readable&rsquo; theme.</li>
<li>Snippets and results.</li>
</ul>
<hr />
<h4 id="1-an-introduction-to-anova">1, An introduction to ANOVA<a class="headerlink" href="#1-an-introduction-to-anova" title="Permanent link">&para;</a></h4>
<p><strong>Working memory experiment</strong></p>
<p>We&rsquo;ll use data from the working memory experiment, which investigates<br />
the relationship between the number of training days and a change in IQ.<br />
There are four independent groups, each of which trained for a different<br />
period of time: 8, 12, 17, or 19 days. The independent variable is the<br />
number of training days and the dependent variable is the IQ gain.</p>
<pre><code class="r"># Print the data set in the console
head(wm)
</code></pre>

<pre><code>##   subject condition   iq
## 1       1    8 days 12.4
## 2       2    8 days 11.8
## 3       3    8 days 14.6
## 4       4    8 days  7.7
## 5       5    8 days 15.7
## 6       6    8 days 11.6
</code></pre>
<pre><code class="r">library(psych)

# Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions)
describeBy(wm, wm$condition)
</code></pre>

<pre><code>## 
##  Descriptive statistics by group 
## group: 12 days
##            vars  n mean   sd median trimmed  mad  min  max range skew
## subject       1 20 30.5 5.92  30.50   30.50 7.41 21.0 40.0  19.0 0.00
## condition*    2 20  NaN   NA     NA     NaN   NA  Inf -Inf  -Inf   NA
## iq            3 20 11.7 2.58  11.65   11.69 2.89  6.9 16.1   9.2 0.05
##            kurtosis   se
## subject       -1.38 1.32
## condition*       NA   NA
## iq            -1.06 0.58
## -------------------------------------------------------- 
## group: 17 days
##            vars  n mean   sd median trimmed  mad  min  max range skew
## subject       1 20 50.5 5.92   50.5    50.5 7.41 41.0 60.0  19.0 0.00
## condition*    2 20  NaN   NA     NA     NaN   NA  Inf -Inf  -Inf   NA
## iq            3 20 13.9 2.26   13.6    13.9 2.00  9.8 18.1   8.3 0.11
##            kurtosis   se
## subject       -1.38 1.32
## condition*       NA   NA
## iq            -0.85 0.50
## -------------------------------------------------------- 
## group: 19 days
##            vars  n  mean   sd median trimmed  mad  min  max range  skew
## subject       1 20 70.50 5.92   70.5   70.50 7.41 61.0 80.0  19.0  0.00
## condition*    2 20   NaN   NA     NA     NaN   NA  Inf -Inf  -Inf    NA
## iq            3 20 14.75 2.50   15.3   14.71 2.15 10.4 19.2   8.8 -0.09
##            kurtosis   se
## subject       -1.38 1.32
## condition*       NA   NA
## iq            -0.99 0.56
## -------------------------------------------------------- 
## group: 8 days
##            vars  n  mean   sd median trimmed  mad min  max range  skew
## subject       1 20 10.50 5.92   10.5   10.50 7.41 1.0 20.0  19.0  0.00
## condition*    2 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf    NA
## iq            3 20 10.91 2.63   11.3   10.97 2.67 5.4 15.7  10.3 -0.21
##            kurtosis   se
## subject       -1.38 1.32
## condition*       NA   NA
## iq            -0.70 0.59
</code></pre>
<pre><code class="r"># Boxplot IQ versus cond
boxplot(wm$iq ~ wm$condition, main=&quot;Boxplot&quot;, xlab=&quot;Group (cond)&quot;, ylab=&quot;IQ&quot;)
</code></pre>

<p><center><img alt="" src="../A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/figure-markdown_strict+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-3-1.png" /></center></p>
<p>Notice that the IQ increases as the amount of training sessions<br />
increases.</p>
<p><strong>t-test vs ANOVA</strong></p>
<p>ANOVA is used when more than two group means are compared, whereas a<br />
t-test can only compare two group means.</p>
<p><strong>Generate density plot of the F-distribution</strong></p>
<p>The test statistic associated with ANOVA is the F-test (or F-ratio).<br />
Recall that when carrying out a t-test, you computed an observed<br />
t-value, then compared that with a critical value derived from the<br />
relevant t-distribution. That t-distribution came from a family of<br />
t-distributions, each of which was defined entirely by its degrees of<br />
freedom.</p>
<p>ANOVA uses the same principle, but instead an observed F-value is<br />
computed and compared to the relevant F-distribution. That<br />
F-distribution comes from a family of F-distributions, each of which is<br />
defined by two numbers (i.e. degrees of freedom).</p>
<p>F-distribution has a different shape than the t-distribution.</p>
<pre><code class="r"># Create the vector x
x &lt;- seq(from = 0, to = 2, length = 100)

# Simulate the F-distributions
y_1 &lt;- df(x, 1, 1)
y_2 &lt;- df(x, 3, 1)
y_3 &lt;- df(x, 6, 1)
y_4 &lt;- df(x, 3, 3)
y_5 &lt;- df(x, 6, 3)
y_6 &lt;- df(x, 3, 6)
y_7 &lt;- df(x, 6, 6)

# Plot the F-distributions
plot(x, y_1, col = 1, 'l')
lines(x,y_2, col = 2, 'l')
lines(x,y_3, col = 3, 'l')
lines(x,y_4, col = 4, 'l')
lines(x,y_5, col = 5, 'l')
lines(x,y_6, col = 6, 'l')
lines(x,y_7, col = 7, 'l')

# Add the legend in the top right corner and with the title 'F distributions'
legend('topright', title = 'F distributions', c('df = (1,1)', 'df = (3,1)', 'df = (6,1)', 'df = (3,3)', 'df = (6,3)', 'df = (3,6)', 'df = (6,6)'), col = c(1, 2, 3, 4, 5, 6, 7), lty = 1)
</code></pre>

<p><center><img alt="" src="../A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/figure-markdown_strict+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-4-1.png" /></center></p>
<p>The F-distribution cannot take negative values, because it is a ratio of<br />
variances and variances are always non-negative numbers. The<br />
distribution represents the ratio between the variance between groups<br />
and the variance within groups.</p>
<p><strong>Between group sum of squares</strong></p>
<p>To calculate the F-value, you need to calculate the ratio between the<br />
variance between groups and the variance within groups. Furthermore, to<br />
calculate the variance (i.e. mean of squares), you first have to<br />
calculate the sum of squares.</p>
<p>Now, remember that the working memory experiment investigates the<br />
relationship between the change in IQ and the number of training<br />
sessions. Calculate the between group sum of squares for the data from<br />
this experiment.</p>
<pre><code class="r"># Define number of subjects in each group
n &lt;- 20

# Calculate group means
Y_j &lt;- as.numeric(tapply(wm$iq, wm$condition, mean))
Y_j
</code></pre>

<pre><code>## [1] 11.700 13.905 14.750 10.910
</code></pre>
<pre><code class="r"># Calculate the grand mean
Y_T &lt;- mean(wm$iq)
Y_T
</code></pre>

<pre><code>## [1] 12.81625
</code></pre>
<pre><code class="r"># Calculate the sum of squares
SS_A &lt;- sum((Y_j - Y_T)^2) * n
SS_A
</code></pre>

<pre><code>## [1] 196.0914
</code></pre>
<p><strong>Within groups sum of squares</strong></p>
<p>To calculate the F-value, you also need the variance within groups.<br />
Similar to the last exercise, we&rsquo;ll start by computing the within groups<br />
sum of squares.</p>
<pre><code class="r"># Create four subsets of the four groups, containing the IQ results

# Make the subset for the group cond = '8 days'
Y_i1 &lt;- subset(wm$iq, wm$condition == '8 days')

# Make the subset for the group cond = '12 days'
Y_i2 &lt;- subset(wm$iq, wm$condition == '12 days')

# Make the subset for the group cond = '17 days'
Y_i3 &lt;- subset(wm$iq, wm$condition == '17 days')

# Make the subset for the group cond = '19 days'
Y_i4 &lt;- subset(wm$iq, wm$condition == '19 days')

# subtract the individual values by their group means
# You have already calculated the group means in the previous exercise so use this result, the vector that contains these group means was called Y_j
S_1 &lt;- Y_i1 - Y_j[1]
S_2 &lt;- Y_i2 - Y_j[2]

# Do it without the vector Y_j, so calculate the group means again.
S_3 &lt;- Y_i3 - mean(Y_i3)
S_4 &lt;- Y_i4 - mean(Y_i4)

#Put everything back in one vector
S_T &lt;- c(S_1,S_2,S_3,S_4)

#Calculate the sum of squares by using the vector S_T
SS_SA &lt;- sum(S_T^2)
</code></pre>

<p><strong>Calculating the F-ratio</strong>&lt;</p>
<p>Calculate the F-ratio.</p>
<pre><code class="r"># Number of groups
a &lt;- 4

# Number of subject in each group
n &lt;- 20

# Define the degrees of freedom
df_A &lt;- a - 1
df_SA &lt;- a*(n - 1)

# Calculate the mean squares (variances) by using the sum of squares SS_A and SS_SA
MS_A &lt;- SS_A/df_A
MS_SA &lt;- SS_SA/df_SA

# Calculate the F-ratio
F &lt;- MS_A/MS_SA
</code></pre>

<p><strong>A faster way: ANOVA in R</strong></p>
<p>Normally, we do not have to do all calculations.</p>
<pre><code class="r"># Apply the aov function
anova.wm &lt;- aov(wm$iq ~ wm$condition)
anova.wm
</code></pre>

<pre><code>## Call:
##    aov(formula = wm$iq ~ wm$condition)
## 
## Terms:
##                 wm$condition Residuals
## Sum of Squares      196.0914  473.4175
## Deg. of Freedom            3        76
## 
## Residual standard error: 2.495832
## Estimated effects may be unbalanced
</code></pre>
<pre><code class="r"># Look at the summary table of the result
summary(anova.wm)
</code></pre>

<pre><code>##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## wm$condition  3  196.1   65.36   10.49 7.47e-06 ***
## Residuals    76  473.4    6.23                     
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
<p>F-value is significant.</p>
<p><strong>Levene&rsquo;s test</strong></p>
<p>The assumptions of ANOVA are relatively simple. Similar to an<br />
independent t-test, we have a continuous dependent variable, which we<br />
assume to be normally distributed. Furthermore, we assume homogeneity of<br />
variance, which can be tested with Levene&rsquo;s test.</p>
<pre><code class="r">library(car)

# Levene's test
leveneTest(wm$iq ~ wm$condition)
</code></pre>

<pre><code>## Levene's Test for Homogeneity of Variance (center = median)
##       Df F value Pr(&gt;F)
## group  3  0.1405 0.9355
##       76
</code></pre>
<pre><code class="r"># Levene's test with the change for the default, namely center = mean
leveneTest(wm$iq ~ wm$condition, center = mean)
</code></pre>

<pre><code>## Levene's Test for Homogeneity of Variance (center = mean)
##       Df F value Pr(&gt;F)
## group  3  0.1598  0.923
##       76
</code></pre>
<p>The assumption of homogeneity of variance hold: the within group<br />
variance equivalent for all groups.</p>
<h4 id="2-post-hoc-analysis">2, Post-hoc analysis<a class="headerlink" href="#2-post-hoc-analysis" title="Permanent link">&para;</a></h4>
<p>Post-hoc tests help finding out which groups differ significantly from<br />
one other and which do not. More formally, post-hoc tests allow for<br />
multiple pairwise comparisons without inflating the type I error.</p>
<p>What does it mean to inflate the type I error?</p>
<p>Suppose the post-hoc test involves performing three pairwise<br />
comparisons, each with the probability of a type I error set at 5%. The<br />
probability of making at least one type I error: if you assume<br />
independence of these three events, the maximum familywise error rate is<br />
then equal to: 1 - (0.95 x 0.95 x 0.95) = 14.26 %.</p>
<p>In other words, the probability of having at least one false alarm (i.e.<br />
type I error) is 14.26%.</p>
<p>What is the maximum familywise error rate for the working memory<br />
experiment, assuming that you do all possible pairwise comparisons with<br />
a type I error of 5%? 26.49%.</p>
<p>Null Hypothesis Significance Testing (NHST) is a statistical method used<br />
to test whether or not you are able to reject or retain the null<br />
hypothesis. This type of test can confront you with a type I error. This<br />
happens when the test rejects the null hypothesis, while it is actually<br />
true in reality. Furthermore, the test can also deliver a type II error.<br />
This is the failure to reject a null hypothesis when it is false. All<br />
hypothesis tests have a probability of making type I and II errors.</p>
<p>Sensitivity and specificity are two concepts that statisticians use to<br />
measure the performance of a statistical test. The sensitivity of a test<br />
is its true positive rate:</p>
<p>
<script type="math/tex; mode=display">sensitivity = \frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives}</script>
</p>
<p>The specificity of a test is its true negative rate:<br />
<script type="math/tex; mode=display">specificity = \frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives}</script>
</p>
<p>Calculate both the sensitivity and specificity of the test based on<br />
numbers displayed in the NHST table?</p>
<p><center><img alt="" src="../A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/NHST_1.png" /></center></p>
<p>The sensitivity is 0.89 and the specificity is 0.85.</p>
<p><strong>Calculate and interpret the results of Tukey</strong></p>
<p>In a situation were you do multiple pairwise comparisons, the<br />
probability of type I errors in the process inflates substantially.<br />
Therefore, it is better to build in adjustments to take this into<br />
account. This is what Tukey tests and other post-hoc procedures do. They<br />
adjust the p-value to prevent inflation of the type I error rate.Use<br />
Tukey&rsquo;s procedure.</p>
<pre><code class="r"># Read in the data set and assign to the object
wm &lt;- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'wm', header = TRUE, startCol = 1, startRow = 1)

# This will print the data set in the console
head(wm)
</code></pre>

<pre><code>##   cond pre post gain train
## 1  t08   8    9    1     1
## 2  t08   8   10    2     1
## 3  t08   8    8    0     1
## 4  t08   8    7   -1     1
## 5  t08   9   11    2     1
## 6  t08   9   10    1     1
</code></pre>
<pre><code class="r"># Revision: Analysis of variance
anova_wm &lt;- aov(wm$gain ~ wm$cond)

# Summary Analysis of Variance
summary(anova_wm)
</code></pre>

<pre><code>##              Df Sum Sq Mean Sq F value Pr(&gt;F)    
## wm$cond       4  274.0   68.51   34.57 &lt;2e-16 ***
## Residuals   115  227.9    1.98                   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
<pre><code class="r"># Post-hoc (Tukey)
TukeyHSD(anova_wm)
</code></pre>

<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = wm$gain ~ wm$cond)
## 
## $`wm$cond`
##               diff         lwr       upr     p adj
## t08-control -0.625 -1.69355785 0.4435578 0.4871216
## t12-control  0.625 -0.44355785 1.6935578 0.4871216
## t17-control  2.425  1.35644215 3.4935578 0.0000001
## t19-control  3.625  2.55644215 4.6935578 0.0000000
## t12-t08      1.250  0.01613568 2.4838643 0.0454650
## t17-t08      3.050  1.81613568 4.2838643 0.0000000
## t19-t08      4.250  3.01613568 5.4838643 0.0000000
## t17-t12      1.800  0.56613568 3.0338643 0.0008953
## t19-t12      3.000  1.76613568 4.2338643 0.0000000
## t19-t17      1.200 -0.03386432 2.4338643 0.0607853
</code></pre>
<pre><code class="r"># Plot confidence intervals
#plot(c(TukeyHSD(anova_wm)$lwr,TukeyHSD(anova_wm)$upr))
plot(TukeyHSD(anova_wm))
</code></pre>

<p><center><img alt="" src="../A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/figure-markdown_strict+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-11-1.png" /></center></p>
<p><strong>Bonferroni adjusted p-values</strong></p>
<p>Just like Tukey&rsquo;s procedure, the Bonferroni correction is a method that<br />
is used to counteract the problem of inflated type I errors while<br />
engaging in multiple pairwise comparisons between subgroups. Bonferroni<br />
is generally known as the most conservative method to control the<br />
familywise error rate.</p>
<p>Bonferroni is based on the idea that if you test <em>N</em> dependent or<br />
independent hypotheses, one way of maintaining the familywise error rate<br />
is to test each individual hypothesis at a statistical significance<br />
level that is deflated by a factor of <script type="math/tex">\frac{1}{n}</script>. So, for a<br />
significance level for the whole family of tests of <em>α</em>, the Bonferroni<br />
correction would be to test each of the individual tests at a<br />
significance level of <script type="math/tex">\frac{\alpha}{n}</script>.</p>
<p>The Bonferroni correction is controversial. It is a strict measure to<br />
limit false positives and generates conservative p-value. Alternative:<br />
increase the sample size, compute the false discovery rate (the expected<br />
percent of false predictions in the set of predictions. For example if<br />
the algorithm returns 100 results with a false discovery rate of .3 then<br />
we should expect 70 of them to be correct.), and the Holm-Bonferroni<br />
method.</p>
<pre><code class="r"># Use `p.adjust` 
bonferroni_ex &lt;- p.adjust(.005, method='bonferroni', n = 8)

bonferroni_ex
</code></pre>

<pre><code>## [1] 0.04
</code></pre>
<pre><code class="r"># Pairwise T-test
pairwise.t.test(wm$gain,wm$cond, p.adjust = 'bonferroni')
</code></pre>

<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  wm$gain and wm$cond 
## 
##     control t08     t12     t17    
## t08 1.00000 -       -       -      
## t12 1.00000 0.05862 -       -      
## t17 5.9e-08 3.8e-09 0.00096 -      
## t19 6.4e-15 2.9e-15 6.7e-09 0.08084
## 
## P value adjustment method: bonferroni
</code></pre>
<h4 id="3-between-groups-factorial-anova">3, Between groups factorial ANOVA<a class="headerlink" href="#3-between-groups-factorial-anova" title="Permanent link">&para;</a></h4>
<p><strong>Data exploration with a barplot</strong></p>
<p>We&rsquo;ll use in this chapter is a randomized controlled experiment<br />
investigating the effects of talking on a cell phone while driving. The<br />
dependent variable in the experiment is the number of driving errors<br />
that subjects made in a driving simulator. There are two independent<br />
variables:</p>
<ul>
<li>Conversation difficulty: None, Low, High</li>
<li>Driving difficulty: Easy, Difficult</li>
</ul>
<!-- -->

<pre><code class="r"># Read in the data set and assign to the object
ab &lt;- readWorksheetFromFile('A Hands-on Introduction to Statistics with R.xls', sheet = 'ab', header = TRUE, startCol = 1, startRow = 1)

ab$subject &lt;- as.integer(ab$subject)
ab$conversation &lt;- as.factor(ab$conversation)
ab$driving &lt;- as.factor(ab$driving)
ab$error &lt;- as.integer(ab$error)

# This will print the data set in the console
head(ab)
</code></pre>

<pre><code>##   subject conversation driving errors error
## 1       1         None    Easy     20    20
## 2       2         None    Easy     19    19
## 3       3         None    Easy     31    31
## 4       4         None    Easy     27    27
## 5       5         None    Easy     31    31
## 6       6         None    Easy     17    17
</code></pre>
<p>Each of the subjects was randomly assigned to one of six conditions<br />
formed by combining different values of the independent variables.</p>
<ul>
<li><code>subject</code>: unique identifier for each subject conversation: level of<br />
    conversation difficulty.</li>
<li><code>driving</code> : level of driving difficulty in the simulator.</li>
<li><code>errors</code>: number of driving errors made.</li>
</ul>
<!-- -->

<pre><code class="r"># Use the tapply function to create your groups
ab_groups &lt;- tapply(ab$errors, list(ab$driving, ab$conversation), sum)

# Make the required barplot
barplot(ab_groups, beside = TRUE, col = c('orange','blue'), main = 'Driving Errors', xlab = 'Conversation Demands', ylab = 'Errors')

# Add the legend
legend('topright', c('Difficult','Easy'), title = 'Driving', fill = c('orange','blue'))
</code></pre>

<p><center><img alt="" src="../A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/figure-markdown_strict+backtick_code_blocks+autolink_bare_uris/unnamed-chunk-14-1.png" /></center></p>
<p>The driving errors made during different driving conditions are<br />
influenced by the level of conversation demand. In other words, the<br />
driving conditions have a different effect on the number of errors made,<br />
depending on the level of conversation demand.</p>
<p><strong>The homogeneity of variance assumption</strong></p>
<p>Before we do factorial ANOVA, we need to test the homogeneity of the<br />
variance assumption. When studying one-way ANOVA, we tested this<br />
assumption with the <code>leveneTest</code> function.</p>
<p>We now have two independent variables instead of just one.</p>
<pre><code class="r"># Test the homogeneity of variance assumption
leveneTest(ab$errors ~ ab$conversation * ab$driving)
</code></pre>

<pre><code>## Levene's Test for Homogeneity of Variance (center = median)
##        Df F value Pr(&gt;F)
## group   5  0.5206 0.7602
##       114
</code></pre>
<p>The homogeneity of variance assumption holds.</p>
<p>By performing a <code>leveneTest</code>, we can check whether or not the<br />
homogeneity of variance assumption holds for a given dataset. The<br />
assumption must hold for the results of an ANOVA analysis to be valid.</p>
<p>Recall from the first chapter that ANOVA makes use of F-statistics, or<br />
F-ratios, in which two types of degrees of freedom are involved.</p>
<pre><code class="r">dim(ab)
</code></pre>

<pre><code>## [1] 120   5
</code></pre>
<pre><code class="r">str(ab$conversation)
</code></pre>

<pre><code>##  Factor w/ 3 levels "High demand",..: 3 3 3 3 3 3 3 3 3 3 ...
</code></pre>
<pre><code class="r">str(ab$driving)
</code></pre>

<pre><code>##  Factor w/ 2 levels "Difficult","Easy": 2 2 2 2 2 2 2 2 2 2 ...
</code></pre>
<p>There are 120 subjets and 2 (Easy, Difficult) * 3 (High Demand, Low<br />
Demand, None) = 6 groups.</p>
<p><strong>The factorial ANOVA</strong></p>
<pre><code class="r"># Factorial ANOVA
ab_model &lt;- aov(ab$errors ~ ab$conversation * ab$driving)

# Get the summary table
summary(ab_model)
</code></pre>

<pre><code>##                             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## ab$conversation              2   4416    2208   36.14 6.98e-13 ***
## ab$driving                   1   5782    5782   94.64  &lt; 2e-16 ***
## ab$conversation:ab$driving   2   1639     820   13.41 5.86e-06 ***
## Residuals                  114   6965      61                     
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
<p>Based on the summary table of the factorial ANOVA, the main effect for<br />
driving difficulty, the main effect for conversation difficulty and the<br />
interaction effect are all significant.</p>
<p><strong>The interaction effect</strong></p>
<p>Now it&rsquo;s time to explore the interaction effect. You will do this with<br />
the help of a simple effects analysis.</p>
<p>Why a simple effects analysis? Well, remember what we had to do when you<br />
had a significant main effect in a one-way ANOVA? There, you just had to<br />
perform some post-hoc tests to see from which level of the categorical<br />
variable the main effect was coming. With an interaction effect, it is<br />
quite similar. Conduct a simple effects analysis of the variable<br />
<code>conversation</code> on the outcome variable <code>errors</code> at each level of<br />
<code>driving</code>.</p>
<pre><code class="r"># Create the two subsets
ab_1 &lt;- subset(ab, ab$driving == 'Easy')  
ab_2 &lt;- subset(ab, ab$driving == 'Difficult')

# Perform the one-way ANOVA for both subsets
aov_ab_1 &lt;- aov(ab_1$errors ~ ab_1$conversation)
aov_ab_2 &lt;- aov(ab_2$errors ~ ab_2$conversation)

# Get the summary tables for both aov_ab_1 and aov_ab_2
summary(aov_ab_1)
</code></pre>

<pre><code>##                   Df Sum Sq Mean Sq F value Pr(&gt;F)  
## ab_1$conversation  2  504.7   252.3   4.928 0.0106 *
## Residuals         57 2918.5    51.2                 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
<pre><code class="r">summary(aov_ab_2)
</code></pre>

<pre><code>##                   Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## ab_2$conversation  2   5551    2776   39.09 2.05e-11 ***
## Residuals         57   4047      71                     
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
<p>There a significant simple effect for the easy driving condition based<br />
on the summary table of <code>aov_ab_1</code>.</p>
<p>There a significant simple effect for the difficult driving condition<br />
based on the summary table of <code>aov_ab_2</code>.</p>
<p><strong>The effect sizes</strong></p>
<p>The definition of an interaction effect states that the effect of one<br />
variable changes across levels of the other variable. For example, we<br />
might expect the effect of conversation to be greater when driving<br />
conditions are difficult than when they are relatively easy.</p>
<p>Unfortunately, it is not quite that simple. In order to really<br />
understand the different effect sizes, you should make use of the<br />
<code>etaSquared</code> function.</p>
<pre><code class="r">library(lsr)

# Calculate the etaSquared for the easy driving case
#easy is ab_1
#difficult is ab_2
etaSquared(aov_ab_1, anova = TRUE)
</code></pre>

<pre><code>##                     eta.sq eta.sq.part      SS df        MS        F
## ab_1$conversation 0.147433    0.147433  504.70  2 252.35000 4.928458
## Residuals         0.852567          NA 2918.55 57  51.20263       NA
##                            p
## ab_1$conversation 0.01061116
## Residuals                 NA
</code></pre>
<pre><code class="r"># Calculate the etaSquared for the difficult driving case
etaSquared(aov_ab_2, anova = TRUE)
</code></pre>

<pre><code>##                      eta.sq eta.sq.part       SS df         MS        F
## ab_2$conversation 0.5783571   0.5783571 5551.033  2 2775.51667 39.09275
## Residuals         0.4216429          NA 4046.900 57   70.99825       NA
##                              p
## ab_2$conversation 2.046097e-11
## Residuals                   NA
</code></pre>
<p>Based on the output of the <code>etaSquared</code> function for the easy driving<br />
condition, the percentage of variance explained by the conversation<br />
variable is 14.7%; the percentage of variance explained by the<br />
conversation variable is 57.8%.</p>
<p><strong>Pairwise comparisons</strong></p>
<p>Finally, let us look at pairwise comparisons for the simple effects. You<br />
can do this with the Tukey post-hoc test.</p>
<pre><code class="r"># Tukey for easy driving
TukeyHSD(aov_ab_1)
</code></pre>

<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = ab_1$errors ~ ab_1$conversation)
## 
## $`ab_1$conversation`
##                         diff        lwr        upr     p adj
## Low demand-High demand -6.05 -11.495243 -0.6047574 0.0260458
## None-High demand       -6.25 -11.695243 -0.8047574 0.0207614
## None-Low demand        -0.20  -5.645243  5.2452426 0.9957026
</code></pre>
<pre><code class="r"># Tukey for difficult driving
TukeyHSD(aov_ab_2)
</code></pre>

<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = ab_2$errors ~ ab_2$conversation)
## 
## $`ab_2$conversation`
##                          diff       lwr        upr     p adj
## Low demand-High demand  -9.75 -16.16202  -3.337979 0.0015849
## None-High demand       -23.45 -29.86202 -17.037979 0.0000000
## None-Low demand        -13.70 -20.11202  -7.287979 0.0000103
</code></pre>
<p>For &lsquo;Easy Driving&rsquo;, two mean differences in terms of number of errors<br />
are significant (below 0.05).</p>
<p>For &lsquo;Difficult Driving&rsquo;, three mean differences in terms of number of<br />
errors are significant (below 0.05).</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_Four%2C_Repeated_Measures_Anova/" class="btn btn-neutral float-right" title="Statistics with R, Course Four, Repeated Measures ANOVA">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_Two%2C_Student_s_T-test/" class="btn btn-neutral" title="Statistics with R, Course Two, Student's t-test"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>© Ugo Spark</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_Two%2C_Student_s_T-test/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../A_Hands-on_Introduction_to_Statistics_with_R%2C_Course_Four%2C_Repeated_Measures_Anova/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="../mathjaxhelper.js"></script>

</body>
</html>
